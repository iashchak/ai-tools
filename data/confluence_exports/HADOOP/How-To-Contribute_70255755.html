<!DOCTYPE html>
<html>
    <head>
        <title>Hadoop : How To Contribute</title>
        <link rel="stylesheet" href="styles/site.css" type="text/css" />
        <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
    </head>

    <body class="theme-default aui-theme-default">
        <div id="page">
            <div id="main" class="aui-page-panel">
                <div id="main-header">
                    <div id="breadcrumb-section">
                        <ol id="breadcrumbs">
                            <li class="first">
                                <span><a href="index.html">Hadoop</a></span>
                            </li>
                                                    <li>
                                <span><a href="Home_65866089.html">Home</a></span>
                            </li>
                                                    <li>
                                <span><a href="Hadoop-Contributor-Guide_89071892.html">Hadoop Contributor Guide</a></span>
                            </li>
                                                </ol>
                    </div>
                    <h1 id="title-heading" class="pagetitle">
                                                <span id="title-text">
                            Hadoop : How To Contribute
                        </span>
                    </h1>
                </div>

                <div id="content" class="view">
                    <div class="page-metadata">
                        
        
    
    
        
    
        
        
            Created by <span class='author'> Akira Ajisaka</span>, last modified by <span class='editor'> Xiaoqiao He</span> on мар 22, 2023
                        </div>
                    <div id="main-content" class="wiki-content group">
                    <h1 id="HowToContribute-HowtoContributetoApacheHadoop">How to Contribute to Apache Hadoop</h1><p class="line862">This page describes the mechanics of <em>how</em> to contribute software to Apache Hadoop. For ideas about <em>what</em> you might contribute, please see the <a class="external-link" href="https://wiki.apache.org/hadoop/ProjectSuggestions" rel="nofollow">ProjectSuggestions</a> page.</p><p class="line862"><span class="anchor"><style type='text/css'>/*<![CDATA[*/
div.rbtoc1683025379863 {padding: 0px;}
div.rbtoc1683025379863 ul {margin-left: 0px;}
div.rbtoc1683025379863 li {margin-left: 0px;padding-left: 0px;}

/*]]>*/</style><div class='toc-macro rbtoc1683025379863'>
<ul class='toc-indentation'>
<li><a href='#HowToContribute-DevEnvironmentSetup'>Dev Environment Setup</a>
<ul class='toc-indentation'>
<li><a href='#HowToContribute-Getthesourcecode'>Get the source code</a></li>
<li><a href='#HowToContribute-ReadBUILDING.txt'>Read BUILDING.txt</a></li>
<li><a href='#HowToContribute-IntegratedDevelopmentEnvironment(IDE)'>Integrated Development Environment (IDE)</a></li>
<li><a href='#HowToContribute-BuildTools'>Build Tools</a></li>
<li><a href='#HowToContribute-Nativelibraries'>Native libraries</a></li>
<li><a href='#HowToContribute-HardwareSetup'>Hardware Setup</a></li>
</ul>
</li>
<li><a href='#HowToContribute-MakingChanges'>Making Changes</a>
<ul class='toc-indentation'>
<li><a href='#HowToContribute-Generatingapatch'>Generating a patch</a>
<ul class='toc-indentation'>
<li><a href='#HowToContribute-Choosingatargetbranch'>Choosing a target branch</a></li>
<li><a href='#HowToContribute-UnitTests'>Unit Tests</a></li>
<li><a href='#HowToContribute-Javadoc'>Javadoc</a></li>
</ul>
</li>
<li><a href='#HowToContribute-Provideapatch'>Provide a patch</a></li>
<li><a href='#HowToContribute-Testingyourpatch'>Testing your patch</a></li>
<li><a href='#HowToContribute-Changesthatspanprojects'>Changes that span projects</a></li>
</ul>
</li>
<li><a href='#HowToContribute-Contributingyourwork'>Contributing your work</a>
<ul class='toc-indentation'>
<li><a href='#HowToContribute-SubmittingpatchesagainstobjectstoressuchasAmazonS3,OpenStackSwiftandMicrosoftAzure'>Submitting patches against object stores such as Amazon S3, OpenStack Swift and Microsoft Azure</a></li>
</ul>
</li>
<li><a href='#HowToContribute-RequestingforaJiraaccount'>Requesting for a Jira account</a></li>
<li><a href='#HowToContribute-JiraGuidelines'>Jira Guidelines</a></li>
<li><a href='#HowToContribute-Stayinvolved'>Stay involved</a></li>
<li><a href='#HowToContribute-SeeAlso'>See Also</a></li>
</ul>
</div></span></p><div><hr/></div><div class="table-of-contents"><h2 class="table-of-contents-heading" id="HowToContribute-DevEnvironmentSetup">Dev Environment Setup</h2></div><p><span class="anchor" style="color: rgb(0,0,0);"> </span>Here are some things you will need to build and test Hadoop. Be prepared to invest some time to set up a working Hadoop dev environment. Try getting the project to build and test locally first before you start writing code.<span class="anchor"> </span><span class="anchor"> </span></p><h3 class="line867" id="HowToContribute-Getthesourcecode">Get the source code</h3><p class="line862">First of all, you need the Hadoop source code. The official location for Hadoop is the Apache Git repository. See <a href="Git-And-Hadoop_89071914.html">Git And Hadoop</a></p><h3 id="HowToContribute-ReadBUILDING.txt">Read BUILDING.txt</h3><p><span class="anchor" style="color: rgb(0,0,0);"> </span>Once you have the source code, we strongly recommend reading BUILDING.txt located in the root of the source tree. It has up to date information on how to build Hadoop on various platforms along with some workarounds for platform-specific quirks. The latest <a class="external-link" href="https://git-wip-us.apache.org/repos/asf?p=hadoop.git;a=blob;f=BUILDING.txt" rel="nofollow">BUILDING.txt</a> for the current trunk can also be viewed on the web.<span class="anchor"> </span><span class="anchor"> </span><span class="anchor"> </span></p><h3 id="HowToContribute-IntegratedDevelopmentEnvironment(IDE)">Integrated Development Environment (IDE)</h3><p class="line874">You are free to use whatever IDE you prefer or your favorite text editor. Note that:<span class="anchor"> </span></p><ul><li>Building and testing is often done on the command line or at least via the Maven support in the IDEs.<span class="anchor"> </span></li><li>Set up the IDE to follow the source layout rules of the project.<span class="anchor"> </span></li><li>Disable any added value &quot;reformat&quot; and &quot;strip trailing spaces&quot; features as it can create extra noise when reviewing patches.<span class="anchor"> </span><span class="anchor"> </span></li></ul><h3 id="HowToContribute-BuildTools">Build Tools</h3><p>Please see BUILDING.txt for the detail.</p><p class="line874">As the Hadoop builds use the external Maven repository to download artifacts, Maven needs to be set up with the proxy settings needed to make external HTTP requests. The first build of every Hadoop project needs internet connectivity to download Maven dependencies.</p><ol><li>Be online for that first build, on a good network<span class="anchor"> </span></li><li><p class="line862">To set the Maven proxy setttings, see <a class="external-link" href="http://maven.apache.org/guides/mini/guide-proxies.html" rel="nofollow">http://maven.apache.org/guides/mini/guide-proxies.html</a><span class="anchor"> </span></p></li><li><p class="line862">Because Maven doesn't pass proxy settings down to the Ant tasks it runs <a class="external-link" href="https://issues.apache.org/jira/browse/HDFS-2381" rel="nofollow">HDFS-2381</a> some parts of the Hadoop build may fail. The fix for this is to pass down the Ant proxy settings in the build Unix: mvn $ANT_OPTS; Windows: mvn %ANT_OPTS%.<span class="anchor"> </span></p></li><li><p class="line862">Tomcat is always downloaded, even when building offline. Setting -Dtomcat.download.url to a local copy and -Dtomcat.version to the version pointed to by the URL will avoid that download.</p></li></ol><p>If you are failing to fetch a artifact from the external maven repository, you may need to delete the related files from your local cache (i.e. ~/.m2 directory).</p><ul><li>Ref: 
<span class="jira-issue resolved" data-jira-key="HADOOP-16577" >
                    <a href="https://issues.apache.org/jira/browse/HADOOP-16577?src=confmacro" class="jira-issue-key"><img class="icon"
                                                                                      src="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype"/>HADOOP-16577</a>
                            -
            <span class="summary">Build fails as can&#39;t retrieve websocket-servlet</span>
                                                <span class="aui-lozenge aui-lozenge-subtle             aui-lozenge-success
     jira-macro-single-issue-export-pdf">Resolved</span>
                </span>
</li></ul><h3 id="HowToContribute-Nativelibraries">Native libraries</h3><p class="line874">On Linux, you need the tools to create the native libraries: LZO headers,zlib headers, gcc, OpenSSL headers, cmake, protobuf dev tools, and libtool, and the GNU autotools (automake, autoconf, etc).<span class="anchor"> </span><span class="anchor"> </span></p><p class="line874">For RHEL (and hence also CentOS):</p><pre><span class="anchor"> </span>yum -y install  lzo-devel  zlib-devel  gcc gcc-c++ autoconf automake libtool openssl-devel fuse-devel cmake</pre><p class="line874">For Debian and Ubuntu:</p><pre><span class="anchor"> </span>apt-get -y install maven build-essential autoconf automake libtool cmake zlib1g-dev pkg-config libssl-dev libfuse-dev</pre><p class="line862">Native libraries are mandatory for Windows. For instructions see <a class="external-link" href="https://wiki.apache.org/hadoop/Hadoop2OnWindows" rel="nofollow">Hadoop2OnWindows</a>.<span class="anchor"> </span><span class="anchor"> </span></p><h3 id="HowToContribute-HardwareSetup">Hardware Setup </h3><ul><li>Lots of RAM, especially if you are using a modern IDE. ECC RAM is recommended in large-RAM systems.<span class="anchor"> </span></li><li>Disk Space. Always handy.<span class="anchor"> </span></li><li>Network Connectivity. Hadoop tests are not guaranteed to all work if a machine does not have a network connection -and especially if it does not know its own name.<span class="anchor"> </span></li><li>Keep your computer's clock up to date via an NTP server, and set up the time zone correctly. This is good for avoiding change-log confusion.<span class="anchor"> </span><span class="anchor"> </span></li></ul><h2 id="HowToContribute-MakingChanges">Making Changes</h2><p class="line862">Before you start, send a message to the <a class="external-link" href="https://hadoop.apache.org/mailing_lists.html" rel="nofollow">Hadoop developer mailing list</a>, or file a bug report in <a class="external-link" href="https://wiki.apache.org/hadoop/Jira" rel="nofollow">Jira</a>. Describe your proposed changes and check that they fit in with what others are doing and have planned for the project. Be patient, it may take folks a while to understand your requirements. If you want to start with pre-existing issues, look for Jiras labeled newbie. You can find them using <a class="external-link" href="https://issues.apache.org/jira/issues/?filter=12331506" rel="nofollow">this filter</a>.<span class="anchor"> </span><span class="anchor"> </span></p><p class="line862">Modify the source code and add some (very) nice features using your favorite IDE.</p><p class="line874">But take care about the following points<span class="anchor"> </span><span class="anchor"> </span></p><ul><li><p class="line862">All public classes and methods should have informative <a class="external-link" href="http://java.sun.com/j2se/javadoc/writingdoccomments/" rel="nofollow">Javadoc comments</a>.<span class="anchor"> </span></p><ul><li>Do not use @author tags.<span class="anchor"> </span></li></ul></li><li><p class="line862">Code must be formatted according to <a class="external-link" href="http://www.oracle.com/technetwork/java/javase/documentation/codeconvtoc-136057.html" rel="nofollow">Sun's conventions</a>, with one exception:<span class="anchor"> </span></p><ul><li>Indent two spaces per level, not four.<span class="anchor"> </span></li></ul></li><li><span class="anchor"><span style="color: rgb(36,41,47);">Code formatter xml is present here:<span> </span></span><a class="external-link" href="https://github.com/apache/hadoop/tree/trunk/dev-support/code-formatter" rel="nofollow" style="text-decoration: none;text-align: left;">https://github.com/apache/hadoop/tree/trunk/dev-support/code-formatter</a><span style="color: rgb(36,41,47);"><span> </span>. IntelliJ users can directly import<span> </span></span><code style="text-align: left;">hadoop_idea_formatter.xml</code></span></li><li>Contributions must pass existing unit tests.<span class="anchor"> </span><ul><li><p class="line862">New unit tests should be provided to demonstrate bugs and fixes. <a class="external-link" href="http://www.junit.org/" rel="nofollow">JUnit</a> is our test framework:<span class="anchor"> </span></p></li></ul><ul><li><p class="line862">You must implement a class that uses @Test annotations for all test methods. Please note, <a class="external-link" href="http://wiki.apache.org/hadoop/HowToDevelopUnitTests" rel="nofollow">Hadoop uses JUnit v4</a>.<span class="anchor"> </span></p></li><li><p class="line862">Define methods within your class whose names begin with test, and call JUnit's many assert methods to verify conditions; these methods will be executed when you run mvn test. Please add meaningful messages to the assert statement to facilitate diagnostics.<span class="anchor"> </span></p></li><li><p class="line862">By default, do not let tests write any temporary files to /tmp. Instead, the tests should write to the location specified by the test.build.data system property.<span class="anchor"> </span></p></li><li><p class="line862">If a HDFS cluster or a MapReduce/YARN cluster is needed by your test, please use org.apache.hadoop.dfs.MiniDFSCluster and org.apache.hadoop.mapred.MiniMRCluster (or org.apache.hadoop.yarn.server.MiniYARNCluster), respectively. TestMiniMRLocalFS is an example of a test that uses MiniMRCluster.<span class="anchor"> </span></p></li><li><p class="line862">Place your class in the src/test tree.<span class="anchor"> </span></p></li><li><p class="line891">TestFileSystem.java and TestMapRed.java are examples of standalone <a class="external-link" href="https://wiki.apache.org/hadoop/MapReduce" rel="nofollow">MapReduce</a>-based tests.<span class="anchor"> </span></p></li><li><p class="line891">TestPath.java is an example of a non <a class="external-link" href="https://wiki.apache.org/hadoop/MapReduce" rel="nofollow">MapReduce</a>-based test.<span class="anchor"> </span></p></li><li><p class="line862">You can run all the project unit tests with mvn test, or a specific unit test with mvn -Dtest=&lt;class name without package prefix&gt; test. Run these commands from the hadoop-trunk directory.<span class="anchor"> </span></p></li></ul></li><li><p class="line862">If you modify the Unix shell scripts, see the <a class="external-link" href="https://wiki.apache.org/hadoop/UnixShellScriptProgrammingGuide" rel="nofollow">UnixShellScriptProgrammingGuide</a>.<span class="anchor"> </span><span class="anchor"> </span></p></li></ul><h3 id="HowToContribute-Generatingapatch">Generating a patch</h3><h4 id="HowToContribute-Choosingatargetbranch">Choosing a target branch</h4><p class="line874">Except for the following situations it is recommended that all patches be based off trunk to take advantage of the Jenkins pre-commit build.<span class="anchor"> </span></p><ol><li>The patch is targeting a release branch that is not based off trunk e.g. branch-3.1, branch-2.10, etc.<span class="anchor"> </span></li><li>The change is targeting a specific feature branch and is not yet ready for merging into trunk.<span class="anchor"> </span><span class="anchor"> </span></li></ol><p class="line862">If you are unsure of the target branch then <strong>trunk</strong> is usually the best choice. Committers will usually merge the patch to downstream branches e.g. branch-3.2 as appropriate.<span class="anchor"> </span><span class="anchor"> </span></p><h4 id="HowToContribute-UnitTests">Unit Tests</h4><p class="line874">Please make sure that all unit tests succeed before constructing your patch and that no new javac compiler warnings are introduced by your patch.<span class="anchor"> </span><span class="anchor"> </span></p><p class="line862">For building Hadoop with Maven, use the following to run all unit tests and build a distribution. The -Ptest-patch profile will check that no new compiler warnings have been introduced by your patch.<span class="anchor"> </span><span class="anchor"> </span></p><pre><span class="anchor"> </span>mvn clean install -Pdist -Dtar -Ptest-patch</pre><p class="line862">Any test failures can be found in the target/surefire-reports directory of the relevant module. You can also run this command in one of the hadoop-common, hadoop-hdfs, or hadoop-mapreduce directories to just test a particular subproject.<span class="anchor"> </span><span class="anchor"> </span></p><p class="line862">Unit tests development guidelines <a class="external-link" href="https://wiki.apache.org/hadoop/HowToDevelopUnitTests" rel="nofollow">HowToDevelopUnitTests</a><span class="anchor"> </span><span class="anchor"> </span></p><h4 id="HowToContribute-Javadoc">Javadoc</h4><p class="line874">Please also check the javadoc.<span class="anchor"> </span><span class="anchor"> </span></p><pre><span class="anchor"> </span>mvn process-sources javadoc:javadoc-no-fork
<span class="anchor"> </span>firefox target/site/api/index.html</pre><p class="line874">Examine all public classes you've changed to see that documentation is complete, informative, and properly formatted. Your patch must not generate any javadoc warnings.<span class="anchor"> </span><span class="anchor"> </span></p><p class="line862">Jenkins includes a javadoc run on Java 8 and Java 11, it will fail if there are unbalanced HTML tags or &lt;p/&gt; clauses (use &lt;p&gt; here.<span class="anchor"> </span><span class="anchor"> </span></p><p class="line874">If Jenkins rejects a patch due to Java 8 or Java 11 javadoc failures, it is considered an automatic veto for the patch.</p><h3 id="HowToContribute-Provideapatch">Provide a patch</h3><p class="line862">You need to create a pull request in <a class="external-link" href="https://github.com/apache/hadoop" rel="nofollow">https://github.com/apache/hadoop</a>. Now attaching a patch in ASF JIRA does not work. You need to set the title which starts with the corresponding JIRA issue number (e.g. HADOOP-XXXXX. Fix a typo in YYY.) to integrate with the issue. If there is no corresponding issue, please create an issue in ASF JIRA before creating a pull request.</p><p class="line874">See also: <a href="GitHub-Integration_89071920.html">GitHub Integration</a></p><h3 id="HowToContribute-Testingyourpatch">Testing your patch</h3><p class="line862">Before submitting your patch, you are encouraged to run the same tools that the automated Jenkins patch test system will run on your patch. This enables you to fix problems with your patch before you submit it. The dev-support/bin/test-patch script in the trunk directory will run your patch through the same checks that Jenkins currently does <em>except</em> for executing the unit tests. (See <a class="external-link" href="https://wiki.apache.org/hadoop/TestPatchTips" rel="nofollow">TestPatchTips</a> for some tricks.)<span class="anchor"> </span><span class="anchor"> </span></p><p class="line862">Run this command from a clean workspace (ie git status shows no modifications or additions) as follows:</p><pre><span class="anchor"> </span>dev-support/bin/test-patch [options] patch-file | defect-number</pre><p class="line862">At the end, you should get a message on your console that is similar to the comment added to Jira by Jenkins's automated patch test system, listing +1 and -1 results. Generally you should expect a +1 overall in order to have your patch committed; exceptions will be made for false positives that are unrelated to your patch. The scratch directory (which defaults to the value of ${user.home}/tmp) will contain some output files that will be useful in determining cause if issues were found in the patch.<span class="anchor"> </span><span class="anchor"> </span></p><p class="line874">Some things to note:</p><ul><li><p class="line862">the optional cmd parameters will default to the ones in your PATH environment variable<span class="anchor"> </span></p></li><li><p class="line862">the grep command must support the -o flag (Both GNU grep and BSD grep support it)<span class="anchor"> </span></p></li><li><p class="line862">the patch command must support the -E flag<span class="anchor"> </span><span class="anchor"> </span></p></li></ul><p class="line874">Run the same command with no arguments to see the usage options.<span class="anchor"> </span><span class="anchor"> </span></p><h3 id="HowToContribute-Changesthatspanprojects">Changes that span projects</h3><p class="line862">You may find that you need to modify both the common project and <a class="external-link" href="https://wiki.apache.org/hadoop/MapReduce" rel="nofollow">MapReduce</a> or HDFS. Or perhaps you have changed something in common, and need to verify that these changes do not break the existing unit tests for HDFS and <a class="external-link" href="https://wiki.apache.org/hadoop/MapReduce" rel="nofollow">MapReduce</a>. Hadoop's build system integrates with a local maven repository to support cross-project development. Use this general workflow for your development:<span class="anchor"> </span><span class="anchor"> </span></p><ul><li>Make your changes in common<span class="anchor"> </span></li><li>Run any unit tests there (e.g. 'mvn test')<span class="anchor"> </span></li><li><p class="line891"><em>Publish</em> your new common jar to your local mvn repository:</p><pre><span class="anchor"> </span>hadoop-common$ mvn clean install -DskipTests</pre></li><li><p class="line862">A word of caution: mvn install pushes the artifacts into your local Maven repository which is shared by all your projects.<span class="anchor"> </span></p></li><li>Switch to the dependent project and make any changes there (e.g., that rely on a new API you introduced in hadoop-common).<span class="anchor"> </span></li><li>Finally, create separate patches for your common and hdfs/mapred changes, and file them as separate JIRA issues associated with the appropriate projects.<span class="anchor"> </span><span class="anchor"> </span></li></ul><h2 id="HowToContribute-Contributingyourwork">Contributing your work</h2><ol><li><p class="line862">Please note that the commits in the GitHub PR should be granted license to ASF for inclusion in ASF works (as per the <a class="external-link" href="http://www.apache.org/licenses/LICENSE-2.0" rel="nofollow">Apache License</a> §5).<span class="anchor"> </span></p></li><li><p class="line862">Folks should run mvn clean install javadoc:javadoc checkstyle:checkstyle before opening PR.<span class="anchor"> </span></p><ol><li>Tests must all pass.<span class="anchor"> </span></li><li><p class="line862">Javadoc should report <strong>no</strong> warnings or errors.<span class="anchor"> </span></p></li><li>The Javadoc on java 8 must not fail.</li><li>Checkstyle's error count should not exceed that listed at lastSuccessfulBuild/artifact/trunk/build/test/checkstyle-errors.html<span class="anchor"> </span></li></ol></li><li>Jenkins's tests are meant to double-check things, and not be used as a primary patch tester, which would create too much noise on the mailing list and in PR.</li><li>If your patch involves performance optimizations, they should be validated by benchmarks that demonstrate an improvement.<span class="anchor"> </span></li><li><p class="line862">If your patch creates an incompatibility with the latest major release, then you must set the <strong>Incompatible change</strong> flag on the issue's Jira 'and' fill in the <strong>Release Note</strong> field with an explanation of the impact of the incompatibility and the necessary steps users must take.<span class="anchor"> </span></p></li><li><p class="line862">If your patch implements a major feature or improvement, then you must fill in the <strong>Release Note</strong> field on the issue's Jira with an explanation of the feature that will be comprehensible by the end user.<span class="anchor"> </span><span class="anchor"> </span></p></li></ol><p class="line862">Once a &quot;+1&quot; comment is received from the automated patch testing system and a code reviewer has set the <strong>Reviewed</strong> flag on the issue's Jira, a committer should then evaluate it within a few days and either: commit it; or reject it with an explanation.<span class="anchor"> </span><span class="anchor"> </span></p><p class="line874">Please be patient. Committers are busy people too. If no one responds to your patch after a few days, please make friendly reminders. Please incorporate other's suggestions into your patch if you think they're reasonable. Finally, remember that even a patch that is not committed is useful to the community.</p><h3 id="HowToContribute-SubmittingpatchesagainstobjectstoressuchasAmazonS3,OpenStackSwiftandMicrosoftAzure">Submitting patches against object stores such as Amazon S3, OpenStack Swift and Microsoft Azure</h3><p class="line862">The modules hadoop-aws, hadoop-openstack and hadoop-azure contain filesystem clients which work with Amazon S3, <a class="external-link" href="https://wiki.apache.org/hadoop/OpenStack" rel="nofollow">OpenStack</a> Swift and Microsoft Azure storage respectively.<span class="anchor"> </span><span class="anchor"> </span></p><p class="line874">The test suites for these modules are not executed on Jenkins because they need credentials to work with.<span class="anchor"> </span><span class="anchor"> </span></p><p class="line867"><strong>Having Jenkins +1 any patch against an object store does not mean the patch works: it must be manually tested by the submitter, the committer and any other reviewers who can do so</strong><span class="anchor"> </span><span class="anchor"> </span></p><p class="line874">If a Yetus patch run says +1 for an object store patch, all it means is &quot;the compilation, javadoc and style checks passed&quot;. It does not mean the patch works, or that it is ready to be committed.<span class="anchor"> </span><span class="anchor"> </span></p><p class="line862">The details of how to test for these object stores are covered <a class="external-link" href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/filesystem/testing.html" rel="nofollow">in the filesystem specification documentation</a>.<span class="anchor"> </span><span class="anchor"> </span></p><p class="line862">When submitting a patch, make sure the patch does not include any of your secret credentials. The Hadoop .gitignore file is set to ignore specific XML test resources for this purpose.<span class="anchor"> </span><span class="anchor"> </span></p><pre><span class="anchor"> </span>hadoop-common-project/hadoop-common/src/test/resources/contract-test-options.xml
<span class="anchor"> </span>hadoop-tools/hadoop-openstack/src/test/resources/contract-test-options.xml
<span class="anchor"> </span>hadoop-tools/hadoop-aws/src/test/resources/auth-keys.xml
<span class="anchor"> </span>hadoop-tools/hadoop-aws/src/test/resources/contract-test-options.xml
<span class="anchor"> </span>hadoop-tools/hadoop-azure/src/test/resources/azure-auth-keys.xml</pre><p class="line874">Please state which infrastructures you have tested against, —including which regions you tested against. If you have not tested the patch yourself, do not expect anyone to look at the patch.<span class="anchor"> </span><span class="anchor"> </span></p><p class="line874">We welcome anyone who can test these patches: please do so and again, declare what you have tested against. That includes in-house/proprietary implementations of the APIs as well as public infrastructures.<span class="anchor"> </span><span class="anchor"> </span></p><h2 id="HowToContribute-RequestingforaJiraaccount">Requesting for a Jira account</h2><p>If you wish to contribute to Hadoop and require a Jira account, such requests can be made via: <a class="external-link" href="https://selfserve.apache.org/jira-account.html" rel="nofollow">https://selfserve.apache.org/jira-account.html</a></p><hr/><p><strong>Note:</strong></p><ul><li><span style="color: rgb(34,34,34);">Jira account is required only if someone has to report a bug or plan to </span><span style="color: rgb(34,34,34);">contribute, other cases are unnecessary to request account.</span></li><li>Serving such requests can take time upto one week or even more during holidays.</li><li>Please refrain from sending the form multiple times or sending follow-ups on the mailing lists.</li></ul><h2 id="HowToContribute-JiraGuidelines">Jira Guidelines</h2><p class="line874">Please comment on issues in Jira, making their concerns known. Please also vote for issues that are a high priority for you.<span class="anchor"> </span><span class="anchor"> </span></p><p class="line874">Please refrain from editing descriptions and comments if possible, as edits spam the mailing list and clutter Jira's &quot;All&quot; display, which is otherwise very useful. Instead, preview descriptions and comments using the preview button (on the right) before posting them. Keep descriptions brief and save more elaborate proposals for comments, since descriptions are included in Jira's automatically sent messages. If you change your mind, note this in a new comment, rather than editing an older comment. The issue should preserve this history of the discussion.<span class="anchor"> </span><span class="anchor"> </span></p><p class="line874">Additionally, do not set the Fix Version. Committers use this field to determine which branches have had patches committed. Instead, use the Affects and Target Versions to notify others of the branches that should be considered.<span class="anchor"> </span><span class="anchor"> </span></p><h2 id="HowToContribute-Stayinvolved">Stay involved</h2><p class="line862">Contributors should join the <a class="external-link" href="https://hadoop.apache.org/mailing_lists.html" rel="nofollow">Hadoop mailing lists</a>. In particular, the commit list (to see changes as they are made), the dev list (to join discussions of changes) and the user list (to help others).<span class="anchor"> </span><span class="anchor"> </span></p><h2 id="HowToContribute-SeeAlso">See Also</h2><ul><li><p class="line891"><a class="external-link" href="http://www.apache.org/dev/contributors.html" rel="nofollow">Apache contributor documentation</a><span class="anchor"> </span></p></li><li><p class="line891"><a class="external-link" href="http://www.apache.org/foundation/voting.html" rel="nofollow">Apache voting documentation</a></p></li></ul>
                    </div>

                                        
                 
                </div>             </div> 
            <div id="footer" role="contentinfo">
                <section class="footer-body">
                    <p>Document generated by Confluence on мая 02, 2023 11:02</p>
                    <div id="footer-logo"><a href="https://www.atlassian.com/">Atlassian</a></div>
                </section>
            </div>
        </div>     </body>
</html>
