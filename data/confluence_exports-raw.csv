title,text,file
Hadoop : Submarine Project Spin-Off to TLP Proposal Meritocracy,"Submarine has been developed as part of Apache Hadoop and thus has been operating as a meritocracy. Many of the developers of Submarine are active Hadoop PMC members, committers or contributors. The Submarine project plans to continue adding new PMC and committers as the project continues to develop.",../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Community,Submarine’s development team seeks to foster the development and user communities. We feel that becoming a separate project will improve both communities by being smaller and more focused than Hadoop and bring tighter integration with various Apache projects and other open source projects that either doesn’t want to or can’t accept the large list of dependencies from Hadoop.,../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Core Developers,"Submarine is being primarily developed by Cloudera, NetEase, LinkedIn, JD, Dahua,Ke.com, Facebook, Alibaba",../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Alignment,"The ASF is a natural host for Submarine given that it is already the home of Hadoop, Spark, Hive, Arrow and other emerging distributed computing software projects. Submarine was designed to offer improved user experiences of deep-learning/machine-learning model training, serving, management which can be part of big data pipeline and leverages the power of Apache Spark, Apache Arrow, Apache Zeppelin, etc.",../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Orphaned Products,"The core developers of the Submarine team are actively working on the project andplan to continue. There is very little risk of Submarine getting orphaned sincelarge companies are using Submarine to train their machine learning models. Forexample, NetEase is using Submarine to run machine learning jobs on their 250 GPU nodes cluster in production and serve their notebook for developers/data-scientists to use (https://www.infoq.cn/article/C11ef0aa1EfSb*6CE9ce).",../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Inexperience with Open Source,"The potential PMC of the new project has extensive experience with Apache projects and includes 2 PMC members from Apache top level projects, and the potential initial committersof Submarine have 7 committers from 3 different Apache top level projects. Theseexperienced committers will be responsible for training the committers thatare less familiar with the Apache Way.",../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Homogeneous Developers,"The developers include employees from Cloudera, NetEase, LinkedIn, JD, Dahua,Ke.com, Facebook, Alibaba. Apache projects encourage an open anddiverse meritocratic community and Submarine team is very motivated to increasethe size and diversity of the development team.",../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Reliance on Salaried Developers,"Most of the work on Submarine has been by salaried developers, but the hope isthat by making Submarine a separate project, it will be more approachable for newdevelopers including non-salaried developers.",../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Relationships with Other Apache Products,"Submarine has a strong relationship and integration with Apache Hadoop, Zeppelin. Being independent of Hadoop will allow other projects to depend on Submarine directly without incurring the cost of depending on the large list of Submarine dependencies. Submarine would like to encourage integration with additional Apache projects: - Apache Arrow for [cross-language development platform for in-memory data cache] - Apache Spark for [Data processing / preprocessing, Feature engineering] - Apache Zeppelin for [Interactive development of algorithms through the way of notebooks] As far as we know, there’s no similar project in Apache positioned to be an end-to-end machine learning platform.",../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal An Excessive Fascination with the Apache Brand,"Submarine wants to become an Apache project in order to help efforts to diversifythe committer-base, and not to capitalize on the Apache brand. The Submarineproject is in production use already inside several large companies and isalready being released by Apache Hadoop. As such, the Submarine project is notseeking to use the Apache brand as a marketing tool.",../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Documentation,"The primary documentation about Submarine is located on thehttps://hadoop.apache.org/submarine/docs/0.2.0/Index/ There have been also been presentations on Submarine: Qcon 2019, Beijing (https://qcon.infoq.cn/2019/beijing/presentation/1440) DataWorks Summit 2019 Barcelona, Spain(https://dataworkssummit.com/barcelona-2019/session/hadoop-submarine-project-running-deep-learning-workloads-on-yarn/)",../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Initial Source,Submarine has been under development as part of Apache Hadoop since 2018. The original inclusion into Hadoop was via YARN-8135 (Now becomes Submarine-2).,../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal List Submarine sub-modules and explain,"CORE Run the machine learning workload on k8s or YARN, and take advantage of k8s' online service and yarn's offline calculation. Submarine Portal Server resource management, personnel role management, algorithm development, model management. Provide interactive data analysis and algorithm development capabilities in notebook mode. Submarine ML Plugin interface Submarine provides a plug-in interface to the machine learning framework, enabling easy integration of various machine learning frameworks into submarines. Submarine Toolkit / SDK Submarine provides development libraries for python, java and scala. You can use it directly in machine learning algorithms. Mlflow, memory cache, metrics in the submarine runtime environment.",../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Major dependencies,Build tools - Apache Maven,../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Other Dependencies,"Apache - Log4j - Hadoop 2.7.x+ Non-Apache - JDK 1.8+ - Protobuf - TonY (BSD 2-clause,https://github.com/linkedin/TonY)",../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Cryptography,Submarine does not currently support encryption.,../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Mailing Lists,private@submarine for private PMC discussions (with moderated subscriptions)dev@submarineuser@submarinecommits@submarine,../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Version Control,"Git is the preferred source control system. We need a separate Git repository after spinoff to a new Apache TLP, Hadoop community has already voted to move Submarine source code to a separate Git repo, which is tracked by:INFRA-18964-Аутентификациядля просмотра подробных данных проблемы. Existing Submarine source code will be moved to the new created repo.",../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Issue Tracking,Submarine already has a separate JIRA instance (SUBMARINE-) to track issues.,../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Other Resources,The existing code already has unit tests so we will make use of existingApache continuous testing infrastructure. The resulting load should not bevery large.,../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Initial PMC,"Wangda Tan (wangda at apache dot org) (Hadoop PMC)Xun Liu (liuxun at apache dot org) (Zeppelin Committer)Sunil Govind (sunilg at apache dot org) (Hadoop PMC)Zhankun Tang (ztang at apache dot org) (Hadoop Committer)Zac Zhou (zhouquan at apache dot org) (Hadoop Committer)Owen O'Malley (omalley at apache dot org) (Ambari PMC, Apsite Committer, Bigtop Committer, Chukwa Committer, Giraph PMC, Hadoop PMC, Hawq Committer, Helix PMC, Hive PMC, Iceberg Committer, Incubator PMC, Kafa Committer, Knox PMC, Kylin PMC, member, Metron Committer, Orc PMC, PMC Chairs, Ranger PMC, Reef, Tez PMC) We’d like to propose Wangda Tan as the initial VP for the Submarine project.",../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Initial Committers,"Szilard Nemeth (snemeth at apache dot org) (Hadoop Committer)Jeff Zhang (zjffdu at apache dot org) (Member, Incubator,  Livy Committer, Pig Committer, Tez PMC, Zeppelin PMC)Yanbo Liang (yliang at apache dot org) (Spark PMC)Naganarasimha Garla (naganarasimha_gr at apache dot org) (Hadoop PMC)Devaraj K (devaraj at apache dot org) (Hadoop PMC)Rakesh Radhakrishnan (rakeshr at apache dot org) (bookkeeper PMC, Hadoop PMC, incubator, Mnemonic PMC, Zookeeper PMC)Vinayakumar B (vinayakumarb at apache dot org) (Hadoop PMC, incubator PMC)Ayush Saxena (ayushsaxena at apache dot org) (Hadoop Committer)Bibin Chundatt (bibinchundatt at apache dot org) (Hadoop PMC)Bharat Viswanadham (bharat at apache dot org) (Hadoop PMC)Brahma Reddy Battula (brahma at apache dot org)) (Hadoop PMC)Abhishek Modi (abmodi at apache dot org) (Hadoop Committer)Wei-Chiu Chuang (weichiu at apache dot org) (Hadoop PMC)Junping Du (junping_du at apache dot org) (Hadoop PMC, member)Rohith Sharma K S (rohithsharmaks@apache.org) (Hadoop PMC)Zhe Zhang (zhz@apache.org) (Hadoop PMC)Sammi Chen (sammichen@apache.org) (Hadoop PMC)Jian He (jianhe@apache.org) (Hadoop PMC)Varun Saxena (varunsaxena@apache.org) (Hadoop PMC)Chen Liang (cliang@apache.org) (Hadoop Committer)Plus All initial PMC members",../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Submarine Project Spin-Off to TLP Proposal Affiliations,"The initial PMC is employed at Cloudera, NetEase, LinkedIn The initial committers are employed by Cloudera, NetEase, LinkedIn, Alibaba, Facebook, Tencent, Intel, Huawei For anybody who wants to be included in this list, please let us know publicly during the proposal voting time.",../data/confluence_exports/HADOOP/Submarine-Project-Spin-Off-to-TLP-Proposal_127405318.html
Hadoop : Hadoop 2.8.0 Release Blocker/Critical (2.8.0):,"JIRA NumberPriorityAssigneeStatusDescriptionHDFS-10684BlockerJohn ZhugeDONEWebHDFS DataNode calls fail without parameter createparentHDFS-11180BlockerAkira AjisakaDONEIntermittent deadlock in NameNode when failover happensYARN-5184BlockerSangjin LeeDONEFix up incompatible changes introduced on ContainerStatus and NodeReportYARN-5559BlockerAkira AjisakaDONEAnalyse 2.8.0/3.0.0 jdiff reports and fix any issuesYARN-3866BlockerWangda TanDONEAM-RM protocol changes to support container resizing (break API compatibility)YARN-6029CriticalTao YangDONECapacityScheduler deadlock when ParentQueue#getQueueUserAclInfo is called by Thread_A at the moment thatThread_B calls LeafQueue#assignContainers to release a reserved containerYARN-6068BlockerJunping DuDONELog aggregation get failed when NM restart even with recoveryYARN-6072BlockerAjith SRM unable to start in secure modeMAPREDUCE-6816BlockerShen YinjieDONEProgress bars in Web UI always at 100%YARN-5709CriticalKarthik KambatlaDONECleanup leader election configs and pluggabilityHADOOP-13250CriticalVinod Kumar VavilapalliMOVE TO LATERjdiff and dependency reports aren't linked in site web pagesHDFS-8674CriticalDaryn SharpDONEImprove performance of postponed block scansHADOOP-13826CriticalSean MackroryDONES3A Deadlock in multipart copy due to thread pool limits.HDFS-11209CriticalXiaoyu YaoMOVE TO LATERSNN can't checkpoint when rolling upgrade is not finalized  For details, please refer:https://s.apache.org/6kwx",../data/confluence_exports/HADOOP/Hadoop-2.8.0-Release_68721069.html
Hadoop : Hadoop 2.8.0 Release TODOsbefore RC,"TODO itemStatus - 1/4/2017Current StatusAll blockers target for 2.8.0 are landed.DONEDONEAll JIRAs with fixVersion 2.8 get landed in branch-2.874 JIRAs identified that no patch landedDONEAll JIRAs with fixVersion 2.7.1, 2.7.2, 2.7.3 have fix version of 2.8.0352 JIRAs identified that not marked with 2.8.0.DONEAll JIRAs with fixVersion of 2.7.1, 2.7.2 and 2.7.3 get landed in branch-2.817 JIRAs from 2.7.1 have no patch landed in 2.8.08 JIRAs from 2.7.2 have no patch landed in 2.8.016 JIRAs from 2.7.3 have no patch landed in 2.8.0DONE",../data/confluence_exports/HADOOP/Hadoop-2.8.0-Release_68721069.html
Hadoop : Hadoop 2.8.0 Release Important Features & Improvements:,"JIRA IDFeature DescriptionJIRA IDFeature DescriptionHADOOP-12909Support async call retry and failover which can be used in async DFSimplementation with retry effort.HDFS-9239DataNode Lifeline Protocol: an alternative protocol for reporting DataNodeliveness. This can prevent the NameNode from incorrectly marking DataNodesas stale or dead in highly overloaded clusters where heartbeat processingis suffering delays.HADOOP-12691HADOOP-13008XFS Filter support in UIs. Cross Frame Scripting (XFS) prevention for UIs can be provided througha common servlet filter.HDFS-9184Logging HDFS operation's caller context into audit logsHADOOP-12723HADOOP-12548HADOOP-12537S3A improvements: add ability to plug in any AWSCredentialsProvider,support read s3a credentials from hadoop credential provider API inaddition to XML configuraiton files, support Amazon STS temporarycredentialsHDFS-9945A new Datanode command for evicting writers which is useful when data nodedecommissioning is blocked by slow writers.HADOOP-12635WASB improvements: adding append API supportYARN-3458NodeManager CPU resource monitoring in Windows.HADOOP-11731HADOOP-12651Build enhancements: replace dev-support with wrappers to Yetus,provide a docker based solution to setup a build environment,remove CHANGES.txt and rework the change log and release notes.YARN-41NM shutdown more graceful: NM will unregister to RM immediately rather thanwaiting for timeout to be LOST (if NM work preserving is not enabled).HADOOP-9477Add posixGroups support for LDAP groups mapping service.YARN-261Add ability to fail a specific AM attempt in scenario of AM attempt get stuck.HADOOP-13037Support integration with Azure Data Lake (ADL) as an alternativeHadoop-compatible file system.YARN-4349CallerContext support in YARN audit log.HDFS-9711HDFS-8155HDFS-9057WebHDFS enhancements: integrate CSRF prevention filter in WebHDFS,support OAuth2 in WebHDFS, disallow/allow snapshots via WebHDFSYARN-3623ATS versioning support: a new configuration to indicate timeline service version.HDFS-9804Allow long-running Balancer to login with keytabMAPREDUCE-6304Allow node labels get specificed in submitting MR jobsHDFS-9835Add ReverseXML processor which reconstructs an fsimage from an XML file.This will make it easy to create fsimages for testing, and manually editfsimages when there is corruptionMAPREDUCE-6415Add a new tool to combine aggregated logs into HAR filesHDFS-9244Support nested encryption zones",../data/confluence_exports/HADOOP/Hadoop-2.8.0-Release_68721069.html
Hadoop : Hadoop 2.8.0 Release Number of Patches,ProjectNumber of Patches Not Released Before (not include 3.0.0-alphaX)Number of Patches Since 2.7.0COMMON655774HDFS9601137MAPREDUCE170232YARN585774TOTAL23702917,../data/confluence_exports/HADOOP/Hadoop-2.8.0-Release_68721069.html
Hadoop : Git And Hadoop Key Git Concepts,"The key concepts of Git. Git doesn't store changes, it snapshots the entire source tree. Good for fast switch and rollback, bad for binaries. (as an enhancement, if a file hasn't changed, it doesn't re-replicate it).Git stores all ""events"" as SHA1 checksummed objects; you have deltas, tags and commits, where a commit describes the status of items in the tree.Git is very branch centric; you work in your own branch off local or central repositoriesYou had better enjoy merging.  GitHubprovide some good lessons on git athttp://learn.github.com Apache serves up read-only Git versions of their source athttp://git.apache.org/. Committers can commit changes to writable Git repository. Seehttps://wiki.apache.org/hadoop/HowToCommit",../data/confluence_exports/HADOOP/Git-And-Hadoop_89071914.html
Hadoop : Git And Hadoop Checking out the source,"You need a copy of git on your system. Some IDEs ship with Git support; this page assumes you are using the command line.Clone a local Git repository from the Apache repository. The Hadoop subprojects (common, HDFS, and MapReduce) live inside a combined repository called `hadoop.git`. git clone https://gitbox.apache.org/repos/asf/hadoop.git  The total download is a few hundred MB, so the initial checkout process works best when the network is fast. Once downloaded, Git works offline -though you will need to perform your initial builds online so that the build tools can download dependencies.",../data/confluence_exports/HADOOP/Git-And-Hadoop_89071914.html
Hadoop : Git And Hadoop Grafts for complete project history,"The Hadoop project has undergone some movement in where its component parts have been versioned. Because of that, commands like `git log --follow` needs to have a little help. To graft the history back together into a coherent whole, insert the following contents intohadoop/.git/info/grafts: # Project split
5128a9a453d64bfe1ed978cf9ffed27985eeef36 6c16dc8cf2b28818c852e95302920a278d07ad0c
6a3ac690e493c7da45bbf2ae2054768c427fd0e1 6c16dc8cf2b28818c852e95302920a278d07ad0c
546d96754ffee3142bcbbf4563c624c053d0ed0d 6c16dc8cf2b28818c852e95302920a278d07ad0c
# Project un-split in new writable git repo
a196766ea07775f18ded69bd9e8d239f8cfd3ccc 928d485e2743115fe37f9d123ce9a635c5afb91a
cd66945f62635f589ff93468e94c0039684a8b6d 77f628ff5925c25ba2ee4ce14590789eb2e7b85b You can then use commands likegit blame --followwith success.",../data/confluence_exports/HADOOP/Git-And-Hadoop_89071914.html
Hadoop : Git And Hadoop Forking onto GitHub,"You can create your own fork of the ASF project. This is required if you want to contribute patches by submitting pull requests. However you can choose to skip this step and attach patch files directly on Apache Jiras. Create a GitHub login athttp://github.com/; Add your public SSH keysGo tohttps://github.com/apache/hadoop/Click fork in the github UI. This gives you your own repository URL.In the existing clone, add the new repository: git remote add -f github git@github.com:MYUSERNAMEHERE/hadoop.git This gives you a local repository with two remote repositories:originandgithub.originhas the Apache branches, which you can update whenever you want to get the latest ASF version: git checkout -b trunk origin/trunk
 git pull origin Your own branches can be merged with trunk, and pushed out to GitHub. To generate patches for attaching to Apache JIRAs, check everything in to your specific branch, merge that with (a recently pulled) trunk, then diff the two: git diff trunk > ../hadoop-patches/HADOOP-XYX.patch",../data/confluence_exports/HADOOP/Git-And-Hadoop_89071914.html
Hadoop : Git And Hadoop Branching,"Git makes it easy to branch. The recommended process for working with Apache projects is: one branch per JIRA issue. That makes it easy to isolate development and track the development of each change. It does mean if you have your own branch that you release, one that merges in more than one issue, you have to invest some effort in merging everything in. Try not to make changes in different branches that are hard to merge, and learn your way round the git rebase command to handle changes across branches. Better yet: do not use rebase once you have created a chain of branches that each depend on each other",../data/confluence_exports/HADOOP/Git-And-Hadoop_89071914.html
Hadoop : Git And Hadoop Creating the branch,"Creating a branch is quick and easy #start off in the apache trunk
git checkout trunk
#create a new branch from trunk
git branch HDFS-775
#switch to it
git checkout HDFS-775
#show what's branch you are in
git branch Remember, this branch is local to your machine. Nobody else can see it until you push up your changes or generate a patch, or you make your machine visible over the network to interested parties.",../data/confluence_exports/HADOOP/Git-And-Hadoop_89071914.html
Hadoop : Git And Hadoop Creating Patches for attachment to JIRA issues,"Assuming your trunk repository is in sync with the Apache projects, you can usegit diffto create a patch file.First, have a directory for your patches: mkdir ../hadoop-patches Then generate a patch file listing the differences between your trunk and your branch git diff trunk > ../hadoop-patches/HDFS-775-1.patch The patch file is an extended version of the unified patch format used by other tools; type {{{git help diff}}} to get more details on it. Here is what the patch file in this example looks like$ cat ../outgoing/HDFS-775-1.patch diff --git src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java
index 42ba15e..6383239 100644
--- src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java
+++ src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java
@@ -355,12 +355,14 @@ public class FSDataset implements FSConstants, FSDatasetInterface {
       return dfsUsage.getUsed();
     }

+    /**
+     * Calculate the capacity of the filesystem, after removing any
+     * reserved capacity.
+     * @return the unreserved number of bytes left in this filesystem. May be zero.
+     */
     long getCapacity() throws IOException {
-      if (reserved > usage.getCapacity()) {
-        return 0;
-      }
-
-      return usage.getCapacity()-reserved;
+      long remaining = usage.getCapacity() - reserved;
+      return remaining > 0 ? remaining : 0;
     }

     long getAvailable() throws IOException {",../data/confluence_exports/HADOOP/Git-And-Hadoop_89071914.html
Hadoop : Git And Hadoop Updating your patch,"If your patch is not immediately accepted, do not be offended: it happens to us all. It introduces a problem: your branches become out of date. You need to check out the latest apache version, merge your branches with it, and then push the changes back to GitHub. git checkout trunk
 git pull apache
 git checkout mybranch
 git merge trunk
 git push github mybranch Your branch is up to date, and new diffs can be created and attached to patches.",../data/confluence_exports/HADOOP/Git-And-Hadoop_89071914.html
Hadoop : Git And Hadoop Deriving Branches from Branches,"If you have one patch that depends upon another, you should have a separate branch for each one. Simply merge the changes from the first branch into the second, so that it is always kept up to date with the first changes. To create a patch file for submission as a JIRA patch, do a diff between the two branches, not against trunk.do not play with rebasing once you start doing this as you will make merging a nightmare",../data/confluence_exports/HADOOP/Git-And-Hadoop_89071914.html
Hadoop : Git And Hadoop What to do when your patch is committed,"Once your patch is committed into Git, you do not need the branch any more. You can delete it straight away, but it is safer to verify the patch is completely merged in. Pull down the latest release and verify that the patch branch is synchronized git checkout trunk
 git pull apache
 git checkout mybranch
 git merge trunk
 git diff trunk The output of the last command should be nothing: the two branches should be identical. You can then prove to git that this is true by switching back to the trunk branch and merging in the branch, an operation which will not change the source tree, but update git's branch graph. git checkout trunk
 git merge mybranch Now you can delete the branch without being warned by git git branch -d mybranch Finally, propagate that deletion to your private github repository git push github --delete mybranch",../data/confluence_exports/HADOOP/Git-And-Hadoop_89071914.html
Hadoop : Secure DataNode Privileged Resources,"Because the DataNode data transfer protocol does not use the Hadoop RPC framework, DataNodes must authenticate themselves using privileged ports which are specified by dfs.datanode.address and dfs.datanode.http.address. This authentication is based on the assumption that the attacker won’t be able to get root privileges on DataNode hosts. When you execute the hdfs datanode command as root, the server process binds privileged ports at first, then drops privilege and runs as the user account specified by HDFS_DATANODE_SECURE_USER. This startup process usesthe jsvc programinstalled to JSVC_HOME. You must specify HDFS_DATANODE_SECURE_USER and JSVC_HOME as environment variables on start up (in hadoop-env.sh).",../data/confluence_exports/HADOOP/Secure-DataNode_75956973.html
Hadoop : Secure DataNode SASL RPC,"As of version 2.6.0, SASL can be used to authenticate the data transfer protocol. In this configuration, it is no longer required for secured clusters to start the DataNode as root using jsvc and bind to privileged ports. To enable SASL on data transfer protocol, set dfs.data.transfer.protection in hdfs-site.xml, set a non-privileged port for dfs.datanode.address, set dfs.http.policy to HTTPS_ONLY and make sure the HDFS_DATANODE_SECURE_USER environment variable is not defined. Note that it is not possible to use SASL on data transfer protocol if dfs.datanode.address is set to a privileged port. This is required for backwards-compatibility reasons. In order to migrate an existing cluster that used root authentication to start using SASL instead, first ensure that version 2.6.0 or later has been deployed to all cluster nodes as well as any external applications that need to connect to the cluster. Only versions 2.6.0 and later of the HDFS client can connect to a DataNode that uses SASL for authentication of data transfer protocol, so it is vital that all callers have the correct version before migrating. After version 2.6.0 or later has been deployed everywhere, update configuration of any external applications to enable SASL. If an HDFS client is enabled for SASL, then it can connect successfully to a DataNode running with either root authentication or SASL authentication. Changing configuration for all clients guarantees that subsequent configuration changes on DataNodes will not disrupt the applications. Finally, each individual DataNode can be migrated by changing its configuration and restarting.",../data/confluence_exports/HADOOP/Secure-DataNode_75956973.html
Hadoop : Hadoop 3.1 Release Blocker/Critical for 3.1.4:,"КлючТемаТип запросаСозданОбновленныйДлительностьИсполнительАвторПриоритетСтатусРезолюцияHADOOP-16754Fix docker failed to build yetus/hadoopдек 09, 2019июл 03, 2020Kevin SuKevin SuResolvedFixedHADOOP-16907Update Netty to 4.1.46Finalмар 04, 2020мар 10, 2020Wei-Chiu ChuangWei-Chiu ChuangResolvedDuplicateHADOOP-16891Upgrade jackson-databind to 2.9.10.3фев 27, 2020мар 04, 2020Siyao MengSiyao MengResolvedFixedHADOOP-16882Update jackson-databind to 2.9.10.2 in branch-3.1, branch-2.10фев 25, 2020фев 29, 2020Lisheng SunWei-Chiu ChuangResolvedFixedHDFS-15181Webhdfs getTrashRoot() causes internal AccessControlExceptionфев 18, 2020фев 19, 2020Kihwal LeeKihwal LeeResolvedDuplicateHDFS-15012NN fails to parse Edit logs after applying HDFS-13101ноя 25, 2019дек 18, 2019Shashikant BanerjeeEric LinResolvedFixedHDFS-15186Erasure Coding: Decommission may generate the parity block's content with all 0 in some caseфев 20, 2020янв 04, 2022Yao GuangdongYao GuangdongResolvedFixedHDFS-15293Relax the condition for accepting a fsimage when receiving a checkpointапр 21, 2020июн 10, 2021Chen LiangChen LiangResolvedFixedYARN-10347Fix double locking in CapacityScheduler#reinitialize in branch-3.1июл 08, 2020июл 14, 2020Masatake IwasakiMasatake IwasakiResolvedFixedHADOOP-16949pylint fails in the build environmentмар 30, 2020апр 04, 2020Akira AjisakaAkira AjisakaResolvedFixedАутентификациядля получения проблемОтображение 10 изПроблем: 11",../data/confluence_exports/HADOOP/Hadoop-3.1-Release_144511191.html
Hadoop : Hadoop 3.1 Release Issues with fixed version 3.1.4,"КлючТемаТип запросаСозданОбновленныйДлительностьИсполнительАвторПриоритетСтатусРезолюцияHDFS-14509DN throws InvalidToken due to inequality of password when upgrade NN 2.x to 3.xмая 23, 2019окт 12, 2022Yuxuan WangYuxuan WangResolvedFixedHADOOP-17014Upgrade jackson-databind to 2.9.10.4апр 26, 2020окт 25, 2021Siyao MengSiyao MengResolvedFixedHADOOP-16109Parquet reading S3AFileSystem causes EOFфев 13, 2019июл 02, 2021Steve LoughranDave ChristiansonResolvedFixedHADOOP-16776backport HADOOP-16775: distcp copies to s3 are randomly corruptedдек 23, 2019дек 04, 2020Amir ShenavandehAmir ShenavandehResolvedFixedHADOOP-16754Fix docker failed to build yetus/hadoopдек 09, 2019июл 03, 2020Kevin SuKevin SuResolvedFixedHADOOP-16982Update Netty to 4.1.48.Finalапр 13, 2020апр 16, 2020Lisheng SunWei-Chiu ChuangResolvedFixedHADOOP-16891Upgrade jackson-databind to 2.9.10.3фев 27, 2020мар 04, 2020Siyao MengSiyao MengResolvedFixedHADOOP-16882Update jackson-databind to 2.9.10.2 in branch-3.1, branch-2.10фев 25, 2020фев 29, 2020Lisheng SunWei-Chiu ChuangResolvedFixedYARN-10055bower install failsдек 23, 2019дек 23, 2019Akira AjisakaAkira AjisakaResolvedFixedHDFS-14910Rename Snapshot with Pre Descendants Fail With IllegalArgumentException.окт 17, 2019окт 24, 2019Wei-Chiu ChuangÍñigo GoiriResolvedFixedАутентификациядля получения проблемОтображение 10 изПроблем: 309",../data/confluence_exports/HADOOP/Hadoop-3.1-Release_144511191.html
Hadoop : Try out Ozone Build From Git Repo,"Get the Apache Ozone source code from the Apache Git repository. Then check out trunk and build it with thehddsMaven profile enabled. git clone https://github.com/apache/ozone.git
  cd hadoop-ozone
  mvn clean install -DskipTests=true -Dmaven.javadoc.skip=true -Pdist -Dtar -DskipShade Initial compilation may take over 30 minutes as Maven downloads dependencies.-DskipShadeis optional - it makes compilation faster for development. This will give you a tarball in your distribution directory. Here is an example of the tarball that will be generated. hadoop-ozone/dist/target/ozone-0.5.0-SNAPSHOT.tar.gz",../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Try out Ozone Build From a Source Release,"Download and extract a source tarball fromhttps://ozone.apache.org/downloads/E.g. tar xf ozone-1.2.1-src.tar.gz
  cd ozone-1.2.1-src/
  mvn clean install -DskipTests=true -Dmaven.javadoc.skip=true -Pdist -Dtar -DskipShade",../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Try out Ozone Download Binary Release,"Download and extract a binary release fromhttps://ozone.apache.org/downloads/E.g. tar xf ozone-1.2.1.tar.gz
  cd ozone-1.2.1/",../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Try out Ozone Start Cluster Using Docker,"If you downloaded or built a source release, run the following commands to start an Ozone cluster in docker containers with 3 datanodes. cd hadoop-ozone/dist/target/ozone-*-SNAPSHOT/compose/ozone
  docker-compose up -d --scale datanode=3  If you downloaded a binary release, run the following instead., cd compose/ozone
  docker-compose up -d --scale datanode=3  For more docker-compose commands, please check the end of theGetting started with dockerguide To Shutdown the cluster, please run the commanddocker-compose down",../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Try out Ozone Single Node Development Cluster,"This is the traditional way to start a development cluster from source code. Once the package is built, you can start Ozone services by going to thehadoop-ozone/dist/target/ozone-*/directory. Your Unix shell should expand the '*' wildcard to the correct Ozone version number.",../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Try out Ozone Configuration,"Save the minimal snippet tohadoop-ozone/dist/target/ozone-*/etc/hadoop/ozone-site.xmlin the compiled distribution. <configuration>
<properties>
<property><name>ozone.enabled</name><value>true</value></property>
<property><name>ozone.scm.datanode.id</name><value>/tmp/ozone/data/datanode.id</value></property>
<property><name>ozone.replication</name><value>1</value></property>
<property><name>ozone.metadata.dirs</name><value>/tmp/ozone/data/metadata</value></property>
<property><name>ozone.scm.names</name><value>localhost</value></property>
<property><name>ozone.om.address</name><value>localhost</value></property>
</properties>
</configuration>",../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Try out Ozone Start Services,"To start ozone, you need to start SCM, OzoneManager and DataNode. In pseudo-cluster mode, all services will be started on localhost. bin/ozone scm --init
  bin/ozone --daemon start scm
  bin/ozone om --init
  bin/ozone --daemon start om
  bin/ozone --daemon start datanode",../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Try out Ozone Run Ozone Commands,"Once you have ozone running you can use theseOzoneshellcommandsto create a volume, bucket and keys. E.g. bin/ozone sh volume create /vol1
  bin/ozone sh bucket create /vol1/bucket1
  dd if=/dev/zero of=/tmp/myfile bs=1024 count=1
  bin/ozone sh key put /vol1/bucket1/key1 /tmp/myfile
  bin/ozone sh key list /vol1/bucket1",../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Try out Ozone Stop Services,"bin/ozone --daemon stop om
  bin/ozone --daemon stop scm
  bin/ozone --daemon stop datanode",../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Try out Ozone Clean up your Dev Environment (Optional),"Remove the following directories to wipe the Ozone pseudo-cluster state. This will also delete all user data (volumes/buckets/keys) you added to the pseudo-cluster. rm -fr /tmp/ozone
rm -fr /tmp/hadoop-${USER}* Note: This will also wipe state for any running HDFS services.",../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Try out Ozone Pre-requisites,Ensure you have password-less ssh setup between your hosts.,../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Try out Ozone ozone-site.xml,"Save the following snippet toetc/hadoop/ozone-site.xmlin the compiled Ozone distribution. <configuration>
<properties>
<property><name>ozone.scm.block.client.address</name><value>SCM-HOSTNAME</value></property>
<property><name>ozone.scm.names</name><value>SCM-HOSTNAME</value></property>
<property><name>ozone.scm.client.address</name><value>SCM-HOSTNAME</value></property>
<property><name>ozone.om.address</name><value>OM-HOSTNAME</value></property>
<property><name>ozone.handler.type</name><value>distributed</value></property>
<property><name>ozone.enabled</name><value>True</value></property>
<property><name>ozone.scm.datanode.id</name><value>/tmp/ozone/data/datanode.id</value></property>
<property><name>ozone.replication</name><value>1</value></property>
<property><name>ozone.metadata.dirs</name><value>/tmp/ozone/data/metadata</value></property>
</properties>
</configuration> Replace SCM-HOSTNAME and OM-HOSTNAME with the names of the machines where you want to start the SCM and OM services respectively. It is okay to start these services on the same host. If you are unsure then just use any machine from your cluster.",../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Try out Ozone ozone-env.sh,"The only mandatory setting in ozone-env.sh is JAVA_HOME. E.g. # The java implementation to use. By default, this environment
# variable is REQUIRED on ALL platforms except OS X!
export JAVA_HOME=/usr/java/latest",../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Try out Ozone workers,"The workers file should contain a list of hostnames in your cluster where DataNode service will be started. E.g. n001.example.com
n002.example.com
n003.example.com
n004.example.com",../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Try out Ozone Initialize the SCM,"Run the following commands on the SCM host bin/ozone scm --init
bin/ozone --daemon start scm",../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Try out Ozone Format the OM,"Run the following commands on the OM host bin/ozone om --init
bin/ozone --daemon start om",../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Try out Ozone Start DataNodes,Run the following command on any cluster host. sbin/start-ozone.sh,../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Try out Ozone Stop Services,Run the following command on any cluster host. sbin/stop-ozone.sh,../data/confluence_exports/HADOOP/Try-out-Ozone_93325864.html
Hadoop : Hadoop 2.8 Release Blocker/Critical (2.8.2):,"JIRA NumberPriorityAssigneeStatusDescriptionHADOOP-14207BlockerSurendra Singh LilhoreDONE""dfsadmin -refreshCallQueue"" command is failing with DecayRpcSchedulerYARN-4006BlockerMOVEDYARN ATS Alternate Kerberos HTTP Authentication ChangesHDFS-11609BlockerKihwal LeeDONESome blocks can be permanently lost if nodes are decommissioned while deadYARN-6102CriticalAjith SMOVEDOn failover RM can crash due to unregistered event to AsyncDispatcherYARN-2919CriticalNaganarasimha G RMOVEDPotential race between renew and cancel in DelegationTokenRenwerYARN-6091CriticalEric BadgerMOVEDthe AppMaster register failed when use Docker on LinuxContainerHADOOP-14024CriticalJohn ZhugeDONEKMS JMX endpoint throws ClassNotFoundExceptionMAPREDUCE-6654CriticalJunping DuMOVEDPossible NPE in JobHistoryEventHandler#handleEventHDFS-7967CriticalDaryn SharpMOVEDReduce the performance impact of the balancerYARN-3760CriticalHaibo ChenDONEFSDataOutputStream leak in AggregatedLogFormat.LogWriter.close()HADOOP-9747CriticalDaryn SharpMOVEDReduce unnecessary UGI synchronizationHDFS-11472CriticalWei-Chiu ChuangDONEFix inconsistent replica size after a data pipeline failureHADOOP-14270CriticalJunping DuDONEHADOOP 2.8 Release tar ball size should be smallerHDFS-11608CriticalXiaobing ZhouDONEHDFS write crashed in the case of huge block sizeYARN-6288CriticalAkira AjisakaDONEExceptions during aggregated log writes are mishandledYARN-7246BlockerShane KumpfDONEFix the default docker binary pathYARN-7230BlockerShane KumpfDONEDocument DockerContainerRuntime for branch-2.8 with proper scope and claim as an experimental featureYARN-7443CriticalJason LoweDONEcontainer-executor fails to remove entries from a directory that is not writable or executableHADOOP-14958BlockerJunping DuDONEFix source-level compatibility after HADOOP-11252  For details, please refer:https://s.apache.org/JM5x",../data/confluence_exports/HADOOP/Hadoop-2.8-Release_66854809.html
Hadoop : Hadoop 2.8 Release TODOsbefore ongoing RC,"TODO itemStatusAll blockers target for 2.8.2 are landed.DONEAll JIRAs with fixVersion in (2.8.0, 2.8.1, 2.8.2, 2.8.3) get landed in branch-2.8DONEAll blocker/critical JIRAs with fixVersion 2.7.4 have fix version of 2.8.2DONEAll blocker/critical JIRAs with fixVersion of 2.7.4 get landed in branch-2.8DONE",../data/confluence_exports/HADOOP/Hadoop-2.8-Release_66854809.html
Hadoop : Hadoop 2.8 Release Number of Patches,ProjectNumber of Patches Since 2.8.1COMMON91HDFS99MAPREDUCE20YARN105TOTAL315     Finished Releases  Hadoop 2.8.0,../data/confluence_exports/HADOOP/Hadoop-2.8-Release_66854809.html
Hadoop : Hadoop 2.9 Release Blocker/Critical (2.9.3):,"КлючТемаТип запросаСозданОбновленныйДлительностьИсполнительАвторПриоритетСтатусРезолюцияYARN-7649RMContainer state transition exception after container updateдек 13, 2017фев 06, 2020Arun SureshWeiwei YangOpenUnresolvedYARN-6918Remove acls after queue delete to avoid memory leakавг 01, 2017ноя 01, 2018Bibin ChundattBibin ChundattPatch AvailableUnresolvedYARN-7601Incorrect container states recovered as LevelDB uses alphabetical orderдек 03, 2017ноя 01, 2018Sampada DehankarSampada DehankarPatch AvailableUnresolvedYARN-7450ATS Client should retry on intermittent Kerberos issues.ноя 06, 2017ноя 01, 2018UnassignedRavi PrakashOpenUnresolvedHDFS-13678StorageType is incompatible when rolling upgrade to 2.6/2.6+ versionsиюн 14, 2018мая 24, 2022UnassignedYiqun LinOpenUnresolvedHADOOP-16320Workaround bug with commons-configuration to be able to emit Ganglia metrics to multiple sink serversмая 18, 2019мая 20, 2019Thomas PoeppingThomas PoeppingPatch AvailableUnresolvedАутентификациядля получения проблемПроблем: 6",../data/confluence_exports/HADOOP/Hadoop-2.9-Release_73637409.html
Hadoop : Hadoop 2.9 Release Blocker/Critical (2.9.2):,"КлючТемаТип запросаСозданОбновленныйДлительностьИсполнительАвторПриоритетСтатусРезолюцияYARN-9106Add option to graceful decommission to not wait for applicationsдек 10, 2018сен 01, 2019Mikayla KonstMikayla KonstPatch AvailableUnresolvedYARN-10394RACK/NODE_LOCAL Request have same nodelabel as ANY Requestавг 11, 2020авг 11, 2020UnassignedchanOpenUnresolvedHADOOP-17423ERROR conf.Configuration: error parsing conf core-site.xml com.ctc.wstx.exc.WstxParsingException: Illegal processing instruction target (""xml""); xml (case insensitive) is reserved by the specs.дек 09, 2020дек 09, 2020UnassignedRavindra Dadahari GaikwadOpenUnresolvedMAPREDUCE-7183Make app master recover history from latest history file that existsфев 06, 2019авг 02, 2019Mikayla KonstMikayla KonstPatch AvailableUnresolvedMAPREDUCE-7173Add ability to shuffle intermediate map task output to a distributed filesystemдек 20, 2018авг 14, 2019Mikayla KonstMikayla KonstPatch AvailableUnresolvedYARN-9927RM multi-thread event processing mechanismокт 22, 2019мая 24, 2021Qi ZhuhcarrotPatch AvailableUnresolvedMAPREDUCE-7171Add additional cleanup hook to the app masterдек 11, 2018авг 14, 2019Mikayla KonstMikayla KonstPatch AvailableUnresolvedMAPREDUCE-7168Add option to not kill already-done map tasks when node becomes unusableдек 01, 2018авг 16, 2019Mikayla KonstMikayla KonstPatch AvailableUnresolvedMAPREDUCE-7295Add support to read history files from the done directoryсен 08, 2020сен 08, 2020UnassignedGaurangi SaxenaOpenUnresolvedАутентификациядля получения проблемПроблем: 9",../data/confluence_exports/HADOOP/Hadoop-2.9-Release_73637409.html
Hadoop : Hadoop 2.9 Release Blocker/Critical (2.9.1):,"JIRASummaryPriorityReporterContributorHADOOP-15177Update the release year to 2018BlockerAkira AjisakaBharat ViswanadhamYARN-7873Revert YARN-6078BlockerBillie RinaldiBillie RinaldiYARN-7692Skip validating priority acls while recovering applicationsBlockerCharan HebriSunil GMAPREDUCE-7028Concurrent task progress updates causing NPE in Application MasterBlockerGergo RepasGergo RepasHDFS-12638Delete copy-on-truncate block along with the original block, when deleting a file being truncatedBlockerJiandan YangKonstantin ShvachkoHADOOP-15189backport HADOOP-15039 to branch-2 and branch-3BlockerGenmao YuGenmao YuYARN-7430Enable user re-mapping for Docker containers by defaultBlockerEric YangEric YangYARN-7782Enable user re-mapping for Docker containers in yarn-default.xmlBlockerEric YangEric YangHADOOP-15080Aliyun OSS: update oss sdk from 2.8.1 to 2.8.3 to remove its dependency on Cat-x ""json-lib""BlockerChris DouglasSammi  ChenHDFS-13112Token expiration edits may cause log corruption or deadlockCriticalDaryn SharpDaryn SharpYARN-7102NM heartbeat stuck when responseId overflows MAX_INTCriticalBotong HuangBotong HuangYARN-7558""yarn logs"" command fails to get logs for running containers if UI authentication is enabled.CriticalNamit MaheshwariXuan GongHDFS-12347TestBalancerRPCDelay#testBalancerRPCDelay fails very frequentlyCriticalXiao ChenBharat ViswanadhamYARN-7591NPE in async-scheduling mode of CapacitySchedulerCriticalTao YangTao YangYARN-7509AsyncScheduleThread and ResourceCommitterService are still running after RM is transitioned to standbyCriticalTao YangTao YangHDFS-9023When NN is not able to identify DN for replication, reason behind it can be loggedCriticalBharat ViswanadhamXiao ChenHDFS-8693refreshNamenodes does not support adding a new standby to a running DNCriticalJian FangAjith SHDFS-12832INode.getFullPathName may throw ArrayIndexOutOfBoundsException lead to NameNode exitCriticalDeng FEIKonstantin ShvachkoHDFS-13100Handle IllegalArgumentException when GETSERVERDEFAULTS is not implemented in webhdfs.CriticalYongjun ZhangYongjun ZhangHDFS-11915Sync rbw dir on the first hsync() to avoid file lost on power failureCriticalKanaka Kumar AvvaruVinayakumar BHDFS-11576Block recovery will fail indefinitely if recovery time > heartbeat intervalCriticalLukas MajercakLukas MajercakHDFS-12788Reset the upload button when file upload failsCriticalBrahma Reddy BattulaBrahma Reddy BattulaYARN-5094some YARN container events have timestamp of -1CriticalSangjin LeeHaibo ChenYARN-7801AmFilterInitializer should addFilter after fill all parametersCriticalSumana SathishWangda Tan",../data/confluence_exports/HADOOP/Hadoop-2.9-Release_73637409.html
Hadoop : Hadoop 2.9 Release HADOOP-15385Test case failures in Hadoop-distcp project doesn’t impact the distcp function in Apache Hadoop 2.9.1 release.,Blocker/Critical (2.9.0): Тип запросаКлючТемаИсполнительАвторПриоритетСтатусРезолюцияСозданОбновленныйДлительностьАутентификациядля получения проблемПроблемы не найдены,../data/confluence_exports/HADOOP/Hadoop-2.9-Release_73637409.html
Hadoop : Hadoop 3.3 Release Release Schedule,Release NumberRelease Date (Planned)Release StatusFeature list3.3.02020-03-15ReleasedJava 11 runtime supportHDFS RBF with securitySupport non-volatile storage class memory(SCM) in HDFS cache directivesApplication Catalog for YARN applicationsScheduling of opportunistic containersSupport Vector Engine ( a new accelerator hardware) based on pluggable device frameworkDynamically add or remove auxiliary services3.3.12021-06-15ReleasedMAPREDUCE-7315 LocatedFileStatusFetcher to collect/publish IOStatisticsHDFS-15759 EC: Verify EC reconstruction correctness on DataNodeHDFS-15711 Add Metrics to HttpFS ServerHADOOP-17292 Using lz4-java in Lz4CodecHADOOP-17125 Using snappy-java in SnappyCodecHADOOP-17076 ABFS: Delegation SAS Generator UpdatesHADOOP-16916 ABFS: Delegation SAS generator for integration with RangerHADOOP-16830 Add Public IOStatistics APIHADOOP-16730 ABFS: Support for Shared Access Signatures (SAS)HADOOP-15891 Provide Regex Based Mount Point In Inode Tree3.3.22022-03-03Released3.3.32022-05-17Released3.3.4,../data/confluence_exports/HADOOP/Hadoop-3.3-Release_144509750.html
Hadoop : Hadoop 3.3 Release Release Tasks,"Read this first:HowToReleaseBefore doing the preparation tasks in the above articles, we have to do the following tasks:Create 3.4.0 and 3.3.1 versions in COMMON/HDFS/YARN/MAPREDUCE projectsAnnounce the process as the past RMs did. For example:""We will cut branch-3.3 and branch-3.3.0 in this weekend.""""branch-3.3 is cut, please commit a change to branch-3.3 if you want to fix the issue in 3.3.x.""""branch-3.3.0 is cut, please do not commit a change to branch-3.3.0 unless it is a blocker/critical issue.""",../data/confluence_exports/HADOOP/Hadoop-3.3-Release_144509750.html
Hadoop : Hadoop 3.3 Release 3.3.1 Blocker/Critical issues,КлючТемаТип запросаСозданОбновленныйДлительностьИсполнительАвторПриоритетСтатусРезолюцияАутентификациядля получения проблемПроблемы не найдены,../data/confluence_exports/HADOOP/Hadoop-3.3-Release_144509750.html
Hadoop : Hadoop 3.2 Release Blocker/Critical for 3.2.3:,"КлючТемаТип запросаСозданОбновленныйДлительностьИсполнительАвторПриоритетСтатусРезолюцияHADOOP-17655Upgrade Jetty to 9.4.40апр 23, 2021фев 02, 2022Akira AjisakaAkira AjisakaResolvedFixedHADOOP-18062Update the year to 2022янв 04, 2022янв 04, 2022UnassignedAkira AjisakaResolvedDuplicateYARN-8990Fix fair scheduler race condition in app submit and queue cleanupноя 08, 2018авг 03, 2021Wilfred SpiegelenburgWilfred SpiegelenburgResolvedFixedHDFS-12920HDFS default value change (with adding time unit) breaks old version MR tarball work with Hadoop 3.xдек 13, 2017мар 18, 2022Akira AjisakaJunping DuResolvedFixedHDFS-15641DataNode could meet deadlock if invoke refreshNameNodeокт 19, 2020июн 10, 2021Hongbing WangHongbing WangResolvedFixedАутентификациядля получения проблемПроблем: 5",../data/confluence_exports/HADOOP/Hadoop-3.2-Release_181307649.html
Hadoop : Hadoop 3.2 Release Issues with fixed version 3.2.3,"КлючТемаТип запросаСозданОбновленныйДлительностьИсполнительАвторПриоритетСтатусРезолюцияHADOOP-17718Explicitly set locale in the Dockerfileмая 20, 2021апр 14, 2022Wei-Chiu ChuangWei-Chiu ChuangResolvedFixedHADOOP-17112whitespace not allowed in paths when saving files to s3a via committerиюл 03, 2020фев 28, 2022Krzysztof AdamskiKrzysztof AdamskiResolvedFixedHADOOP-17655Upgrade Jetty to 9.4.40апр 23, 2021фев 02, 2022Akira AjisakaAkira AjisakaResolvedFixedHDFS-15113Missing IBR when NameNode restart if open processCommand async featureянв 11, 2020сен 22, 2021Xiaoqiao HeXiaoqiao HeResolvedFixedYARN-8990Fix fair scheduler race condition in app submit and queue cleanupноя 08, 2018авг 03, 2021Wilfred SpiegelenburgWilfred SpiegelenburgResolvedFixedHADOOP-17661mvn versions:set fails to parse pom.xmlапр 24, 2021июн 10, 2021Wei-Chiu ChuangWei-Chiu ChuangResolvedFixedHADOOP-17614Bump netty to the latest 4.1.61мар 31, 2021июн 10, 2021Wei-Chiu ChuangWei-Chiu ChuangResolvedFixedHDFS-15643EC: Fix checksum computation in case of native encodersокт 19, 2020июн 10, 2021Ayush SaxenaAhmed HusseinResolvedFixedHDFS-16422Fix thread safety of EC decoding during concurrent preadsянв 13, 2022апр 26, 2023янв 13, 2022daimindaiminResolvedFixedHADOOP-15775[JDK9] Add missing javax.activation-api dependencyсен 20, 2018ноя 07, 2022Akira AjisakaAkira AjisakaResolvedFixedАутентификациядля получения проблемОтображение 10 изПроблем: 313",../data/confluence_exports/HADOOP/Hadoop-3.2-Release_181307649.html
Hadoop : Hadoop 2.x to 3.x Upgrade Efforts Upgrade Tests for HDFS/YARN,"The following scenarios were tested while upgrading from Hadoop 2.8.4 to Hadoop 3.1.0  TypeComponentScenarioIssues FoundResolutionOverall StatusEXPRESS/ROLLING UPGRADEHDFSStarting 3.1.0 NameNode/DataNode with custom MetricsPlugin configured in hadoop2-metrics.propertiesHADOOP-15502-Rolling upgrade from HADOOP 2.x to 3.x breaks due to backward in-compatible change in MetricsPlugin interfaceOpenWorkaround is applicable only for EXPRESS UPGRADE -Replace MetricsPlugin implementation jars( eg: HadoopTimelineMetricsSink) with recompiled jars which use package ""org.apache.commons.configuration2""EXPRESS UPGRADEYARNStarting Hadoop 3.1.0 YARN daemonsROLLING UPGRADEHDFS3.1.0 NN is started with rollingUpgrade with default policy configured for Erasure codingHDFS-13596-NN restart fails after RollingUpgrade from 2.x to 3.xResolvedWorkaround Not knownROLLING UPGRADEYARNStart 3.1.0 NM in batches after starting RM.YARN-8346-Upgrading to 3.1 kills running containers with error ""Opportunistic container queue is full""ResolvedFixedEXPRESS/ROLLINGUPGRADEYARNRM started with recovery enabledYARN-8068-Application Priority field causes NPE in app timeline publish when Hadoop 2.7 based clients to 2.8+ResolvedFixed",../data/confluence_exports/HADOOP/Hadoop-2.x-to-3.x-Upgrade-Efforts_85472630.html
Hadoop : Hadoop 2.x to 3.x Upgrade Efforts Workloads,"Application TypeUpgrade TypeIssues FoundStatusOverall StatusMREXPRESS/ROLLING UPGRADEYARN-8346-Upgrading to 3.1 kills running containers with error ""Opportunistic container queue is full""ResolvedFixedHIVE on TEZHive with older versions of Tez (0.7, 0.8.x) with Hadoop 2 client ran into UT failuresTez 0.10.0 will support Hadoop 3TEZ-3923Move master to Hadoop 3+ and create separate 0.9.x lineTEZ-3252-[Umbrella] Enable support for Hadoop-3.xSpark 2.2/2.3Spark 2.2/2.3 has a fork of older version of Hive (1.2) which does not work with Hadoop 3Ongoing efforts in community to build/validate Spark with Hadoop 3 LibrariesSPARK-23534Umbrella jira to Build/test with Hadoop 3SPARK-23710Upgrade to Hive 2.x ( which builds with Hadoop 3)IN-PROGRESSPIGSupport for Hadoop 3 In-Progress in the community - targeted forPIG 0.18.0PIG-5253Pig Hadoop 3 supportIN-PROGRESSOOZIEDependent on PIG support for Hadoop 3Support for Hadoop 3 In-Progress in the community - Targeted for OOZIE-5.1.0OOZIE-2973Make sure Oozie works with Hadoop 3IN-PROGRESSMR with Native Task OptimizationValidation PendingMR with Shared Cache ManagerValidation Pending",../data/confluence_exports/HADOOP/Hadoop-2.x-to-3.x-Upgrade-Efforts_85472630.html
Hadoop : GitHub Integration Git setup,"This describes setup for one local repo and two remotes. It allows you to push the code on your machine to either your GitHub repo or apache/hadoop GitHub repo. The ASF official repository isgitbox.apache.org, however, the repository can be writable from both GitBox and GitHub if you are a committer. You will want to fork GitHub'sapache/hadoopto your own account on GitHub, this will enable Pull Requests of your own. Cloning this fork locally will set up ""origin"" to point to your remote fork on GitHub as the default remote. So if you perform `git push origin trunk` it will go to your fork.To attach to the Apache git repo do the following: git remote add apache https://github.com/apache/hadoop.git To check your remote setup: git remote -v you should see something like this: origin    https://github.com/your-github-id/hadoop.git (fetch)
origin    https://github.com/your-github-id/hadoop.git (push)
apache    https://github.com/apache/hadoop.git (fetch)
apache    https://github.com/apache/hadoop.git (push) Now if you want to experiment with a branch everything, by default, points to your github account becauseoriginis the. You can work as normal using only github until you are ready to merge with the apache remote. Some conventions will integrate with Apache Jira ticket numbers. git checkout -b feature/hadoop-xxxx #xxxx typically is a Jira ticket number
#do some work on the branch
git commit -a -m ""doing some work""
git push origin feature/hadoop-xxxx # notice pushing to **origin** not **apache** Once you are ready to commit to the apache remote, you can create a PR.",../data/confluence_exports/HADOOP/GitHub-Integration_89071920.html
Hadoop : GitHub Integration How to create a PR,"Push your branch to GitHub: git checkout feature/hadoop-xxxx
git fetch apache
git rebase apache/trunk # to make it apply to the current trunk
git push origin feature/hadoop-xxxx  Go to yourfeature/hadoop-xxxxbranch on Github. Since you forked it from Github'sapache/hadoopit will default any PR to go toapache/trunk.Click the green ""Compare, review, and create pull request"" button.You can edit the to and from for the PR if it isn't correct. The ""base fork"" should beapache/hadoopunless you are collaborating separately with one of the committers on the list. The ""base"" will be trunk. Don't submit a PR to one of the other branches unless you know what you are doing. The ""head fork"" will be your forked repo and the ""compare"" will be your `feature/hadoop-xxxx` branch.Click the ""Create pull request"" button and name the request ""HADOOP-XXXX"" all caps. This will connect the comments of the PR to the mailing list and Jira comments.From now on the PR lives on github'sapache/hadooprepository. You use the commenting UI there.",../data/confluence_exports/HADOOP/GitHub-Integration_89071920.html
Hadoop : GitHub Integration How to run Jenkins precommit job for a PR,"The precommit job is run automatically when opened a PR and when there is any change in your branch. If you are a committer and want to run Jenkins precommit job manually, log in tohttps://ci-hadoop.apache.org/job/hadoop-multibranchand run the job corresponding with the pull request ID. If there is no such job, click ""Scan Repository Now"" to scan the pull requests. If you are not committer, please create an empty commit on your branch.",../data/confluence_exports/HADOOP/GitHub-Integration_89071920.html
Hadoop : GitHub Integration Merging a PR (for committers),"In most cases, clicking the ""Squash and merge button"" is fine. Before merging the PR, the committer must check the title and the commit message, and fix them if needed. You can add ""Signed-off-by"", ""Reviewed-by"", and ""Co-authored-by"" when merging the commit. When you need to commit the change locally and push them, start with readinghttps://help.github.com/articles/checking-out-pull-requests-locally/. Remember that pull requests are equivalent to a remote GitHub branch with potentially a multitude of commits. In this case it is recommended to squash remote commit history to have one commit per issue, rather than merging in a multitude of contributor's commits. In order to do that, as well as close the PR at the same time, it is recommended to use squash commits. Merging pull requests are equivalent to a ""pull"" of a contributor's branch: git checkout trunk      # switch to local trunk branch
git pull apache trunk   # fast-forward to current remote HEAD
git pull --squash https://github.com/cuser/hadoop cbranch  # merge to trunk The--squashoption ensures all PR history is squashed into single commit, and allows committer to use his/her own message. Read git help for merge or pull for more information about--squashoption. In this example we assume that the contributor's GitHub handle is ""cuser"" and the PR branch name is ""cbranch"". Next, resolve conflicts, if any, or ask a contributor to rebase on top of trunk, if PR went out of sync.If you are ready to merge your own (committer's) PR you probably only need to merge (not pull), since you have a local copy that you've been working on. This is the branch that you used to create the PR. git checkout trunk      # switch to local trunk branch
git pull apache trunk   # fast-forward to current remote HEAD
git merge --squash feature/hadoop-xxxx Remember to run regular patch checks, build with tests enabled, and change CHANGES.TXT (not applicable for Hadoop versions 2.8.0 and later) for the appropriate part of the project.If everything is fine, you now can commit the squashed request along the lines git commit -a -m ""HADOOP-XXXX description (cuser via your-apache-id) closes apache/hadoop#ZZ"" HADOOP-XXXX is all caps and where ZZ is the pull request number on apache/hadoop repository. Including `closes apache/hadoop#ZZ` will close the PR automatically. More information is found athttps://help.github.com/articles/closing-issues-via-commit-messages. Next, push togitbox.apache.org: push apache trunk (this will require Apache handle credentials).The PR, once pushed, will get mirrored to GitHub. To update your personal GitHub version push there too: push origin trunk Note on squashing: Since squash discards remote branch history, repeated PRs from the same remote branch are difficult for merging. The workflow implies that every new PR starts with a new rebased branch. This is more important for contributors to know, rather than for committers, because if new PR is not mergeable, github would warn to begin with. Anyway, watch for dupe PRs (based on same source branches). This is a bad practice.",../data/confluence_exports/HADOOP/GitHub-Integration_89071920.html
Hadoop : GitHub Integration Closing a PR without committing (for committers),"Now Hadoop committer can directly close GitHub pull requests. If you are a committer and don't have the privilege, you need to link your ASF and GitHub account viahttps://gitbox.apache.org/setup/",../data/confluence_exports/HADOOP/GitHub-Integration_89071920.html
Hadoop : GitHub Integration Apache/github integration features,"Readhttps://blogs.apache.org/infra/entry/improved_integration_between_apache_and. Comments and PRs with Hadoop issue handles should post to mailing lists and Jira. Hadoop issue handles must in the form `HADOOP-YYYYY` (all capitals). Usually it makes sense to file a JIRA issue first, and then create a PR with description HADOOP-YYYY: <jira-issue-description> In this case all subsequent comments will automatically be copied to JIRA without having to mention the JIRA issue explicitly in each comment of the PR.",../data/confluence_exports/HADOOP/GitHub-Integration_89071920.html
Hadoop : GitHub Integration Avoiding accidentally committing private branches to the ASF repo,"Its dangerously easy —especially when using IDEs— to accidentally commit changes to the ASF repo, be it direct to the `trunk`, `branch-2` or other standard branch on which you are developing, or to a private branch you had intended to keep on GitHub (or a private repo).Committers can avoid this by having the directory in which they develop code set up with read only access to the ASF repository on GitHub, without the Apache repository added. A separate directory should be set up with write access to the ASF repository as well as read access to your other repositories. Merging operations and pushes back to the ASF repo are done from this directory —so isolated from all local development.If you accidentally commit a patch to an ASF branch, do not attempt to roll back the branch and force out a new update. Simply commit and push out a new patch revoking the change.If you do accidentally commit a branch to the ASF repo, the infrastructure team can delete it —but they cannot stop it propagating to GitHub and potentially being visible. Try not to do that.",../data/confluence_exports/HADOOP/GitHub-Integration_89071920.html
"Hadoop : GitHub Integration Avoiding accidentally committing private keys to Amazon AWS, Microsoft Azure or other cloud infrastructures","All the cloud integration projects underhadoop-toolsexpect a resource file,resources/auth-keys.xmlto contain the credentials for authenticating with cloud infrastructures. These files are explicitly excluded from git through entries in.gitignore. To avoid running up large bills and/or exposing private data, it is critical to keep any of your credentials secret.For maximum security here, clone your Hadoop repository into create separate directory for cloud tests, one with read-only access. Create theauth-keys.xmlfiles there. This guarantees that you cannot commit the credentials, albeit with a somewhat more complex workflow, as patches must be pushed to a git repository before being pulled and tested into the cloud-enabled directory.Accidentally committing secret credentials is a very expensive mistake. You will not only need to revoke your keys, you will need to kill all bitcoining machines created on all EC2 zones, and all outstanding spot-price bids for them.",../data/confluence_exports/HADOOP/GitHub-Integration_89071920.html
Hadoop : Roadmap Upcoming releases,"3.4.03.3.23.2.3 Feature freeze dateNACode freeze dateNAPlanned release dateActual release dateRelease Manager:Branchbranch-3.2Release StatusHadoop 3.2 Release 2.10.2 For more details, please checkHadoop Active Release Lines",../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap Old releases (Released),Information about old releases only for the records here. For the final features included in a given release please check the official release notes.,../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap 3.2.2,"Feature freeze dateSep 30, 2020Code freeze dateOct 15, 2020Planned release dateDec 20, 2020Actual release dateJan 15, 2021Release Manager:He XiaoqiaoBranchbranch-3.2",../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap 3.1.4,"Feature freeze dateWednesday, 11 March 2020Code freeze dateWednesday, 22 April 2020Planned release dateWednesday, 29 April 2020Actual release dateRelease Manager:Gabor BotaBranchbranch-3.1 Planned features:Hadoop 3.1 Release",../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap 3.3.0,"Feature freeze dateFeb 28, 2020Code freeze dateMarch 10, 2020Planned release dateMarch 15, 2020Actual release dateJuly 14,2020Release Manager:Brahma Reddy BattulaBranchtrunk Planned features:",../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap YARN features,JIRASummaryComponentFeature OwnerCommentsStatus1.YARN-9414Application Catalog for YARN applicationsYARNEric YangMerged2.YARN-5542Scheduling of opportunistic containersYARNKonstantinos Karanasos/Abhishek ModiMerged3.YARN-9473Support Vector Engine ( a new accelerator hardware) based on pluggable device frameworkYARNPeter BacskoMerged4.YARN-9264[Umbrella] Follow-up on IntelOpenCL FPGA pluginYARNPeter BacskoMerged5.YARN-9145[Umbrella] Dynamically add or remove auxiliary servicesYARNBillie RinaldiMerged6.YARN-9050[Umbrella] Usability improvements for scheduler activitiesYARNTao YangMerged7.YARN-8851[Umbrella] A pluggable device plugin framework to ease vendor plugin developmentYARNZhankun TangMerged8.YARN-9014runC container runtimeYARNEric BadgerMerged Partially,../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap HDFS Features:,JIRASummaryComponentFeature OwnerCommentsStatus1.HDFS-13891HDFS RBF stabilization phaseHDFSBrahma Reddy BattulaMerged2.HDFS-12345Scale testing HDFS NameNode with real metadata and workloads (Dynamometer)HDFSErik KrogenMerged3.HDFS-13762Support non-volatile storage class memory(SCM) in HDFS cache directivesHDFSFeilong HeMerged,../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap Common Features:,JIRASummaryComponentFeature OwnerCommentsStatus1.HADOOP-16095Support impersonation for AuthenticationFilterCommonEric YangMerged2.HADOOP-15620Über-jira: S3A phase VI: Hadoop 3.3 featuresCommonSteve LoughranMergedPartially3.HADOOP-15763Über-JIRA: abfs phase II: Hadoop 3.3 features & fixesCommonSteve LoughranMerged Partially4.HADOOP-15338Java 11 runtime supportCommonAkira AjisakaMerged5.HADOOP-15619Über-JIRA: S3Guard Phase IV: Hadoop 3.3 featuresCommonSteve LoughranMerged Partially6.HADOOP-13363Upgrade protobuf from 2.5.0 to something newerCommonVinayMerged,../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap 2.10.0,"Feature freeze dateCode freeze datePlanned release date2019Actual release dateOct 31, 2019Release Manager:Jonathan HungBranchbranch-2 Planned features:",../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap YARN features,JIRASummaryComponentFeature OwnerCommentsStatus1.YARN-8200Backport resource types/GPU features to branch-2YARNJonathanMerged to branch-2.10,../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap HDFS Features:,JIRASummaryComponentFeature OwnerCommentsStatus1.HDFS-12943Consistent Reads from Standby NodeHDFSKonstantinMerged2.HDFS-13541NameNode Port based selective encryptionHDFSChenMerged3.HDFS-14403Cost-Based RPC Fair Call QueueHDFSErikMerged,../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap 3.1.3,"Feature freeze dateAug 25, 2019Code freeze dateAug 30, 2019Planned release dateSep 07, 2019Actual release dateRelease Manager:Zhankun TangBranchbranch-3.1 Planned features:",../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap 3.2.1,"Feature freeze dateAug 30, 2019Code freeze dateAug 30, 2019Planned release dateSept 7, 2019Actual release dateSept 22, 2019Release Manager:Rohith Sharma K SBranchbranch-3.2",../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap 3.2.0,"Feature freeze dateSep 7, 2018Code freeze dateSep 14, 2018Planned release dateSept 30, 2018Actual release dateJan 8, 2019Release Manager:Sunil GovindanBranchtrunk",../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap YARN features,JIRASummaryComponentFeature OwnerCommentsStatus1.YARN-3409Node Attributes Support in YARNYARNNaganarasimha/ SunilMerged2.YARN-1011Resource OvercommitmentYARNHaibo / MiklosMoved Out3.YARN-8135Hadoop Submarine project for DeepLearning workloadsYARNWangdaMerged4.YARN-7129Application Catalog for YARN applicationsYARNEric YangMoved Out5.YARN-7512Support service upgrade via YARN Service API and CLIYARNChandniMerged6.YARN-5742Serve aggregated logs of historical apps from ATSv2YARNRohith SharmaMerged,../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap HDFS Features:,JIRASummaryComponentFeature OwnerCommentsStatus1.HDFS-10285HDFS Storage Policy SatisfierHDFSUmaMerged2.HDFS-12615Router-based HDFS federationHDFSInigoImprovement WorksMerged,../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap Common Features:,JIRASummaryComponentCommentsComments1.HADOOP-15226S3Guard Phase IIICommonSteve LoughranMerged2.HADOOP-15220S3a phase VCommonSteve LoughranMerged3.HADOOP-15407Support Windows Azure Storage - Blob file system in HadoopCommonDa Zhou/Steve LoughranMerged,../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap Version 3.1,"Feature freeze dateJan 30, 2018Code freeze dateFeb 08, 2018Planned release dateFeb 28, 2018Release Manager:Wangda TanBranchtrunk",../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap YARN features,"JIRASummaryComponentFeature OwnerCommentsStatus1.YARN-5079,YARN-4793,YARN-4757,YARN-6419YARN native servicesYARNJian HeMerged2.YARN-5734Dynamic scheduler queue configurationYARNJonathan / XuanAlso 2.9Merged3.YARN-5881Add absolute resource configuration to CapacitySchedulerYARNSunil GMerged4.YARN-5673Container-executor rewrite for better security,extensibility and portabilityYARNVarun VMove out5.YARN-6223GPU IsolationYARNWangdaMerged6.YARN-5972Support Pausing/Freezing of opportunistic containersYARNArun SureshMove out7.YARN-3926Resource Profile and multiple resource typesYARNVarun V / SunilMaybe 3.0Merged8.YARN-1011ResourceovercommittmentYARNHaibo / Karthik KUnder discussionMove out9.YARN-6592Support rich placement constraints in YARNYARNArun Suresh / Kostas / WangdaTentativeMerged10.YARN-7117Capacity Scheduler: Support Auto Creation of Leaf Queues While Doing Queue MappingYARNSumaMerged11.YARN-5983FPGA supportMerged",../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap HDFS Features:,JIRASummaryComponentFeature OwnerComments1.HDFS-10285HDFS Storage Policy SatisfierHDFSUmaMove out2.HDFS-9806HDFS tiered storageHDFSChris DouglasMerged3.HDFS-7240Ozone: Object store for HDFSHDFSAnuMove out,../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap Common Features:,JIRASummaryComponentCommentsComments1.HADOOP-13786Add S3Guard committer for zero-rename commits toS3 endpointsHDFSSteve LoughranMerged,../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap Version 2.9,"Feature freeze date6th October 201720th October 2017Code freeze date20th October 201727th October 2017Planned release date3rd November 201717th November 2017Release Manager:Subru Krishnan, Arun SureshTracking PageHadoop 2.9 ReleaseBranchbranch-2",../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap Planned features:,"JIRASummaryComponentFeature OwnerProduction Deployment Sponsor(*)Status1.YARN-2928/YARN-5355ATS v2YARNVrushali / VarunJoep (Twitter)Merged in branch-22.YARN-2877/YARN-5220Opportunistic ContainersYARNKonstantinos / ArunRoni (Microsoft)Merged in branch-23.YARN-5085Container UpdateYARNArun / WangdaRoni / Sarvesh (Microsoft)Merged in branch-24.YARN-2915Yarn FederationYARNSubru / CarloSarvesh (Microsoft)Merged in branch-25.YARN-5139Global Scheduling (Capacity Scheduler)YARNWangdaMerged in branch-26.YARN-3368YARN Web UI v2YARNSunil / WangdaJoep (Twitter)Merged in branch-27.YARN-5734API based (Capacity) Scheduler configurationYARNJonathan / XuanJonathan (LinkedIn)Merged in branch-28.YARN-4726Allocation reuse for container upgradesYARNArun / JianSarvesh (Microsoft)Merged in branch-29.HDFS-10467HDFS Router based federationHDFSInigo / ChrisInigo (Microsoft)Merged in branch-2 (*)Production Deployment Sponsor:""We will favor inclusion of a feature in the 2.9.0 release if it is associated with an immediate production deployment requirement. Other cases: We would recommend moving it to the next major release.""",../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap Version 3.0,"Feature freeze dateSep 15, 2017Code freeze datePlanned release date2017-11-01Release Manager:Andrew WangTracking pageHadoop 3.0.0 releaseBranchbranch-3.0",../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Roadmap Planned features:,"JIRASummaryComponentFeature OwnerStatus1.HADOOP-11656Classpath isolationHADOOPMerged2.HADOOP-13345S3GuardHADOOPMerged3.HADOOP-9902Shell script rewriteHADOOPMerged4.HADOOP-13578Add Codec for ZStandard CompressionHADOOPMerged5.HADOOP-10950,MAPREDUCE-5785Reworked daemon and task heap managementHADOOP, MAPREDUCEMerged6.MAPREDUCE-2841MapReduce task-level native optimizationMAPREDUCEMerged7.HADOOP-11804Shaded client jarsHADOOPMerged8.YARN-2915Yarn FederationYARNMerged9.YARN-5520Support for Opportunistic Containers and Distributed Scheduling.YARNMerged10.YARN-5355TimelineService v2 alpha 2YARNMerged12.YARN-5734API based scheduler configurationYARNMerged12.YARN-5079,YARN-4793,YARN-4757,YARN-6419Yarn native servicesYARNPENDING13.HADOOP-12756Support for Microsoft Azure Data Lake and Aliyun Object Storage System filesystem connectorsHADOOPMerged14.HDFS-7285Support for Erasure Codes in HDFSHDFSMerged15.HDFS-5570Removal of hftp in favor of webhdfsHDFSMerged16.HDFS-6440Support for more than two standby NamenodesHDFSMerged17.HDFS-1312Intra-datanode balancerHDFSMerged18.HDFS-9427Move default ports out of ephemeral rangeHDFSMerged19.HDFS-10467HDFS Router based federationHDFSInigo / ChrisMerged",../data/confluence_exports/HADOOP/Roadmap_73637259.html
Hadoop : Hadoop Release Validation Overview,"By ASF policy the PMC votes on release artifacts hosted atdist.apache.org. E.g. forApache Hadoop 3.1.0, the following artifacts are covered by this policy: hadoop-3.1.0-src.tar.gzhadoop-3.1.0-src.tar.gz.aschadoop-3.1.0-src.tar.gz.mdshadoop-3.1.0.tar.gzhadoop-3.1.0.tar.gz.aschadoop-3.1.0.tar.gz.mds Additionally it is a good idea to verify the Maven artifacts onrepository.apache.orgas these will be consumed by downstream projects.",../data/confluence_exports/HADOOP/Hadoop-Release-Validation_80446904.html
Hadoop : Hadoop Release Validation Step-by-step guide,You don't need to verify all of the below before voting on a release.,../data/confluence_exports/HADOOP/Hadoop-Release-Validation_80446904.html
Hadoop : Hadoop Release Validation Check release artifacts,"Verify that the release bits were correctly generated. These steps don't check for release functionality. Verify signatures using the instructions athttps://www.apache.org/info/verification.html#CheckingSignatures. You will need GPG installed. On MacOS machines, you can install GPG withHomeBrewusingbrew install gpg.Verify checkums for the source and binary artifacts from the corresponding.mdsfiles. E.g. a quick way to do so using gpg is:gpg --print-mds hadoop-3.1.1-src.tar.gz > mds.tmp
diff hadoop-3.1.1-src.tar.gz.mds mds.tmpVerify that there are no MD5 signatures provided.Verify that jars have been correctly staged torepository.apache.org.For 3.x releases and later, verify that the shaded fat jars look correct.Sanity check theCHANGES.mdandRELEASENOTES.mdfiles.Verify that source and binary tarballs include LICENSE.txt and NOTICE.txt files.",../data/confluence_exports/HADOOP/Hadoop-Release-Validation_80446904.html
Hadoop : Hadoop Release Validation Verify Source Release Bits,"Extract the source tarball and build the release from sources using:mvn clean install package -Pdist -DskipTests=trueThe binary tarball will be available athadoop-dist/target/hadoop-<version>.tar.gz.Follow all the steps from theVerify Binary Releasesection below.Verify that the source distribution has no extra files/changes by diffing against the git tag in a local clone of the Hadoop repo. E.g.$ git checkout release-3.1.0-RC1
$ diff -r $PWD /tmp/hadoop-3.1.0  # Assuming RC src tarball was unpacked in /tmp/hadoop-3.1.0",../data/confluence_exports/HADOOP/Hadoop-Release-Validation_80446904.html
Hadoop : Hadoop Release Validation Verify Binary Release Bits,"Install the binary release to a cluster. E.g. this may be a single-node pseudo-cluster, a 3-node cluster of VMs or a real cluster of arbitrary size.Start up HDFS and YARN services and check release functionality. Some example functionality that applies to all releases:Try outfile system shell commands.Try out someadmin commands.Check the various service web UIs.Enable NameNode HA.Launch example MapReduce jobs.Repeat the above steps withKerberos Securityenabled.<Add more yarn-specific checks here>Check release specific features.",../data/confluence_exports/HADOOP/Hadoop-Release-Validation_80446904.html
Hadoop : Hadoop Release Validation Verify Site Documentation,Generate and stage thesite documentationwith the following commands and verify site docs look okay:mvn site:sitemkdir -p /tmp/site && mvn site:stage -DstagingDirectory=/tmp/siteBrowse tofile:///tmp/site/hadoop-project/index.html.,../data/confluence_exports/HADOOP/Hadoop-Release-Validation_80446904.html
Hadoop : Hadoop Release Validation Related articles,Страница:Hadoop Release Validation,../data/confluence_exports/HADOOP/Hadoop-Release-Validation_80446904.html
Hadoop : Hadoop Mentorship Expectations,"Mentors should meet with their mentees once every week or two. We recommend a three month period of doing this. Being available via email is good, but meetings should be over phone or video conferencing if possible. (In person is even better, but often not possible.) We recommend taking shared notes to track progress and outstanding questions. Mentees should come to the first meeting with some goals they have. Examples goals are: Submit code or documentation patches, do some code reviews, answer community questions, etc.. The mentor will try to help the mentee achieve these goals over the three month period by answering questions and providing reviews and constructive feedback. We want to make it easy for more mentors to get involved. Keep in mind that mentors are typically very busy people. The idea is that just an hour or two a week is enough to help newer developers get involved, but can still fit into peoples' busy schedules. Mentees should try to be proactive and drive the process with the help of their mentors.",../data/confluence_exports/HADOOP/Hadoop-Mentorship_100829292.html
Hadoop : Hadoop Mentorship Getting Started,"If you want to participate, please fill out thissurvey Other Resources TheHadoop Contributor Guideis a good place to start.TODO Link to newbie JIRAs",../data/confluence_exports/HADOOP/Hadoop-Mentorship_100829292.html
Hadoop : Hadoop 3.0 Release Important:,HDFS-12990 changes NameNode RPC port back to 8020. It will make Apache Hadoop 3.0.0 deprecated.,../data/confluence_exports/HADOOP/Hadoop-3.0-Release_75969397.html
Hadoop : Hadoop 3.0 Release Incompatible Changes,JIRASummaryPriorityComponentReporterContributorHDFS-12990Change default NameNode RPC port back to 8020CriticalnamenodeXiao ChenXiao Chen,../data/confluence_exports/HADOOP/Hadoop-3.0-Release_75969397.html
Hadoop : Hadoop 3.0 Release Blocker/Critical (3.0.1):,"JIRASummaryPriorityComponentReporterContributorHADOOP-15189backport HADOOP-15039 to branch-2 and branch-3Blocker.Genmao YuGenmao YuHDFS-12638Delete copy-on-truncate block along with the original block, when deleting a file being truncatedBlockerhdfsJiandan YangKonstantin ShvachkoHADOOP-15058create-release site build outputs dummy shaded jars due to skipShadeBlocker.Andrew WangAndrew WangYARN-7664Several javadoc errorsBlocker.Sean MackrorySean MackroryHADOOP-15122Lock down version of doxia-module-markdown pluginBlocker.Elek, MartonElek, MartonYARN-7692Skip validating priority acls while recovering applicationsBlockerresourcemanagerCharan HebriSunil GMAPREDUCE-7028Concurrent task progress updates causing NPE in Application MasterBlockermr-amGergo RepasGergo RepasYARN-7765Atsv2GSSException: No valid credentials provided - Failed to find any Kerberos tgt thrown by Timelinev2Client & HBaseClient in NMBlocker.Sumana SathishRohith Sharma K SYARN-7873Revert YARN-6078Blocker.Billie RinaldiBillie RinaldiYARN-8022ResourceManager UI cluster/app/\<app-id\> page fails to renderBlockerwebappTarun ParimiTarun ParimiHADOOP-15177Update the release year to 2018BlockerbuildAkira AjisakaBharat ViswanadhamYARN-7782Enable user re-mapping for Docker containers in yarn-default.xmlBlockersecurity, yarnEric YangEric YangHDFS-12990Change default NameNode RPC port back to 8020CriticalnamenodeXiao ChenXiao ChenHDFS-9023When NN is not able to identify DN for replication, reason behind it can be loggedCriticalhdfs-client, namenodeSurendra Singh LilhoreXiao ChenHDFS-12832INode.getFullPathName may throw ArrayIndexOutOfBoundsException lead to NameNode exitCriticalnamenodeDENG FEIKonstantin ShvachkoYARN-7558""yarn logs"" command fails to get logs for running containers if UI authentication is enabled.Critical.Namit MaheshwariXuan GongHDFS-12347TestBalancerRPCDelay#testBalancerRPCDelay fails very frequentlyCriticaltestXiao ChenBharat ViswanadhamHDFS-11915Sync rbw dir on the first hsync() to avoid file lost on power failureCritical.Kanaka Kumar AvvaruVinayakumar BHDFS-13039StripedBlockReader#createBlockReader leaks socket on IOExceptionCriticaldatanode, erasure-codingLei (Eddy) XuLei (Eddy) XuYARN-7102NM heartbeat stuck when responseId overflows MAX_INTCritical.Botong HuangBotong HuangYARN-7790Improve Capacity Scheduler Async Scheduling to better handle node failuresCritical.Sumana SathishWangda TanMAPREDUCE-7033Map outputs implicitly rely on permissive umask for shuffleCriticalmrv2Jason LoweJason LoweHDFS-13100Handle IllegalArgumentException when GETSERVERDEFAULTS is not implemented in webhdfs.Criticalhdfs, webhdfsYongjun ZhangYongjun ZhangYARN-7801AmFilterInitializer should addFilter after fill all parametersCritical.Sumana SathishWangda TanHDFS-8693refreshNamenodes does not support adding a new standby to a running DNCriticaldatanode, haJian FangAjith SHDFS-13112Token expiration edits may cause log corruption or deadlockCriticalnamenodeDaryn SharpDaryn SharpYARN-7032ATSv2NPE while starting hbase co-processor when HBase authorization is enabled.Critical.Rohith Sharma K SRohith Sharma K SHDFS-12919RBF: Support erasure coding methods in RouterRpcServerCritical.Íñigo GoiriÍñigo GoiriYARN-5094some YARN container events have timestamp of -1CriticaltimelineserverSangjin LeeHaibo Chen",../data/confluence_exports/HADOOP/Hadoop-3.0-Release_75969397.html
Hadoop : Hadoop 3.0.0 release Release schedule,"For Hadoop 3, we are planning to ""release early, release often"" to quickly iterate on feedback collected from downstream projects. To this end, we will be releasing a series of alpha and beta releases leading up to an eventual Hadoop 3.0.0 GA. This is aplannedrelease schedule. Future release dates are subject to change. ReleaseDateReleased?3.0.0-alpha12016-09-033.0.0-alpha22017-01-253.0.0-alpha32017-05-263.0.0-alpha42017-07-073.0.0-beta12017-10-033.0.0 GA2017-12-13  JIRA Dashboard Hadoop 3 release status updates JACC compatibility report on Jenkins JIRA fix version vs. git log validation on Jenkins  The goal for beta1 is for these downstreams to have their unit tests and integration tests run and triaged. This is so we can discuss and address any burdensome incompatible changes before compatibility is locked. ProjectVersioncompilesunit test statusbasic functional testingnon-HA insecure integration testsHA secure integration testsHBase2.0.0Spark2.0Hive2.1.0Oozie5.0Pig0.16Solr6.xKafka0.10",../data/confluence_exports/HADOOP/Hadoop-3.0.0-release_66852646.html
Hadoop : How to generate and push ASF web site Create a new releases,"To create a new item on the hadoop site just create a new markdown file with the standard header and put it to the src/releases folder. For example: --- title: Release 2.8.1 availabledate: 2017-06-08linked: true---<!---Licensed under the Apache License, Version 2.0 (the ""License"");you may not use this file except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an ""AS IS"" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License. See accompanying LICENSE file.-->This is a security release in the 2.8.0 release line. It consists of2.8.0 plus security fixes. Users on 2.8.0 are encouraged to upgrade to2.8.1.Please note that 2.8.x release line continues to be not yet ready forproduction use. Critical issues are being ironed out via testing anddownstream adoption. Production users should wait for a subsequentrelease in the 2.8.x line. Please note, thelinked: trueattribute in the header. Only the linked releases will be available from the menu. Most probably you would like to remove this attribute from the previous release file.",../data/confluence_exports/HADOOP/How-to-generate-and-push-ASF-web-site_73633757.html
Hadoop : How to generate and push ASF web site Sparse checkout,The repository contains a lot of javadoc files. If you would like to modify only the site content you can do a sparse checkout with the following command: git clonehttps://gitbox.apache.org/repos/asf/hadoop-site.git--no-checkout cd hadoop-site git config core.sparsecheckout true echo '/*' > .git/info/sparse-checkout echo '!content/docs' >>  .git/info/sparse-checkout git checkout asf-site,../data/confluence_exports/HADOOP/How-to-generate-and-push-ASF-web-site_73633757.html
Hadoop : How to generate and push ASF web site Git log without the content,"The repository containers both the source and the rendered version of the site. If you are interested about the pure change in the source side, you can use git log with an exclude pattern: git log -p -- . "":(exclude)content""",../data/confluence_exports/HADOOP/How-to-generate-and-push-ASF-web-site_73633757.html
Hadoop : Hadoop Contributor Guide Need Further Help?,Emailcommon-dev@hadoop.apache.orgor any of the sub-project-specific mailing lists: hdfs-dev@hadoop.apache.orgyarn-dev@hadoop.apache.orgmapreduce-dev@hadoop.apache.org,../data/confluence_exports/HADOOP/Hadoop-Contributor-Guide_89071892.html
Hadoop : Hadoop Contributor Guide Other Links,Hadoop Common issues are tracked in theHADOOPJira instance.HDFS issues are tracked in theHDFSJira instance.YARN issues are tracked in theYARNJira instance.MapReduce issues are tracked in theMAPREDUCEJira instance.HDDS/Ozone issues are tracked in theHDDSJira instance.,../data/confluence_exports/HADOOP/Hadoop-Contributor-Guide_89071892.html
Hadoop : Develop on Apple Silicon (M1) macOS Protocol Buffers (protobuf),"Hadoop 3 has moved to newer version of protobuf package (version 3.7.0 in trunk as of Mar 2021), though you can use old versions especially if you need to build both on Hadoop 2 and Hadoop 3 with one protobuf version. However, Apple M1 does not work with protobuf 2.5.0 - when installing protobuf from source you will get unsupported CPU architecture error. The simply solution is to use the same new protobuf version.",../data/confluence_exports/HADOOP/177047419.html
Hadoop : Develop on Apple Silicon (M1) macOS protobuf-maven-plugin,"You may get following error: [ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.5.1:compile-custom (default) on project hadoop-yarn-csi: Missing:
[ERROR] ----------
[ERROR] 1) io.grpc:protoc-gen-grpc-java:exe:linux-aarch_64:1.15.1 This is because the grpc package is not releasing binaries for ARM64. One way of solving this is to specify the propertiesos.detected.classifierto be osx-x86_64 instead of osx-aarch_64 For example, you can put following code in your~/.m2/settings.xmlfile: <settings><activeProfiles><activeProfile>apple-silicon</activeProfile></activeProfiles><profiles><profile><id>apple-silicon</id><properties><os.detected.classifier>osx-x86_64</os.detected.classifier></properties></profile></profiles></settings>",../data/confluence_exports/HADOOP/177047419.html
Hadoop : Develop on Apple Silicon (M1) macOS frontend-maven-plugin,"This plugin is used to install node.js and yarn locally.YARN-6278addedyarn-uiprofile to pom.xml leveraging this plugin to automatically install node.js and yarn locally undertarget/webappdirectory. Latest versions of this plugin supports Apple M1 chip but Hadoop is still using an older version that does not. As a result,when detecting an ARM CPU on your Apple M1, this plugin will generate a download link for a Darwin ARM64 build of Node, which doesn’t exist. So the workaround is to manually upgrade this version to 1.10+. For this you can update the version inhadoop-project/pom.xmlfile. Later Hadoop release will have this plugin upgrade. UPDATE: Fixed by YARN-10706.",../data/confluence_exports/HADOOP/177047419.html
Hadoop : Hadoop Java Versions Supported Java Versions,Apache Hadoop 3.3 and upper supports Java 8 and Java 11 (runtime only)Please compile Hadoop with Java 8. Compiling Hadoop with Java 11 is not supported:HADOOP-16795-Java 11 compile supportOpenApache Hadoop from 3.0.x to 3.2.x now supports only Java 8Apache Hadoop from 2.7.x to 2.10.x support both Java 7 and 8,../data/confluence_exports/HADOOP/Hadoop-Java-Versions_100827883.html
Hadoop : Hadoop Java Versions Supported JDKs/JVMs,"Now Apache Hadoop community is using OpenJDK for the build/test/release environment, and that's why OpenJDK should be supported in the community.https://github.com/apache/hadoop/blob/rel/release-3.2.1/dev-support/docker/Dockerfile#L92Other jdks/jvms should work well. If you find they don't work as expected, please file a JIRA.",../data/confluence_exports/HADOOP/Hadoop-Java-Versions_100827883.html
Hadoop : Hadoop Java Versions Java Incompatible Changes,This document is for users who upgrade Java version of their Hadoop cluster. It documents the incompatible changes of Java that affect Apache Hadoop.,../data/confluence_exports/HADOOP/Hadoop-Java-Versions_100827883.html
Hadoop : Hadoop Java Versions Java 8,"VersionsIncompatible changesRelated JDK bug system ticketsRelated JIRAs1.8.0_242The visibility of sun.nio.ch.SocketAdaptor is changed from public to package-private.TestIPC#testRTEDuringConnectionSetup is affected.JDK-8237177HADOOP-15787-[JDK11] TestIPC.testRTEDuringConnectionSetup failsResolved1.8.0_242Kerberos Java client will fail by ""Message stream modified (41)"" when the client requests a renewable ticket and the KDC returns a non-renewable ticket. If your principal is not allowed to obtain a renewable ticket, you must remove ""renew_lifetime"" setting from your krb5.conf.JDK-81310511.8.0_191All DES cipher suites were disabled. If you are explicitly using DES cipher suites, you need to change cipher suite to a strong one.HADOOP-16016-TestSSLFactory#testServerWeakCiphers sporadically fails in precommit buildsResolved1.8.0_171In Apache Hadoop 2.7.0 to 2.7.6, 2.8.0 to 2.8.4, 2.9.0 to 2.9.1, 3.0.0 to 3.0.2, and 3.1.0, KMS fails by java.security.UnrecoverableKeyException due toEnhanced KeyStore Mechanisms. You need to set the system property ""jceks.key.serialFilter"" to the following value to avoid this error:java.lang.Enum;java.security.KeyRep;java.security.KeyRep$Type;javax.crypto.spec.SecretKeySpec;org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata;!*""HADOOP-15473-Configure serialFilter in KeyProvider to avoid UnrecoverableKeyException caused by JDK-8189997Resolved",../data/confluence_exports/HADOOP/Hadoop-Java-Versions_100827883.html
Hadoop : Hadoop Java Versions Daily Builds,Java 11:https://ci-hadoop.apache.org/job/hadoop-qbt-trunk-java11-linux-x86_64/,../data/confluence_exports/HADOOP/Hadoop-Java-Versions_100827883.html
Hadoop : S3A: FileNotFound Exception on Read Example Stack Trace,"java.io.FileNotFoundException: Reopen at position 0 on s3a://bucket-name/test/some-file: com.amazonaws.services.s3.model.AmazonS3Exception: The specified key does not exist. (Service: Amazon S3; Status Code: 404; Error Code: NoSuchKey; Request ID: 58552EC03A3499D7), S3 Extended Request ID: thYjg0cDPGceq5M3n5T2nLmRDfFnoAeyiVMx8rOvYv/IHDPZiBnL5oAOPjdw44rQgzngDk4wELY= at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1588) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1258) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1030) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:742) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:716) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:699) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:667) at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:649) at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:513) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4221) at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4168) at com.amazonaws.services.s3.AmazonS3Client.getObject(AmazonS3Client.java:1378) at org.apache.hadoop.fs.s3a.S3AInputStream.reopen(S3AInputStream.java:160) at org.apache.hadoop.fs.s3a.S3AInputStream.onReadFailure(S3AInputStream.java:350) at org.apache.hadoop.fs.s3a.S3AInputStream.read(S3AInputStream.java:323) atjava.io.FilterInputStream.read(FilterInputStream.java:83)",../data/confluence_exports/HADOOP/73630383.html
Hadoop : S3A: FileNotFound Exception on Read Causes of This Error,"The file was deleted after the client successfully opened it.  Upon trying to read the data, it is discovered the file is no longer present.If you are running with S3Guard enabled, there are two possible causes:Eventual consistency:  Although the open() was successful because the file's metadata was found in S3Guard's MetadataStore, the file is still not available in S3 by the time the client tried to read its data.  This is expected to be rare, as GET is generally consistent on S3.  There may be a case where a previous GET of the same path before it existed (negative GET) is cached by S3 infrastructure and it improperly treats the file as still missing.  Unfortunately, S3 documentation on this behavior is hard to find.S3Guard MetadataStore is out of sync with S3.   This can happen if there are clients modifying the same bucket that do not have S3Guard enabled.  This can cause the S3Guard MetadataStore to get out of sync with the S3 metadata.  This condition can also occur if the client (S3A) crashes between updating S3 and updating the S3Guard MetadataStore.  This case can be resolved by clearing the MetadataStore (e.g. drop the DynamoDB table) and then re-running your job.",../data/confluence_exports/HADOOP/73630383.html
Apache Tomcat : FAQ Table of Contents,"FAQ TOPICS Bugs- What to do when you believe you have a bug or there is a difference in behavior with another servlet container.Character Encoding- How to handle non-standard characters.Class Not Found- What to do with Class Not Found errors.Clustering- Cluster and clustering related questionsConnectors- You want to connect tomcat to Apache, IIS, or have questions about tomcat-standalone.Database- Getting tomcat to talk to a database.Deployment- Questions related to web application deployment.Developing- Various IDE integration questions.FDA Validation- Questions related to running Tomcat in an FDA validated environment.How To- Miscellaneous common how to's.Known Issues- Known issuesLinux / Unix- Common questions for Linux / Unix related platforms.Logging- Common questions related to logging in Tomcat.Memory- Common memory related questions.Meta- About Tomcat and this FAQ.Miscellaneous- Miscellaneous questions that weren't categorized.Monitoring- Tips on Monitoring a running Tomcat instance.Other Operating Systems- Information about scripts and support for other operating systems.Other Resources- A lot of links to tomcat related documentation and experiences.Performance & Monitoring- Performance & Monitoring questions.Security- Common security issues.Specifications- A reference page that provides links to specifications, implemented by Apache Tomcat.Tomcat User- More information about the tomcat-user mailing list.Troubleshooting & Diagnostics- Tools and techniques for diagnosing problems.Version- About the different tomcat versions.WebDav- Questions on using Tomcat Webdav servlet.Windows- Common Windows questions.",../data/confluence_exports/TOMCAT/FAQ_103098750.html
Apache Tomcat : Design and Development Issues Development Processes,Building the Tomcat Native Connector binaries for WindowsBuilding the isapi_redirector.dll (mod_jk) for WindowsManaging translationsRelease process for Apache Tomcat,../data/confluence_exports/TOMCAT/Design-and-Development-Issues_61320762.html
Apache Tomcat : Design and Development Issues In progress,Removing unpackWARshttp workshop 2019Jakarta EE Release Numbering,../data/confluence_exports/TOMCAT/Design-and-Development-Issues_61320762.html
Apache Tomcat : Design and Development Issues Notes,Encoding and URIsJakarta EE TCKs  There is also anArchiveof non-current issues.,../data/confluence_exports/TOMCAT/Design-and-Development-Issues_61320762.html
Apache Tomcat : AJP Client Library and Command Line Tool Introduction,AJP Client Tool will help to investigate issues on AJP connector in Tomcat and could even use to do some simple stress testing on AJP connector.,../data/confluence_exports/TOMCAT/AJP-Client-Library-and-Command-Line-Tool_80447061.html
Apache Tomcat : AJP Client Library and Command Line Tool Deliverable Phase,AJP client library: Which is developed methods and classes for AJP ProtocolJMeter AJP connector: Handle connections between JMeter and Tomcat with AJP ProtocolCLI wrapper: AJP command converterCLI: a command interfaceDemo client: For the demonstrations,../data/confluence_exports/TOMCAT/AJP-Client-Library-and-Command-Line-Tool_80447061.html
Apache Tomcat : AJP Client Library and Command Line Tool Command lines Options,"-T                      : Connection timeout -O or --output          : Output location, default on the console -H or --headers         : Header name and header file -r or --requests-file   : Set the request.xmlconfiguration file -rounds                 : The number of times url[s] given in the command are repeatedly fetched -http-version           : HTTP 1.0 or 1.1 -query                  : parameter values -verbose                : -user-agent or -u       : -m or --method          : set the HTTP method -h or --help            :",../data/confluence_exports/TOMCAT/AJP-Client-Library-and-Command-Line-Tool_80447061.html
Apache Tomcat : SSLWithFORMFallback Setup,"So, to get the fallback login working you will need the following:  Tomcat InstallationYour WebapplicationThe Java ClassSSLWithFormFallbackAuthenticator(download from here)Server Certificate & Private KeyClient Certificate & Private KeyCertification Authority Public CertificatesWorking authentication realm  It is assumed that your web-application is working, and you are currently using FORM based authentication. Your login config in your web.xml deployment descriptor should therefore look something like this:  <login-config>
	  <auth-method>FORM</auth-method>
	  <form-login-config>
	  	<form-login-page>/pub/login.jsp</form-login-page>
	  	<form-error-page>/pub/loginerror.jsp</form-error-page>
	  </form-login-config>
</login-config>  It is further assumed that your web-application contains at least one protected page, requiring authentication, and that the login currently works using your FORM based login with an appropriate Login Realm.",../data/confluence_exports/TOMCAT/SSLWithFORMFallback_103100112.html
Apache Tomcat : SSLWithFORMFallback Basic SSL Setup,"First, setup SSL in Tomcat, as described in:http://tomcat.apache.org/tomcat-5.5-doc/ssl-howto.html  Make sure basic SSL is working, without client authentication. You can test this using your browser. While you are at it, add the server's certification authority to your Browser's list of trusted certificates, if it is not there already. You should be able to access your application normally, but via the https (SSL) protocol. If you access a protected page, you should be prompted for a login using the FORM login you configured.",../data/confluence_exports/TOMCAT/SSLWithFORMFallback_103100112.html
Apache Tomcat : SSLWithFORMFallback SSL Client Authentication,"In the tomcat server.xml file, configure the server to use client authentication:  <Connector port=""8443"" minProcessors=""5"" maxProcessors=""75""
    enableLookups=""true"" disableUploadTimeout=""true""
    acceptCount=""100"" debug=""0"" scheme=""https"" secure=""true"";
    clientAuth=""want"" sslProtocol=""TLS""
    keystoreFile=""/etc/mykeystore.jks"" keystorePass=""changeit""
    truststoreFile=""/etc/mytruststore.jks"" truststorePass=""changeit""/>  Note the use of clientAuth=""want"" to request a certificate, but not fail if none is presented.  Configure your Realm to accept the client certificate. Depending on which realm you are using, you will need to add a user for the certificate in different ways. The default Tomcat Realms use the ""SubjectDN"" field of the certificate as a ""username"" to look up the user.",../data/confluence_exports/TOMCAT/SSLWithFORMFallback_103100112.html
Apache Tomcat : SSLWithFORMFallback Testing Client Authentication,"Fire up your browser and install the client certificate and private key in your certificate store.  Now change your auth-method from ""FORM"" to ""CLIENT-CERT"" and restart/redeploy your web-app. If you access your protected page you should now be prompted for a certificate by your browser. Select the installed certificate. If everything was configured correctly you should be authenticated based on your certificate, and taken to the protected page.  If things go wrong, here's some places to look:  Is the client certificate properly installed? If not, your browser will not offer the certificate for you to choose on login.Is the client certificate authority properly imported in the truststore on the server? If not, the browser will not know to use your installed client certificate.Is the server certificate valid and properly installed?Is your client certificate's SubjectDN configured as a user in the Realm you are using? Depending on which realm you are using you will have to add the user in different ways (for example, to the file ""tomcat-users.xml"" if using Tomcat'sMemoryUserDatabase).",../data/confluence_exports/TOMCAT/SSLWithFORMFallback_103100112.html
Apache Tomcat : SSLWithFORMFallback Adding Fallback to Form Authentication,"Make sure the class you downloaded, ""SSLWithFormFallbackAuthenticator.java"" is compiled and installed, for example in the server/classes folder of your Tomcat installation. Alternatively, you can pack the compiled class in a JAR file, and place the JAR in the server/lib folder of your Tomcat installation.  The class implements a Tomcat ""Valve"", and needs to live within the server directory of your Tomcat and cannot be part of your web application's WAR, since it is used by Tomcat for authentication, which happens before your web application is called, and hence outside your web application's classloader.  Configure your Web-Application to use this class for authentication. This is done in two steps:  Remove the <auth-method> element from your web application's deployment descriptor. The <login-config> element should still contain the <form-login-config> elements to configure the form, butNO<auth-method> tag.  <login-config>
	  <!-- auth-method is commented out for fallback authentication -->
	  <!-- <auth-method>FORM</auth-method> -->
	  <form-login-config>
	  	<form-login-page>/pub/login.jsp</form-login-page>
	  	<form-error-page>/pub/loginerror.jsp</form-error-page>
	  </form-login-config>
</login-config>  2. Configure the authentication valve. The authentication valve can be configured in two places. Either in your Tomcat server.xml file, or your application's context.xml file. The context.xml file lives in the directory ""META-INF"" within your WAR file. The following is a sample context.xml file for fallback authentication:  <?xml version=""1.0"" encoding=""UTF-8""?>
<Context path=""/mycontextpath"" >
	<Valve className=""at.telekom.tomcat.security.SSLWithFormFallbackAuthenticator"" />
</Context>  If you have configured the <Context> element in your Tomcat server.xml file, you can also place the <Valve> element there.  You will need to restart tomcat to apply these changes.",../data/confluence_exports/TOMCAT/SSLWithFORMFallback_103100112.html
Apache Tomcat : SSLWithFORMFallback Testing Fallback Authentication,"This is best tested with two different browsers (eg Firefox and IE):  Install the client certificate in one of the Browsers (if it isn't already)Fire up this browser and visit your protected pageYou should be promted for the certificate as before, select it as beforeYou should be logged into the site, as beforeNow fire up the other browser, the one without the certificateAttempt to access your protected pageDepending on your Browser you may be promted about the certificate - click ""cancel"" if this is the caseYou should be taken to the login form of your applicationLog in using the form. You should be granted access, as before",../data/confluence_exports/TOMCAT/SSLWithFORMFallback_103100112.html
Apache Tomcat : SSLWithFORMFallback How does it work?,"The code is tested with Tomcat 5.5.17, 5.5.20 and 5.5.25. It will probably work with only minor modifications for other Tomcat 5.5 versions. It has been tested using Java 1.5.  In short, this code works because:  Tomcat uses the auth-config element of the deployment descriptor to create an Authentication ValveIf this element is missing, Tomcat does not complain, and simply installs no AuthenticatorBy manually adding our own Authentication Valve we add authentication back to the applicationOur authentication valve inherits from theFormAuthenticationValve, and adds functionality from the SSL authenticatorIt simply first tries SSL Auth, and failing that makes a second attempt using form auth",../data/confluence_exports/TOMCAT/SSLWithFORMFallback_103100112.html
"Apache Tomcat : SSLWithFORMFallback Comments, Feedback, Support","This code is supplied back to the apache foundation, without any support or warranty. Use at your own risk. The author and his employer assume no responsibility for damages resulting in the use of this code or these instructions.  Feel free to use the code in any way you want but do not expect support.  Should you have questions about the code, please feel free to contact me (the Author) at:  runger -AT- aon.at   CategoryFAQ",../data/confluence_exports/TOMCAT/SSLWithFORMFallback_103100112.html
Apache Tomcat : UsefulLinks Configuring for Windows,Tomcat5 Multi-Instances on Win32(in German)Got time for a translation? Contactthe web page authorTomcat Service Manager(software) - by David BoyerThis software beendiscontinuedin favor of theJava Service ManagerTomcat Service Manager (tcsm)(software) - by Dave Oxley,../data/confluence_exports/TOMCAT/UsefulLinks_103100724.html
Apache Tomcat : UsefulLinks Configuring for UNIX and Linux,"Installing Web Services with Linux / Apache / Tomcat / Mod_jk / Struts / Postgre{{`SQL / JDBC / Open}}`SSLHow to run Tomcat on Port 80Integrating Tomcat and Apache on Red Hat 9.0Java and Jakarta Tomcat on Free``BSDTomcat / Clustered JDBCConfiguring and Using Apache Tomcat   For general information, see theRUNNING.txtfile in the Tomcat distribution.TomcatOnMacOS: Running Tomcat on MacOS XInstalling Tomcat on MacOS XRunning Tomcat in Eclipse for development with Tapestry   How to deal withOutOfMemoryErrors   CategoryFAQ",../data/confluence_exports/TOMCAT/UsefulLinks_103100724.html
Apache Tomcat : WebSocket 2.0 TCK Tomcat,Set the following system properties org.apache.tomcat.websocket.DISABLE_BUILTIN_EXTENSIONS=trueorg.apache.tomcat.websocket.ALLOW_UNSUPPORTED_EXTENSIONS=trueorg.apache.tomcat.websocket.DEFAULT_PROCESS_PERIOD=0,../data/confluence_exports/TOMCAT/WebSocket-2.0-TCK_158863846.html
Apache Tomcat : WebSocket 2.0 TCK Test Suite,"Download the Jakarta WebSocket 2.0 TCK https://download.eclipse.org/ee4j/jakartaee-tck/jakartaee9/promoted/websocket-tck-2.0.0.zip https://download.eclipse.org/ee4j/jakartaee-tck/jakartaee9-eftl/staged-910/jakarta-websocket-tck-2.0.1.zip Extract to WEBSOCKET_TCK_HOME  Edit $WEBSOCKET_TCK_HOME/bin/ts.jte You'll need to set the following properties (adjust the paths and values for your environment) webServerHost=localhost webServerPort=8080 securedWebServicePort=8443 websocket.api=/home/mark/repos/asf-public/tomcat/trunk/output/build/lib/websocket-api.jar websocket.classes=/home/mark/repos/asf-public/tomcat/trunk/output/build/lib/tomcat-websocket.jar:/home/mark/repos/asf-public/tomcat/trunk/output/build/lib/servlet-api.jar:/home/mark/repos/asf-public/tomcat/trunk/output/build/lib/tomcat-util.jar:/home/mark/repos/asf-public/tomcat/trunk/output/build/lib/tomcat-api.jar:/home/mark/repos/asf-public/tomcat/trunk/output/build/bin/tomcat-juli.jarAdd the following to the command.testExecute property (to prevent entropy issues slowing the tests down)-Djava.security.egd=file:/dev/./urandom Do not reduce ws_wait below the default of 5s as it is likely to trigger test failures.  set JAVA_HOME Run ant gui Accept the defaults and then run the tests  A default 10.0.x build (as of 10.0.5) running with the 2.0.1 TCK build triggers 0 test failures with the following JRE(s): Adopt OpenJDK 11.0.9 b11 (TCK and Tomcat) 5 Tests 'fixed' by appropriate system property configuration (see above). 4 x extensions (TCK assumes invalid extensions are ignored)Consider making TCK smarter so it ignores other extensions the server may advertise. Better still, detect them first and then exclude them from this test.Need to discuss expected behaviour if an unknown extension is specified. Are these required or optional?1 x timeout related test expects more frequent expiration checks",../data/confluence_exports/TOMCAT/WebSocket-2.0-TCK_158863846.html
Apache Tomcat : Community Review of DISA STIG Introduction,"This is a community review of the (USA) Defense Information Systems Agency (DISA) Security Technical Implementation Guide (STIG) dated 2021-12-27 which can be browsed conveniently at this address: https://www.stigviewer.com/stig/apache_tomcat_application_sever_9/2021-12-27/ Similar documents have been drafted in the past and various members of this community including project maintainers, users, and administrators have expressed frustration regarding many of the items both included and not included in these guides. This document is intended to be a community-sourced review of the latest (at the time of this writing) version of the STIG, including specific recommendations from this community's members for improvements and modifications to the STIG. Recommendations and suggestions are welcome from any member of the community.We ask that if your recommended change is discussed and the group consensus is that no change should be recommended, please remove that suggestion from this document and do not bring it back up for discussion without some substantive change to the suggestion or the background/context of that suggestion. We ask this to avoid arguments raging in the history of this document. Feel free to have conversations on the various Tomcat-related mailing lists and post the results of those conversations into this document instead of using the document as a vehicle for communication. This is a living document and changes can be made at any point, including changes that do not reflect any particular consensus of the Apache Tomcat Project Management Committee (PMC), the Apache Tomcat Security Team, etc. Any official recommendation made to DISA will be made for aspecific versionof this document which has supported consensus from one of more of the aforementioned groups.",../data/confluence_exports/TOMCAT/Community-Review-of-DISA-STIG_199536782.html
Apache Tomcat : Community Review of DISA STIG Instructions,"For anyone contributing to this review, please read the instructions below and participate in a constructive way. In order to coordinate our recommendations, it will be helpful if everyone follows a few simple rules: To recommend an improvement or modification to the STIG, please file your commentary under a sub-section of this document whose name is the same as theFinding ID, which is how STIGs are generally organized. Please keep these Finding IDs in alphanumeric order for easy identification.In a single short sentence, describe the overall change that is being recommended. For example ""This finding is invalid,"" or ""The instructions for identifying this finding are incomplete,"" etc.Provide appropriate background and context if appropriate. Remember that extraordinary claims require extraordinary evidence.Provide a specific recommendation or recommendations at the end of the section for the finding.",../data/confluence_exports/TOMCAT/Community-Review-of-DISA-STIG_199536782.html
Apache Tomcat : Community Review of DISA STIG General Recommendations,No general recommendations at this point.,../data/confluence_exports/TOMCAT/Community-Review-of-DISA-STIG_199536782.html
Apache Tomcat : Community Review of DISA STIG V-222931,"This finding contains multiple inaccuracies. This finding claims that Apache Tomcat ""currently operates only on JKS, PKCS11, or PKCS12 format keystores."" Further, it claims that the finding only applies to JKS keystores. Finally, it claims that Tomcat will use a default password of ""changeit"" if the keystore was not created with a password. Many of the above are factually incorrect. Apache Tomcat can use JKS and PKCS12 format keystores, but PKCS11 is another thing entirely which can be accessed in various ways. Apache Tomcat also supports the use of JCEKS keystores and PEM-encoded DER files which can include password-protected content similar to the other keystores. In all of these cases, Apache Tomcat will use any password provided by the administrator in the appropriate position in the configuration file. Apache Tomcat will indeed use ""changeit"" as the default passwordif none is specified in the configurationto load the keys from a keystore. Apache Tomcat does not create keystores, and any administrator would have to explicitly specify ""changeit"" as the password used to protect a given keystore. The recommendations should be made to protect the keystore with a password (and explicitlynotto use ""changeit"" which is the commonly-used password in all Java-keystore-related HOWTOs and other guides) and has nothing to do really with Apache Tomcat's behavior at all. Furthermore,allkeys should be recommended to be protected, not just those in JKS and PKCS12 key stores. JCEKS and PEM-encoded-DER files should be given the same prescription.",../data/confluence_exports/TOMCAT/Community-Review-of-DISA-STIG_199536782.html
Apache Tomcat : Community Review of DISA STIG V-222938,"The finding ignores valid settings that meet the requirement and the fix text is inaccurate. Access logging may be configured per Engine, Host or Context. It is sufficient to configure one access log per Engine. It is also possible to meet the requirement to log all requests with various combinations of access logs at different levels. The fix text states every element needs an access log valve. This is incorrect. The fix text - for the current meaning - should be every Host element needs an access log valve. However, note from the previous paragraph that other approaches are also possible. A better way of phrasing this could be: For every possible combination of Engine, Host and Context that a request may be mapped to, at least one Container (Engine, Host and Context are all containers) must have a nested access log valve. The recommended approach is one per Host.",../data/confluence_exports/TOMCAT/Community-Review-of-DISA-STIG_199536782.html
"Apache Tomcat : Community Review of DISA STIG V-222939,V222942","The findings ignore valid settings that meet the requirement. The findings require that specific values are included in the pattern for the AccessLogValve but does not take into account that the values ""common"" and ""combined"" also include the required values. The findings need to be edited to accept ""common"" or ""combined"". The pattern example included in the finding is the ""common"" pattern.",../data/confluence_exports/TOMCAT/Community-Review-of-DISA-STIG_199536782.html
Apache Tomcat : Community Review of DISA STIG V-222950,"This finding confuses HTTP TRACE and software application stack traces and recommends a course of action that does not provide any security benefit. This finding claims that ""stack tracing provides debugging information ... that could be used to compromise the system."" The recommended mitigation is to set the allowTrace attribute of all <Connector> elements to ""false"". The claim is that by setting this attribute, application stack traces will be suppressed. The claim is incorrect. While revealing stack traces may be helpful to a potential attacker, setting allowTrace=""false"" does not disable application stack traces. Instead, it disables the HTTP TRACE verb which provides little to no security benefit. Application stack traces, if they had been possible to generate by an attacker before the configuration change, are still entirely possible for an attacker to cause. We recommend that V-222950 be completely removed from the STIG as it has no perceivable benefit.",../data/confluence_exports/TOMCAT/Community-Review-of-DISA-STIG_199536782.html
Apache Tomcat : Community Review of DISA STIG V-222964,This finding misses multiple TLS settings for remote JMX. The finding should also cover: com.sun.management.jmxremote.registry.sslcom.sun.management.jmxremote.ssl.enabled.protocolscom.sun.management.jmxremote.ssl.enabled.cipher.suites,../data/confluence_exports/TOMCAT/Community-Review-of-DISA-STIG_199536782.html
Apache Tomcat : Community Review of DISA STIG V-222968,"This finding requires that FIPS-validated cipher (suites) are used on secure connectors, but gets confused after that. It's not clear whether this Finding requires that a FIPS-validated cryptographic module be used in FIPS mode, or that a FIPS-validated cryptographic module must be used (in any mode), or that only cipher suites be FIPS-140-2 validated (whatever that means). It also states that setting FIPSMode=""on"" on the AprLifecycleListener (a) will enable FIPS and (b) FIPS mode is not possible without it. Enabling the AprLifecycleListener and using FIPSMode=""on"" is only applicable if OpenSSL is being used as the underlying cryptographic module, and that module has been built with FIPS support. There are other ways to use FIPS-valiated cryptographic modules that do not use this setting. This recommendation should clarify what is actually required, and be specific about how to ensure that a FIPS-validated module is actually in use.",../data/confluence_exports/TOMCAT/Community-Review-of-DISA-STIG_199536782.html
Apache Tomcat : Community Review of DISA STIG V-222981,"The title of this finding describes a requirement that cannot be met. The LockOutRealm does not distinguish between admin and non-admin users. The lock out rules apply equally to all users. The title should be edited to remove the text ""...for admin users"".",../data/confluence_exports/TOMCAT/Community-Review-of-DISA-STIG_199536782.html
Apache Tomcat : Community Review of DISA STIG Remote JMX,"The findings cover a number of settings regarding the use of remote JMX but does not cover all risks. The default remote JMX authentication mechanism does not implement any access logging so it is not possible to audit use, or attempted use, of the interface. The default remote JMX authentication mechanism does not implement any form of account lock-out and is therefore vulnerable to brute-force attacks. Remote JMX has a very crude access control model. Access is either ""read everything"" or ""read/write everything"". Even read access provides access to potentially sensitive data. Any access via remote JMX should be considered equivalent to ""super user"" access. There is no way to mitigate this risk short of disabling remote JMX entirely.  There is a strong case to be made for accessing JMX via some form of (local to Tomcat) proxy that addresses this risk.  It is likely such a proxy woudl address other risks as well.",../data/confluence_exports/TOMCAT/Community-Review-of-DISA-STIG_199536782.html
Apache Tomcat : Security Preface,"This FAQ section provides help with some security-related issues. If you hear of a vulnerability or its exploitation, please see thesecurity page.",../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security The Record,"There have been no public cases of damage done to a company, organization, or individual due to a Tomcat security issue. There have been no documented cases of data loss or application crashes caused by an intruder. While there have been numerous analyses conducted on Tomcat, partially because this is easy to do with Tomcat's source code openly available, there have been onlytheoreticalvulnerabilities found. All of those were addressed even though there were no documented cases of actual exploitation of these vulnerabilities. That said, There have been several reports of a compromise done via guess of the password of a user of the Manager web application. There was once a bug that blindly clicking-trough the Windows installer configured a manager user with blank password (CVE-2009-3548). This was fixed by April 2010 (Tomcat 5.5.29, 6.0.24 and later are safe). Please see ""Security considerations"" pages in Tomcat documentation (linked below) for a reference on how access to Management Applications in Tomcat should be secured. There have been several reports of compromises via vulnerabilities in 3-rd party web applications deployed on Tomcat. E.g. vulnerabilities in Apache Struts framework were a popular attack target several times in years 2013-2017. E.g.Equifax breachin year 2017. It is unknown whether Equifax has run their application on Tomcat, but there have been a number of similar compromise reports from Tomcat users. Those are not caused by a vulnerability in Tomcat.",../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security Role of Customization,"We believe, and the evidence suggests, that Tomcat is more than secure enough for most use-cases. However, like all other components of Tomcat, you can customize any and all of the relevant parts of the server to achieve even higher security. For example, the session manager implementation is pluggable, and even the default implementation has support for pluggable random number generators. If you have a special need that you feel is not met by Tomcat out of the box, consider these customization options. At the same time, please bring up your requirements on the user mailing list, where we'll be glad to discuss it and assist in your approach/design/implementation as needed. It is also possible to configure Tomcat insecurely. Please see ""Security considerations"" pages in Tomcat documentation (linked below) for the list of security-sensitive options.",../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security Links,"Known vulnerabilities:https//tomcat.apache.org/security.htmlSecurity considerations (Tomcat documentation) —Tomcat 9,Tomcat 8.5,Tomcat 7.",../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security Questions,How do I use OpenSSL to set up my own Certificate Authority (CA)?Oh no! Port 8005 is available for anyone on localhost to shutdown my tomcat!What about Tomcat running as root?How do I force all my pages to run under HTTPS?What is the default login for the manager and admin app?How do I restrict access by ip address or remote host?How do I use jsvc/procrun to run Tomcat on port 80 securely?Has Tomcat's security been independently analyzed or audited?How do I change the Server header in the response?Why are passwords in plain text?How can I restrict the list of ciphers used for HTTPS?Which cipher suites should I use?Is Tomcat affect by Log4Shell CVE-2021-44228?I found a vulnerability in JMXProxy,../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security How do I use OpenSSL to set up my own Certificate Authority (CA)?,Using OpenSSL to set up your own CA.,../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security Oh no! Port 8005 is available for anyone on localhost to shutdown my tomcat!,See these 2 discussions. Possible to switch off tcp/ip server shutdown?Tomcat shutdown & security,../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security What about Tomcat running as root?,See these threads: Tomcat as root and security issues,../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security How do I force all my pages to run under HTTPS?,Use security-constraint in web.xml.,../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security What is the default login for the manager and admin app?,"The admin and manager application do not provide a default login. Doing so would be a security flaw. You need to edit $CATALINA_HOME/conf/tomcat-users.xml file if you are using the default install. SeeConfiguring Manager Application Accessfor details. Note that there exists malware that tries to guess the manager password. There was once a bug that blindly clicking-trough the Windows installer configured a manager user with blank password (CVE-2009-3548). This was fixed by April 2010 (Tomcat 5.5.29, 6.0.24 and later are safe).",../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security How do I restrict access by ip address or remote host?,"By using theRemoteHostValveorRemoteAddrValve. Warning, these valves rely on accurate incoming ip addresses or hostnames. So they can fall victim to spoofing! See alsoRemoteIpValve.Valve Reference Link",../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security How do I use jsvc/procrun to run Tomcat on port 80 securely?,"Fairly easilySee the Setup page in the docs for your tomcat release, and readthis mailing list postfor a complete setup example with permissions etc.",../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security Has Tomcat's security been independently analyzed or audited?,"Yes, by numerous organizations and individuals, many times. Trythis Google searchand you'll see many references, guides, and analyses.",../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security How do I change the Server header in the response?,"Inserver.xmlfile add a ""server"" attribute to the Connector element.https://tomcat.apache.org/tomcat-9.0-doc/config/http.html",../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security Why are passwords in plain text?,We have a page dedicated to this topic.Password,../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security How can I restrict the list of ciphers used for HTTPS?,SeeHowTo SSLCiphers.,../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security Which cipher suites should I use?,SeeSecurity/Ciphers.,../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security Is Tomcat affect by Log4Shell CVE-2021-44228?,"Out of the box  - No. But that doesn't prevent an application deployed to Tomcat from using log4j2. In which case, please use general guidance on various remediations. As of this writing, they include any of these Upgrading log4j2 to 2.16.0 (or better)This is the best fix (2.15.0 was also insufficient due to CVE-2021-45046)Use system property log4j2.formatMsgNoLookups=true to disable message formattingThis is a reasonable workaround - But due to CVE-2021-45046 you may still have other vulnerabilitiesSetting a shell environment variableLOG4J_FORMAT_MSG_NO_LOOKUPS=trueshould work tooRemove the following files from your jar file: log4j-core-2.XX.Y.jar  (This is a hack. A simple one but effective. This also remediates CVE-2021-45046)org/apache/logging/log4j/core/net/JndiManager$1.classorg/apache/logging/log4j/core/util/JndiCloser.classorg/apache/logging/log4j/core/net/JndiManager$JndiManagerFactory.classorg/apache/logging/log4j/core/lookup/JndiLookup.classorg/apache/logging/log4j/core/net/JndiManager.classorg/apache/logging/log4j/core/selector/JndiContextSelector.classEnsuring your JRE is 1.8.121 or better. (This version doesn't fix the issue, it just eliminates one very easy to exploit attack vector.)Not recommended but better than nothing. When combined with the limiting of creating socket connections, this will make payloads harder (but not impossible) to deploy.But these 2 system properties must also be set to false (or unset) com.sun.jndi.rmi.object.trustURLCodebase,com.sun.jndi.cosnaming.object.trustURLCodebaseNewer JVM's can still fall victim to this attack with the older log4j2. It is beyond the scope of this FAQ to explain how. But the TL;DR is it the exploit becomes more application or application server specific. More details on these CVE's via the ASF blog",../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Security I found a vulnerability in JMXProxy,"JMXProxy is a powerful servlet which has full access to all JMX capabilities.By design, enabling it opens you to a lot of security challenges. The equivalent of enabling generic remote JMX access at the JVM level.  With that in mind, if you enable it: You shouldat a minimumrequire an extremely strong password to protect this URL as well restrict the IP client list which may access it. (Ideally restricting it to localhost if possible)",../data/confluence_exports/TOMCAT/Security_103099051.html
Apache Tomcat : Archive Not started,These pages are for ideas that - for whatever reason - never got started. AJP.nextAJP Client Library and Command Line Tool(proposed GSoC project that was not selected)Nested FilesystemTomcatGrid,../data/confluence_exports/TOMCAT/Archive_110694813.html
Apache Tomcat : Archive Completed,These pages were used to track one-off tasks that have been completed or where work has ceased. CookiesGit migrationJava 9 Status trackingWAR URLs,../data/confluence_exports/TOMCAT/Archive_110694813.html
Apache Tomcat : Archive No longer relevant,The topics of these pages have been over taken by events and are no longer relevant. Managing Tomcat's Dependency on the Eclipse JDT Core Batch Compiler,../data/confluence_exports/TOMCAT/Archive_110694813.html
Apache Tomcat : Deployment Preface,This section of the FAQ discusses common questions related to web application deployment.,../data/confluence_exports/TOMCAT/Deployment_103098853.html
Apache Tomcat : Deployment Questions,Why does tomcat 5 create context configuration files?Why does the memory usage increase when I redeploy a web application?,../data/confluence_exports/TOMCAT/Deployment_103098853.html
"Apache Tomcat : Deployment Tomcat 5, Tomcat 6:","Unlike tomcat 4.x, tomcat 5.x creates context configuration files for you in itsconf/[Engine name]/[Host name]directory. This is part of the change in tomcat's configuration mechanism from version 4.x to make overall configuration more robust, flexible, and enterprise-friendly. Note, however, that this has changed the recommended deployment practices for web applications. These context configuration files are created by tomcat, but not removed by tomcat, because the user may have changed them or other files in the conf directory. The suggested practice for tomcat 5 is to place context configuration files asMETA-INF/context.xmlin your webapp, and use Tomcat's Manager webapp to deploy/undeploy your applications. More details can be found here:MARC Archive",../data/confluence_exports/TOMCAT/Deployment_103098853.html
Apache Tomcat : Deployment Tomcat 7 and later:,"In Tomcat 7 the default behaviour has been changed to do not auto-create those context configuration files. The recommended practice of usingMETA-INF/context.xmlfiles is still the same. Those files are discovered and processed in the same way. The difference is that they are not copied to theconf/[Engine name]/[Host name]directory. This is convenient, as you do not need to care of those copied files when undeploying your application, and you do not need to care whether theconfdirectory is writeable. This change in behaviour is documented in theMigration Guide.",../data/confluence_exports/TOMCAT/Deployment_103098853.html
Apache Tomcat : Deployment Why does the memory usage increase when I redeploy a web application?,"That is because your web application has a memory leak. A common issue are ""PermGen"" memory leaks. They happen because the Classloader (and the Class objects it loaded) cannot be recycled unless some requirements are met (*). They are stored in the permanent heap generation by the JVM, and when you redeploy a new class loader is created, which loads another copy of all these classes. This can causeOufOfMemoryErrorseventually. (*) The requirement is that all classes loaded by this classloader should be able to be gc'ed at the same time. Starting with Tomcat 6.0.25 there is a tool in the Manager webapp to help diagnose such misbehaving applications. SeeFAQ/MemoryandMemoryLeakProtection.",../data/confluence_exports/TOMCAT/Deployment_103098853.html
Apache Tomcat : WAR URLs Background,"Tomcat defines, for internal use, a WAR URL similar to JAR URLs except it uses ""*/"" rather than ""!/"" to allow URLs for JARs nested in WARs without confusing the JREs handling of JAR URLs. The use of ""*"" causes problems in some environments so we are considering alternatives. Whatever is used needs to be valid in a URL. The relevant ABNF from RFC 3986 is: absolute-path  = 1*( ""/"" segment )pchar          = unreserved / pct-encoded / sub-delims / "":"" / ""@""unreserved     = ALPHA / DIGIT / ""-"" / ""."" / ""_"" / ""~""sub-delims     = ""!"" / ""$"" / ""&"" / ""'"" / ""("" / "")""/ ""*"" / ""+"" / "","" / "";"" / ""=""",../data/confluence_exports/TOMCAT/WAR-URLs_67638939.html
Apache Tomcat : WAR URLs Single character options,"Exclude ""*"" as that is the option we are trying to replace. Exclude ""!"" as it is used in JAR URLs. Exclude ""+"" as it is used to encode space. Exclude "";"" as it is used to delimit path parameters and for file versions in VMS. Exclude ALPHA, DIGIT, ""."", ""-"", ""_"" as they are frequently used in file names. The remaining options are: "":"" / ""@"" /  ""~"" / ""$"" / ""&"" / ""'"" / ""("" / "")"" / "","" / ""=""",../data/confluence_exports/TOMCAT/WAR-URLs_67638939.html
Apache Tomcat : WAR URLs Multiple character options,"Use "".war/""",../data/confluence_exports/TOMCAT/WAR-URLs_67638939.html
Apache Tomcat : WAR URLs Alternatives,Make the single character configurable.,../data/confluence_exports/TOMCAT/WAR-URLs_67638939.html
Apache Tomcat : JSP TCK (2.3 & 3.0) Common Configuration for all Tomcat versions,"None. Note: If you re-run the TCK you must stop Tomcat, empty the work directory and then restart Tomcat. This is because the TCK assumes it is running for the first time and a number of tests check various compilation activities.",../data/confluence_exports/TOMCAT/103089960.html
Apache Tomcat : JSP TCK (2.3 & 3.0) Tomcat 9.0.x,None required (clean 9.0.x build).,../data/confluence_exports/TOMCAT/103089960.html
Apache Tomcat : JSP TCK (2.3 & 3.0) Tomcat 10.0.x,Remove the request-character-encoding and response-character-encoding settings from conf/web.xml (the JSP TCK has one test that depends on the default encoding).,../data/confluence_exports/TOMCAT/103089960.html
Apache Tomcat : JSP TCK (2.3 & 3.0) Test Suite,Download one of the following: Jakarta EE 8 (Tomcat 9.0.x):http://download.eclipse.org/ee4j/jakartaee-tck/jakartaee8/promoted/pages-tck-2.3.0.zipJakarta EE 9 (Tomcat 10.0.x):https://download.eclipse.org/ee4j/jakartaee-tck/jakartaee9/promoted/pages-tck-3.0.0.zip Extract to JSP_TCK_HOME  Edit $JSP_HOME/bin/ts.jte You'll need to set the following properties (adjust the paths and values for your environment) webServerHome=/path/to/tomcat webServerHost=localhost webServerPort=8080 jspservlet.classes=${webServerHome}/lib/servlet-api.jar${pathsep}${webServerHome}/lib/jsp-api.jar el.classes=${webServerHome}/lib/el-api.jar sigTestClasspath=${el.classes}${pathsep}${jspservlet.classes}${pathsep}${JAVA_HOME}/lib/rt.jar${pathsep}${JAVA_HOME}/lib/modules set JAVA_HOME cd $JSP_TCK_HOME/bin ant gui Accept the defaults and then run the tests  A default 9.0.x build with the above configuration passes the Jakarta EE 8 JSP TCK. A default 10.0.x build with the above configuration passes the Jakarta EE 9 JSP TCK with the following JREs: Adopt OpenJDK 8u275 b01Adopt OpenJDK 11.0.9 b11 (TCK and Tomcat)  The Apache Tomcat project has reported the following bugs to the Eclipse Jakarta EE TCK project which have since been fixed. 1 x regressionshttps://github.com/eclipse-ee4j/jakartaee-tck/issues/535(PR to fix -https://github.com/eclipse-ee4j/jakartaee-tck/pull/536)1 x signature testhttps://github.com/eclipse-ee4j/jakartaee-tck/issues/48https://github.com/eclipse-ee4j/jakartaee-tck/issues/294(PR to fix -https://github.com/eclipse-ee4j/jakartaee-tck/pull/295),../data/confluence_exports/TOMCAT/103089960.html
Apache Tomcat : TomcatCon London 2017 Sessions,"State of the Cat New and upcoming Reverse Proxies, Load-balancing & Custering From the cluster to the cloud Ask Us Anything",../data/confluence_exports/TOMCAT/TomcatCon-London-2017_74680879.html
Apache Tomcat : TomcatCon London 2017 Financials,"Income Item£Ticket sales - 16 @ £50 per ticket800.00Speaker Sponsorship (c2b2)Note: Also purchased 2 tickets included in ticket sales above200.00Grand Total1000.00 Expenses Item£Venue - Provided entirely by Liferay who also handled all venue associated organisationIncluded breakfast pastries, 2 coffee breaks and lunch0.00Eventbrite fees63.04Badge holders - 2 boxes of 25 @ £5.75 per box11.50Blank badges (200)18.61USB microphone15.49Badge printing – 23 @ $0.25 per badge4.47Return train fare for markt48.60Taxi for markt14.40Tube ticket for markt2.40Grand Total178.51 Surplus for future events = £821.49",../data/confluence_exports/TOMCAT/TomcatCon-London-2017_74680879.html
Apache Tomcat : UsingDataSources How do I use DataSources with Tomcat?,"When developing JavaEE web applications, the task of database connection management can be daunting. Best practice involves using a JavaEE DataSource to provide connection pooling, but configuring DataSources in web application servers and connecting your application to them is often a cumbersome process and poorly documented. The usual procedure requires the application developer to set up a DataSource in the web application server, specifying the driver class, JDBC URL (connect string), username, password, and various pooling options. Then, the developer must reference the DataSource in his application's web.xml configuration file, and then access it properly in his servlet or JSP. Particularly during development, setting all of this up is tedious and error-prone. With Tomcat the process is vastly simplified. Tomcat allows you to configure DataSources for your JavaEE web application in a context.xml file that is stored in your web application project. You don't have to mess with configuring the DataSource separately in the Tomcat server.xml, or referencing it in your application's web.xml file. Here's how:",../data/confluence_exports/TOMCAT/UsingDataSources_103100747.html
Apache Tomcat : UsingDataSources Install the JDBC Driver,"Install the .jar file(s) containing the JDBC driver in Tomcat's$CATALINA_BASE/libfolder. You do not need to put them in your application'sWEB-INF/libfolder. When working with JavaEE DataSources, the web application server manages connections for your application.",../data/confluence_exports/TOMCAT/UsingDataSources_103100747.html
Apache Tomcat : UsingDataSources Create META-INF/context.xml,"In the root of your web app directory structure, create a folder named META-INF (all caps). Inside that folder, create a file named context.xml that contains a Resource like this: <?xml version=""1.0"" encoding=""UTF-8""?>
<Context>

  <Resource name=""jdbc/WallyDB"" auth=""Container""
            type=""javax.sql.DataSource"" username=""wally"" password=""wally""
            driverClassName=""com.microsoft.sqlserver.jdbc.SQLServerDriver"" 
            url=""jdbc:sqlserver://localhost;DatabaseName=mytest;SelectMethod=cursor;""
            maxActive=""8"" 
            />

</Context> This example shows how to configure a DataSource for a SQL Server database named mytest located on the development machine. Simply edit the Resource name, driverClassName, username, password, and url to provide values appropriate for your JDBC driver.",../data/confluence_exports/TOMCAT/UsingDataSources_103100747.html
Apache Tomcat : UsingDataSources From a Servlet,"Here's how you might access the data in a servlet: InitialContext ic = new InitialContext();
  DataSource ds = (DataSource) ic.lookup(""java:comp/env/jdbc/WallyDB"");
  Connection c = ds.getConnection();
  ...
  c.close(); Notice that, when doing the DataSource lookup, you must prefix the JNDI name of the resource withjava:comp/env/",../data/confluence_exports/TOMCAT/UsingDataSources_103100747.html
Apache Tomcat : UsingDataSources Sample Project,Here's a sample web application project that shows where all the files go. This one shows how to access data from from a JSP page:datasourcedemo.war,../data/confluence_exports/TOMCAT/UsingDataSources_103100747.html
Apache Tomcat : UsingDataSources Known-Working examples for other Databases,"<Resource name=""jdbc/denali"" auth=""Container"" type=""javax.sql.DataSource""
              username=""denali"" url=""jdbc:postgresql://localhost:5432/demo""
              factory=""org.apache.commons.dbcp.BasicDataSourceFactory""
              driverClassName=""org.postgresql.Driver""
              maxActive=""20"" maxIdle=""10""/> <Resource name=""jdbc/ccsdatasource"" auth=""Container"" type=""javax.sql.DataSource""
              username=""ccs"" password=""secret"" url=""jdbc:mysql://localhost:3306/ccs""
              driverClassName=""com.mysql.jdbc.Driver""
              maxActive=""20"" maxIdle=""10""/>",../data/confluence_exports/TOMCAT/UsingDataSources_103100747.html
Apache Tomcat : UsingDataSources Please Note,"This technique is Tomcat-specific. If you deploy your web application to another application server, you will need to configure the database according to your application server's documentation, and reference it in your application's web.xml.",../data/confluence_exports/TOMCAT/UsingDataSources_103100747.html
Apache Tomcat : AddOns UrlRewrite,"UrlRewriteFilterby Paul Tuckey (also hosted onGitHub) is a Filter that performs URL rewriting and redirections, and other typical tasks, like setting request attributes, headers or cookies.  It is similar tomod_rewritemodule of Apache HTTPD server.",../data/confluence_exports/TOMCAT/AddOns_103098456.html
Apache Tomcat : AddOns Security Filter,SecurityFilteris a Filter that mimics container managed security.,../data/confluence_exports/TOMCAT/AddOns_103098456.html
Apache Tomcat : AddOns Apache Shiro,"Apache Shirois a Java security framework that performs authentication, authorization, cryptography, and session management. It can be configured as a Filter to secure a web application.",../data/confluence_exports/TOMCAT/AddOns_103098456.html
Apache Tomcat : AddOns Spring Security,"Spring Securityis a powerful and highly customizable authentication and access-control framework. It is usually used withSpring Framework. In simple configuration it may be used as a filter to perform both authentication and authorization. It also can be used to perform authorization control at business methods of an application, and to implement domain object security.",../data/confluence_exports/TOMCAT/AddOns_103098456.html
Apache Tomcat : AddOns OrientDB Realm,OrientDBRealm- Realms for Tomcat 6 and 7 that connect to an OrientDB,../data/confluence_exports/TOMCAT/AddOns_103098456.html
Apache Tomcat : AddOns PicketLinkVault,PicketLink Vault extension for Apache Tomcat- Provides a customPropertySourcethat can be used with Tomcat. SeeFAQ/Password.,../data/confluence_exports/TOMCAT/AddOns_103098456.html
Apache Tomcat : AddOns PSI Probe manager application,"PSI Probeis advanced manager and monitor web application for Apache Tomcat.  (Historically, the name is a tribute to Lambda Probe project, which they continued as a fork. The original Lambda Probe project (www.lambdaprobe.org) closed more than 10 years ago (2006) and is not applicable to current versions of Tomcat).",../data/confluence_exports/TOMCAT/AddOns_103098456.html
Apache Tomcat : AddOns Logging to Syslog,See aStackOverflow questionandAgafua-syslogproject (GitHub).   CategoryFAQ,../data/confluence_exports/TOMCAT/AddOns_103098456.html
Apache Tomcat : JNDI startTLs HowTo JNDI StartTLSHowTo,"In reference to:http://www.mail-archive.com/users@tomcat.apache.org/msg80660.htmlthis Howto describes the configuration of a JNDI Realm connecting to an LDAP directory using StartTLS for connection establishment. StartTLS is the method of negotiating a TLS connection. For LDAP it was first time in RFC 2830, then refined in RFC 4513. Tomcat does not support this out of the box. Using JNDI Realm'scontextFactoryfeature however, we can still achieve this: <Realm className=""org.apache.catalina.realm.JNDIRealm""
     connectionURL=""ldap://primary.ldap.dir:389""
     alternateURL=""ldap://secondary.ldap.dir:389""
     connectionName=""uid=binddn"" connectionPassword=""password.""
     userBase=""ou=people,dc=brainsware,dc=org"" userSearch=""uid={0}""
     contextFactory=""tc.startTLS.LdapTlsContextFactory /> Using the code provided by Felix Schumacher in this post:http://www.mail-archive.com/users@tomcat.apache.org/msg80693.html- You can download it here:LdapTlsContextFactory.java. We have to compile it into a JAR and put in a place where Tomcat can find it:lib. Then we simply reference its full name incontextFactory.LdapTlsContextFactorywill now do the negotiation initialization. Afterwards the created object will be used for every authentication attempt. Beware that the code will not check the hostname of the server with respect to its certificate. If you don't want this behaviour remove the call totls.setHostNameVerifier(...).",../data/confluence_exports/TOMCAT/JNDI-startTLs-HowTo_103099456.html
Apache Tomcat : JNDI startTLs HowTo Further Steps,"The code probably needs auditing. More testing. And definitely more tightening: e.g.: When starting the negotiation the client (Tomcat +LdapTlsContextFactory) sends anSSLv2Hello, which is anything but desirable. This could be due to Sun’s poor defaults in their SSL implementation, an oversight in the code, or because I’ve missed out a JVM startup options.",../data/confluence_exports/TOMCAT/JNDI-startTLs-HowTo_103099456.html
Apache Tomcat : ServletProxy References :,"[1] J2EP documentation.                                                                                                                                                        [2]http://noodle.tigris.org/, “home page – noodle”.                                                                                                                                                        [3]http://edwardstx.net/wiki/Wiki.jsp?page=HttpProxyServlet, “ Http proxy Servlet –      main page”.                                                                                                                                                        [4]http://portals.apache.org/applications/webcontent2/reverse-proxy-module.html, “Apache Portals Web Content Application 2 - Http proxy Servlet Reverse Proxy Module”.                                                                              CategoryGSOC",../data/confluence_exports/TOMCAT/ServletProxy_103100148.html
Apache Tomcat : About Preface,"This FAQ is maintained by the Tomcat Committers. The content for this FAQ is usually discovered by lurking in the tomcat-user list. If you wish to make a comment about the FAQ, make the comment on the tomcat-user list. Do not e-mail any of the committers directly and do not e-mail the tomcat-dev list. (Unless other people liked your suggestion and it was accidentally missed by the committers who read the tomcat-user list) Emailing the tomcat-user list will allow a larger audience to immediately learn and critique your findings.",../data/confluence_exports/TOMCAT/About_103098752.html
Apache Tomcat : About Questions,How did Tomcat get its name?How do I contribute a question?,../data/confluence_exports/TOMCAT/About_103098752.html
Apache Tomcat : About How did Tomcat get its name?,"He (James Duncan Davidson) came up with ""Tomcat"" since the animal represented something that could take care of itself and fend for itself. That's how he came up with the name.",../data/confluence_exports/TOMCAT/About_103098752.html
Apache Tomcat : About How do I contribute a question?,"Make sure the question has been asked more than onceMake sure the answer(s) given are correctSelect the right FAQ section in ConfluenceEdit Confluence, keeping to the existing format and layoutAdd your question to the list at the start of the pageDon't forget to add an anchor to the start of your answer",../data/confluence_exports/TOMCAT/About_103098752.html
Apache Tomcat : Performance and Monitoring Preface,"This is about Tomcat performance. Tomcat Performance ConcernsHow do I increase performance on Tomcat? Other (non-exhaustive) notes: Stress test your webapp. You can do this viaJMeter,siege,flood, and other tools. Google is your friend.Tweak your UNIX box! Look at ulimit and kernel parameters.Bad design will hurt performance.Look at profiling tools for Java.",../data/confluence_exports/TOMCAT/Performance-and-Monitoring_103099040.html
Apache Tomcat : Performance and Monitoring Questions,Is Tomcat faster than serving static HTML pages than Apache httpd?How do I configure apache tomcat connectors for a heavy load site?How do I make Tomcat start up faster?,../data/confluence_exports/TOMCAT/Performance-and-Monitoring_103099040.html
Apache Tomcat : Performance and Monitoring Is Tomcat faster than serving static HTML pages than Apache httpd?,Yes depending on how you tune it. And NO depending on how you tune it. Anything less starts a religious war. We recommend performing your own benchmarks andsee for yourself.,../data/confluence_exports/TOMCAT/Performance-and-Monitoring_103099040.html
Apache Tomcat : Performance and Monitoring How do I configure apache tomcat connectors for a heavy load site?,"The followingexcellentarticle was written by Mladen Turk. He is a Developer and Consultant for JBoss Inc in Europe, where he is responsible for native integration. He is a long time commiter for Jakarta Tomcat Connectors, Apache Httpd and Apache Portable Runtime projects. Fronting Tomcat with Apache or IIS - Best Practices https://people.apache.org/~mturk/docs/article/ftwai.html",../data/confluence_exports/TOMCAT/Performance-and-Monitoring_103099040.html
Apache Tomcat : Performance and Monitoring How do I make Tomcat start up faster?,SeeHowTo FasterStartUp,../data/confluence_exports/TOMCAT/Performance-and-Monitoring_103099040.html
Apache Tomcat : Troubleshooting and Diagnostics Table of Contents,"Techniques & ReferenceToolsJMX ClientsJDK toolsProfilers & Heap AnalyzersNotes on using JMX clientsCommon Troubleshooting ScenarioTroubleshooting unexpected Response state problemsTroubleshooting ""Too many open file descriptors""",../data/confluence_exports/TOMCAT/Troubleshooting-and-Diagnostics_103099080.html
Apache Tomcat : Troubleshooting and Diagnostics Techniques & Reference,How To: Capture a thread dumpHow To: Capture a heap dumpHow To: Examine a StacktraceHow To: Configure Tomcat for debuggingFAQ: DevelopingFAQ: MemoryTomcat Memory Leak ProtectionNotes on using JMX clients,../data/confluence_exports/TOMCAT/Troubleshooting-and-Diagnostics_103099080.html
Apache Tomcat : Troubleshooting and Diagnostics JMX Clients,"JJConsole:DocumentationVisualVM:Documentation,Project",../data/confluence_exports/TOMCAT/Troubleshooting-and-Diagnostics_103099080.html
Apache Tomcat : Troubleshooting and Diagnostics JDK tools,jinfo - Prints JVM process infojstack - Prints thread stack tracesjmap - Dumps heap and shows heap statusjhat - Heap Analyzer Tooljcmd - Multitool intended to replace the above JDK tools,../data/confluence_exports/TOMCAT/Troubleshooting-and-Diagnostics_103099080.html
Apache Tomcat : Troubleshooting and Diagnostics Profilers & Heap Analyzers,Eclipse Memory Analyzer (MAT)YourKit ProfilerVisualVM Docs,../data/confluence_exports/TOMCAT/Troubleshooting-and-Diagnostics_103099080.html
Apache Tomcat : Troubleshooting and Diagnostics Notes on using JMX clients,"When running a JMX client (JConsole, VisualVM) on the same machine as the target JVM process it is possible to connect without pre-configuring a JMX port, using the local connector stub. This method relies on being able to create a protected temporary file, accessible only to a user with administrator privileges. Java processes which are accessible via the local connector will automatically appear in the client. NB(1) On Windows, this means that the temporary directory must be located on an NTFS formatted disk. See the following link for more details. NB(2) On Windows, if Tomcat is started using a service wrapper, this will prevent JConsole & VisualVM from using the local JMX connector stub. Java 5 JConsole and Remote Management FAQ From Java 6 onward a process does not need to have the management agent enabled when it starts, as the Attach API permits the management agent to be activated on demand.",../data/confluence_exports/TOMCAT/Troubleshooting-and-Diagnostics_103099080.html
Apache Tomcat : Troubleshooting and Diagnostics Common Troubleshooting Scenario,"If youhave already looked into Tomcat logs, there are no error messages, and you just want to find out what is going on, you may try the following Look intoTomcat access log(the log file generated byAccessLogValve).If your request is not listed there, then it has not been processed by Tomcat. You need to look elsewhere (e.g. at your firewall).You will see what IP address your client is using, and whether it is using an IPv4 (127.0.0.1) or IPv6 address (0:0:0:0:0:0:0:1).Modern operating systems can use IPv6 addresses for localhost / local network access, while external network is still using IPv4.Take a thread dump. This way you will find out what Tomcat is actually doing.If you are troubleshooting some process that takes noticeable time, takeseveral(three) thread dumps with some interval between them. This way you will see if there are any changes, any progress.Trydebugging.A good place for a breakpoint isorg.apache.catalina.connector.CoyoteAdapter.service()method. That is the entry point from Tomcat connectors and into the Servlet engine. At that place your request has already been received and its processing starts.If you did a long-awaited upgrade, jumping over several years worth of Tomcat releases, and something broke, and you have no clue,ReadingMigration guidesmay help.It may help to do abinary search(akabisecting) to locate the version of Tomcat that triggered the change. If your issue is easy to reproduce, it may be pretty fast. Just 7-8 tries may cover a range of 100 versions. Once you know the version and its release date, the following resources are available:The release announcement.See ""former announcements"" link at the bottom of the front page of theApache Tomcat site.An announcement mail message can also be found in the archives of the ""announce@""mailing list.The changelog. A release announcement usually has a link to it.Archives of the ""users@""mailing list. You may look for discussions that happened a month or two after the release.",../data/confluence_exports/TOMCAT/Troubleshooting-and-Diagnostics_103099080.html
Apache Tomcat : Troubleshooting and Diagnostics Troubleshooting unexpected Response state problems,"If you encounter problems that manifest themselves as accessing a request or response that is an inconsistent state, the main suspect isyour own web application(or a library that it uses) keeping a reference to Request or Response objects outside of their life cycle. Examples:BZ 61289,BZ 58457. The lifetime of the Response object is documented in theServlet specification. Quoting from section ""5.8 Lifetime of the Response Object"" of Servlet 4.0 specification: ""Each response object is valid only within the scope of a servlet’s service method, or within the scope of a filter’s doFilter method, unless the associated request object has asynchronous processing enabled for the component. If asynchronous processing on the associated request is started, then the response object remains valid until complete method on AsyncContext is called."" In case of asynchronous processing, when an error occurs Tomcat notifies all registeredAsyncListeners and then callscomplete()automatically if none of the listeners have called it yet. (Reference:61768) Also see sections ""2.3.3.4 Thread Safety"" and ""3.13 Lifetime of the Request Object"" of the same specification. To troubleshoot the issue: Make sure that your Tomcat is configured to discard facades to its internal objects when request processing completes. This makes it easier to spot illegal access when it happens, instead of waiting until side effects of such access become visible. Essentially, it protects Tomcat internals from misbehaving web applications.This feature is always on when you are running Tomcat with aJava Security Manager being enabled. Starting with Tomcat 10.0 this feature is enabled by default. It isdisabledby default in earlier versions of Tomcat. The way this feature is configured differs between versions: it is controlled by an attribute onConnectorelement or by asystem property.If you are running Tomcat 9.0 or earlier, do both of the following:- Set the followingsystem propertyin Tomcat configuration:org.apache.catalina.connector.RECYCLE_FACADES=true- Add the following attribute to allConnectorelements:discardFacades=""true""The Connector attribute was added in Tomcat 10.0.0-M1, 9.0.31, 8.5.51 and 7.0.100. The system property is an older way to configure this feature. In case of a doubt, or if you are switching back and forth between versions while troubleshooting the issue, it is safer to configure both of them.This feature is also mentioned on theSecurity Considerationspage in Tomcat documentation. You can also search the archives of the Tomcat users'mailing listsfor previous discussions mentioning the RECYCLE_FACADES flag. Accessing response objects after their lifetime can lead to security issues in your application, such as sending responses to wrong clients, mixing up responses. If you can reproduce the issue and the above diagnostic does not show your own bug, but a bug in Apache Tomcat, if the problem manifests as a security issue, seehow to report it. There are some known examples of broken libraries / APIs: Read aboutJava ImageIOissue — an issue withjavax.imageio.ImageIOAPI. It may have already been fixed as it is an old issue, but there are no clear records of it.Read about anissue in PD4ML,a library that is used to generate PDF files, — fixed in their version 3.8.0, earlier versions may be affected.",../data/confluence_exports/TOMCAT/Troubleshooting-and-Diagnostics_103099080.html
"Apache Tomcat : Troubleshooting and Diagnostics Troubleshooting ""Too many open file descriptors""",The code that opens the descriptors can be identified using a tool such ashttp://file-leak-detector.kohsuke.org/,../data/confluence_exports/TOMCAT/Troubleshooting-and-Diagnostics_103099080.html
Apache Tomcat : Verifying a Release Build Verifiable Items,"All release artifacts should be verifiable. That includesapache-tomcat-*.zip,apache-tomcat-*.tar.gz, in both source and binary distributions, as well as the Windows .exe installer and uninstaller binaries.",../data/confluence_exports/TOMCAT/Verifying-a-Release-Build_240880954.html
Apache Tomcat : Verifying a Release Build Prerequisites,"There are several things which you will need in order to perform a Tomcat release build. A build environment running on x86 or x86-64 architecture. This is required to build the Windows installer and uninstaller binaries which require NSIS which is an x86-only product. Use of Microsoft Windows is not required; builds can be completed on MacOS or Linux with wine installed. Please seeTomcat's Release Processfor how to install and configure wine if you choose this option.A Java Development Kit (JDK) which matches the version used to build the release. This version information can be found in each release's source artifact in the build.properties.release file.Apache Ant which matches the version used to build the release. This version information can be found in each release's source artifact in the build.properties.release file.GnuPG and your own private key. You will be asked to sign the release that you build locally during the build process. You will need to configure your build environment. The easiest way to do that is to create abuild.propertiesfile in your user's home directory which contains the following configuration: gpg.exec=(full path to gpg.exe on your system)
base.path=(full path to a temporary directory where Tomcat can download dependencies e.g. /tmp/tomcat-build-libs)",../data/confluence_exports/TOMCAT/Verifying-a-Release-Build_240880954.html
Apache Tomcat : Verifying a Release Build How to Verify,"You will need to download the source distribution of the release. This is the apache-tomcat-x.y.z-src.zip or apache-tomcat-x.y.z-src.tar.gz file which you can find in the downloads area for Apache Tomcat. For example, the source ZIP file for Apache Tomcat 10.1.5 can be found athttps://dist.apache.org/repos/dist/dev/tomcat/tomcat-10/v10.1.5/src/apache-tomcat-10.1.5-src.zip Expand the ZIP or tar.gz archive andcdinto that directory. Inspect thebuild.properties.releasefile to ensure you are using the same versions of both the JDK and Apache Ant. Perform a release build: ant release This will take some time, but when it has completed you should have a number of release artifacts in theoutput/releasedirectory. These are the artifacts you will be verifying. To verify release artifact files, you will need to download those files you wish to verify. You have already downloaded the source artifact, so that one is very easy to verify: diff output/release/apache-tomcat-x.y.z-src.zip ../apache-tomcat-x.y.z-src.zip Or diff output/release/apache-tomcat-x.y.z-src.tar.gz ../apache-tomcat-x.y.z-src.tar.gz If you have downloaded all the release artifacts to a directory calledrelease-verificationthen you should be able to run this command on UNIX-like systems (or in Windows using bash): (cd output/bin ; find . \( -name ""*.asc"" -prune \) -o \( -type f -exec diff -qs ""/path/to/release-verification/{}"" ""{}"" \; \) )
(cd output/src ; find . \( -name ""*.asc"" -prune \) -o \( -type f -exec diff -qs ""/path/to/release-verification/{}"" ""{}"" \; \) ) This will print out the names of all files compared and whether or not they match each other. We skip the.ascfiles because those were produced by the release manager using their private GPG key and will not be reproducible by you.",../data/confluence_exports/TOMCAT/Verifying-a-Release-Build_240880954.html
Apache Tomcat : Verifying a Release Build How does all this work?,"When the release-manager performs the release, all parts of the build are tweaked so that reproducibility is possible. There is support from Apache Ant, the javac compiler, and other parts of the toolchain to ensure that every file generated during the build is identical. This begins by using the same versions of the toolchain that were used by the release-manager. It's possible that two different versions of e.g. javac willl generate different output, so it's necessary to use the same versions for verification. The second step is found withinbuild.properties.releasewhich contains the timestamp that the release was prepared by the release-manager. This ensures that all files generated during the build process have consistent file timestamps which is especially important when they are put into archives which store those file timestamps. The third step is with the file signature-generation. Apache Tomcat releases have several different types of file signatures: Simple hashes, such as files with names ending in.sha512. These are directly reproducible by re-hashing the files once built, and verifiable withdiff.GPG signatures, which are not reproducible but still verifiable withgpg --verify.Windows .exe digital signatures, which are the most complicated. The release-manager generates so-called detached signatures (because the file contains the signature only) and bundles these in the release tag in revision-control, as well as in the source release. You got a copy of these files when you expanded the source archive. Whenyouperform a release-build, the release process uses the existing detached signature files to merge with the unsigned .exe files and the result is a byte-for-byte copy of the signed installer and uninstalled binaries. You can verify these withdiffor by asking Windows to confirm the digital signature of the files.",../data/confluence_exports/TOMCAT/Verifying-a-Release-Build_240880954.html
Apache Tomcat : PoweredBy Sites / Systems / Applications With Associated Publications,"This section contains cases that publicly described their Tomcat experience or installation.  AppFuse: Raible Designs'AppFuseruns best on Tomcat. They wrotean interesting postcomparing standards-compliance and performance on Tomcat and other containers.Bonhams: Bonhams is an auction house founded in 1793, running Tomcat and other open-source software as detailed inthis articlefromCIO Magazine.CardinalHealth:CardinalHealthas documented by Sue Hildreth inthis article.WalMart.com: as documented by Eugene Ciurana onTheServerSide.The Weather Channel: and a nicearticlefrom Computer World on their shift from proprietary to open source.E*Trade: - a financial services company,",../data/confluence_exports/TOMCAT/PoweredBy_103099576.html
Apache Tomcat : PoweredBy Other Known Sites / Systems / Applications,"This section contains cases added by users without further external information, or found on theTomcat mailing lists.  AS-ComTec IT Service: Tomcat Hosting , Telefonanlagen & DSL Zugänge Server Solution and more in Germany Frankfurt - Offenbach - Darmstadt3ware: is a software house specialized in web based application.g.PROis our framework based on pure HTML5/CSS3 standards for the client side and on Java/J2EE for the server side, proudly running on Tomcat application server.AccesStream: is an open-source Identity and Access Management suite.Agendize: is a platform that offers conversion & communication tools such as Click to Call, Live Chat and Online Appointment Booking that can easily be added to any website. Agendize has powered over 20,000,000 customer interactions and the application runs on Tomcat.agileBase:  is an open source, commercially hosted/supported platform for building lean, agile business software.Agility Bug Tracker: from The Agile Edge.Alfresco: is an open-source enterprise content management system.AndroidPIT.de:  is the largest German-speaking Android community and secondary Android App Store andAndroidPIT.comone of the largest worldwide with about 1.5 million page views per day handled by Tomcat 6.0.Astradyne Systems: A front for a crime family?The AstroGrid: project uses Tomcat for its web applications, as documented on theirwiki.Automated HomeFinder: A Colorado real estate company, used Tomcat/JBoss for their production website for years, allowing millions of home buyers to search a robust database for Denver real estate, Boulder real estate, and other real estate listings around Colorado.Become.com: The statistics servers for this massively-scaled shopping web crawler:Read About ItBergen Jersey Foreclosures: The most popular site for free NJ foreclosure listings has been powered by Tomcat for years since it was first deployed.Bildergalerie:  is a German site providing free hosting of Picture Galleries, developed by two students. It uses Apache Tomcat 7.Borneosoft: Easy-to-use Web-based CRM with Form Builder and Admin Tools.BreakBIT: Fairwizard - an integrated enterprise management system for Fair and Exhibitions.Jakarta Cactus:  A test framework for unit testing server-side java code.CampRate.com: Online Campground Directory and Reviews; Running Tomcat 5 and Struts.CarGurus.com: Next-generation automotive social network: where autmotive knowledge is shared!CHADIS:  is a web-based screening, diagnostic and management system that administers and analyzes pre-visit, online questionnaires completed by parents, teens or teachers and provides Clinicians with instant access to valuable clinical data and resources.Chambres d'Hotes .org: Chambres D'Hotes .org runs Tomcat for it's proprietary Online Reservations and Availabilities system.CiteSeerX: Scientific Literature Digital Library and Search Engine uses Tomcat among other Open Source tools.Read more about CiteSeerXPolska Strefa - Ogloszenia:  Classified ads database. Running on Tomcat 5 andApache CocoonCodeCollaborator: is a popular peer code review tool fromSmart Bear Software.Cofax: Content Object FactoryColorado HomeFinder:  is the most popular real estate website in Colorado, delivering millions of page views each month through a custom high performance Tomcat/Struts solution that allows users to search and browse real estate listings in Denver, Boulder, Fort Collins, and all across the Colorado front range.Colorado Home Helper: is a Premier Colorado Real estate site run out of Boulder and Broomfield. The site runs a complex property database and mapping system using Tomcat and other open source programs.Delta Virtual Airlines:  is the world's largest virtual airline, with over 2,200 members. This is a group of flight simulation enthusiasts using Microsoft Flight Simulator to fly the current and historic routes of Delta Air Lines.Demeures:  runs Tomcat for it's classified ads system.Derecho.com: Spanish law page, the biggest ebook publisher, Law content provider and eLaw firm in Spain.Diecast: Stockists of all major diecast brands.DSpaceis an open-source repository for research materials. It is installed in numerous places, includingMITand the European University Institute.DURAMENTAL Glutathion: runs on Tomcat and is realized with JSP-Pages. The Backend is a client solution based on swing - a complete Java-App! It was implemented by the TYPO3- and Magento-Agency ""björn hahnefeld IT""Electricshopping.com: is a UK based internet retailer of home appliances and merchandise. Started in early 2001, it provides a low price guarantee along with free shipping in the UK and aims establish itself as the premier destination for electric home goods in Europe. It currently uses Tomcat for is online store.Employee Benefits Institute of America: -En Buenos Aires - Apartments / Real Estate:  is a free property publications web site based in Buenos Aires Argentina, we're running Apache, Tomcat on Debian Linux doing around 120K visitors / month / 1.5 million page views / month and Tomcat is working very well, keep it up!eSage Group: A consulting company, most of our projects are built on Tomcat.Temizlik: uses Tomcat for own production systems and development services.eTools.ch: is a fast and transparent metasearch engine that simultaneously queries major search engines.FarmDirectory.org: Connects producers of agricultural goods with consumers.Farmer Guy Hams and Gammons: is a small web site offering home made hams and gammons for the christmas season, we only have a small jsp order form on tomcat but it works really well. Many thanks to the Tomcat team.Fichedepersonnalite.com: use Tomcat for the database process of result.Fluencyis a dynamic user interface builder that uses Tomcat for its infrastructure.Frankfurt IT-Service: offers on site support & IT services.F.W.Davison & Co.:  develops payroll and human resource software, including HRPyramid Web Edition which is an employee and manager web self-services software built on Tomcat.The Grasshopper: Developer Zone web site is an ASP.NET application running on Tomcat using Grasshopper that portsMonoopen source .NET framework to Tomcat.General Motors: One of the largest car-makers and financiers in the world.Hotels and Accommodation: is an Australian accommodation site. The website utilizes a hotel database system using Tomcat.Homegevityis a National Real Estate and Rental site. The site runs a complex property database system using Tomcat.HubSpot: is an inbound marketing system to help your small or medium sized business get found on the Internet by the right prospects and convert more of them into leads and customers for maximum marketing ROI.Ihre-Apotheke.in:  is a pharmacy finder aimed to the central european market.IMS Neptune:  is a powerful Java-based content management system built on Tomcat and PostgreSQL.IPHOTEL Hospedagem de Sites:  Brazilian Webhosting Company.iPoint Portal: iPoint Portal is an Open Source Collaborative Portal which is compatible with JSR168 and runs on Tomcat.itanum: delivers all Content Management solutions and Web business applications on Tomcat.Jakarta Slide: is an open-source content repository that can serve as the basis for a content management system.Jasig uPortal: also has aconfiguration guide.Javaranch.com: A friendly place for Java Greenhorns.JBoss: -jobbank.com: is an employment site for job searches, posting jobs, posting resumes, and career tools. This MVC site uses Struts and Velocity on Tomcat to render its view.KonaKart: is a free java based online shopping cart application that provides everything that store owners need to sell their products over the Internet. It runs on Tomcat and includes a Java API and SOAP Web Service interface.LeadsAndDeals.com: facilitates a common international business-to-business (B2B) trade platform by connecting global buyers with suppliers. It uses tomcat in several day to day IT operations.LUXMS: Luxury messaging:  Wide range of wireless messaging services, including email SMS notification and delivery of important information from desktop to your phone.Lagerverkauf-Finden: German Searchsite for factory outlet stores. Based on a custom developed lucene based nosql database and running on tomcat.Handytarife-Finden: This ist a very fast search site for mobile phones and bundles. It works with an lucene based nosql database and running on an System consisting of nginx and tomcat.Mhdbdb: Middle-High German Conceptual DatabaseMillennium Pharmaceuticals:  a leading biopharmaceutical company.Webdesign: delivers all Content Management solutions and Web business applications on Tomcat.ngasi.com: is home to NGASIAppServerManager. NGASIAppServerManager automates the installation and management of Tomcat across multiple systems. It also creates and manages Load-Balanced farms of Tomcat instances.MyOtherDrive: an Online Backup and File Sharing provider, runs on Apache Tomcat.myWMS: is an open-source Warehouse Management System using a variety of tools including Tomcat.n0r1sk.com: some nice open source projects! (SMS Gateway / PDF Split / ...)Optusnet.com.au:  An Australian ISP portal with multiple brandings serving over 20 million page impressions per month.Orangepics.com:  Simple, Safe Photo SharingPcgesund.de: PC Gesund is the leading remote support computer helpdesk in Germany.PopcornMonsters.com:  A free movie information portal that averages 100,000 unique visitors a month.Qcadoo.com: qcadoo MES - easy to use, extensible and open source manufacturing management system for small and medium companies. qcadoo Framework - an open source framework for rapid development of data intensive and modular business web applicationsQuickCreative.net:  Advertising Agency offering online ordering of business cards, brochures, logos, postcards and other printed materials.Razuna: is the Open Source alternative to Digital Asset Management. The standalone server download comes bundled with Tomcat 6.x, also their Virtual Server Image comes preinstalled with Tomcat 6.x. Their Razuna Hosted Platform offering, a hosted Digital Asset Management solution, runs on Tomcat as well.RecipeLand.com: One of the oldest and largest recipe web sites.RedEndo BMX: Online Opt-in advertising BMX JournalThe ResCarta Foundation: Community Standards through Collaborative Efforts.The Sakai Project: building a collaboration and learning environment for higher education.Service-Repository: Registry of public SOAP Web Services and dynamic client UIShoppingkaiser: Global Searchsite for currently expiring eBay Auctions. Based on a nosql lucene database.Specto Design, Inc.: delivers all ecommerce solutions and financial applications on Tomcat.springsource: Enterprise Ready Server™ (ERS) is the most comprehensive and widely distributed solution for Apache Web and Tomcat Application Server management. ERS provides better performance and security and improves service quality, reliability and scalability by uniquely enhancing Apache and Tomcat for enterprise use while reducing the costs and complexity of sophisticated Web infrastructures. Includes multiple versions of Apache Tomcat. To download go tohttp://www.springsource.com/products/ers/apachedownloads.STRABAG SE - BRVZ Gmbh IT: BRVZ Gmbh IT is the IT service provider of the Strabag Societas Europea building company. A lot of internal services are running on 150+ Tomcat instances.Synetek.com: uses Tomcat for theirLeaseEagleproduct.Tixeo: uses Tomcat for theirWorkSpace3Dproducts, solutions for web conferencing, video conferencing, desktop sharing and realtime collaborative work in 3D.Teamdev.com: uses Tomcat for own production systems and development services.The Stocks Profit:  uses Tomcat for own back bone production system.TravPro: a web booking client for tour operators runs on Tomcat.Trentisa: a leading consultant and software company specialized in IBM and JAVA solutions.TrycksaksTorget: webbased print solutions.VirtualPairProgrammers- power their E-Commerce operation on Tomcat.Voicent Communications: Voicent uses Tomcat to Power: Call Center Software, IVR, and Auto dialer, as well as many other titles.Volagratis.it: is an E-Commerce site.WebShots: A large online photo service siteWolfram Research: makers ofMathematica, recommend using Tomcat for webMathematica, and have a configuration guide available.XWiki:  is an open-source Java wiki system.",../data/confluence_exports/TOMCAT/PoweredBy_103099576.html
Apache Tomcat : PoweredBy Hosting providers,"2020Media.com:  Longest established UK Tomcat host. Over a decade of Tomcat hosting expertise. Plans on shared, private and dedicated Tomcat servers. Customer has full control over tomcat, SSH access and Tomcat version choice.A2 Hosting: Tomcat Hosting is available on CentOS, Debian, Fedora or Ubuntu VPS Hosting packages. Install Tomcat in moments with A2 Hosting's exclusive Quickinstaller tool.AcuGIS: Tomcat hosting solutions with emphasis on GIS (GeoServer and PostGIS). Tomcat 6 and 7 options with private JVM. High performance hosting with SAS 15k disks and 1 Gb public/private network on all hosting plans.Axon Datacenters: Axon Datacenters is datacenter offering virtual hosting, dedicated servers and cloud services powered by Tomcat and other open-source software via their servers page at Axon Servers.Bestinweb: Bestinweb is a little hosting provider that provides, hosting apart, many other services like: Web Design, Sviluppo siti internet and Posizionamento siti.Bjorn hahnefeld IT: björn hahnefeld IT runs on Tomcat. Hosting-Plans can be booked under Tomcat Co-Location and Serverhousing.BODHost: Proud to be Apache Tomcat Hosting ProviderDailyRazor:DailyRazoris the leading provider of Tomcat and Java hosting solutions with fantastic support for MySQL and PostgreSQL database. Tomcat 5, 6, and 7 all supported. All plans feature Private JDK on a Private JVM.Eapps.com: eApps Hosting has provided hosting for Java applications using Tomcat since 2000 and now offers Tomcat hosting in a reliable, economical VPS container with 24/7 support by in-house staff.Enciva Solutions Ltd.: Enciva Solutions Ltd. offers premium U.S. and U.K based Tomcat hosting solutions. MySQL, Oracle, and PostgreSQL database options. Tomcat 5, 6, and 7. All plans feature private JVM and 1Gbps network speed. VPS and dedicated solutions also available.Energized Hosting: Energized Hosting uses Tomcat for its Servlet/JSP hosting needs.FutureHosting.com: A U.S./U.K./Australian managed hosting provider that operates Tomcat and JBOSS VZ-based virtual containers.goJava.net - Java / Tomcat Hosting: goJava.net - Java / Tomcat Hosting, Dedicated JAVA / Tomcat hosting company. Private Tomcat instances, versions 5.5 / 6.0 / 7.0 supported, Linux CentOS 6.x 64bit servers, minimum 2 x Quad Core CPUs and 24-48 GB RAM. Only Private Tomcat plans. Private Tomcat Lite $7.99/pm (128 MB heap size), Pro $14.99 (256 MB heap size), extra 128 MB heap size for $5. 14 days Trial available!Great JSP Hosting: Great JSP Hosting was created to allow for quick and painless deployment of Java applications. GreatJSPhosting offers java hosting based on Apache Tomcat 7 and JDK 1.7, and Java Xen VPS with pre-installed Tomcat version 7 and full support for Apache Tomcat servlet container. Dedicated Tomcat and Java Hosting company!HomeHost - Hospedagem de Sites:HomeHost- Hospedagem de Sites provides webhosting with support to JSP/Servlets by using Tomcat.HostingInCanada.com:HostingInCanada.com has been providing Java Hosting services based on Apache Tomcat since 1999. At that time only a few companies offered JSP and Java Hosting services. HiC offer: Shared Java Hosting (based on Apache Tomcat), Private Tomcat Hosting (dedicated Tomcat instance) and JBoss. 14 days trial available (only for Shared and Private Tomcat packages).Planet-hosting.net: Hosting Planet Limited is hosting company offering Java hosting using private JVM. All hosting plans including Tomcat as JSP/Servlet container.Hyve Managed Hosting: Hyve Managed Hosting A Mission Critical UK based hosting company specialising in Java and Tomcat hosting in the cloud. Dedicated servers and private instances available.InnoShare:InnoShare- A full-service web design, email newsletter, web hosting and search engine optimization provider.Infrenion Networks: Infrenion Networks - Cheap UK and US Web Hosting service provider with FFMPEG and TOMCAT v6Java Hosting:WebApplicationhosting company. We created Java Panel with Tomcat 6.JavaServletHosting: You'll find professional-grade Java Hosting at very affordable prices. For more than 15 years, they've been a leader in commercial web hosting and n-tier application development. They are one of the original three hosting providers featured on this page back in 2005.Java Hoster: Hosting provider fully dedicated to Java/J2EE hosting located in France with support to Tomcat 6.0.Affordable, Automated and Rock-solid Java Hosting: Cheap Java and JSP hosting company with its own JVM and application server control panel JVMCP. All versions of Tomcat and all versions of JDK supported.KingHost - Hospedagem de Sites:KingHost- Hospedagem de Sites provides webhosting with JSP/Servlets support using Tomcat 5.0, 5.5 and 6.0.LocaData- Hospedagem de Sites provides webhosting with JSP/Servlets support using Tomcat 5.0, 5.5 and 6.0.MetaWerx.net:MetaWerx.net - Australia's first and largest Java hosting providerHospedagem de Sites: Hospedagem de Sites provides webhosting with JSP/Servlets support using Tomcat 5.0, 5.5 and 6.0.MIVITEC GmbH - ISP from Germany / Munich: MIVITEC GmbH - ISP from Germany / Munich - One of the leading business hosting providers in Germany, uses Tomcat and Tomcat clusters for client's sites, CMS systems like OpenCMS and Magnolia etc.NetHosted Ltd:NetHostedLtd Providers of high quality Tomcat hosting on our UK VPS hosting and UK Dedicated Server solutions.NEXCESS.NET: NEXCESS.NET is now supporting JSP/Servlets using Tomcat.olvius.com: olvius.com CMS Website Builder with HTML/CSS editors provides user-maintained hosting using Tomcat 6 / CentOS 5.Opticalhost - Solucoes para o seu site: Opticalhost - Soluçoes para o seu site provides webhosting with JSP/Servlets support using Tomcat.Oxxus.net Tomcat Hosting: Oxxus.net Tomcat Hosting - Offers latest Tomcat 6 hosting services on private JVM.Merkaweb: Merkaweb - Spanish based web hosting company supporting Tomcat in all cPanel web hosting plans.Apache Tomcat Hosting: Apache Tomcat Hosting - India based web hosting company supporting Tomcat in all cPanel web hosting plans.Pickaweb: Pickaweb - Supporting Tomcat in all of our web hosting plans. 24x7 Support via chat, phone & email.Revion.com Oracle and Tomcat Web Hosting: Revion.com Oracle and Tomcat Web Hosting - Revion.com is a premier provider of Oracle hosting, Tomcat, Jboss, website, database, and application hostingRimuHosting.com VPS with Tomcat Web Hosting:RimuHosting.com VPS with Tomcat Web Hosting - provides a VPS and support for you to host your Tomcat the way you want.Rosehosting.com:RoseHosting.com - specializes in high quality managed Tomcat Hosting.RoseHosting.com has been in service since 2001 providing reliable Tomcat Linux VPS hosting solutions to individuals and businesses.RSHosting.co.uk UK Tomcat Web Hosting: RSHosting.co.uk UK Tomcat Web Hosting - provides Tomcat Web Hosting on our Linux and Windows servers in UK.RSHosting.com US based Tomcat Web Hosting: RSHosting.com US based Tomcat Web Hosting - provides Tomcat Web Hosting on our Linux and Windows servers in US datacenter.Starhost - Hospedagem de Sites: Starhost - Hospedagem de Sites Brazilian webhost provides JSP/Servlets support using Tomcat 5.0, 5.5 and 6.0.Hospedagem de Sites STUDIOSERVER: Hospedagem de Sites STUDIOSERVER provides webhosting with support to JSP/Servlets by using Tomcat.hospedagem de sites: hospedagem de sites Hospedagem de sites -TeHospedo.WebAppCabaret:WebAppCabaretis the oldest JAVA Web hosting provider. Features include one-click installation of many versions of Tomcat.WebHostUK LTD:  ""UK Web Hosting""WebHostUK LTD - Proud to be Apache Tomcat UK Web Hosting provider.Web Hosting US:  ""US Web Hosting"" Web Hosting US - Proud to be one of the best Apache Tomcat US Web Hosting provider.Webhostinguk.com: Web Hosting UK - Tomcat Hosting offers on reliable & fast servers at WebhostingUK Limited.Zaidsoft.net: Zaidsoft offers JSP hosting using Tomcat. All of Zaidsoft web based software applications including Zaidsoft iMLM are powered by Tomcat as JSP/Servlet container.",../data/confluence_exports/TOMCAT/PoweredBy_103099576.html
Apache Tomcat : PoweredBy Surveys and Other Evidence of Usage,"JBoss Infrastructure Survey:A surveyof infrastructure tools, adoption, mindshare, and more from JBoss finds Tomcat again among the leading products.O'Reilly On Java:Over 86%of respondents in O'Reilly's On Java 2004 Survey.TheServerSide.com:Tomcat is among the market leadersin this survey byTheServerSide.com.",../data/confluence_exports/TOMCAT/PoweredBy_103099576.html
Apache Tomcat : PoweredBy More Stuff,ASlashDotdiscussion about using Tomcat in production.Anarticlefrom The Value Manager for IT Insights on Tomcat's value.Dan Hansen wrotean articleforJavaWorldin 2004 showing how to use Tomcat to put together a build system in two days.Get Reviews for the Hosts offering Tomcat Hosting on Shared Servers –Web Hosting Reviews,../data/confluence_exports/TOMCAT/PoweredBy_103099576.html
"Apache Tomcat : Summit-na-2014 When: Friday April 11, 2014",Follows on fromApacheCon NA 2014,../data/confluence_exports/TOMCAT/Summit-na-2014_103100259.html
Apache Tomcat : Summit-na-2014 Registration,Via theApacheCon NA 2014site.,../data/confluence_exports/TOMCAT/Summit-na-2014_103100259.html
Apache Tomcat : Summit-na-2014 Scedule,08.30 Breakfast09.00 Introduction09.15 Agree agenda09.30 Topic discussions (see below)10.30 Morning break11.00 Resume topic discussions12.30 Lunch13.30 Resume topic discussions15.00 Afternoon break15.30 Resume topic discussions17.00 Close,../data/confluence_exports/TOMCAT/Summit-na-2014_103100259.html
Apache Tomcat : Summit-na-2014 Catering,Thanks to kind sponsorship byPivotalthe following food and beverages will be provided:,../data/confluence_exports/TOMCAT/Summit-na-2014_103100259.html
Apache Tomcat : Summit-na-2014 All Day,Freshly brewed regular & decaffeinated starbucks serena organic blend® coffeetazo® hot teaassorted coca-cola® productsbottle waters,../data/confluence_exports/TOMCAT/Summit-na-2014_103100259.html
Apache Tomcat : Summit-na-2014 Breakfast,Chilled fruit juicesPersonal yogurtGranolaHome-style muffinsHearty bagelsCream cheeseJamsSweet butter,../data/confluence_exports/TOMCAT/Summit-na-2014_103100259.html
Apache Tomcat : Summit-na-2014 Morning break,Whole fresh fruitNatural granola bars,../data/confluence_exports/TOMCAT/Summit-na-2014_103100259.html
Apache Tomcat : Summit-na-2014 Afternoon break,Home made cookiesAssorted cupcakes,../data/confluence_exports/TOMCAT/Summit-na-2014_103100259.html
Apache Tomcat : Summit-na-2014 Topics,"The topics to be discussed will be finalized by the attendees on the day. The provisional list of topics is:  Road map for future development prior to the next round of JavaEE specificationsAJP support for HTTP upgradeMake JSR 356 Java WebSocket 1.0 server implementation depend on Tomcat internalsNo longer container neutralSimpler codeBetter performanceCookiesTomcat 8 stabilityTCK statusRefactoring request/reponse recycling so Coyote and Catalina request and response and recycled at the same timeAdd full version information at the start of the logReview current enhancement requests in BugZillaresurrectJuiceto provide a OpenSSL provider for tomcat (something to have in tc-native)Change the way Apache Commons DBCP 2 and Pool 2 are consumed (svn copy and then merge subsequent changes)Add support for JSR196 JASPICmavenization of buildArquillian testsAdditions to authentication/realm API to improve extension capabilities (e.g. support bcrypt, pbkdf2, etc.)Monitoring of Tomcat Cluster(Tribes) by mbean.Improvement of  Cluster Deployer(FarmWarDeployer).When adding new cluster membe, synchronize the war that is deployed on the master node.Add support for parallel deployment.Please add your topics here...",../data/confluence_exports/TOMCAT/Summit-na-2014_103100259.html
Apache Tomcat : TomcatGridDesign Directory structure,"Tentative directory structures are desbribed below for the Manager installations, and for the Agents.  Maybe we could merge both the Manager and Agent structure so all machines could run the CLI and Web Managers by definition.",../data/confluence_exports/TOMCAT/TomcatGridDesign_103100543.html
Apache Tomcat : TomcatGridDesign Manager,"The Manager directory structure contains the managers applications, the configuration, and all the deployables:  <MANAGER_HOME>
  +- bin                           : Grid executables
  | +- grid.sh                     : CLI Manager script (can also start/stop the Web Manager)
  | +- grid.bat                    : CLI Manager script (can also start/stop the Web Manager)
  | +- grid-<version>.jar          : CLI Manager library
  | +- grid-<version>.war          : Web Manager application
  +- conf                          : Configurations dir
  | +- grid.xml                    : Grid configuration file
  +- deployables                   : Contains installables and releases files
    +- agent                       : Agent package (installable)
    | +- agent-<version>.tgz       : Agent executables
    |   +- agent.sh                : Agent script
    |   +- agent.bat               : Agent script
    |   +- agent-<version>.jar     : Agent library
    +- layouts                     : Layouts (Tomcat installables) dir
    | +- tomcat-8.0.13.tgz         : A Tomcat layout (installable)
    | +- tomcat-8.0.13-core.tgz    : A Tomcat layout (installable)
    | +- tomcat-8.0.21-core.tgz    : A Tomcat layout (installable)
    +- libraries                   : Uploaded libraries (ready to install)
    | +- ojdbc6.jar                : A library
    | +- apache-activemq-4.1.0.jar : A library
    +- hooks                       : Stores all hooks scripts
    +- releases                    : Stores the releases
      +- <version-id1>             : A release version (autogenerated or specified)
      | +- <application1>          : A war application alias
      | | +- <app1-file>.war       : War application (file or dir)
      | +- <application2>          : A war application alias
      |   +- <app2-file>.war       : War application (file or dir)
      +- <version-id2>             : A release version (autogenerated or specified)
        +- <application1>          : A war application alias
        | +- <app1-file>.war       : War application (file or dir)
        +- <application2>          : A war application alias
          +- <app2-file>.war       : War application (file or dir)",../data/confluence_exports/TOMCAT/TomcatGridDesign_103100543.html
Apache Tomcat : TomcatGridDesign Agent,"The Agent directory structure contains the agent application, the grid and agent configurations, all deployables, and all the local Tomcat instances:  <AGENT_HOME>
  +- bin                           : Agent executables
  | +- agent.sh                    : Agent script
  | +- agent.bat                   : Agent script
  | +- agent-<version>.jar         : Agent library
  +- conf                          : Configurations dir
  | +- grid.xml                    : Grid configuration file
  | +- agent.xml                   : Agent data file (includes agent id, agent_home, etc.)
  +- deployables...                : Replicated copy of the ""deployables"" dir of the Manager
  +- instances                     : Contains all the local Tomcat instances
    +- <tomcat-1>                  : Tomcat installation for the instance
    +- <tomcat-2>                  : Tomcat installation for the instance
    +- <tomcat-N>                  : Tomcat installation for the instance",../data/confluence_exports/TOMCAT/TomcatGridDesign_103100543.html
Apache Tomcat : TomcatGridDesign Configuration files,"Two configuration files are envisioned. All extra configuration, such as libraries, installables, web applications, hooks, etc are referenced on the configuration file but are stored in the standard file system locations as described above.",../data/confluence_exports/TOMCAT/TomcatGridDesign_103100543.html
Apache Tomcat : TomcatGridDesign Grid configuration file,"The following example grid.xml configuration file, the main configuration file considers a Tomcat Grid with four instances in two machines:  terminus(machine):tomcat101(tomcat instance): runs thecustomer facingwar.tomcat102(tomcat instance): runs thecustomer carewar (lighter config).anacreon(machine):tomcat201(tomcat instance): runs thecustomer facingwar.tomcat202(tomcat instance): runs thecustomer carewar (lighter config), and thebackendwar.  As an overview, the configuration file shown below includes all machines and all Tomcat instances for each one. Particularly each Tomcat instance specifies:  Its name.The Tomcat layout (core Tomcat installable) to use.All additions to the layout (libraries, drivers, etc.)The full JVM profile (java options) to run the Tomcat instance.The full Tomcat server profile (aka server.xml) to use.All the web applications to deploy, including their context configurations, and virtual hosts.  This information covers all the aspects needed to run each instance.  Considering the profiles are usually shared between instances (the customer facing profiles are probably identical for all customer facing instances), they are defined only once. Each instance references them, so they can be easily tuned on a single place instead of being copied all over the place.  <?xml version=""1.0""?>

<!DOCTYPE tomcat-grid SYSTEM ""http://tomcat.apache.org/tomcat/grid/grid.dtd"">

<tomcat-grid name=""AMT Benefits"" environment=""prod""
  manager-port=""6001"">

  <layouts>
    <layout name=""core"" package=""apache-tomcat-7.0.42-core.tar.gz"" />
    <layout name=""realms"" package=""apache-tomcat-7.0.42-realms.tar.gz"" />
  </layouts>

  <libraries>
    <libraries name=""Oracle 6 JDBC driver"" package=""ojdbc6.jar"" />
    <libraries name=""ActiveMQ 4.1 driver"" package=""apache-activemq-4.1.0-incubator.jar"" />
  </libraries>

  <application-configurations>

    <application-configuration name=""customer-facing"">
    <![CDATA[
      <Context>
        <Manager pathname="""" />
        <Parameter name=""UPLOAD_SHARED_MOUNT_PATH"" value=""/shared/brdsmnt071/attachments""
          override=""true"" />
        <Parameter name=""LOG_FILE""
          value=""/shared/brdsmnt071/logs/${service.name}.log"" override=""true"" />
        <Environment name=""maxAttachmentSize"" value=""20000000""
          type=""java.lang.Integer"" override=""false"" />
        <Resource name=""jdbc/customerDS"" auth=""Container""
          type=""javax.sql.DataSource"" maxActive=""100"" maxIdle=""30""
          maxWait=""10000"" username=""benefits"" password=""benefits""
          driverClassName=""oracle.jdbc.driver.OracleDriver""
          url=""jdbc:oracle:thin:@10.14.121.60:1521:MFUNDS4"" />
      </Context>
      ]]>
    </application-configuration>

    <application-configuration name=""customer-care"">
      <![CDATA[
      <Context>
        <Manager pathname="""" />
        <Parameter name=""UPLOAD_SHARED_MOUNT_PATH"" value=""/shared/brdsmnt071/attachments""
          override=""true"" />
        <Parameter name=""LOG_FILE""
          value=""/shared/brdsmnt071/logs/${service.name}.log"" override=""true"" />
        <Environment name=""maxAttachmentSize"" value=""20000000""
          type=""java.lang.Integer"" override=""false"" />
        <Resource name=""jdbc/customerDS"" auth=""Container""
          type=""javax.sql.DataSource"" maxActive=""100"" maxIdle=""30""
          maxWait=""10000"" username=""benefits"" password=""benefits""
          driverClassName=""oracle.jdbc.driver.OracleDriver""
          url=""jdbc:oracle:thin:@10.14.121.60:1521:MFUNDS4"" />
      </Context>
      ]]>
    </application-configuration>

    <application-configuration name=""backend"">
      <![CDATA[
      <Context>
        <Manager pathname="""" />
        <Parameter name=""UPLOAD_SHARED_MOUNT_PATH"" value=""/shared/brdsmnt071/attachments""
          override=""true"" />
        <Environment name=""maxAttachmentSize"" value=""20000000""
          type=""java.lang.Integer"" override=""false"" />
        <Resource name=""jdbc/personnelDS"" auth=""Container""
          type=""javax.sql.DataSource"" maxActive=""100"" maxIdle=""30""
          maxWait=""10000"" username=""benefits"" password=""benefits""
          driverClassName=""oracle.jdbc.driver.OracleDriver"" url=""jdbc:oracle:thin:@10.14.121.60:1521:MFUNDS4"" />
        <Resource name=""jms/extWiresQueue"" auth=""Container""
          type=""org.apache.activemq.command.ActiveMQQueue"" description=""External Wires Queue""
          factory=""org.apache.activemq.jndi.JNDIReferenceFactory""
          physicalName=""EXTWIREQ"" />
      </Context>
      ]]>
    </application-configuration>

  </application-configurations>

  <applications>

    <application name=""site"" package=""benefits.war""
      standard-source=""/work/dists/benefits.war"">
    </application>

    <application name=""backend"" package=""benefits-aide.war""
      standard-source=""/work/dists/benefits-aide.war"">
    </application>

  </applications>

  <java-profiles>

    <java-profile name=""site-face"">
      <![CDATA[
      -Xms1024m -Xmx2048m
      -XX:PermSize=192m -XX:MaxPermSize=256m
      -XX:+DisableExplicitGC
      -XX:+PrintGCDetails
      -XX:+PrintGCTimeStamps
      -XX:+HeapDumpOnOutOfMemoryError
      -Dsun.rmi.dgc.server.gcInterval=3600000
      -Dsun.lang.ClassLoader.allowArraySyntax=true
      -Djava.net.preferIPv4Stack=true
      -Dlog.file=$GRID_HOME/logs/${service.name}.log
      ]]>
    </java-profile>

    <java-profile name=""internal"">
      <![CDATA[
      -Xms512m -Xmx768m
      -XX:PermSize=128m -XX:MaxPermSize=192m
      -XX:+DisableExplicitGC
      -XX:+PrintGCDetails
      -XX:+PrintGCTimeStamps
      -XX:+HeapDumpOnOutOfMemoryError
      -Dsun.rmi.dgc.server.gcInterval=3600000
      -Dsun.lang.ClassLoader.allowArraySyntax=true
      -Djava.net.preferIPv4Stack=true
      -Dlog.file=$GRID_HOME/logs/${service.name}.log
      ]]>
    </java-profile>

    <java-profile name=""processes"">
      <![CDATA[
      -Xms1024m -Xmx1024m
      -XX:PermSize=128m -XX:MaxPermSize=192m
      -XX:+DisableExplicitGC
      -XX:+PrintGCDetails
      -XX:+PrintGCTimeStamps
      -XX:+HeapDumpOnOutOfMemoryError
      -Dsun.rmi.dgc.server.gcInterval=3600000
      -Dsun.lang.ClassLoader.allowArraySyntax=true
      -Djava.net.preferIPv4Stack=true
      -Dlog.file=$GRID_HOME/logs/${service.name}.log
      ]]>
    </java-profile>

  </java-profiles>

  <server-profiles>

    <server-profile name=""web"">
      <![CDATA[
      <Server port=""${server.port}"" shutdown=""SHUTDOWN"">
        <Listener className=""org.apache.catalina.core.AprLifecycleListener"" SSLEngine=""on"" />
        <Listener className=""org.apache.catalina.core.JasperListener"" />
        <Listener className=""org.apache.catalina.core.JreMemoryLeakPreventionListener"" />
        <Listener className=""org.apache.catalina.mbeans.GlobalResourcesLifecycleListener"" />
        <Listener className=""org.apache.catalina.core.ThreadLocalLeakPreventionListener"" />
        <GlobalNamingResources>
          <Resource name=""UserDatabase"" auth=""Container""
            type=""org.apache.catalina.UserDatabase"" description=""User database that can be updated and saved""
            factory=""org.apache.catalina.users.MemoryUserDatabaseFactory""
            pathname=""conf/tomcat-users.xml"" />
        </GlobalNamingResources>
        <Service name=""Catalina"">
          <Connector name=""http"" executor=""tomcatThreadPool""
            port=""${http.port}"" protocol=""HTTP/1.1"" connectionTimeout=""20000"" />
          <Engine name=""Catalina"" defaultHost=""localhost"">
            <Realm className=""org.apache.catalina.realm.LockOutRealm"">
              <Realm className=""org.apache.catalina.realm.UserDatabaseRealm""
                resourceName=""UserDatabase"" />
            </Realm>
            <Host name=""localhost"" appBase=""webapps"" unpackWARs=""true"" autoDeploy=""true"">
              <Valve className=""org.apache.catalina.valves.AccessLogValve""
                directory=""logs"" prefix=""${service.name}-localhost_access_log.""
                suffix="".txt"" pattern=""%h %l %u %t &quot;%r&quot; %s %b"" />
            </Host>
          </Engine>
        </Service>
      </Server>
      ]]>
    </server-profile>

    <server-profile name=""auxiliary"">
      <![CDATA[
      <Server port=""${server.port}"" shutdown=""SHUTDOWN"">
        <Listener className=""org.apache.catalina.core.AprLifecycleListener"" SSLEngine=""on"" />
        <Listener className=""org.apache.catalina.core.JasperListener"" />
        <Listener className=""org.apache.catalina.core.JreMemoryLeakPreventionListener"" />
        <Listener className=""org.apache.catalina.mbeans.GlobalResourcesLifecycleListener"" />
        <Listener className=""org.apache.catalina.core.ThreadLocalLeakPreventionListener"" />
        <GlobalNamingResources>
          <Resource name=""UserDatabase"" auth=""Container""
            type=""org.apache.catalina.UserDatabase"" description=""User database that can be updated and saved""
            factory=""org.apache.catalina.users.MemoryUserDatabaseFactory""
            pathname=""conf/tomcat-users.xml"" />
        </GlobalNamingResources>
        <Service name=""Catalina"">
          <Connector name=""http"" executor=""tomcatThreadPool""
            port=""${http.port}"" protocol=""HTTP/1.1"" connectionTimeout=""20000"" />
          <Engine name=""Catalina"" defaultHost=""localhost"">
            <Realm className=""org.apache.catalina.realm.LockOutRealm"">
              <Realm className=""org.apache.catalina.realm.UserDatabaseRealm""
                resourceName=""UserDatabase"" />
            </Realm>
            <Host name=""localhost"" appBase=""webapps"" unpackWARs=""true"" autoDeploy=""true"">
              <Valve className=""org.apache.catalina.valves.AccessLogValve""
                directory=""logs"" prefix=""${service.name}-localhost_access_log.""
                suffix="".txt"" pattern=""%h %l %u %t &quot;%r&quot; %s %b"" />
            </Host>
          </Engine>
        </Service>
      </Server>
      ]]>
    </server-profile>

  </server-profiles>

  <machines>

    <machine id=""1"" name=""Box #1 - Terminus"" host=""10.208.14.91""
      agent-port=""6060"" reported-hostname=""terminus"">

      <tomcat name=""tomcat101"" layout=""core"">
        <addition library=""Oracle 6 JDBC driver"" />
        <deployment application=""site"" engine=""Catalina""
          virtual-host=""localhost"" appBase=""webapps"" configuration=""customer-facing"" />
        <java-options profile=""site-face"" />
        <server-configuration profile=""web"">
          <parameter name=""server.port"" value=""8005"" />
          <parameter name=""http.port"" value=""8080"" />
        </server-configuration>
      </tomcat>

      <tomcat name=""tomcat102"" layout=""core"">
        <addition library=""Oracle 6 JDBC driver"" />
        <deployment application=""site"" engine=""Catalina""
          virtual-host=""localhost"" appBase=""webapps"" configuration=""customer-care"" />
        <java-options profile=""internal"" />
        <server-configuration profile=""web"">
          <parameter name=""server.port"" value=""8105"" />
          <parameter name=""http.port"" value=""8180"" />
        </server-configuration>
      </tomcat>

    </machine>

    <machine id=""2"" name=""Box #2 - Anacreon"" host=""10.208.14.92""
      agent-port=""6060"" reported-hostname=""anacreon"">

      <tomcat name=""tomcat201"" layout=""core"">
        <addition library=""Oracle 6 JDBC driver"" />
        <deployment application=""site"" engine=""Catalina""
          virtual-host=""localhost"" appBase=""webapps"" configuration=""customer-facing"" />
        <java-options profile=""site-face"" />
        <server-configuration profile=""web"">
          <parameter name=""server.port"" value=""8005"" />
          <parameter name=""http.port"" value=""8080"" />
        </server-configuration>
      </tomcat>

      <tomcat name=""tomcat202"" layout=""realms"">
        <addition library=""Oracle 6 JDBC driver"" />
        <addition library=""ActiveMQ 4.1 driver"" />
        <deployment application=""site"" engine=""Catalina""
          virtual-host=""localhost"" appBase=""webapps"" configuration=""customer-care"" />
        <deployment application=""backend"" engine=""Catalina""
          virtual-host=""localhost"" appBase=""webapps"" configuration=""backend"" />
        <java-options profile=""processes"" />
        <server-configuration profile=""auxiliary"">
          <parameter name=""server.port"" value=""8105"" />
          <parameter name=""http.port"" value=""8280"" />
        </server-configuration>
      </tomcat>

    </machine>

  </machines>

</tomcat-grid>",../data/confluence_exports/TOMCAT/TomcatGridDesign_103100543.html
Apache Tomcat : TomcatGridDesign Agent configuration file,"The Agent configuration file contains minimal information needed to identify the local configuration from the general grid configuration file.  <?xml version=""1.0""?>

<!DOCTYPE tomcat-grid-agent SYSTEM ""http://tomcat.apache.org/tomcat/grid/agent.dtd"">

<tomcat-grid-agent id=""1"">
</tomcat-grid-agent>",../data/confluence_exports/TOMCAT/TomcatGridDesign_103100543.html
Apache Tomcat : Database Preface,"This page is to discuss database error and DBCP. Until more content can appear here, here are some links to the mail archives to perform some searches: databasedatasourceoraclemysqldbpool Other Links of interest: JNDI Datasource HOW-TO",../data/confluence_exports/TOMCAT/Database_103098844.html
Apache Tomcat : Database Questions,Is it safe to use JDBC-ODBC bridge in production environment?How to use Orache thin driver?How to use JDBC 3 driver with Tomcat 8?,../data/confluence_exports/TOMCAT/Database_103098844.html
Apache Tomcat : Database Is it safe to use JDBC-ODBC bridge in production environment?,"No, do not use JDBC-ODBC bridge bundled with Sun's JDK with Tomcat. It was never meant for a production server environment. If you ask a question about it, everyone will tell you to not use it. If you do need to use ODBC, there are 3rd party drivers which do a pretty good job at being thread safe.",../data/confluence_exports/TOMCAT/Database_103098844.html
Apache Tomcat : Database How to use Orache thin driver?,"If you use the Oracle thin driver, be sure to rename it to a jar file from a zip file. Tomcat only auto-magically loads files ending in .jar placed in a lib directory. It ignores all other file extensions.",../data/confluence_exports/TOMCAT/Database_103098844.html
Apache Tomcat : Database How to use JDBC 3 driver with Tomcat 8 / DBCP 2?,"One of the connection pool implementations in Tomcat 8 is DBCP 2, and DBCP 2 calls Connection.isValid(int) method when no validationQuery is specified. IsValid(int) method is introduced with JDBC 4. If you must use JDBC 3 driver with Tomcat 8 / DBCP 2, make sure that you specifyvalidationQueryattribute in pool configuration.",../data/confluence_exports/TOMCAT/Database_103098844.html
Apache Tomcat : http workshop 2019 H2 - the important bits,Implementations have max stream values that range from 30 to 256 with most using 100. We currently use 200. Action:Consider reducing default max streams to 100. Done Implementations have max header list sizes that range from 16kB to 4GB. Most use 16kB or 32kB. We use 4GB. Action:Consider reducing default max header list size.Done Other values were consistent with typical implementations. Connection coalescence. Action: Consider implementing RFC 8336 Action: Review HTTP/2 extension for secondary certificates,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 Signed Exchanges,Could be handled with a dedicated Servlet.  Possibly the Default Servlet in a similar manner to compression. Might need changes to mapping.  Or at least how users think about mapping.  One way to route correctly would be to have the default host handle all the signed exchanges. No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 Traffic Analysis,No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 Embedding decryption keys in PCAP,OpenSSL has a callback for writing the TLS key log for debug purposes. Action: We should add support for this to Tomcat-Native.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 Generic overlay networks,No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 Intercepting QUIC,Testing QUIC / HTTP/3 implementations is going to be tricky. No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 HTTP State Tokens,Use an opaque token on the client to reference state held on the server. Sort of equivalent to a cookie that only holds a session ID. Would require servers and proxies to maintain much more state. There are several new(ish) cookie features we aren't using and should consider. Seehttps://scotthelme.co.uk/tough-cookies/ Action: Review new(ish) cookie features,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 Prefer-Push,Server push has minimal benefit for browsers. Neater solution to transclusion. Likely to require application level implementation since you need to know which resources (if any) to push. No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 WebSocket,I was surprised at the level of dislike of WebSocket. Most people in the room wanted to see it go. No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 hx URIs,No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 Extending h2 for bi-directional messaging,This is a potential replacement for WebSocket Proxy may interfere with HTTP/2 push Would require Tomcat changes to implement. No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 Partial POST replay,Handles failover for POST requests that take longer than the standard drain time. Would require Tomcat changes to implement. No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 HTTP retries,Aims to provide better signalling. Would require Tomcat changes to implement. May require Servlet spec changes. May need to relax HTTP spec. No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 Sec-Fetch-*,Aim to protect authenticated client session Should be possible to implement in an application. Not sure if a general (e.g. filter) implementation would be possible. No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 Origin Policy,"Aims to headers that apply site-wide from every response. e.g. CSP, CORS (4k for Twitter!) Would be static files in a known location for Tomcat. Could also be configuration driven and served by a standard servlet. No action required from us at this point.",../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 Structured Headers,Actively being worked on in the HTTP working group Would require a new header parser for Tomcat. No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 Web-compatible header values,Standard approach to header parsing. No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 Client Hints,Only on secure connections. Reduce client fingerprinting. Replace UA and accept-* headers Could make use of / be replaced by origin policy May impact on Tomcat's current content negotiation. No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 Partially Reliable HTTP,Used video as an example. If some TCP frames are lost it isn't always worth trying to recover them. Jitter is worse than lost data. Would need Servlet API changes to support this. No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 SEARCH method,c.f. POST retries Many POSTs are for searches and could be cached and could be safely re-tried Is a new method (SEARCH) required. Depends on being able to normalize the search query. No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 VTest,Possible platform for common test cases Little varnish focused at the moment. No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 CDNs,Proxy status header is being discussed. There is a caching test suite we could use. Action: Run the caching test suite against Tomcat,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 EOS detection in H2,"Consider the following sequence: client req then EOSserver response then EOSserver shutsdownclient sends window updateTCP resetreponse truncated Send a ping and wait for ACK before shutting down server. Half-close didn't cover all use cases. Implies TCP reset packages ""jump"" the queue. Action: Check Tomcat's close behaviour",../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 Response sources,pre-fetch - new document pre-load - resource in document Need to be aware of browser behaviour for push.  See links in day2 notes for additional reading. No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 Early hints,103 status code RFC 8297 Implementation would require Tomcat changes. No action required from us at this point.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 HTTP/2 Priority,"Includes the browser's view of prioritisation. Audio and video add a time factor that isn't currently taken into account. Action: Test Tomcat with the prioritisation test page The larger the send buffer, the harder it is to do prioritisation. Action: Review Tomcat's buffering. Committed buffers make (re-)prioritisation harder.",../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 QUIC & HTTP/3,Consensus among those I spoke to was that Tomcat should wait as things are still in flux. It may be possible to use an external QUIC implementation (like we do OpenSSL). Action: Investigate TCP fast open. Can Tomcat make use of it? What is involved? Action: Investigate TLS 1.3 early data. Can Tomcat make use of it? What is involved?,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : http workshop 2019 Alt-Src,Discussions continue about different channels for passing this to clients. It will be needed to HTTP/3. No action needed until we start HTTP/3 implementation.,../data/confluence_exports/TOMCAT/http-workshop-2019_110694819.html
Apache Tomcat : TomcatCon Finances,Considered various options to reduce upfront costs to zero for the first event:http://markmail.org/thread/b4lii3tkgiv5epgp Opened a separate (personal) bank account to use to receive funds from ticket sales and to reimburse expenses. While some items were paid for in advance (e.g. materials for name badges) no expenditure was committed to unless there was sufficient projected income fromconfirmedticket sales to cover. i.e. at no point was anyone at risk of financial loss. Tracked all expenses and income for the event and reported it publicly. Reporting was after the event. Events aim to break even with surplus from previous event used to pay upfront costs for the next. Sponsorship allows larger / nicer / better located venues and/or funding travel for additional speakers.,../data/confluence_exports/TOMCAT/TomcatCon_74680877.html
Apache Tomcat : TomcatCon Ticket Management,Ticket sales via EventBrite. Ticket price set at £50. High enough to discourage no shows but low enough that attendees should find it easy to get the expense approved.,../data/confluence_exports/TOMCAT/TomcatCon_74680877.html
Apache Tomcat : TomcatCon Session topics,The topics for the first event were selected based on a combination of know popular topics from past events and speaker choice. Also had an ask me anything session at the end of the day. Received useful community feedback from this session on possible future features.,../data/confluence_exports/TOMCAT/TomcatCon_74680877.html
Apache Tomcat : TomcatCon Venue,Venue for first event handled entirely by sponsor. Looking at Premier Inn meeting rooms as possible venues for future events.,../data/confluence_exports/TOMCAT/TomcatCon_74680877.html
Apache Tomcat : TomcatCon Recording,"Purchased a USB microphone and used this, combined with screen recording software on speaker's laptops to record session where possible.",../data/confluence_exports/TOMCAT/TomcatCon_74680877.html
Apache Tomcat : SummerOfCode2010 Important Notes,The application process has not yet started. This page is to record possible projects assuming the ASF and the Tomcat project does participate in this year's summer of code. The actual list of projects will be determined as the application process progresses.  Anyone is free to add ideas to this list. If you are not a Tomcat committer please do not add a committer as a potential mentor for a project unless they have agreed to do so.,../data/confluence_exports/TOMCAT/SummerOfCode2010_103100253.html
Apache Tomcat : SummerOfCode2010 Ideas,"TitleMentorJIRADescriptionJMXmarktCOMDEV-23Update the JMX descriptors to more accurately reflect the properties and methods of the underlying objects, taking particular care about which attributes should be read-only and which read-write. Then add additional functionality as necessary to enable a complete Tomcat instance to be configured entirely via JMX (ie start with a Server with no config and create everything via JMX).Security docsmarktCOMDEV-24Update the security pages etc to include svn revisions for each fix, update the release notes to include the CVE reference with the fix, update the svn logs to include the CVE reference. This will require a lot of cross-checking to ensure that the correct CVEs and svn references are added, particularly for the older vulnerabilities. This would include the 5.5.x, 6.0.x and 7.0.x code-bases.JSR196marktCOMDEV-25Provide aJSR196implementationSPDYjfclereCOMDEV-26Create Connector that supports the SPDY protocol (Seehttp://dev.chromium.org/spdy/spdy-whitepaper)   CategoryGSOC",../data/confluence_exports/TOMCAT/SummerOfCode2010_103100253.html
Apache Tomcat : ClusteringOverview General Thoughts and the Mailing List,"At each stage of the setup, I recommend testing before moving on to the next section. The mailing list is a good source of helpful information for when you have problems, provided that you do the following. Ask simple, focused questionsProvide all of the requested information (inline, since the mailing list strips most attachments)Try to format configuration files reasonablyRemove commentsBe aware of word wrapDon't use rich text / html text Be aware that people on the mailing list arevolunteers. Do not expect homework answers, complete web applications, or systems architecture from the list. Do expect to put in at least as much effort on solving the problem as the people helping you are. Also, there is a lot of misleading, incomplete, and just flat wrong information concerning Tomcat floating around on the Internet. You might get accurate information elsewhere, but from what I've seen this is not very likely. The authoritative source for information is always: https://tomcat.apache.org/",../data/confluence_exports/TOMCAT/ClusteringOverview_103098638.html
Apache Tomcat : ClusteringOverview Linux in General,"You'll find that Linux is a different beast than Windows (even Windows 7). In particular file permissions, file ownerships, and SELinux present quite a different security model than the typical Windows installation. It's best to be aware of this from the start.",../data/confluence_exports/TOMCAT/ClusteringOverview_103098638.html
Apache Tomcat : ClusteringOverview Purpose,"The first question to answer is what is the purpose of this setup? If you're running a test or production platform then set up a service account and install all Tomcats there. A service account is an unprivileged account used only for configuring and running Tomcat servers. Make this account inaccessible from the network, and give the account its own group. If you're setting up a development platform, then just create a directory and unpack multiple Tomcat servers in that directory. This will make dealing with permissions easier, which is an important consideration when integrating Tomcat servers with IDEs such as Eclipse or NetBeans.",../data/confluence_exports/TOMCAT/ClusteringOverview_103098638.html
Apache Tomcat : ClusteringOverview Apache HTTPD,"Rather than building your own Apache HTTPD, I recommend that you get and install the distribution package for Apache HTTPD. This will place files in line with the rest of your system, and it will also have a serviceable default configuration. On Fedora, the Apache HTTPD packages include: aprapr-utilapr-develapr-util-develhttpdhttpd-develhttpd-tools The -devel packages are very important, since you will be building mod_jk from source. Most package managers will pull in the correct dependencies, however you will have to specify the development packages separately.",../data/confluence_exports/TOMCAT/ClusteringOverview_103098638.html
Apache Tomcat : ClusteringOverview Java,"There are several versions and several ways to install Java on a Linux platform. While both OpenJDK and Oracle's Java are reported to work well with Tomcat, some people have reported problems when using GJC. Make sure you have a reasonable version of Java installed, and that it's used by default. Type: java -version
javac -version to make sure you get the version you expect. While the Tomcat startup scripts will do a good job at finding the desired Java, it's advantageous to set an environment variable to point to the desired JRE. In a bash shell, just type the following: export JRE_HOME=[where your JRE is installed] This will ensure that Tomcat uses the Java you want it to use. It's also a useful way to try new versions of Java.",../data/confluence_exports/TOMCAT/ClusteringOverview_103098638.html
Apache Tomcat : ClusteringOverview Tomcat,"As I've noted above if you're setting up a pure development environment it's just best to create a directory in your home directory and unpack a Tomcat there. For now, just start with one. If you have Java installed correctly, Tomcat comes ready to run out of the box. Just unpack it, cd to the bin directory, and run startup.sh. You should be able to browse to localhost:8080/ and see the Tomcat welcome page. Now stop the Tomcat server with shutdown.sh and set up the manager application. This involves editing the tomcat-users.xml file found in the conf directory. Instructions for editing that file (for Tomcat 7) can be found here: https://tomcat.apache.org/tomcat-7.0-doc/manager-howto.html#Configuring_Manager_Application_Access Once it's running and you can access the manager application, then associate the installation with your IDE (if you're setting up a development environment). Now you can develop, debug, test, and deploy from within your IDE without running into permission issues. Unlike Apache HTTPD, most people on the mailing list do NOT recommend that you use the Linux distribution packaged versions of Tomcat. In general people have found that these are much more difficult to work with than just getting a stock Tomcat fromhttps://tomcat.apache.org/",../data/confluence_exports/TOMCAT/ClusteringOverview_103098638.html
Apache Tomcat : ClusteringOverview mod_jk build,"While other people have had difficulty (search the mailing list) building mod_jk on various platforms, I've never had much trouble. The steps are quite simple, provided you have the appropriate packages installed on your system. Download the source fromhttps://tomcat.apache.org/download-connectors.cgiUnpack it into a directorycd to[tomcat-connectors-1.xx]/nativeReadBUILDING.txt FollowingBUILDING.txt: ./configure --with-apxs=/usr/sbin/apxsmakesu to rootcd back to where you weremake install This will put everything in the right place.",../data/confluence_exports/TOMCAT/ClusteringOverview_103098638.html
Apache Tomcat : ClusteringOverview mod_jk configuration,"Recent versions of mod_jk come with some very nice and well-commented examples. They can be found in[tomcat-connectors-1.xx]/conf. Read them, follow them, use them. The defaults have been chosen to work in most general use cases. In order to test mod_jk, map all of the examples (not just the *.jsp files). Restart Apache HTTPD, and then start Apache Tomcat. You should be able to browse to localhost/examples and get the Apache Tomcat examples. DoNOTproceed with clustering until you successfully connect one Apache HTTPD with one Tomcat, and execute the examples. Note that if you place all of your mappings in a uriworkermap.properties file, Apache HTTPD will reread this once per minute (by default). This is nice if you tend to add and delete Tomcat applications. It's probably not quite as nice for a production system (but I don't know what the overhead is).",../data/confluence_exports/TOMCAT/ClusteringOverview_103098638.html
Apache Tomcat : ClusteringOverview Clustering,"There are a lot of pieces to put together in order to get clustering to work. As noted in the introduction, this Wiki article just gives the lay of the land. Future articles will go into more detail.",../data/confluence_exports/TOMCAT/ClusteringOverview_103098638.html
Apache Tomcat : ClusteringOverview Linux,"You have to ensure that multicast is enabled, and that multicast routing is set up. If you run iptables, you may run into firewall issues, but on the same system probably not.",../data/confluence_exports/TOMCAT/ClusteringOverview_103098638.html
Apache Tomcat : ClusteringOverview mod_jk,Each Tomcat gets at least one worker. This worker must be configured to talk to a particular Tomcat on the correct AJP/1.3 port. There are some other constraints for load balancing workers that should be noted. In workers.properties: Each worker name in a cluster must be unique 2. The worker name must be the same as the jvmRoute attribute set in the Engine element of the target Tomcat 3. The worker name must be in a list of worker names for a loadbalancer The mapping in uriworkermap.properties should point to the new loadbalancer worker instead of an individual worker. See the following for documentation on worker name versus jvmRoute attribute: https://tomcat.apache.org/tomcat-7.0-doc/cluster-howto.html#Cluster_Basics See the following for mod_jk loadbalancer configuration: https://tomcat.apache.org/connectors-doc/reference/workers.html,../data/confluence_exports/TOMCAT/ClusteringOverview_103098638.html
Apache Tomcat : ClusteringOverview Another Tomcat,"While the easiest way to get started with clustering is just to unpack another copy of Tomcat in another directory, it's probably far more useful to use the concept of CATALINA_HOME and CATALINA_BASE. See RUNNING.txt in the Tomcat directory for details. However another Tomcat is installed, the following needs to be done. Change the SHUTDOWN port to not conflict with the first TomcatChange the HTTP/1.1 port to not conflict with the first Tomcat (useful for testing)Change the AJP/1.3 port to match that configured in workers.propertiesChange the jvmRoute attribute of the Engine element to match the correct worker nameConfigure the manager application (useful for testing)",../data/confluence_exports/TOMCAT/ClusteringOverview_103098638.html
Apache Tomcat : ClusteringOverview Tomcat clustering configuration,"The basic clustering documentation can be found here: https://tomcat.apache.org/tomcat-7.0-doc/cluster-howto.html It's long, involved, and needs to be read carefully. However, as a first pass the following will work. Make sure that each Tomcat has a unique jvmRoute in the Engine element (see above)Make sure that the jvmRoute matches the correct worker name (see above)Make sure that each Tomcat has a unique shutdown port (see above)Make sure that each Tomcat has a unique AJP/1.3 port (see above)Make sure that each AJP/1.3 port matches the correctly named worker in workers.properties (see above)Make sure that each HTTP/1.1 port is unique (nice for manager access)Copy the example configuration fromhttps://tomcat.apache.org/tomcat-7.0-doc/cluster-howto.htmlto inside the Host element in each server.xml, but omit (for now) the Deployer element. The cluster configuration can be placed inside the Host or Engine element. However, the Farm Deployer element will only work if the cluster configuration is inside the Host element.Start up your Tomcat servers. If everything is set up correctly, cluster notifications will appear in the logs. It's also useful to have the access log enabled for your Tomcat servers. You can then tell which Tomcat is receiving the request from Apache HTTPD.  This overview should point you in the right direction to get a two node cluster up and running. Again, start simply. Stock Apache HTTPD installation (and verify)Stock Apache Tomcat installation (and verify)mod_jk installation (and verify)Second Apache Tomcat installation (and verify)Cluster",../data/confluence_exports/TOMCAT/ClusteringOverview_103098638.html
Apache Tomcat : TomcatOnSolaris10 THE ENVIRONMENT,"The following assumes you have Java already installed. java is usually installed in /usr/java. This is a symbolic link from the actual installation in /usr/jdk/jdk1.6.0_[Update]                                                                             THE PROBLEM  Solaris comes with apache and tomcat preinstalled, just not in the locations you would expect unless your are familiar with Solaris. Solaris 10 comes with a new method for starting and stopping services on system startup, called SMF(Service Management Facility).  Software installation directory  /usr/apache/tomcat55  Configuration, logs, and deployed applications (webapps directory)  /var/apache/tomcat55  If you where to run multiple instances of tomcat your would want to duplicate the whole/var/apache/tomcat55directory, create new startup.sh shutdown.sh scripts setting a CATALINA_BASE environment variable pointing to your new instance.",../data/confluence_exports/TOMCAT/TomcatOnSolaris10_103100601.html
Apache Tomcat : TomcatOnSolaris10 Configuring SMF to start and stop tomcat,"Create file in the following location. (The location does not really matter but the convention says so.)  /var/svc/manifest/application/web/tomcat.xml  <?xml version=""1.0""?>
<!DOCTYPE service_bundle SYSTEM ""/usr/share/lib/xml/dtd/service_bundle.dtd.1"">
<!--
    tomcat_svr.xml : Tomcat service  manifest, Daniel Berg
-->

<service_bundle type='manifest' name='Tomcat55'>
<service name='application/web/tomcat' type='service' version='1'>

   <single_instance />

   <exec_method
      type='method'
      name='start'
      exec='/usr/apache/tomcat55/bin/startup.sh'
      timeout_seconds='30' />

   <exec_method
      type='method'
      name='stop'
      exec='/usr/apache/tomcat55/bin/shutdown.sh'
      timeout_seconds='30' />

   <instance name='default' enabled='false' />

   <stability value='Unstable' />

   <template>
      <common_name>
         <loctext xml:lang='C'>Apache Tomcat 5.5.27</loctext>
      </common_name>
      <documentation>
         <manpage title='tomcat' section='1' manpath='/usr/man' />
      </documentation>
   </template>

</service>
</service_bundle>  To import this file run  #svccfg import /var/svc/manifest/application/web/tomcat.xml",../data/confluence_exports/TOMCAT/TomcatOnSolaris10_103100601.html
Apache Tomcat : TomcatOnSolaris10 Starting and stopping tomcat,"To start tomcat you can now run  # svcadm enable tomcat  To stop tomcat your would run  # svcadm disable tomcat  To se the current status:  #- svcs -lp tomcat  Output is something like the following  fmri         svc:/application/web/tomcat:default
name         Apache Tomcat 5.5.27
enabled      true
state        online
next_state   none
state_time   Fri Feb 26 13:25:28 2010
logfile      /var/svc/log/application-web-tomcat:default.log
restarter    svc:/system/svc/restarter:default
contract_id  183
process      8303 /usr/jdk/jdk1.6.0_18/bin/sparcv9/java -Djava.util.logging.config.file=/var/apac  You can see the log output from SMF in the filereferences as logfile in the above output for any troubleshooting.  Running tomcat through SMF negates the need to run tomcat using nohup on Solaris.   CategoryFAQ",../data/confluence_exports/TOMCAT/TomcatOnSolaris10_103100601.html
Apache Tomcat : Building the isapi_redirect (mod_jk) binaries for Windows Building,"The isapi_redirector.dll requires no other external dependencies (it does depend on PCRE but this is included in the source). Obtain the tcnative source from one of: the win32-src.zip source bundle for the version you wish to build;https://svn.apache.org/repos/asf/tomcat/jk E.g.: To build the latest 1.2.x development build from trunk Obtain the source code: c:cd \svn co https://svn.apache.org/repos/asf/tomcat/jk/trunk/ tomcat-jk-1.2.xcd tomcat-jk-1.2.x\native\iis  Buildisapi_redirector.dll: c:\cmsc\setenv.bat /x86nmake -f Makefile.x86
 
 c:\cmsc\setenv.bat /x64
 nmake -f Makefile.amd64 Tomcat isapi_redirect DLLs may then be found in C:\tomcat-jk-1.2.x\native\iis\Release_[amd64|x86]. Construct the binary distributions set VER=1.2.43mkdir tomcat-connectors-%VER%-windows-i386-iiscopy ..\..\LICENSE tomcat-connectors-%VER%-windows-i386-iis\copy ..\..\NOTICE tomcat-connectors-%VER%-windows-i386-iis\copy README tomcat-connectors-%VER%-windows-i386-iis\copy Release_x86\isapi_redirect.dll tomcat-connectors-%VER%-windows-i386-iis\ mkdir tomcat-connectors-%VER%-windows-x86_64-iiscopy ..\..\LICENSE tomcat-connectors-%VER%-windows-x86_64-iis\copy ..\..\NOTICE tomcat-connectors-%VER%-windows-x86_64-iis\copy README tomcat-connectors-%VER%-windows-x86_64-iis\copy Release_amd64\isapi_redirect.dll tomcat-connectors-%VER%-windows-x86_64-iis\ mkdir tomcat-connectors-%VER%-windows-i386-symbolscopy ..\..\LICENSE tomcat-connectors-%VER%-windows-i386-symbolscopy ..\..\NOTICE tomcat-connectors-%VER%-windows-i386-symbolscopy Release_x86\isapi_redirect.pdb tomcat-connectors-%VER%-windows-i386-symbols\ mkdir tomcat-connectors-%VER%-windows-x86_64-symbolscopy ..\..\LICENSE tomcat-connectors-%VER%-windows-x86_64-symbolscopy ..\..\NOTICE tomcat-connectors-%VER%-windows-x86_64-symbolscopy Release_amd64\isapi_redirect.pdb tomcat-connectors-%VER%-windows-x86_64-symbols\ SET JAVA_HOME=C:\Java\adopt-8.0.242.09-x64set PATH=%PATH%;%JAVA_HOME%\bincd tomcat-connectors-%VER%-windows-i386-iisjar -cMf ..\tomcat-connectors-%VER%-windows-i386-iis.zip *cd ..\tomcat-connectors-%VER%-windows-x86_64-iisjar -cMf ..\tomcat-connectors-%VER%-windows-x86_64-iis.zip * cd ..\tomcat-connectors-%VER%-windows-i386-symbolsjar -cMf ..\tomcat-connectors-%VER%-windows-i386-symbols.zip *cd ..\tomcat-connectors-%VER%-windows-x86_64-symbolsjar -cMf ..\tomcat-connectors-%VER%-windows-x86_64-symbols.zip *  The Windows binary distributions may then be found in C:\tomcat-jk-1.2.x\native\iis These need to be signed and hashed before uploading for the release vote.",../data/confluence_exports/TOMCAT/65872535.html
Apache Tomcat : WebSocket 1.1 TCK Tomcat,"Set the following system properties org.apache.tomcat.websocket.DISABLE_BUILTIN_EXTENSIONS=trueorg.apache.tomcat.websocket.ALLOW_UNSUPPORTED_EXTENSIONS=trueorg.apache.tomcat.websocket.DEFAULT_PROCESS_PERIOD=0 Make the following changes to server.xml add backgroundProcessorDelay=""1"" to the <Engine ... > element",../data/confluence_exports/TOMCAT/WebSocket-1.1-TCK_103088446.html
Apache Tomcat : WebSocket 1.1 TCK Test Suite,Download latest promoted build http://download.eclipse.org/ee4j/jakartaee-tck/jakartaee8/promoted/websocket-tck-1.1.1.zip Extract to WEBSOCKET_TCK_HOME  Edit $WEBSOCKET_TCK_HOME/bin/ts.jte You'll need to set the following properties (adjust the paths and values for your environment) webServerHost=localhost webServerPort=8080 securedWebServicePort=8443 websocket.api=/home/mark/repos/asf-public/tomcat/trunk/output/build/lib/websocket-api.jar websocket.classes=/home/mark/repos/asf-public/tomcat/trunk/output/build/lib/tomcat-websocket.jar:/home/mark/repos/asf-public/tomcat/trunk/output/build/lib/servlet-api.jar:/home/mark/repos/asf-public/tomcat/trunk/output/build/lib/tomcat-util.jar:/home/mark/repos/asf-public/tomcat/trunk/output/build/lib/tomcat-api.jar:/home/mark/repos/asf-public/tomcat/trunk/output/build/bin/tomcat-juli.jarAdd the following to the command.testExecute property (to prevent entropy issues slowing the tests down)-Djava.security.egd=file:/dev/./urandom Do not reduce ws_wait below the default of 5s as it is likely to trigger test failures.  set JAVA_HOME Run ant gui Accept the defaults and then run the tests  A default 9.0.x build (as of yyyy-mm-dd) without any configuration triggers 20 test failures To be confirmed. Tomcat 9 was last tested with a nightly build of the WebSocket 1.1 TCK. It has yet to be tested with the 1.1.1 release of the TCK. 2 unclear specification 2 x URIhttps://github.com/eclipse-ee4j/websocket-api/issues/228 11 Faulty tests 1 xhttps://github.com/eclipse-ee4j/jakartaee-tck/issues/33(batching)2 xhttps://github.com/eclipse-ee4j/jakartaee-tck/issues/35(batching)8 xhttps://github.com/eclipse-ee4j/jakartaee-tck/issues/37(async concurrency) 5 Tests 'fixed' by appropriate system property configuration (see above) 4 x extensions (TCK assumes invalid extensions are ignored)1 x timeout related test expects more frequent expiration checks 2 Tests 'fixed' by appropriate server.xml configuration 2 x timeout related tests requiring even more frequent expiration checksStill see intermittent failures on these A further 19 test failures caused by Tomcat bugs have been fixed 4 x Deployment failure didn't undeploy all WebSockets4 x @PathParam not validated for correct type6 x Throw DeploymentException rather than IAE1 x Improve checking of multiple @OnMessage for the same type4 x exception in encoder leading (incorrectly) to empty message,../data/confluence_exports/TOMCAT/WebSocket-1.1-TCK_103088446.html
Apache Tomcat : AJP.next Goals,Consolidate enhancements to the AJP protocol in a central location.,../data/confluence_exports/TOMCAT/AJP.next_61320760.html
Apache Tomcat : Building Tomcat on MacOS Building Tomcat,"Building Tomcat itself is fairly straightforward. Simply download the source distribution of Tomcat and follow the instructions in the BUILDING.txt file bundled with the distribution.  Briefly, you'll need:  The Tomcat source tarball (the ZIP file is fine, but we're on UNIX, so the tarball is more natural)A Java Development Kit, available fromJava.netApache ant, available from theApache ant downloadspage  Once you have all that,  ant deploy  You may have to set yourJAVA_HOMEenvironment variable and/or specify the full path to yourantbinary.",../data/confluence_exports/TOMCAT/Building-Tomcat-on-MacOS_103098603.html
Apache Tomcat : Building Tomcat on MacOS Building libtcnative,"Buildinglibtcnativeit fairly straightforward as well, but you will need a number of prerequisites that are not terribly obvious as to how to get them.  Xcode command-line tools, available fromApple's developer tools download site. Make sure you get the proper version for your XCode version (if you have XCode already installed) and your OS version (10.x)Apache Portal Runtime (APR), available either directly from Apache (APR downloads) or by usinghttps://brew.sh/(brew install apr)1. (Optional) OpenSSL 1.1.1 (latest at the time of this writing), available either directly from OpenSSL (OpenSSL Downloads) or by usingbrew({{{brew install openssl1  Download and unpack the libtcnative sources:  $ tar xzf tomcat-native-x.y.x.tar.gz$ cd tomcat-native-x.y.z/native",../data/confluence_exports/TOMCAT/Building-Tomcat-on-MacOS_103098603.html
Apache Tomcat : Building Tomcat on MacOS Using OpenSSL,If using OpenSSL:                                                                             $ ./configure --with-ssl=[path to OpenSSL] --with-apr=[path to APR] --with-java-home=[your java home]                                                                             For example:  $ ./configure --with-ssl=/usr/local/Cellar/openssl@1.1/1.1.1 --with-apr=/usr/local/Cellar/apr/1.6.5 --with-java-home=/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home,../data/confluence_exports/TOMCAT/Building-Tomcat-on-MacOS_103098603.html
Apache Tomcat : Building Tomcat on MacOS Without OpenSSL (LibreSSL),"If not using OpenSSL:                                                                             ./configure --with-ssl=yes --with-apr=[path to APR] --with-java-home=[your java home]                                                                             ./configure --with-ssl=yes --with-apr=/usr/local/Cellar/apr/1.6.5 --with-java-home=/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home  In either case (OpenSSL or not), proceed with the build process:  make  Once this process has completed, your built libraries can be found in.libs/.",../data/confluence_exports/TOMCAT/Building-Tomcat-on-MacOS_103098603.html
Apache Tomcat : Building Tomcat on MacOS Installing libtcnative,"While you can setld.library.pathto include libraries from all over the place, I recommend that you copy everything into one place so you can easily find everything and it won't interfere with anything else on your system.  First, copy thelibtcnativebinaries from where they were built:  $ cp -aR tomcat-native-x.y.z/native/.libs/* apache-tomcat-x.y.z-src/output/build/bin/  Next, copy the APR libraries:  $ cp -aR $APR_HOME/libexec/lib/* apache-tomcat-x.y.z-src/output/build/bin/  Finally, if you are using a custom OpenSSL build, copy those libraries as well:  $ cp -aR $OPENSSL_HOME/lib/* apache-tomcat-x.y.z-src/output/build/bin/",../data/confluence_exports/TOMCAT/Building-Tomcat-on-MacOS_103098603.html
Apache Tomcat : Building Tomcat on MacOS Running Tomcat with libtcnative,"Tomcat (really Java) needs to know where to find these native libraries. We do that by setting thejava.library.pathenvironment variable for the JVM during startup. The easiest way to do this is by settingCATALINA_OPTSon startup. This can be done by adding this line tobin/setenv.sh:  export CATALINA_OPTS=""-Djava.library.path=$CATALINA_HOME/bin""  Then you can startup Tomcat as usual, either:  $ bin/startup.sh  or  $ bin/catalina.sh start",../data/confluence_exports/TOMCAT/Building-Tomcat-on-MacOS_103098603.html
Apache Tomcat : Developing About,This section of the FAQ discusses common questions related to Tomcat development.,../data/confluence_exports/TOMCAT/Developing_103098864.html
Apache Tomcat : Developing Questions,HackingHow do I start hacking Tomcat in Eclipse?DebuggingHow do I configure Tomcat to support remote debugging?How do I remotely debug Tomcat using Eclipse?How do I remotely debug Tomcat using NetBeans?OtherHow do I change the monitoring interval for modified resources and application reloading?Official Eclipse IDE Web Tools FAQ for Tomcat,../data/confluence_exports/TOMCAT/Developing_103098864.html
Apache Tomcat : Developing How do I start hacking Tomcat in Eclipse?,"Briefly: $ git clone https://github.com/apache/tomcat.git
  (or whatever branch you want: clearly, this would be better
  to do directly from within Eclipse but it's easier to describe
  as a command)

$ cd tomcat

$ echo ""base.path=/path/to/where/tomcat/can/put/its/3rd-party/libs"" > build.properties

$ ant ide-eclipse Then, in Eclipse, go to Preferences | Java/Build Path/Classpath Variables and set the following variables: ANT_HOME=path to your Ant install (where lib/ant.jar can be found)
TOMCAT_LIBS_BASE=[whatever you set base.path to above] If you look in (project root)/res/ide-support/eclipse/java-compiler-errors-warnings.txt, you'll see a set of compiler warnings and import organization rules that you will have to set up manually in your project. If you set those up properly, you are more likely to submit cleaner patches. A full explanation for Tomcat in Eclipse can be found here: Tomcat 9.0.x:https://tomcat.apache.org/tomcat-9.0-doc/building.html#Building_with_EclipseTomcat 8.5.x:https://tomcat.apache.org/tomcat-8.5-doc/building.html#Building_with_EclipseTomcat 7.0.x:https://tomcat.apache.org/tomcat-7.0-doc/building.html#Building_with_Eclipse",../data/confluence_exports/TOMCAT/Developing_103098864.html
Apache Tomcat : Developing How do I configure Tomcat to support remote debugging?,"The short answer is to add the following options when the JVM is started: -Xdebug -Xrunjdwp:transport=dt_socket,address=8000,server=y,suspend=n There are a number of ways you can do this depending on how you normally start Tomcat: If you are usingshell scriptsto start Tomcat, start it with the following command:catalina jpda startIt will start Tomcat so that a remote debugger can be connected to port 8000.The above mentioned options can be provided by setting certain environment variables. See the comments at the top ofcatalina.shor.batfile for details.For example, the port number and JPDA transport implementation can be set withJPDA_ADDRESS=8000andJPDA_TRANSPORT=dt_socket.If you run Tomcat usingservice wrapper, add the above JVM options before any other JVM options. Check the documentation for the service wrapper to determine how to set JVM options.If you start Tomcat from within an IDE, check the documentation for the IDE to determine how to set the required JVM options. The port does not need to be set to 8000, it may be any value appropriate for your system. If you need to debug Tomcat startup or the auto-deployment of an application (where a breakpoint set in a debugger would be reached before you have time to connect with the debugger), then set the parametersuspend=yinstead ofsuspend=n. Doing this will cause the JVM to pause execution very early in the launch process and wait until a debugger is attached before proceeding. Whilst this is very useful in development it should not be used in production because of both security and performance implications.",../data/confluence_exports/TOMCAT/Developing_103098864.html
Apache Tomcat : Developing How do I remotely debug Tomcat using Eclipse?,"This answer assumes that you have a project set up and have some idea of what you are doing in this respect. If not then that is really outside the scope of this topic and you need to go toeclipse.organd read up on how to use your IDE, and maybe practice a little bit before you come back to this. We are also going to assume that you have some idea of what a debugger is and how to use one. Make sure that Tomcat is started in remote debugging mode as described above and that your app is deployed.Make sure that you have the sources for the code that you are trying to debug in your IDE. For the libraries and for Tomcat itself you can ""attach"" the sources to the jar files: open a class file and then click ""Attach Source..."" button.If you have a servlet or something, set a breakpoint where it is sure to hit on the next request.Go to ""Run->Debug Configurations..."". Click on ""Remote Java Applications"", then click ""New"". Type in the title. Note that port 8000 from the Tomcat instructions. Save and run.Eclipse will connect to the JVM that Tomcat is running under. Wow, that was easy! Now go type the url to submit to your servlet or whatever in your browser. Boom you hit the breakpoint right? Have fun!",../data/confluence_exports/TOMCAT/Developing_103098864.html
Apache Tomcat : Developing How do I remotely debug Tomcat using NetBeans?,"This answer assumes that you know how to work with a NetBeans Project, and also how to use the NetBeans debugger. If not, please go tohttp://www.netbeans.org/kb/using-netbeans/40/debug.htmland read up on how to use NetBeans and its debugger. Starting with Tomcat trunk revision 1484409, the Tomcat source includes Ant tasks to configure your source directory as a NetBeans Free-Form Project. After you have successfully run Ant with the default build target (deploy), you can then run theide-netbeanstarget. This task will configure your sandbox copy of Tomcat so the source can be inspected, maintained and debugged under the NetBeans IDE. You should follow the detailed instructions in the README.txt file that will have been installed in your new nbproject directory. Make sure that Tomcat is started in debug mode as described above, that your application is deployed, and that the sources are all defined as resources in your application. If you have a servlet or JSP file, set a breakpoint where you think a problem might be occurring. Go to ""Run->Attach Debugger"". A dialog pops up to let you specify the following options: Debugger: JPDA DebuggerConnector: SocketAttachHost: The IP address of the host your Tomcat installation is running on (127.0.0.1 if it is your local machine).Port: The port of your Tomcat debugging interface, which should be 8000 if you've followed the instructions above. When you press OK, you have a debugging connection very similar to local debugging. Note that NetBeans has a second option – you can debug JSP files and servlets locally using a Tomcat server that is bundled with the IDE. When you debug a JSP file or servlet in the IDE, the bundled Tomcat server automatically starts in debug mode, and the debugger connects to it. The Tomcat NetBeans targets have not yet been back-ported to Tomcat 7. You can copy the files from your copy of the trunk and they should only require minimal editing of paths to work with older versions of Tomcat.",../data/confluence_exports/TOMCAT/Developing_103098864.html
Apache Tomcat : Developing How do I change the monitoring interval for modified resources and application reloading?,Monitoring interval for application reloading is controlled by thebackgroundProcessorDelayproperty onContextelement or on its parent containers:HostandEngine. SeeTomcat Configuration Referencefor details. By default there is a single background processing thread that is run by Engine. See itsconfigurationfor the default delay value. Interval that controls reloading of the changed JSP pages is set in theJasper configurationinweb.xml.,../data/confluence_exports/TOMCAT/Developing_103098864.html
Apache Tomcat : Developing Official Eclipse IDE Web Tools FAQ for Tomcat,"Eclipse IDE has support for development of Web applications and running them on Apache Tomcat. This support is provided byEclipse Web Tools Platform Project. An easy way to get Web Tools is to download ""for Java EE Developers"" edition ofEclipse IDE. The Web Tools project has a FAQ page. WTP Tomcat FAQInEclipse Helpsee ""Web Tools Platform User Guide"" > ""Using the server tools"" > ""Testing and publishing on your server""",../data/confluence_exports/TOMCAT/Developing_103098864.html
Apache Tomcat : TomcatAtApacheConUs2009 Attendees,"Please registeron the linkif you plan to attend.  Already registered attendees (updated the 27th of October):  Jean-Frederic Clere, Rainer Jung, Jason Brittain, Sateesh Narahari, Costin Manolache, Mark Thomas, Aleksandar Gargenta, Luca Candela, Juan Manuel, Manoj Kumar, Bem Jones-Bey, Jason Richey,  For more information on other BoF / MeetUp / GetTogetther schedules please consult the main ApacheCon wiki...  http://wiki.apache.org/apachecon/ApacheMeetupsUs09",../data/confluence_exports/TOMCAT/TomcatAtApacheConUs2009_103100492.html
Apache Tomcat : Tomcat Training Course Background,BGD01: What is Tomcat? Quickstart. Tour of a typical installation,../data/confluence_exports/TOMCAT/Tomcat-Training-Course_70254775.html
Apache Tomcat : Tomcat Training Course Deployment,DEP01: Packaging and deploymentUnderstanding web.xmlDeployment with and without web.xml,../data/confluence_exports/TOMCAT/Tomcat-Training-Course_70254775.html
Apache Tomcat : Tomcat Training Course Logging,"LOG01: Tomcat and app logging config with rotation, archive, clean-up etc for experienced sysadmins",../data/confluence_exports/TOMCAT/Tomcat-Training-Course_70254775.html
Apache Tomcat : Tomcat Training Course TLS,TLS01: TLS backgroundTLS02: Step-by-step instructions for JSSE style configTLS03: Step-by-step instructions for OpenSSL style configTLS04: Step-by-step instructions for LetsEncryptTLS05: Instructions for other CAs,../data/confluence_exports/TOMCAT/Tomcat-Training-Course_70254775.html
Apache Tomcat : Tomcat Training Course Performance,"PER01: Performance metrics, tuning parameters",../data/confluence_exports/TOMCAT/Tomcat-Training-Course_70254775.html
Apache Tomcat : Tomcat Training Course Community,"COM01: State of the Cat (review of current state of development, highlights of recent activity, plans for the future)COM02: How to contributeCOM03: How to become a committer",../data/confluence_exports/TOMCAT/Tomcat-Training-Course_70254775.html
Apache Tomcat : Tomcat Training Course Security,SEC01: Authn & authzLDAPSEC02: HardeningFile system permissions,../data/confluence_exports/TOMCAT/Tomcat-Training-Course_70254775.html
Apache Tomcat : Tomcat Training Course Tomcat for system administrators,"Target Audience: System administrators with little or no experience of Apache Tomcat Modules: BGD01, DEP01, LOG01, TLS01, TLS03, PER01",../data/confluence_exports/TOMCAT/Tomcat-Training-Course_70254775.html
Apache Tomcat : Managing Tomcat's Dependency on the Eclipse JDT Core Batch Compiler Update: April 2019,"For some time now, Eclipse has been publishing the JDT JAR to Maven central. Therefore the following simplified policy is now followed: Use the latest available JAR from Maven central that supports the minimum Java version required by Tomcat.POMs, build dependencies and any other dependency reference refers to the JAR available from Maven central.",../data/confluence_exports/TOMCAT/61320771.html
Apache Tomcat : Managing Tomcat's Dependency on the Eclipse JDT Core Batch Compiler Background,Tomcat depends on JDT to enable it to support the compilation of JSPs while running on a JRE. This ensures JSP compilation works out of the box. Tomcat can be configured to use javac if running on a JDK but that has proved problematic in the past - mainly around ensuring that tools.jar is on the class path. For folks that use Tomcat with Maven in embedded mode it is important that the Maven dependencies are valid and correct. There have been problems in this area in the past such asbug 50604. The root cause of the issue is that thesmall (~2MB) JDT JARwe use is not officially released to Maven Central by Eclipse. One or more volunteers upload the JAR and it typically appears 2-4 weeks after the Eclipse release. There is alarger JAR (5.3MB)with contains the same functionality that is officially uploaded by Eclipse. It usually appears in Maven Central sooner that the smaller JAR.,../data/confluence_exports/TOMCAT/61320771.html
Apache Tomcat : Managing Tomcat's Dependency on the Eclipse JDT Core Batch Compiler Policy,"Tomcat releases will always package the 'small' JDT JAR as they have done for several years.The POMs used when uploading a release to Maven Central will always include references to valid JARs that are known to already exist in Maven central.The POMs will continue to reference the 'small' Eclipse JAR as a non-optional dependency by default.Tomcat's dependency on JDT will not normally be updated until the 'small' JAR is available in Maven Central. At that point all the dependencies (JAR that ships with Tomcat, JAR defined as dependency for Jasper JARs, JAR defined as dependency for emebedded JARs) will be updated together.If we need to update the JDT dependency before the 'small' JAR is available in Maven central (e.g. security issue, access new features such as Java 9 support for testing etc.) then we will use the following fallback options for the POM references:if the 'small' JDT JAR is not available, use the 'large' one;if neither the 'small' nor the 'large' JDT JARs are available continue to reference the pre-update version of the 'small' JDT JAR.The changelog should make clear which dependencies have been updated and which have not.",../data/confluence_exports/TOMCAT/61320771.html
Apache Tomcat : Nested Filesystem Functional Requirements,A FileSystem view of an archive may be created by calling the newFileSystem(Path) method on the provider.The FileSystem underlying the Path must support random access via the SeekableByteChannel returned from newByteChannel()The provider's newByteChannel() operation must return a SeekableByteChannel that supports random accessA FileSystem view of an archive may be created by calling the newFileSystem(URI) method on the provider.The URI must be able to be converted to a Path using the Paths.get(URI) API.The FileSystem backing such a Path must meet the constraints defined for newFileSystem(Path)The URIs for Paths returned by the provider must use standard URI syntax and support resolving of relative references,../data/confluence_exports/TOMCAT/Nested-Filesystem_61320783.html
Apache Tomcat : Nested Filesystem Non-Functional Requirements,"The provider will be identified by the URI scheme ""archive""The provider should avoid unnecessary buffering of data in memory or on diskBuffering modes should be configurable by the userPerformance should be comparable to that achievable by extracting the archive to diskMount performance should be comparable to the time and resources taken to extract the archive's contentFile open performance should be comparable to the time taken to open a file on the default filesystemFile read performance should be comparable to the time taken to read from a file on the default filesystemFile seek performance should be comparable to the time taken to position within a file on the default filesystem",../data/confluence_exports/TOMCAT/Nested-Filesystem_61320783.html
Apache Tomcat : Nested Filesystem Zip Structure,PKWARE's documentation on the format can be found athttp://www.pkware.com/documents/casestudies/APPNOTE.TXT,../data/confluence_exports/TOMCAT/Nested-Filesystem_61320783.html
Apache Tomcat : TomcatTrackUs09 PMC Sessions Tomcat community overview,"by Mladen Turk  Presentation - 60 minutes. The presentation gives current overview of the Apache Tomcat Project and directions it is heading.  It explains how we work as a community of developers and how we interact with our community of users. It gives brief overview of each technology Apache Tomcat Project is developing and maintaining  Tomcat versions  Connectors  Security  Bugzilla  Development  Audience should be both generic (at least for the technical and community overview I plan to do) and technical for an in-depth presentations offered by Mark, Filip and Rainer.  Presenter  Mladen Turk is a Principal Software Engineer at JBoss, a division of Red Hat (Switzerland), where he is responsible for Native Integration, Enterprise Web Services and Multiplatform technologies. He is member of JBoss Application Server team and gives more then 20 years of experience in client/server technologies. Mladen is currently acting as Apache Tomcat PMC chair, and beside Tomcat, he actively contributes to APR, Httpd and Commons projects.",../data/confluence_exports/TOMCAT/TomcatTrackUs09-PMC-Sessions_103100657.html
Apache Tomcat : TomcatTrackUs09 PMC Sessions All you want to know about reverse proxies,By Rainer Jung,../data/confluence_exports/TOMCAT/TomcatTrackUs09-PMC-Sessions_103100657.html
Apache Tomcat : TomcatTrackUs09 PMC Sessions mod_jk in depth,"By Rainer Jung  Based on some users list feedback: I could either present on mod_jk (in case there's no conflict with Mladen), or - thinking about André's suggestion, I could give a ""Reverse Proxies"" talk.  That talk would be more for sys admins, not for developers, and would explain the special situation a reverse proxy is in:  Request Routing  Loadbalancing/Stickyness procedures  Changing URLs, Cookies, etc.  Content rewriting  AJP protocol  Keep-Alive issues  Resource issues (blocking)  etc. It would include examples from mod_jk, mod_proxy_*, mod_substitute/mod_sed, mod_proxy_html (a module done by Nick Kew) and if available then even the GSOC Tomcat reverse proxy.  I think it would be fun, doing a talk that is not directly related to one single software component or project but more focused on a real-life topic and solution ingredients.+++Yes, for the two proposals coming from me (working titles):  All you want to know about reverse proxies  mod_jk in depth  Both of my talks are more directed to admins.",../data/confluence_exports/TOMCAT/TomcatTrackUs09-PMC-Sessions_103100657.html
Apache Tomcat : TomcatTrackUs09 PMC Sessions Securing your Tomcat installation,"By Tim Funk  Whether you are new to Tomcat or used it for a long time, we'll walk through ways to make your installation more secure. Even though we make Tomcat secure out of the box, you will make configuration changes. In this talk, we'll give you hints to make sure your changes don't expose you to security issues. We'll also cover items developers should and should not do in their code to keep the installation secure. Out of scope in this discussion: SSL.  Presenter  Tim Funk has been a Tomcat user for over 7 years and committer since 2003. Tim currently resides in Pennsylvania and works for Armstrong World Industries performing a range of roles, enjoying the role of developer the most. Tim has a Masters of Software Engineering from Penn State.",../data/confluence_exports/TOMCAT/TomcatTrackUs09-PMC-Sessions_103100657.html
Apache Tomcat : TomcatTrackUs09 PMC Sessions mod_jk / mod_proxy and others,"By Mladen Turk/Rainer Jung/Jean-Frederic Clere  Tomcat is often used as a cluster and/or is the back-end server of a front-end reserve proxy.  Several front-end can be used, mod_jk, mod_proxy, mod_serf and mod_cluster, quick presentation of each of them, featuring: loadbalancing, failover, QoS etc.  ""Inktomi Traffic Server"" if incubated will be presented too.",../data/confluence_exports/TOMCAT/TomcatTrackUs09-PMC-Sessions_103100657.html
Apache Tomcat : MemoryLeakProtection ThreadLocalleaks,"Classloader leaks because of uncleanedThreadLocalvariables are quite common. Depending on the use cases, they can be detected or not.",../data/confluence_exports/TOMCAT/MemoryLeakProtection_103099526.html
Apache Tomcat : MemoryLeakProtection CustomThreadLocalclass,"Suppose we have the following 3 classes in our webapp :  public class MyCounter {
	private int count = 0;

	public void increment() {
		count++;
	}

	public int getCount() {
		return count;
	}
}

public class MyThreadLocal extends ThreadLocal<MyCounter> {
}

public class LeakingServlet extends HttpServlet {
	private static MyThreadLocal myThreadLocal = new MyThreadLocal();

	protected void doGet(HttpServletRequest request,
			HttpServletResponse response) throws ServletException, IOException {

		MyCounter counter = myThreadLocal.get();
		if (counter == null) {
			counter = new MyCounter();
			myThreadLocal.set(counter);
		}

		response.getWriter().println(
				""The current thread served this servlet "" + counter.getCount()
						+ "" times"");
		counter.increment();
	}
}  If theLeakingServletis invoked at least once and the Thread that served it is not stopped, then we created a classloader leak !  The leak is caused because we have a custom class for theThreadLocalinstance, and also a custom class for the value bound to the Thread. Actually the important thing is that both classes were loaded by the webapp classloader.  Hopefully tomcat 6.0.24 can detect the leak when the application is stopped: each Thread in the JVM is examined, and the internal structures of the Thread andThreadLocalclasses are introspected to see if either theThreadLocalinstance or the value bound to it were loaded by theWebAppClassLoaderof the application being stopped.  In this particular case, the leak is detected and a message is logged.  Tomcat 6.0.24 to 6.0.26 modify internal structures of the JDK (ThreadLocalMap) to remove the reference to theThreadLocalinstance, but this is unsafe (see#48895) so that it became optional and disabled by default from 6.0.27. Starting with Tomcat 7.0.6, the threads of the pool are renewed so that the leak is safely fixed.  Mar 16, 2010 11:47:24 PM org.apache.catalina.loader.WebappClassLoader clearThreadLocalMap
SEVERE: A web application created a ThreadLocal with key of type [test.MyThreadLocal] (value [test.MyThreadLocal@4dbb9a58]) and a value of type [test.MyCounter] (value [test.MyCounter@57922f46]) but failed to remove it when the web application was stopped. To prevent a memory leak, the ThreadLocal has been forcibly removed.  Note: this particular leak was actually already cured by previous versions of tomcat 6, because static references of classes loaded by the webappclassloader are nullified (see later).",../data/confluence_exports/TOMCAT/MemoryLeakProtection_103099526.html
Apache Tomcat : MemoryLeakProtection Webapp class instance asThreadLocalvalue,"Suppose that we have the following class in the common classpath (for instance in a jar in tomcat/lib) :  public class ThreadScopedHolder {
	private final static ThreadLocal<Object> threadLocal = new ThreadLocal<Object>();

	public static void saveInHolder(Object o) {
		threadLocal.set(o);
	}

	public static Object getFromHolder() {
		return threadLocal.get();
	}
}  And those 2 classes in the webapp :  public class MyCounter {
	private int count = 0;

	public void increment() {
		count++;
	}

	public int getCount() {
		return count;
	}
}
public class LeakingServlet extends HttpServlet {

	protected void doGet(HttpServletRequest request,
			HttpServletResponse response) throws ServletException, IOException {

		MyCounter counter = (MyCounter)ThreadScopedHolder.getFromHolder();
		if (counter == null) {
			counter = new MyCounter();
			ThreadScopedHolder.saveInHolder(counter);
		}

		response.getWriter().println(
				""The current thread served this servlet "" + counter.getCount()
						+ "" times"");
		counter.increment();
	}
}  If the servlet is invoked at least once, the webapp classloader would not be GCed when the app is stopped: since the classloader ofThreadScopedHolderis the common classloader, it remains forever which is as expected. But itsThreadLocalinstance has a value bound to it (for the non-terminated thread(s) that served the sevlet), which is an instance of a class loaded by the webapp classloader...  Here again, tomcat >=6.0.24 will detect the leak :  Mar 17, 2010 10:23:13 PM org.apache.catalina.loader.WebappClassLoader clearThreadLocalMap
SEVERE: A web application created a ThreadLocal with key of type [java.lang.ThreadLocal] (value [java.lang.ThreadLocal@44676e3f]) and a value of type [test.leak.threadlocal.value.MyCounter] (value [test.leak.threadlocal.value.MyCounter@62770d2e]) but failed to remove it when the web application was stopped. To prevent a memory leak, the ThreadLocal has been forcibly removed.",../data/confluence_exports/TOMCAT/MemoryLeakProtection_103099526.html
Apache Tomcat : MemoryLeakProtection Webapp class instance indirectly held through aThreadLocalvalue,"Suppose we have the sameThreadScopedHolderclass (in the common classloader) andMyCounterclass in the webapp, but with the following servlet :  public class LeakingServlet extends HttpServlet {

	protected void doGet(HttpServletRequest request,
			HttpServletResponse response) throws ServletException, IOException {

		List<MyCounter> counterList = (List<MyCounter>) ThreadScopedHolder
				.getFromHolder();
		MyCounter counter;
		if (counterList == null) {
			counter = new MyCounter();
			ThreadScopedHolder.saveInHolder(Arrays.asList(counter));
		} else {
			counter = counterList.get(0);
		}

		response.getWriter().println(
				""The current thread served this servlet "" + counter.getCount()
						+ "" times"");
		counter.increment();
	}
}  We have more or less the same kind of leak as the previous one, but this time tomcat does not detect the leak when stopping the application. The problem is that when it inspects the entries ofThreadLocalMap, it checks whether either the key or the value is an instance of a class loaded by the webapp classloader. Here the key is an instance ofThreadLocal, and the value is an instance of java.util.ArrayList.  The ""Find leaks"" button in tomcat manager will report the leak when asked :  The following web applications were stopped (reloaded, undeployed), but their
classes from previous runs are still loaded in memory, thus causing a memory
leak (use a profiler to confirm):
/testWeb  But it does not give any clue about what caused the leak, we would need to make a heapdump and analyse it with some tool likeEclipse MAT.  Tomcat 7.0.6 and later fix this leak by renewing threads in the pool.",../data/confluence_exports/TOMCAT/MemoryLeakProtection_103099526.html
Apache Tomcat : MemoryLeakProtection ThreadLocalpseudo-leak,"Suppose we have the sameMyCounterclass as above (in the webapp) and the following servlet :  public class LeakingServlet extends HttpServlet {
	private ThreadLocal<MyCounter> myThreadLocal = new ThreadLocal<MyCounter>();

	protected void doGet(HttpServletRequest request,
			HttpServletResponse response) throws ServletException, IOException {

		MyCounter counter = myThreadLocal.get();
		if (counter == null) {
			counter = new MyCounter();
			myThreadLocal.set(counter);
		}

		response.getWriter().println(
				""The current thread served this servlet "" + counter.getCount()
						+ "" times"");
		counter.increment();
	}

	@Override
	public void destroy() {
		super.destroy();
		// normally not needed, just to make my point
		myThreadLocal = null;
	}
}  Notice that theThreadLocalinstance is referenced through an instance variable, not a static one.  Sun's implementation ofThreadLocal(andWeakHashMap) too is such thatThreadLocalMapentries whose key is GCed are not immediately removed. (The key is a weak reference to theThreadLocalinstance, seejava.lang.ThreadLocal.ThreadLocalMap.Entry<T> in JDK 5/6. And there's no daemon thread waiting on aReferenceQueue). Instead, it's only during subsequent uses ofThreadLocalfeatures that each Thread removes the abandonedThreadLocalMap.Entryentries (seeThreadLocalMap.expungeStaleEntries().  If many threads were used to serve our leaking webapp, but after we stop it only a couple of threads are enough to serve other webapps, one could have some threads that are no longer used, waiting for some work. Since those threads are blocked, they have no interaction with theirThreadLocalMap(i.e. there's noThreadLocalvalue bound to them or removed), so that there's no opportunity toexpungeStaleEntries().  Tomcat 6.0.24-6.0.26 ""speeds up"" the removal of stale entries (and thus fixes the pseudo-leak), by callingexpungeStaleEntries()for each thread that has some stale entries. Since it's not thread-safe, it has been made optional and disabled by default from 6.0.27.  Tomcat 7.0.6 and later fix the problem by renewing threads in the pool.",../data/confluence_exports/TOMCAT/MemoryLeakProtection_103099526.html
Apache Tomcat : MemoryLeakProtection Threads spawned by webapps,"If a webapp creates a thread, by default its context classloader is set to the one of the parent thread (the thread that created the new thread). In a webapp, this parent thread is one of tomcat worker threads, whose context classloader is set to the webapp classloader when it executes webapp code.  Furthermore, the spawned thread may be executing (or blocked in) some code that involves classes loaded by the webapp, thus preventing the webapp classloader from being collected.  So, if the spawned thread is not properly terminated when the application is stopped, the webapp classloader will leak because of the strong reference held by the spawned thread.  Example :  public class LeakingServlet extends HttpServlet {
	private Thread leakingThread;

	protected void doGet(HttpServletRequest request,
			HttpServletResponse response) throws ServletException, IOException {

		if (leakingThread == null) {
			synchronized (this) {
				if (leakingThread == null) {
					leakingThread = new Thread(""leakingThread"") {

						@Override
						public void run() {
							synchronized (this) {
								try {
									this.wait();
								} catch (InterruptedException e) {
									e.printStackTrace();
								}
							}
						}
					};
					leakingThread.setDaemon(true);
					//leakingThread.setContextClassLoader(null);
					leakingThread.start();
				}
			}
		}
		response.getWriter().println(""Hello world!"");
	}
}  Here, when the app is stopped, the webapp classloader is still referenced by the spawned thread both through its context classloader and its current call stack (the anonymous Thread subclass is loaded by the webapp classloader).  When stopping an application, tomcat checks the context classloader of every Thread, and if it is the same as the app being stopped, it logs the following message :  Mar 18, 2010 11:13:07 PM org.apache.catalina.core.ApplicationContext log
INFO: HTMLManager: stop: Stopping web application at '/testWeb'
Mar 18, 2010 11:13:07 PM org.apache.catalina.loader.WebappClassLoader clearReferencesThreads
SEVERE: A web application appears to have started a thread named [leakingThread] but has failed to stop it. This is very likely to create a memory leak.  Now, if we uncomment the lineleakingThread.setContextClassLoader(null);in the above example, tomcat (6.0.24) no longer detect the leak when the application is stopped because the spawned thread context classloader is no longer the webapp's. (the ""Find leaks"" feature in the manager will report it though)",../data/confluence_exports/TOMCAT/MemoryLeakProtection_103099526.html
Apache Tomcat : MemoryLeakProtection Threads spawned by classes loaded by the common classloader,"Suppose, we have theCommons Poollibrary in the classpath of the server (e.g. the jar is in tomcat/lib), and the following servlet :  public class LeakingServlet extends HttpServlet {
	private GenericObjectPool objectPool;

	protected void doGet(HttpServletRequest request,
			HttpServletResponse response) throws ServletException, IOException {
		response.getWriter().println(""Number of idle objects in the pool :""+objectPool.getNumIdle());
	}

	@Override
	public void init() throws ServletException {
		objectPool = new GenericObjectPool();
		objectPool.setFactory(new BasePoolableObjectFactory() {
			private AtomicInteger counter = new AtomicInteger(0);

			@Override
			public Object makeObject() throws Exception {
				String str = ""Object #"" + counter.incrementAndGet();
				System.out.println(""Creating ""+str);
				return str;
			}

			@Override
			public void destroyObject(Object obj) throws Exception {
				System.out.println(""Destroying ""+obj);
			}
		});
		objectPool.setMinIdle(3);
		objectPool.setTimeBetweenEvictionRunsMillis(1000);
	}

	@Override
	public void destroy() {
		try {
			objectPool.close();
		} catch (Exception e) {
			e.printStackTrace();
		}
	}
}  The call toGenericObjectPool.setTimeBetweenEvictionRunsMillis)actually starts or reuses ajava.lang.Timershared between allGenericObjectPoolinstances. As long as a pool is running and at least one pool is using the timer eviction feature, the Timer lives.  If there's no other webapp using commons-pool, there's no leak : when we stop the webapp, the servlet is stopped and the pool is closed. If it was the only pool in use, the Timer thread is also stopped and there's no leak.  Now, imagine that there are 2 webapps using commons-pool with the timer eviction feature (imagine the above servlet is deployed in 2 webapps A and B). Suppose webapp A is deployed, then B. Since the commons-pool jar is shared between both webapps, only one Timer thread is spawned, with 2TimerTask, one for each webapp, to handle the eviction of each pool instance.  Then, if we stop webapp A, boom it's leaking! the Timer thread has its context classloader set to theWebAppClassLoaderof webapp A. This is somehow abug of commons-pool, but tomcat 6.0.24 tries to help :  INFO: HTMLManager: stop: Stopping web application at '/testWeb'
Destroying Object #3
Destroying Object #2
Destroying Object #1
Mar 21, 2010 9:26:36 PM org.apache.catalina.loader.WebappClassLoader clearReferencesStopTimerThread
SEVERE: A web application appears to have started a TimerThread named [Timer-0] via the java.util.Timer API but has failed to stop it. To prevent a memory leak, the timer (and hence the associated thread) has been forcibly cancelled.  So the leak is fixed, but unfortunately there's a side effect :it broke webapp B eviction timer. That's why stoppingTimerThreadhas been made optional from 6.0.27.",../data/confluence_exports/TOMCAT/MemoryLeakProtection_103099526.html
Apache Tomcat : MemoryLeakProtection Threads spawned by JRE classes,"Just like third-party libraries may spawn threads that provoke leaks, some JRE classes also spawn threads that inherit from the current class loader and thus provoke leaks.  Instead of trying to stop such threads, tomcat prefers to force the creation of such threads when the container is started, before webapps are started. TheJreMemoryLeakPreventionListenerdoes it for a few known offenders in the JRE.",../data/confluence_exports/TOMCAT/MemoryLeakProtection_103099526.html
Apache Tomcat : MemoryLeakProtection static class variables,"When an app is stopped, Tomcat (even before 6.0.24) nullifies the value of all static class variables of classes loaded by theWebAppClassLoader. In some cases, it may fix a classloader leak (for example because of a customThreadLocalclass, see above), but even if we still have a leak, it may decrease the amount of memory lost:  Imagine a class with the following variable :  private final static byte[] BUFFER = new byte[1024*1024]; //1MB buffer  Normally, the 1MB buffer should be freed when the app is stopped, but only if the classloader itself can be garbage-collected. Since there are still possibilities to have a leak of the classloader, clearing the BUFFER variable allows to recover 1MB of memory.",../data/confluence_exports/TOMCAT/MemoryLeakProtection_103099526.html
Apache Tomcat : MemoryLeakProtection LogFactory,to be completed,../data/confluence_exports/TOMCAT/MemoryLeakProtection_103099526.html
Apache Tomcat : MemoryLeakProtection JavaBeanIntrospector cache,Tomcat callsjava.beans.Introspector.flushCaches();when an app is stoppped.,../data/confluence_exports/TOMCAT/MemoryLeakProtection_103099526.html
Apache Tomcat : MemoryLeakProtection JDBC driver registration,"If a webapp contains a JDBC driver (e.g. in WEB-INF/lib), the driver will be registered with theDriverManagerwhen it is first used. When the application is stopped, the driver should be deregistered withDriverManagerto avoid a classloader leak. Since applications usually forget this, tomcat helps by deregistering the driver.",../data/confluence_exports/TOMCAT/MemoryLeakProtection_103099526.html
Apache Tomcat : MemoryLeakProtection RMI target,to be completed,../data/confluence_exports/TOMCAT/MemoryLeakProtection_103099526.html
Apache Tomcat : MemoryLeakProtection References,"Mark Thomas interview on DZoneEclipse Memory Analysis Tool  Related issues  49159- ImproveThreadLocalmemory leak clean-upSun bug 4957990- In some cases the Server JVM fails to collect classloaders. According tothis pageit should have been fixed with java 6u16 but actually it was not. It seems to be fixed with 6u21 (documentedhereand verified by the author of this wiki page).Sun bug 6916498- An exception can keep a classloader in memory if the stack trace that was recorded when it was created contains a reference to one of its classes. Fixes were done in Tomcat for its own classes that had this issue (seeBZ 50460), but some library or JRE code may still create a leak that is undetected by tools because of this JVM bug. See alsoBZ 53936for a workaround that you can implement if you are unable to fix a buggy library.   CategoryFAQ",../data/confluence_exports/TOMCAT/MemoryLeakProtection_103099526.html
Apache Tomcat : Apache Tomcat Home Недавно обновлено,"ReleaseProcessмар 04, 2023•обновленоChristopher Schultz•измененияDevelopingфев 07, 2023•обновленоChristopher Schultz•измененияVerifying a Release Buildянв 12, 2023•обновленоChristopher Schultz•измененияSpecificationsдек 06, 2022•обновленоKonstantin Kolinko•измененияFinancial Summaryокт 14, 2022•обновленоMark Thomas•измененияTroubleshooting and Diagnosticsавг 05, 2022•обновленоKonstantin Kolinko•измененияAccessLogValveавг 05, 2022•обновленоKonstantin Kolinko•измененияCharacter Encodingиюл 21, 2022•обновленоKonstantin Kolinko•измененияSecurityиюн 13, 2022•обновленоTimothy A. Funk•измененияTomcatVersionsапр 24, 2022•создано посредствомMark ThomasTomcat Versionsмар 28, 2022•обновленоChristopher Schultz•измененияCommunity Review of DISA STIGмар 03, 2022•обновленоMark Thomas•измененияCommunity Review of DISA STIGмар 01, 2022•обновленоChristopher Schultz•измененияReleaseProcessсен 14, 2021•обновленоKonstantin Kolinko•измененияApache Tomcat Homeсен 14, 2021•обновленоKonstantin Kolinko•измененияПоказать больше",../data/confluence_exports/TOMCAT/Apache-Tomcat-Home_61320756.html
Apache Tomcat : Servlet TCK 4.0 setenv.[sh|bat],Set the following system properties -Dorg.apache.catalina.STRICT_SERVLET_COMPLIANCE=true-Dorg.apache.tomcat.util.http.ServerCookie.FWD_SLASH_IS_SEPARATOR=false-Duser.language=en-Duser.country=US,../data/confluence_exports/TOMCAT/Servlet-TCK-4.0_103089318.html
Apache Tomcat : Servlet TCK 4.0 context.xml,"Make the following changes: <Context crossContext=""true"" resourceOnlyServlets=""jsp""> <CookieProcessor className=""org.apache.tomcat.util.http.LegacyCookieProcessor"" alwaysAddExpires=""true"" forwardSlashIsSeparator=""false"" /> ... </Context>",../data/confluence_exports/TOMCAT/Servlet-TCK-4.0_103089318.html
Apache Tomcat : Servlet TCK 4.0 tomcat-users.xml,"Make the following changes: <user username=""CN=CTS, OU=Java Software, O=Sun Microsystems Inc., L=Burlington, ST=MA, C=US"" roles=""Administrator""/><user username=""j2ee"" password=""j2ee"" roles=""Administrator,Employee"" /><user username=""javajoe"" password=""javajoe"" roles=""VP,Manager"" />",../data/confluence_exports/TOMCAT/Servlet-TCK-4.0_103089318.html
Apache Tomcat : Servlet TCK 4.0 server.xml,"Enable h2c on port 8080, and add some trailer headers <Connector ... allowedTrailerHeaders=""myTrailer, myTrailer2""> <UpgradeProtocol className=""org.apache.coyote.http2.Http2Protocol"" /> </Connector> Enable TLS on port 8443 <Connector port=""8443"" protocol=""HTTP/1.1"" SSLEnabled=""true""> <SSLHostConfig truststoreFile=""conf/cacerts.jks""> <Certificate certificateKeystoreFile=""conf/clientcert.jks"" certificateKeystorePassword=""changeit"" type=""RSA"" /> </SSLHostConfig> </Connector> Remove the lock-out realm Client certificate tests: see below",../data/confluence_exports/TOMCAT/Servlet-TCK-4.0_103089318.html
Apache Tomcat : Servlet TCK 4.0 Test Suite,"Download latest nightly build https://download.eclipse.org/ee4j/jakartaee-tck/8.0.1/nightly/servlettck-4.0_latest.zip Extract to SERVLET_TCK_HOME Import bin/cts_cert to a truststore doing: ""keytool -import -alias cts -file cts_cert -storetype JKS -keystore cacerts.jks"" password should be ""changeit"" Create the truststore using  ""keytool -import -alias cts -file cts_cert -storetype JKS -keystore cacerts.jks"" password should be ""changeit"" Place cacerts.jks truststore in $SERVLET_TCK_HOME/bin/certificates Add $SERVLET_TCK_HOME/bin/certificates/cacerts.jks and $SERVLET_TCK_HOME/bin/certificates/clientcert.jks in the Tomcat conf folder  Edit $SERVLET_TCK_HOME/bin/ts.jte You'll need to set the following properties (adjust the paths and values for your environment)  web.home=/path/to/tomcat servlet.classes=${web.home}/lib/servlet-api.jar:${web.home}/lib/annotations-api.jar webServerHost=localhost webServerPort=8080 securedWebServicePort=8443 command.testExecute += -Djava.endorsed.dirs=${ts.home}/endorsedlib -Djavax.net.ssl.trustStore=${ts.home}/bin/certificates/cacerts.jks  set JAVA_HOME cd $SERVLET_TCK_HOME/bin ant gui Accept the defaults and then run the tests  A default 9.0.x build with the above configuration triggers 10 test failures 2 Expected failures 1 x signature test as Tomcat has added a missing \@Deprecated annotation1 x default context path test as Tomcat configuration always overrides this 8 TCK bugs 1 x case sensitive HTTP header checkshttps://github.com/eclipse-ee4j/jakartaee-tck/issues/415 * com/sun/ts/tests/servlet/spec/security/denyUncovered/* use URLs which don't match the WAR name; it needs to be renamed from servlet_sec_denyUncovered_web.war to servlet_sec_denyUncovered.war (there is a proprietary descriptor to rectify the mapping on deployment for Glassfish, but it is not portable)https://github.com/eclipse-ee4j/jakartaee-tck/issues/45 2 * com/sun/ts/tests/servlet/spec/security/secbasic/client.java#test7[_anno]Assertion is explained in the common class:https://github.com/eclipse-ee4j/jakartaee-tck/blob/master/src/com/sun/ts/tests/common/jspservletsec/SecBasicClient.java#L340I am not aware of this special behavior being mentioned anywherehttps://github.com/eclipse-ee4j/jakartaee-tck/issues/44 Note the configuration above also works around 3 additional TCK bugs 2 x cookie tests assume server is running in en_US locale (fixed by setenv.sh changes that start Tomcat in that locale)https://github.com/eclipse-ee4j/jakartaee-tck/issues/461 x missing endorsedLib configuration (fixed by command.testExecute change above)https://github.com/eclipse-ee4j/jakartaee-tck/issues/39 1 Tomcat bug has also been fixed as a result of running the TCK 1 x Enable a PushBuilder to manipluate cookies via HTTP headers",../data/confluence_exports/TOMCAT/Servlet-TCK-4.0_103089318.html
Apache Tomcat : TomcatOnMacOS Running Tomcat on Mac OS X,For an updated guide for installing Tomcat 5/6 on Mac OS X 10.6 usingMacPortscheckhttp://serverfault.com/questions/183496/full-guide-for-installing-tomcat-on-os-x                                                                             [See below for later updates to this 2004 posting]                                                                             These notes are the result of several weeks playing with different things and asking a LOT of questions on several mailing lists.  I hope these notes are of use to someone out there.,../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : TomcatOnMacOS THE ENVIRONMENT,"Java is installed by default on every MacOS X installation.  MacOS X 10.2 (Jaguar) comes with Java 1.3; MacOS X 10.3 (Panther) comes with Java 1.4.  MacOS X 10.4 (Tiger) comes with Java 1.4.2 installed, but Java 1.5 can be downloaded.  MacOS X 10.5 (Leopard) comes with Java 1.5.  MacOS X 10.5 (Snow Leopard) comes with Java 1.6.  It is possible to run Java 1.4 on MacOS XJaguarbut it may interfere with operation of the standard environment.  JAVA_HOMEis at/Library/Java/Home, but this is a link into a directory/System/Frameworks/JavaJVM.Framework/Versionswhich is used to switch between versions easily.  There is a linkCurrentandCurrentVersionwhich matches up the current Java environment with the appropriate version.",../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : TomcatOnMacOS THE SITUATION,"I wanted to learn about Java Servlet Pages (JSP) on my Macintosh (with MacOS X). I installed it the first time on a Windows XP Professional system, but my development machine is a Macintosh.",../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : TomcatOnMacOS THE SOLUTION,"Since I had already installed and played with a separate install of Tomcat (the version installed by Apple's XCode was out of date, as was the version I downloaded from another site), I was ready to give up!  In my case I wanted to get rid of all the other Tomcat installations and start from scratch.  Step 1: Download Tomcat  Download Jakarta Tomcat from the Tomcat home page, and get the binary version - I_downloadedjakarta-tomcat-5.0.27.tar.gz.  Extract the files (unless your browser automatically extracts them).  Navigate (using the Finder) toMacintosh HD > Library.  If there is not already a folder inMacintosh HD > LibrarycalledTomcat, then create one.  If there is already aTomcatfolder (presumably from an out-of-date installation), delete all the files already in that folder.  Step 2: Extract tomcat archive  Copy all the unstuffed files from thejakarta-tomcat-5.0.27.tar.gzdownload and put all the files into theTomcatfolder.  I just named itTomcatas it saves some typing in the Terminal  So the directory structure becomes:  Macintosh HD > Library > Tomcat ><extracted files>  Step 3: Configure tomcat-users.xml  Open the/Library/Tomcat/conf/tomcat-users.xmlfile with a plain text editor. The default Tomcat installation only has the basic roles and users set up.  What is configured as default is:  <?xml version='1.0' encoding='utf-8'?>
 <tomcat-users>
   <role rolename=""tomcat""/>
   <role rolename=""role1""/>
   <user username=""tomcat"" password=""tomcat"" roles=""tomcat""/>
   <user username=""both"" password=""tomcat"" roles=""tomcat,role1""/>
   <user username=""role1"" password=""tomcat"" roles=""role1""/>
 </tomcat-users>  An example of what itshouldlook like follows:  <?xml version='1.0' encoding='utf-8'?>
 <tomcat-users>
  <role rolename=""tomcat""/>
  <role rolename=""role1""/>
  <role rolename=""manager""/>
  <role rolename=""admin""/>
  <user username=""tomcat"" password=""tomcat"" roles=""tomcat,admin,manager""/>
  <user username=""both"" password=""tomcat"" roles=""tomcat,role1""/>
  <user username=""role1"" password=""tomcat"" roles=""role1""/>
 </tomcat-users>  In this case a role, a user and password have been set up with the nametomcat. You can change this later.  Step 4: Start Tomcat  To start Tomcat, open a shell command prompt (using, for instance, the Terminal application).  The path to Tomcat via the Finder isMacintosh HD > Library > Tomcat.  But to get to that directory using the Terminal, type in:  cd /Library/Tomcat/bin  Do anls- you should see a file calledstartup.sh.  Any file in this directory ending in.shcan be executed in the terminal by putting a period and a slash before the file name (eg:startup.sh).  The following example executes the tomcat startup script:  ./startup.sh && tail -f ../logs/catalina.out  Terminal should display four lines looking something like this:  Using CATALINA_BASE:   /library/tomcat
Using CATALINA_HOME:   /library/tomcat
Using CATALINA_TMPDIR: /library/tomcat/temp
Using JAVA_HOME:       /Library/Java/Home
Users-Computer:/library/tomcat/bin user$  There are some notes and a couple of Preference Panes for automating the starting and stopping of Tomcat. After trying them all on five different computers - this is the simplest!  Step 5: Test installationOpen a browser window, and enterhttp://127.0.0.1:8080- the default Tomcat page should open.  If you click theTomcat AdministratororTomcat Managerlinks in the upper left hand of the default Tomcat page, you will be asked for a user name and password.  As mentioned above, usetomcatfor the user name, andtomcatfor the password.  By : Brynley Blake (31 August 2004)  Withconsiderablehelp from Joachim, Tracy, Tom, Samuel and Jonel from the Apple Web and Java Developer mailing lists.",../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : TomcatOnMacOS Updated for 2006,"Basil Bourque says:  Today (2006-04-05) I disovered a somewhat different recipe to running Tomcat 5.5.16 on Mac OS X 10.4.6.  Today's Tomcat (v5.5) wants to run on Java 5, but Apple's command line defaults to Java 4 (1.4.2). Surprisingly, Tomcat's script works around this.  I did not need to set any environment variables (JAVA_HOME).  But I did need to fix vital file permissions in the downloaded Tomcat folder.",../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : TomcatOnMacOS Also Updated in 2010,By Pid: I removed some of the original statements about setting JAVA_HOME as they did not match my experience of installing Tomcat on OS X. It's also certainly overkill to install an application just to be able to set an environment variable.,../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : TomcatOnMacOS (1) Upgrade to Mac OS X 10.4.6,"I do not know if 10.4.6 is required, but it sure made an easy Tomcat install.  Apply all of Apple's Software Updates, especially the Java 5 update.",../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : TomcatOnMacOS (2) Download apache-tomcat-5.5.16,Unzip.  Move it to your home folder.,../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : TomcatOnMacOS (3) Fix the Unix file permissions,"Download the freeware BatChmod, a GUI wrapper around the Unix ""chmod"" command.  http://macchampion.com/arbysoft/  Drag and drop the entire Tomcat folder onto the BatChmod icon.  Check *all* the checkboxes.  Click ""Apply"" and give your Administrator password.",../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : TomcatOnMacOS (4) Start Tomcat,"Launch /Applications/Utilities/Terminal.  Navigate to the Tomcat folder (apache-tomcat-5.5.16):cd ~/apache-tomcat-5.5.16Run the startup script:./bin/startup.shYou should see something like this:Using CATALINA_BASE:   /Users/basilbourque/apache-tomcat-5.5.16
Using CATALINA_HOME:   /Users/basilbourque/apache-tomcat-5.5.16
Using CATALINA_TMPDIR: /Users/basilbourque/apache-tomcat-5.5.16/temp
Using JRE_HOME:       /System/Library/Frameworks/JavaVM.framework/Versions/1.5/Home  Notice that somehow the startup script(s) have used Java 5 on Mac OS X even though it is not the default Java runtime at the command line. I hope to parse that script later to discover how they did this.",../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : TomcatOnMacOS (5) Test Tomcat,"Launch a web browser such as Safari.  Gotohttp://127.0.0.1:8080/  You should see the cute Tomcat logo on the welcome page.  You are technically done at this point. But as a good pratice, I also did the following additional steps.",../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : TomcatOnMacOS (6) Start the Firewall,"System Preferences > Sharing > Firewall > Start  Also, click the Advanced button on that panel, to (a) block UDP, and (b) start logging. It is fascinating to see how quickly a computer directly on the Internet starts to get queried/tested/attacked. Open /Applications/Utilities/Console to see the log.",../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : TomcatOnMacOS (7) Enable Port-Forwarding,"Add a rule to do port-forwarding from port 80 to Tomcat's default port 8080. Type this in the Terminal:sudo ipfw add 100 fwd 127.0.0.1,8080 tcp from any to any 80 inOr use this nifty program, another GUI wrapper around the ipfw command line, Simple Port Forwarder:http://www.4dresources.com/files/  The Sharing SysPref will get cranky when you add an ipfw rule behind its back; it disables its user interface. To use the Sharing SysPref again you'll have to clear that rule, the one we added and numbered 100. To delete a rule, either read the ipfw man page, or use Simple Port Forwarder again. Quit the System Preferences program, and re-launch it to re-enable its Firewall panel.",../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : TomcatOnMacOS (8) Test Tomcat,"Launch a web browser such as Safari.  Goto:http://127.0.0.1/  Notice that we left off the port number this time, so the web browser defaults to port 80. Or you can explicitly say port 80:http://127.0.0.1:80/  You should see the cute Tomcat logo on the welcome page.  To be sure it is working, and not cached by the browser:  • Try another browser.  • Click the Reload button in the toolbar of the browser.  • Choose the Reload command, such as View > Reload Page in Safari.",../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : TomcatOnMacOS (9) Walk the dog,"Enjoy an all-too-rare moment of computing satisfaction.  As best as I can recall, that is all I had to do today. I hope I am not steering you wrong. I did assume at first that Java 5 would not be used be default, so I wasted time messing with the path ($PATH) and setting the environment variable JAVA_HOME. But I undid those, as they failed. I then tried the simple steps above, and it worked.  I hope to refresh my own web site with this info:http://www.BasilBourque.org/  Tip: You can drag and drop from the Finder to the Terminal, to avoid typing file and folder names.  --Basil Bourque",../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : TomcatOnMacOS Updated for 2008,"Michael Valentiner says:  Today (2008-02-03) I verified running Tomcat 6.0.14 on Mac OS X 10.5.1.  Like Basil, I did not need to set up JAVA_HOME and I did need to fix Unix file permissions.  Unlike Basil, the System Preferences > Sharing > Firewall has moved to System Preferences > Security > Firewall.  I was able to enable logging, but haven't figured out how to block UDP.",../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : TomcatOnMacOS Updated for 2009,"Chris Latko says:  To have Tomcat 6.0.x launch on boot on Mac OS X 10.5.x, you need to add a LaunchDaemon. Create a file called org.apache.tomcat.plist in /Library/LaunchDaemons/ with the following content:  <?xml version=""1.0"" encoding=""UTF-8""?>
<!DOCTYPE plist PUBLIC ""-//Apple Computer//DTD PLIST 1.0//EN"" ""http://www.apple.com/DTDs/PropertyList-1.0.dtd"">
<plist version=""1.0"">
<dict>
	<key>Disabled</key>
	<false/>
	<key>Label</key>
	<string>com.apache.tomcat</string>
	<key>ProgramArguments</key>
	<array>
		<string>/Library/Tomcat/bin/startup.sh</string>
	</array>
	<key>RunAtLoad</key>
	<true/>
</dict>
</plist>",../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : TomcatOnMacOS Updated for 2009,"Greg Woolsey says:  For Tomcat 5.5.x (and probably 6.0, as the scripts don't appear to have changed much) you need a slightly modified plist, that calls catalina.sh in a manner compatible with the requirements of launchd:http://developer.apple.com/MacOsX/launchd.html.  Specifically, the script must be run with the ""run"" parameter instead of ""start"", as start backgrounds the Java process and exits the script.  This causes launchd to think the service is done, and kills the java process as part of it's cleanup.  My plist assumes Tomcat is installed in /Library/Tomcat/tomcat-version, with a symbolic link called ""Home"" pointing to the version to use.  Save this file as /Library/LaunchDaemons/org.apache.tomcat.plist:  <?xml version=""1.0"" encoding=""UTF-8""?>
<!DOCTYPE plist PUBLIC ""-//Apple Computer//DTD PLIST 1.0//EN"" ""http://www.apple.com/DTDs/PropertyList-1.0.dtd"">
<plist version=""1.0"">
<dict>
	<key>Disabled</key>
	<false/>
	<key>Label</key>
	<string>org.apache.tomcat</string>
	<key>ProgramArguments</key>
	<array>
		<string>/Library/Tomcat/Home/bin/catalina.sh</string>
		<string>run</string>
	</array>
	<key>RunAtLoad</key>
	<true/>
</dict>
</plist>  If you have a web app that doesn't want to shut down nicely (my company's product is one), then to avoid the need to force quit Tomcat just to turn off your Mac, use a startup script instead of a daemon plist.  Add this line to /etc/hostconfig as root:  TOMCAT=YES  Then, as root, create /Library/StartupItems/Tomcat, writable only by the owner (root).  Inside this, create two files - Tomcat (script) and StartupParameters.plist.  Again, make them writable only by root, and the script executable by everyone.  Contents of StartupParameters.plist:  <?xml version=""1.0"" encoding=""UTF-8""?>
<!DOCTYPE plist PUBLIC ""-//Apple Computer//DTD PLIST 1.0//EN"" ""http://www.apple.com/DTDs/PropertyList-1.0.dtd"">
<plist version=""1.0"">
<dict>
	<key>Description</key>
	<string>Tomcat Server</string>
	<key>OrderPreference</key>
	<string>Late</string>
	<key>Provides</key>
	<array>
		<string>Tomcat</string>
	</array>
	</dict>
</plist>  Contents of the Tomcat script file:  #!/bin/sh
#
# /Library/StartupItems/Tomcat/Tomcat
#
# A script to automatically start up Tomcat on system bootup
# for Mac OS X. This is actually just a wrapper script around
# the standard catalina.sh script, which is included in
# the distribution.
#

# Suppress the annoying ""$1: unbound variable"" error when no option
# was given
if [ -z $1 ] ; then
	echo ""Usage: $0 [start|stop|restart] ""
	exit 1
fi

# Source the common setup functions for startup scripts
test -r /etc/rc.common || exit 1
. /etc/rc.common

# The path to the catalina.sh script. 
# The currently used version is in /Library/Tomcat/Home/bin
SCRIPT=""/Library/Tomcat/Home/bin/catalina.sh""

# file to hold the process ID on start so it can be killed by stop.
export CATALINA_PID=""/Library/Tomcat/Home/server.pid""

StartService ()
{
	if [ ""${TOMCAT:=-NO-}"" = ""-YES-"" ] ; then
		ConsoleMessage ""Starting Tomcat server""
		$SCRIPT start > /dev/null 2>&1
	fi
}

StopService ()
{
	ConsoleMessage ""Stopping Tomcat server""
	$SCRIPT stop -force > /dev/null 2>&1
}

RestartService ()
{
	ConsoleMessage ""Restarting Tomcat server""
	StopService
	StartService
}

if test -x $SCRIPT ; then
	RunService ""$1""
else
	ConsoleMessage ""Could not find Tomcat control script!""
fi   CategoryFAQ",../data/confluence_exports/TOMCAT/TomcatOnMacOS_103100575.html
Apache Tomcat : Manchester - April 2018 Financials,Income Item£9 Tickets at £100 per ticket900.00Interest 2017-11-020.15Interest 2017-12-020.17Interest 2018-01-020.17Interest 2018-02-020.17Interest 2018-03-020.13Interest 2018-04-020.07Total to Date900.86  Expenses Item£9 Ticket fees at £8.39 per ticket75.51Meeting room (Portico)180.00Projector50.00Lunch 10 @ £16.96 per person169.60Hotel room for markt92.25Train ticket for markt61.90Pens (12)9.77Tomcat notebooks133.56Parking for markt12.00Mileage to station (37 @ 45p)16.65Coffee breaks72.00Dinner for markt12.00Total to Date885.24 Ticket fees are (2.5% + £1.99) + VAT + 3% Final balance is a surplus of £15.62,../data/confluence_exports/TOMCAT/Manchester---April-2018_75966764.html
Apache Tomcat : All about bugs Preface,"If you think you found a bug,please first read this page.",../data/confluence_exports/TOMCAT/All-about-bugs_103098759.html
Apache Tomcat : All about bugs Questions,"I have a bug, what do I do?Why does feature ABC work in Servlet Container XYZ but not in Tomcat?I submitted a bug, why is it ignored?What does it mean to contact the user list?",../data/confluence_exports/TOMCAT/All-about-bugs_103098759.html
Apache Tomcat : All about bugs Answers,"I have a bug, what do I do? Unless you have the source code reference in Tomcat which is wrong, it may not be a bug. E-mail theTomcat user listand confirm its a bug. Alsoread thisfirst from the tomcat site about reporting a bug. Why does feature ABC work in Servlet Container XYZ but not in Tomcat? There could be a good chance that the other servlet container could be implementing the spec wrong. For help, please consult theTomcat user list. I submitted a bug, why is it ignored? Any of the following may affect someone acting on a bug: Is it a bug?Is your description good?Is your description complete?Can a developereasilyreproduce the bug in their own environment?Have you provided a patch? Is it against the current sources from git?Is your patch coded well?Is your solution ""good""? Some developers are uncomfortable submitting patches to code they are unfamiliar with. The committers are not experts in all areas of Tomcat. It may be that the developer who has the ability to adequately review the patch may be too busy or unable to review the bug. That is the bad news, the good news is Tomcat is Open Source and you can build and use your own release until the fix gets back into the official release. What does it mean to contact the user list? If you are reading this - you were probably instructed to contact the user list for help. Bugzilla is not a support forum. Bugzilla is intended to fix ""real bugs"" in Tomcat. It is not intended to help you diagnose errors you are experiencing. The outcome of a diagnosis can be a new bug in Bugzilla. Configuration questions do not belong in Bugzilla. SeeTomcat Userfor what the Tomcat User list is.",../data/confluence_exports/TOMCAT/All-about-bugs_103098759.html
Apache Tomcat : TomcatAtApacheConNA2011 Attendees,Please registeron the linkif you plan to attend.,../data/confluence_exports/TOMCAT/TomcatAtApacheConNA2011_103100481.html
Apache Tomcat : KnownIssues Questions,What are the known issues in any given Tomcat version?What are the known issues with the Oracle JRE?What are the known issues with the OpenJDK?I'm using the Java ImageIO to dynamically serve images and get strange Exceptions from time to time. Is this a bug in Tomcat?,../data/confluence_exports/TOMCAT/KnownIssues_103098892.html
Apache Tomcat : KnownIssues What are the known issues in any given Tomcat version?,"To determine the known issues for any given Tomcat version, you'll need to review the following: The currently open bugs and enhancement requests in BugzillaThe latest (from svn) change log entries for all newer versions See chapterLooking for known issueson Tomcat web site.",../data/confluence_exports/TOMCAT/KnownIssues_103098892.html
Apache Tomcat : KnownIssues What are the known issues with the Oracle JRE?,jps.exe and jvisualvm.exe cannot detect tomcat using jdk1.6.0_23 onwards— Fixed in Java 1.6.0_25.,../data/confluence_exports/TOMCAT/KnownIssues_103098892.html
Apache Tomcat : KnownIssues What are the known issues with the OpenJDK?,"There have been reports that java.util.logging does not work properly in OpenJDK 1.7.0.9 and OpenJDK6 1.6.0_32. The symptom is ""java.lang.ClassNotFoundException: 1catalina.org.apache.juli.FileHandler"" errors when you start Tomcat. See these threads fromMarch 2013andJuly 2013. This issue was absent in earlier versions and should be fixed in a later version of those JDKs.",../data/confluence_exports/TOMCAT/KnownIssues_103098892.html
Apache Tomcat : KnownIssues I'm using the Java ImageIO to dynamically serve images and get strange Exceptions from time to time. Is this a bug in Tomcat?,"Imagine you have a servlet which dynamically generates images and serves them via javax.imageio.ImageIO. To write the image to the OutputStream, perhaps you are doing something like this: protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {
        BufferedImage img = createMyImage(); // makes a BufferedImage
        
        response.setContentType(""image/png"");
        try (OutputStream out = response.getOutputStream()) { // try-with-resources
            ImageIO.write(img, ""PNG"", out);
        } catch (IOException ex) {
            // Client aborted connection
        }
    } Now, although there shouldn't be any Exception logged (because the IOException which occurs when the client aborted the connection is ignored), you see strange Exceptions in Tomcat's log which may belong to other Servlets/JSP (at least with Sun/Oracle JVM on Windows), saying that the response has already been committed, although you didn't write anything to it at that time. For example: 13.07.2011 00:13:51 org.apache.catalina.core.StandardWrapperValve invoke
SEVERE: Servlet.service() for servlet [myApp.MyServlet] in context with path [] threw exception
java.lang.IllegalStateException: Cannot create a session after the response has been committed
	at org.apache.catalina.connector.Request.doGetSession(Request.java:2734)
        ... or maybe you use the ISAPI Redirector for IIS on Windows, and get these logs: [Tue Jul 12 06:04:49.812 2011] [4124:2444] [error] ajp_connection_tcp_get_message::jk_ajp_common.c (1296): wrong message format 0xdaed from 127.0.0.1:8019",../data/confluence_exports/TOMCAT/KnownIssues_103098892.html
Apache Tomcat : KnownIssues Is this a bug in Tomcat?,"Actually, it's a bug (or at least a strange behavior) in the Java ImageIO. When the ImageIO writes to an OutputStream and gets an IOException during writing, it could happen that some later time, when the ImageWriter is garbage-collected, the flush() method is called on that OutputStream. Tomcat recycles OutputStream objects to save resources, so it could be that when flush() is called from the ImageIO, the particular OutputStream object already belongs to another Response, which can produce the above errors, when the Servlet tries to get a Session for example, or can generally lead to broken responses. See alsohereor thisBug report.",../data/confluence_exports/TOMCAT/KnownIssues_103098892.html
Apache Tomcat : KnownIssues So how to resolve the errors?,"To resolve this, I'm using an OutputStream decorator class which decorates Tomcat's OutputStream and prevents any flush() calls. Additionally, when close() is called on that Stream, it nulls-out the reference to Tomcat's OutputStream and prevents any other operations: /**
 * A OutputStream which can be used to write Images
 * with the ImageIO in servlets.
 */
public class MyImageIOOutputStream extends OutputStream {

    private OutputStream out;
    private volatile boolean isActive = true;

    public MyImageIOOutputStream(OutputStream out) {
        this.out = out;
    }

    @Override
    public void close() throws IOException {
        if (isActive) {
            isActive = false; // deactivate
            try {
                out.close();
            } finally {
                out = null;
            }
        }
    }

    @Override
    public void flush() throws IOException {
      if(isActive) {
        out.flush();
      }
      // otherwise do nothing (prevent polluting the stream)
    }

    @Override
    public void write(byte[] b, int off, int len) throws IOException {
        if (isActive)
            out.write(b, off, len);
    }

    @Override
    public void write(byte[] b) throws IOException {
        if (isActive)
            out.write(b);
    }

    @Override
    public void write(int b) throws IOException {
        if (isActive)
            out.write(b);
    }
} Now you just have to use this Decorater class instead of using Tomcat's OutputStream directly: response.setContentType(""image/png"");
    try (OutputStream out = new MyImageIOOutputStream(response.getOutputStream())) {
        ImageIO.write(img, ""PNG"", out);
    } catch (IOException ex) {
        // Client aborted connection
    } and the errors should be gone away. An alternative would be to write the Image contents to a ByteArrayOutputStream, and using its writeTo() method to write the contents to the Servlet's Response. However that would require some additional memory, as the contents have to be buffered.",../data/confluence_exports/TOMCAT/KnownIssues_103098892.html
Apache Tomcat : KnownIssues Are there any other corresponding cases of this bug?,"The third party PDF generating software module PD4ML has had a corresponding problem when calling the render() methods in class org.zefer.pd4ml.PD4ML with response.getOutputStream() as argument. That causes the response stream to be closed from a finalizer() method of a class called PD4Device. When using an Apache/Tomcat connector, this unexpected stream close from the finalizer thread has occationally caused responses to be sent to wrong requestor (request/response mix up). The workarounds described above for ImageIO works perfectly in this case too. A general way to protect the response output streams from misbehaving web applications is to set the system property org.apache.catalina.connector.RECYCLE_FACADES=true, since that makes Tomcat create new stream instances for each request (of course at the cost of performance). PD4ML has fixed this bug in their latest releases, but sites using older versions of the library can still be affected. PD4ML version 3.2.3 definitely has this flaw, but the currently latest version 3.8.0 is fixed. The release notes document gives no clues where in between the problem was fixed, and the vendor was not able to tell either inthis bug report.",../data/confluence_exports/TOMCAT/KnownIssues_103098892.html
Apache Tomcat : TomcatTrackUs09 Draft Schedule,"08:00 - 09:00REGISTRATION09:00 - 09:30Opening09:30 - 10:30Keynote10:30 - 11:00Break11:00 - 12:00PMC track: Tomcat community overview, Mladen Turk12:00 - 13:30LUNCH13:30 - 14:30457: Introduction to Apache Tomcat 7, Mark Thomas14:30 - 15:30518: Asynchronous servlet processing in Apache Tomcat 7.0,  Filip Hanik15:30 - 16:00BREAK16:00 - 17:00PMC track: Securing your Tomcat installation, Tim Funk17:00 - 18:00PMC track: mod_jk / mod_proxy and others, Mladen Turk/Rainer Jung/Jean-Frederic Clere  Seehttp://wiki.apache.org/tomcat/TomcatTrackUs09_Sessionsandhttp://wiki.apache.org/tomcat/TomcatTrackUs09_PMC_Sessionsfor the long description of the sessions.",../data/confluence_exports/TOMCAT/TomcatTrackUs09_103100645.html
Apache Tomcat : Connectors Preface,"Please see theOther Resources Linkfor other pages describing how they were able to link Tomcat with a connector. With luck, someone documented their experience in an environment which is similar to yours. Here is a link to theApache Tomcat Connectors(aka JK Connectors) project page. It contains more configuration and installation information. Please note, jk2 is no longer supported. Please use mod_jk instead.",../data/confluence_exports/TOMCAT/Connectors_103098826.html
Apache Tomcat : Connectors Questions,"What is JK (or AJP)?Which connector: mod_jk or mod_proxy?What about mod_jserv, mod_jk2, mod_webapp (aka warp)?Why should I integrate Apache HTTP Server with Apache Tomcat? (or not)At boot, is order of start up (Apache HTTP Server vs Apache Tomcat) important?Is there any way to control the content of automatically generated mod_jk.conf-auto?How do I bind to a specific IP address?Where can I download a binary distribution of my connector?I'm having strange UTF-8 issues with my request parameters.How do I configure apache tomcat connectors for a heavy load site?",../data/confluence_exports/TOMCAT/Connectors_103098826.html
Apache Tomcat : Connectors What is JK (or AJP)?,"AJP is a wire protocol. It an optimized version of the HTTP protocol to allow a standalone web server such asApacheto talk to Tomcat. Historically, Apache has been much faster than Tomcat at serving static content. The idea is to let Apache serve the static content when possible, but proxy the request to Tomcat for Tomcat related content.",../data/confluence_exports/TOMCAT/Connectors_103098826.html
Apache Tomcat : Connectors Which connector: mod_jk or mod_proxy?,"mod_jk is mature, stable and extremely flexible. It is under active development by members of the Tomcat community.mod_proxy_ajp is distributed with Apache httpd 2.2 and later. Note that the communication protocol used is AJP.mod_proxy_http is a cheap way to proxy without the hassles of configuring JK. If you don't need some of the features of mod_jk, this is a very simple alternative. Note that the communication protocol used is HTTP/1.1.mod_proxy_http2 uses HTTP/2 as communication protocol. I has support for both secure (h2) and cleartext (h2c) variants of HTTP/2. Both are understood by Tomcat 8.5 and later. Here are some anecdotal comments from members of the Tomcat community: _ I have been using mod_jk for a very long time and I saw (at the time) only one reason to make the switch to mod_proxy_ajp: it is bundled with Apache and so you (likely) don't have to build the module yourself. That said, simple configurations are *way* more simple in mod_proxy_ajp than with mod_jk, although the (somewhat) recent addition ofJkWorkerPropertyandJkMount""extensions"" do help quite a bit. mod_proxy_ajp can also be trivially swapped-out with mod_proxy_http just by changing the URLs in yourProxyPassandProxyPassReversedirectives to say http:// (or https://) instead of ajp://. This might help you if you need to switch protocols for debugging purposes or if you suddenly need switch to HTTPS to secure the traffic without any external configuration (e.g. stunnel or VPN). (SeeAJP with stunnel.) mod_proxy also supportsProxyPassMatchdirective which lets you use regular expressions in your URL mappings, which mod_jk'sJkMountdoes not (though you *can* use<LocationMatch>along withSetHandlerin order to achieve the same result, it's a cleaner configuration with mod_proxy). That said, I have found that mod_jk supports more complicated configurations where I have struggled to get mod_proxy_ajp to do the same. Specifically, overlapping URL spaces that must be mapped to separate workers. Technically speaking, I suppose you could use lots ofProxyPassMatchdirectives and/or have a complex regular expression to direct the various URLs, but again you end up with a rather messy configuration that way. Messy configurations are a maintenance risk as well as at risk of becoming ""arcane knowledge"" that nobody actually understands and so they are afraid to modify it for any reason. Generally, mod_jk will get fixed faster than mod_proxy_ajp due to its independent release cycle: the httpd folks might have a fix for a problem but it doesn't get released for a while due to testing of other components, etc. At this point, mod_proxy_ajp has (IMHO) reached a point of stability that this is less of an issue than it used to be. At this stage, there is no reason for me to move any of my projects from mod_jk to mod_proxy_ajp but if I were starting from scratch, I might choose mod_proxy_ajp solely due to its binary availability and simple configuration. If the configuration became complicated to the extent that switching to mod_jk were a good option, then I'd move. As for performance, I have no data on that one way or another. I would suspect that mod_jk has a slight performance advantage because it has been especially designed for the purpose rather than mod_proxy_ajp which must support the mod_proxy API and might have a bit more plumbing code to accomplish that. I would be surprised if you could detect any performance difference between the two if you were to test them both faithfully and with compatible configurations. If anyone has relative performance data between mod_jk and mod_proxy_ajp, I'd be happy to read it._(Source:https://tomcat.markmail.org/message/u5v4aiejluzy7tde)",../data/confluence_exports/TOMCAT/Connectors_103098826.html
"Apache Tomcat : Connectors What about mod_jserv, mod_jk2, mod_webapp (aka warp)?","All of these connectors have been abandoned long ago. Do not use any of them. mod_jk2 sounds like it could be an updated version of mod_jk, it is not: it was an aborted effort whose features have been re-incorporated into mod_jk. For historical purposes, and emphasis: mod_jserv is unsupported and will not be supported in Tomcat 5 onward. mod_jserv was the original connector which supported the ajp protocol.Do not use mod_jserv.Stay away from mod_webapp, aka warp. It is deprecated and unsupported due to lack of developer interest and there are better options such as jk and mod_proxy. It WILL NOT run on windows.Do not use mod_webapp or warp.jk2 is a refactoring of mod_jk and uses the Apache Portable Runtime (apr). But due to lack of developer interest, it is unsupported.Do not use mod_jk2.",../data/confluence_exports/TOMCAT/Connectors_103098826.html
Apache Tomcat : Connectors Why should I integrate Apache HTTP Server with Apache Tomcat? (or not),"There are many reasons to integrate Tomcat with Apache. And there are reasons why it should not be done too. Needless to say, everyone will disagree with the opinions here. With the performance of Tomcat 5 and 6, performance reasons become harder to justify. So here are the issues to discuss in integrating vs not. Clustering. By using Apache as a front end you can let Apache act as a front door to your content to multiple Tomcat instances. If one of your Tomcats fails, Apache ignores it and your Sysadmin can sleep through the night. This point could be ignored if you use a hardware loadbalancer and Tomcat's clustering capabilities.Clustering/Security. You can also use Apache as a front door to different Tomcats for different URL namespaces (/app1/, /app2/, /app3/, or virtual hosts). The Tomcats can then be each in a protected area and from a security point of view, you only need to worry about the Apache server. Essentially, Apache becomes a smart proxy server.Security. This topic can sway one either way. Java has the security manager while Apache has a larger mindshare and more tricks with respect to security. I won't go into this in more detail, but let Google be your friend. Depending on your scenario, one might be better than the other. But also keep in mind, if you run Apache with Tomcat - you have two systems to defend, not one.Add-ons. Adding on CGI, perl, PHP is very natural to Apache. Its slower and more of a kludge for Tomcat. Apache also has hundreds of modules that can be plugged in at will. Tomcat can have this ability, but the code hasn't been written yet.Decorators! With Apache in front of Tomcat, you can perform any number of decorators that Tomcat doesn't support or doesn't have the immediate code support. For example, mod_headers, mod_rewrite, and mod_alias could be written for Tomcat, but why reinvent the wheel when Apache has done it so well?Speed. Apache is faster at serving static content than Tomcat. But unless you have a high traffic site, this point is useless. But in some scenarios, tomcat can be faster than Apache httpd. So benchmark YOUR site.Tomcat can perform at httpd speeds when using the proper connector (APR with sendFile enabled). Speed should not be considered a factor when choosing between Apache httpd and TomcatSocket handling/system stability. Apache has better socket handling with respect to error conditions than Tomcat. The main reason is Tomcat must perform all its socket handling via the JVM which needs to be cross platform. The problem is socket optimization is a platform specific ordeal. Most of the time the java code is fine, but when you are also bombarded with dropped connections, invalid packets, invalid requests from invalid IP's, Apache does a better job at dropping these error conditions than JVM based program. (YMMV)",../data/confluence_exports/TOMCAT/Connectors_103098826.html
"Apache Tomcat : Connectors At boot, is order of start up (Apache HTTP Server vs Apache Tomcat) important?",No. This way either Apache HTTPd or Apache Tomcat can be restarted at any time independently of one another.,../data/confluence_exports/TOMCAT/Connectors_103098826.html
Apache Tomcat : Connectors Is there any way to control the content of automatically generated mod_jk.conf-auto? I need my own specific commands added to it.,There really is no need to. Just copy the automatically generated mod_jk.conf-auto and edit it manually to your preference. None of production tomcat installations really use mod_jk.conf-auto as it is.,../data/confluence_exports/TOMCAT/Connectors_103098826.html
Apache Tomcat : Connectors How do I bind to a specific IP address?,Each Connector element allows anaddressproperty. See theHTTP Connector docsor theAJP Connector docs.,../data/confluence_exports/TOMCAT/Connectors_103098826.html
Apache Tomcat : Connectors Where can I download a binary distribution of my connector?,You cannot: you need to download the source and compile it for your platform. The source distributions are available from thestandard location. Note that JPackage.org has RPM distributions for the connectors as well as tomcat itself:JPackage.org,../data/confluence_exports/TOMCAT/Connectors_103098826.html
Apache Tomcat : Connectors I'm having strange UTF-8 issues with my request parameters.,SeeCharacter Encoding,../data/confluence_exports/TOMCAT/Connectors_103098826.html
Apache Tomcat : Connectors How do I configure Apache Tomcat connectors for a heavy load site?,SeePerformance and Monitoring,../data/confluence_exports/TOMCAT/Connectors_103098826.html
Apache Tomcat : Cookies Implementation Progress,"I started work on this in a local branch. Patches for the changes made there can be found here:http://people.apache.org/~jboynes/patches/Of these, patches 01 to 12 have been applied.  There is substantial refactoring in there to simply the current implementation. Actual changes are:  C3 '=' is now disallowed in Netscape cookie names (it was already not allowed in RFC2109 names)C4 Attribute names are allowed as cookies namesCookie names starting with '$' are allowed in Netscape and RFC6265 mode and will still throw an IAE in RFC2109 mode",../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : Cookies Round Trip Behaviour,"The following tables document how a value is sent in a Set-Cookie header, what gets stored by a typical browser, the Cookie header that is generated by the browser and then the final value returned to a Servlet application.  The browser tested here is Chrome-31",../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : Cookies Default Configuration (no properties set),"GenerationBrowser ValueParsingVersionValueSet-Cookie HeaderCookie HeaderResulting Value0bartest=barbartest=barbar0""bar""test=""bar""""bar""test=""bar""bar0""""test=""""""""test=""""emptyString0a""btest=""a\""b""; Version=1""a\""b""test=""a\""b""a""b0a\btest=""a\b""; Version=1""a\b""test=""a\b""ab0a?btest=""a?b""; Version=1""a?b""test=""a?b""a?b1bartest=bar; Version=1bartest=barbar",../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : Cookies ALLOW_HTTP_SEPARATORS_IN_V0=true,"GenerationBrowser ValueParsingVersionValueSet-Cookie HeaderCookie HeaderResulting Value0bartest=barbartest=barbar0""bar""test=""bar""""bar""test=""bar""bar0""""test=""""""""test=""""emptyString0a""btest=a""ba""btest=a""ba""b0a\btest=a\ba\btest=a\bab0a?btest=a?ba?btest=a?ba?b0a;btest=""a;b""; Version=1""atest=""ano cookie0a,btest=""a,b""; Version=1""a,b""test=""a,b""a,b1bartest=bar; Version=1bartest=barbar",../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : Cookies Proposed Changes,"The intent of the following changes is improve interoperability of cookies with other servers and with client-side JavaScript. The primary changes are a switch to the RFC6265 format for transmission of V0 cookies to be more in line with browser behaviour, and support for UTF-8 encoded values that are now specified by HTML-5.  This is very preliminary and intended primary to focus discussion on something concrete now the behavior has been clarified.",../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : Cookies Changes to Cookie class,"C1 Stricter default validation of name::  *  <p>Change the default value of STRICT_NAMING to be true even if STRICT_SERVLET_COMPLIANCE is false. Application impact is that applications that wish to set cookies with names that are valid per Netscape's rules but that are not valid ""tokens"" per RFC2109 or RFC6265 will need to explicitly set this system property. The intent of the change is to notify application developers that they are using a cookie name that is likely to have interoperability issues.</p>  *  <p><strong>Alternative C1a:</strong> remove option for Netscape naming entirely. Applications that need to set names that do not comply with RFC2109 and RFC6265 would need to sub-class Cookie themselves. If this is common, then we could provide a default implementation of that behaviour (e.g. o.a.t.NetscapeCookie).</p>  *  <p><strong>Alternative C1b:</strong> Make STRICT_NAMING a enum specifying which standard's rules to enforce: values are ""netscape"" ""rfc2109"" or ""rfc6265"" with the default being ""rfc6265."" Maintain compatibilty by allowing ""true"" as an alias for ""rfc2109"" and ""false"" as an alias for ""netscape"" with the option defaulting to ""rfc6265"" or to ""rfc2109"" if STRICT_SERVLET_COMPLIANCE is true. ""rfc2109"" and ""rfc6265"" are both based on ""token"" rules, except ""rfc2109"" disallows values starting with '$' character.</p>  C2 Always allow ""/"" in Netscape cookie names::  *  <p>Discontinue use of FWD_SLASH_IS_SEPARATOR to configure whether a ""/"" character can appear in a name when STRICT_NAMING is false and instead always allow it. No negative application impact and matches the behaviour of the RI. This property was introduced to prevent quoting of tokens used in Path values as that is not supported by IE but that behaviour is not needed for names.</p>  C3 Always disallow ""="" in Netscape cookie names::  *  <p>Now throw IllegalArgumentException if a ""="" character is present. Application impact is that an attempt to use ""="" will now trigger an IAE before the cookie is sent rather than having the browser set a cookie with an inconsistent name and value. When parsing the received Set-Cookie header, browsers treat all characters up to the first ""="" character as the name and the remainder as the value. Having a ""="" character in the name will result in an incorrect split.</p>  C4 Always allow attribute names (e.g. ""Expires"") as cookie names::  *  <p>Stop throwing IAE if an attribute name is used as the cookie name. No application impact as more values are allowed. No confusion with cookie protocols as they are unambiguous in Set-Cookie and are never used as part of a Cookie header (attributes in the RFC2109 Cookie header begin with '$').</p>  C5 Allow unnamed cookies in C1b ""netscape"" mode::  *  <p>Allow cookies whose name is null or the empty string. Browsers will store a single cookie that has no name whose value is sent as simply «value» (i.e. without any '=' delimiter). This would now be supported if STRICT_NAMING is set to ""netscape"" but would remain disallowed in ""rfc2109"" or ""rfc6265"" modes. If allowed, the Set-Cookie header would contain just the value (no '=' present and an IAE if value contained an '=') and any such cookie found during parsing would be included in the result of <span class=""error"">&#91;HttpServletRequest&#93;</span>#getCookies().</p>",../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : Cookies Changes to generation of Set-Cookie header,"G1 Use RFC6265 format header for V0 cookies::  *  <p>When version == 0 always generate a RFC6265 header, raising an exception from addCookie if the value is invalid rather than attempting to upgrade to a RFC2109 header to use quoting. Application impact is that they will now fail fast with an error rather than inconsistent data as described in Bug 55920; applications that do not set invalid values will not be impacted.</p>  *  <p><strong>Alternative G1a:</strong> Generate an RFC6265 header if possible but provide an option (disabled by default) to allow switching to an RFC2109 header if a valid RFC6265 header is not possible.</p>  G2 Use RFC2109 format header only for V1 cookies::  *  <p>When version == 1 always generate a RFC2109 header, raising an exception from addCookie if the value is invalid. This preserves existing behaviour for applications that use V1 cookies.</p>  G3 Stop adding quotes or escaping to values::  *  <p>The value supplied by the application will be validated to the relevant specification and will result in a IAE if it does not conform. The value will never be modified to add quotations or escape characters, Application impact is that an attempt to set an invalid value will result in an early error rather than inconsistent data.</p>  *  <p><strong>Alternative G3a:</strong> Quotes and/or escaping only to be added to RFC2109 headers. API to remain symmetric and quoting/escaping to remain transparent to applicatons.</p>  G4 Use UTF-8 encoding for values::  *  <p>The value (which is a UCS-16 Java String) will be encoded using UTF-8 when being added to the header. Application impact is that non-ASCII characters will no longer cause an IAE. For V0 cookies, this is an extension to RFC6265 required to support HTML-5. V1 cookies already allow 8-bit characters if quoted and this is likely to be needed to avoid an IAE as the value would still be validated; it would be the application's responsibility to quote the value.</p>  *  <p><em>kkolinko</em>: Using UTF-8 in HTTP headers is not allowed by RFC 2616. On page 32 it says:</p>            message-header = field-name "":"" [field-value ]            field-value    = *( field-content | LWS )  field-content  = <the OCTETs making up the field-value and consisting of either *TEXT or combinations of token, separators, and quoted-string>  The tokens are US-ASCII (0-127 minus CTLs or separators) (pages 16-17).            The TEXT is defined on page 16 where it says: ""Words of *TEXT MAY contain characters from character sets other than ISO-8859-1 [22] only when encoded according to the rules of RFC 2047 [14].""            The quoted-string is TEXT in double quotes (page 16).  *  <p><em>kkolinko</em>: Javadoc for <span class=""error"">&#91;HttpServletResponse&#93;</span>.setHeader() method also mentions that the value of a header should be encoded according to RFC 2047. <span class=""nobr""><a href=""http://www.ietf.org/rfc/rfc2047.txt"" class=""external-link"" rel=""nofollow"">http://www.ietf.org/rfc/rfc2047.txt<sup><img class=""rendericon"" src=""/confluence/images/icons/linkext7.gif"" height=""7"" width=""7"" align=""absmiddle"" alt="""" border=""0""/></sup></a></span></p>  G5 Validate domain per RFC6265::  *  <p>The domain will now be validated per RFC1034 rather than simply as a value. Application impact is that an invalid domain will now raise an IAE rather than be rejected by the browser. No semantic validation (e.g. number of dots) will be performed. A valid domain name is a ""token"" and so no quotation would be needed.</p>  G6 Do not quote Path values for V0 cookies::  *  <p>The quotes needed to make a Path value valid per RFC2109 will no longer be added for V0 cookies. No application impact. Paths contain ""/"" characters which require quoting in a RFC2109 value which causes issues for IE which does no expect the quotes; the FWD_SLASH_IS_SEPARATOR property was used to prevent them being added. This behaviour is not needed for a V0 cookie.</p>  G7 Always set both Max-Age and Expires as a pair::  *  <p>When maxAge &gt;= 0, then always set both Expires (Netscape) and Max-Age (RFC2109 and RFC6265) attributes to support older and newer browsers. This removes the need for the ALWAYS_ADD_EXPIRES system property.</p>",../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : Cookies Changes to parsing of Cookie header,"P1 Parse non-RFC2109 cookies using a lenient RFC6256 parser::  *  <p>Parse any header not starting with ""$Version=1"" as a RFC6256 style header and do not attempt to apply RFC2109 rules. Application impact will be that more consistent parsing with the way user-agents are generating the header which will mean more cookies to be accepted; cookie values containing quotes or escape characters may be changed.</p>  The lenient parser for V0 cookies will allow:  A cookie name will accept all CHARs up to semicolon or equals. Leading and trailing spaces will be trimmed. This relaxes RFC6265 to accommodate additional characters browsers can use in names but does not allow for 8-bit characters in names.A cookie value will accept all octets up to a semicolon decoded using UTF-8. Leading and trailing spaces will be trimmed. This relaxes RFC6265 to accommodate HTML5.Issue:Any commas added by header folding prior to receipt will be accepted as part of the previous cookie's value. This is indicative of a mis-behaving proxy or user-agent and conflicts with browsers that accept commas in values.Issue:There is a conflict with Safari (WebKit?) here in that it does allow semicolons in values that start/end with DQUOTE. However, the other browsers will truncate the value at the semicolon when it is being set resulting in a mismatched pair.Any invalid character will result in the cookie being dropped. Parsing will restart at the next semicolon.Any cookie whose name begins with ""$"" will be dropped to avoid confusion with RFC2109 attributes. RFC6265 and Netscape do not support $Path and $Domain attributes and so any value they define will not be used to set fields in a Cookie object.Cookies whose name matches an attribute (e.g. ""Expires"") will be permitted.  This generally matches the rules defined by Netscape and RFC6265 for the Cookie header.  P2 Parse RFC2109 requests (determined by the presence of a ""$Version=1"" attribute) using the current parser::  *  <p>Retains current behaviour if the browser sends a RFC2109 header.</p>  *  <p><strong>Issue:</strong> The notes below that shaped this proposal have not be checked against a browser that actually sends a RFC2109 format header.</p>  P3 Do not throw IAE from the parser::  *  <p>Invalid syntax will result in a user-data log entry and cookies being dropped rather than throwing of an IAE. Application impact is that requests with an invalid Cookie header will now be dispatched to the application. ""Dropping a cookie"" means an invalid cookie will not appear in the list returned by HttpServletRequest#getCookies(). An application will still be able to access the original Cookie header and may perform its own parsing.</p>  P4 Ensure that the cookie header is always available for the application to parse manually.::  *  <p>Stop modifying the header in-situ as part of the de-escaping process (<span class=""nobr""><a href=""https://bz.apache.org/bugzilla/show_bug.cgi?id=57896"" class=""external-link"" rel=""nofollow"">Bug 57896<sup><img class=""rendericon"" src=""/confluence/images/icons/linkext7.gif"" height=""7"" width=""7"" align=""absmiddle"" alt="""" border=""0""/></sup></a></span>) so that an application can elect to perform its own parsing by calling getHeader(""Cookie""). Eliminate the need for the PRESERVE_COOKIE_HEADER property that currently controls whether a copy of the header is made if modifications are needed. Perform de-escaping during the copy needed to convert the MessageBytes to the String in Cookie#value, possibly during any conversation process needed to handle UTF-8.</p>",../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : Cookies Impact of proposal on existing issues,"IssueImpactBug 55917Parsing will no longer cause an IAE. 8-bit values will be interpreted as a UTF-8 value and the cookie would be dropped if they are not a valid encoding.Bug 55918The cookie would be dropped rather than accepted.Bug 55920Valid values would be round tripped including quotes supplied by the application. Attempts to set invalid values would result in a IAE from addCookie. Invalid values sent by the browser would result in the cookie being ignored.Bug 55921Attempts to set a cookie containing raw JSON would results in an IAE due to the DQUOTE characters. A cookie sent from the browser containing JSON would be accepted although any semicolons in the data would result in early termination (note, browsers other than Safari do not allow semicolons in values anyway).",../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : Cookies Parsing the Cookie header by Tomcat,"The various specifications define the following formats for the Cookie header sent by the user-agent:  SpecificationFormat of Cookie headerNetscapeCookie: NAME1=OPAQUE_STRING1; NAME2=OPAQUE_STRING2 ...RFC2109{{""Cookie:"" ""$Version"" ""="" value 1*(("";"""","") cookie-value)}}RFC6265""Cookie:"" OWS cookie-pair *( "";"" SP cookie-pair ) OWS  Chrome-31, Firefox-26, Firefox Aurora-28, Internet Explorer-11 and Safari-7.01 all send a single header in Netscape/RFC6265 format with name=value pairs separated by semicolon and space. The name and value correspond to whatever was stored in the browser when the ""Set-Cookie"" header was parsed. These may contain commas, spaces, other separators or 8-bit characters.  None of them add any of the ""$"" attributes (""$Version"" ""$Domain"" or ""$Path) from RFC2109 and specifically do not send the leading ""$Version"" attribute that is part of that specification's syntax. All except Safari support a unnamed ""value-only"" cookie that is sent as is (without a name or ""=""); i.e. a unnamed cookie with value ""foo"" (including quotes) is sent as the line:  Cookie: ""foo""  When set through JavaScript, any Unicode codepoints in the text are encoded as UTF-8 in the header. For example, in Chrome the statementdocument.cookie = ""foo=b\u00e1r"";will result in a header containing the octets  43 6f 6f 6b 69 65 3a 20 66 6f 6f 3d 62 c3 a1 72  showing codepoint U+00E1 being converted to its UTF-8 equivalent 0xC3 0xA1. This matches the behaviour defined byHTML5.  IssueCurrent behaviour (8.0.0-RC10/7.0.50)Proposed new behaviourServlet + Netscape + RFC2109Servlet + RFC 62650x80 to 0xFF in cookie value (Bug 55917)IAETBDNetscape yes. RFC2109 requires quotes.RFC 6265 never allowed.CTL allowed in quoted cookie values (Bug 55918)AllowedTBDNot allowed.Not allowed.Quoted values in V0 cookies (Bug 55920)Quotes removed.TBDNetscape - quotes are part of value.Quotes are not part of value.Raw JSON in cookie values (Bug 55921)TBDTBDTBDTBDAllow equals in valueNot by default. Allowed if property set.TBDNetscape is ambiguous. RFC2109 requires quoting.Allowed.Allow separators in V0 names and valuesNot by default. Allowed if property set.TBDYes except semi-colon, comma and whitespace.Never in names. Yes in values except semi-colon, comma and whitespace, double-quote and backslash.Always add expiresEnabled by default. Disabled by property.TBDNetsacpe uses expires. RFC2109 uses Max-Age.Allows either, none or both./ is separatorEnabled by default. Disabled by property.TBDNetscape allowed in names and values. RFC2109 allowed in values if quoted.Allowed in values.Strict naming (as per Servlet spec)Enabled by default. Disabled by property.TBDNetscape allows names the Servlet spec does not. RFC2109 is consistent with the Servlet spec.Consistent with the Servlet spec.Allow name onlyDisabled by default. Enabled by property.TBDNetscape allowed and equals sign expected before empty value. RFC2109 not allowed.Allowed but equals sign required before empty value.  Issues to add to the table above  Bug 55951regarding UTF-8 encoded values from HTML5Any further issues raised on mailing lists",../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : Cookies Requirements as defined by the specifications,"RequirementServletNetscapeRFC2109RFC6265Format of nameMust conform to RFC2109. Vendors may provide option to allow Netscape formatA sequence of characters excluding semi-colon, comma and white space. Browsers generally stop at first equals,tokentokenFormat of valueThe value can be anything the server chooses to send. With Version 0 cookies, values should not contain white space, brackets, parentheses, equals signs, commas, double quotes, slashes, question marks, at signs, colons, and semicolons. Empty values may not behave the same way on all browsers.This string is a sequence of characters excluding semi-colon, comma and white space.tokenquoted-stringcookie-valueDomainString, per RFC2109domain=DOMAIN_NAME""Domain"" ""="" value""Domain="" domain-valuePathString, per RFC2109path=PATH""Path"" ""="" value""Path="" path-valueSecurebooleansecure""Secure""""Secure""HttpOnlybooleanN/AN/A""HttpOnly""ExpiresN/Aexpires=DATE as ""Wdy, DD-Mon-YYYY HH:MM:SS GMT""N/A""Expires="" sane-cookie-dateMax-Ageint in secondsN/A""Max-Age"" ""="" value""Max-Age="" non-zero-digit *DIGITCommentStringN/A""Comment"" ""="" valueallowed by extensionVersionint (0 or 1)N/A""Version"" ""="" 1*DIGITallowed by extensionExtensionN/AN/AN/Aany CHAR except CTLs or "";""  The RI defines a vendor system property ""org.glassfish.web.rfc2109_cookie_names_enforced"" (default true) that controls the characters permitted in the name argument. If true, RFC2616 separators (including ""/"") will trigger an IllegalArgumentException; if false, only comma, semicolon and space are considered invalid.",../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : Cookies Cookie,"The constructor of javax.servlet.http.Cookie will throw an IllegalArgumentException if any of the following conditions are met:  name is null or zero lengthif name is not a tokenif name equalsIgnoreCase any of ""Comment"" ""Discard"" ""Domain"" ""Expires"" ""Max-Age"" ""Path"" ""Secure"" ""Version""if name startsWith ""$""            By default, a token comprises characters 0x21..0x7E except comma, semicolon and space. If STRICT_NAMING is true, then token also excludes characters from ""()<>@,;:\\\""[]?={} \t"" which corresponds to RFC2616 separators without ""/"" (i.e. ""/"" is allowed); if FWD_SLASH_IS_SEPARATOR is true than ""/"" is also excluded. These properties will default to true if STRICT_SERVLET_COMPLIANCE is true.            Issues    *  <p>the ""HttpOnly"" attribute is not covered by the check</p>  *  <p>by default, a ""="" character is allowed in a name (browsers treat the name as everything up to the first equals)</p>  No checks are made in any of the other setters.  The domain value is converted to lower case (per Locale.ENGLISH) when set as ""IE allegedly needs this.""  Neither the Cookie class or any of its methods are declared final so any of this behaviour can be overridden if an application sub-classes Cookie; for example, the checks performed on the name can be bypassed by overriding the getName() method.",../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : Cookies HttpServletResponse,"This is typically implemented by o.a.c.connector.Response whose addCookie method delegates generation of the Set-Cookie header to o.a.t.util.http.ServerCookie#appendCookieValue. This first appends the name (relying on checks performed by Cookie), ""="" and then the value using RFC2109 quoting rules:  if the value is null or empty, append empty quoted-string """"if the value starts and ends with '""', output as is after escaping any '""' characters between the outer quotesif ALLOW_HTTP_SEPARATORS_IN_V0 is false and the value contains a RFC2616 separator, output as a quoted-string after escaping '""' and force Version=1if ALLOW_HTTP_SEPARATORS_IN_V0 is true and the value contains a Netscape separator, output as a quoted-string after escaping '""' and force Version=1otherwise, output as is  Netscape separators are {',', ';', ' ', '\t'}RFC2616 separators by default do not include ""/"" unless FWD_SLASH_IS_SEPARATOR is set (or implied by STRICT_SERVLET_COMPLIANCE). Characters outside the set { HT, 0x20..0x7E } will result in a IllegalArgumentException when the check for token characters is performed.  The same quoting rules are applied when outputting any Domain or Path value.  If maxAge >=, then the Max-Age attribute will be set for V1 cookies and the Expires attribute for V0 cookies. If the property ALWAYS_ADD_EXPIRES is true then Expires will also be set for V1 cookies.  Issues::  *  <p>relies on the browser supporting RFC2109 quoting rules when Version=1 (most apply Netscape rules)</p>  *  <p>Domain is not strictly checked</p>  *  <p>Path is quoted using the same rules as Value; browsers treat them differently (e.g. IE treats quoted paths as invalid)</p>",../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : Cookies Proposed Implementation,TBD,../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : Cookies RFC2616 definitions,"token          = 1*<any CHAR except CTLs or separators>
separators     = ""("" | "")"" | ""<"" | "">"" | ""@"" | "","" | "";"" | "":"" | ""\"" | <""> | ""/"" | ""["" | ""]"" | ""?"" | ""="" | ""{"" | ""}"" | SP | HT
CHAR           = <any US-ASCII character (octets 0 - 127)>
CTL            = <any US-ASCII control character (octets 0 - 31) and DEL (127)>
quoted-string  = ( <""> *(qdtext | quoted-pair ) <""> )
qdtext         = <any TEXT except <"">>
quoted-pair    = ""\"" CHAR
TEXT           = <any OCTET except CTLs, but including LWS>
rfc1123-date   = wkday "","" SP date1 SP time SP ""GMT""",../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : Cookies RFC2109 definitions,"cookie-value   = NAME ""="" VALUE ["";"" path] ["";"" domain]",../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : Cookies RFC6265 definitions,"cookie-pair       = cookie-name ""="" cookie-value
cookie-value      = *cookie-octet / ( DQUOTE *cookie-octet DQUOTE )
cookie-octet      = %x21 / %x23-2B / %x2D-3A / %x3C-5B / %x5D-7E
domain-value      = <subdomain> ; defined in [RFC1034], Section 3.5, as enhanced by [RFC1123], Section 2.1
path-value        = <any CHAR except CTLs or "";"">
sane-cookie-date  = <rfc1123-date, defined in [RFC2616], Section 3.3.1>",../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : Cookies References,RFC6265 discussion on 0x80-0xFF,../data/confluence_exports/TOMCAT/Cookies_61320785.html
Apache Tomcat : OutOfMemory The General Rule,"The first thing to do is to set the basis for these patterns to be recognized. This way, the developer will be able to find even those mistakes that are not listed in this page, and why not, add them here An Out Of Memory can be thrown by several causes: A servlet trying to load a several GBytes file into memory will surely kill the server. These kind of errors must be considered a simple bug in our program.To compensate for the data your servlet tries to load, you increase the heap size so that there is no room to create the stack size for the threads that need to be created. The memory required by each thread will vary by OS but can be as high as 2M by default and in some OS's (like Debian Sarge) is not reducible with the -Xss parameter. Rule of Thumb, use no more than 1G for heap space in a 32-bit web application.Deep recursive algorithms can also lead to Out Of Memory problems. In this case, the only fixes are increasing the thread stack size (-Xss), or refactoring the algorithms to reduce the depth, or the local data size per call.A webapp that uses lots of libraries with many dependencies, or a server maintaining lots of webapps could exhauste the JVMPermGenspace. This space is where the VM stores the classes and methods data. In those cases, the fix is to increase this size. The Sun VM has the flag-XX:MaxPermSizethat allows to set its size (the default value is 64M)PermGen has been integrated into a new concept called MetaSpace from Java 8 on. The old setting will generate a warning and will be ignored by newer JVMs.Hard references to classes can prevent the garbage collector from reclaiming the memory allocated for them when aClassLoaderis discarded. This will occur on JSP recompilations, and webapps reloads. If these operations are common in a webapp having these kinds of problems, it will be a matter of time, until thePermGenspace gets full and an Out Of Memory is thrown. This last case is the one we intend to address here. It is directly related to the fact that the webapp is running on a managed environment, in which code changes can be commited without the application being stopped at all. Once said this, the patterns to be included here will be those that, although being safe and legal on a standalone application, need to be refactored to make them ""compatible"" with the servlet container.",../data/confluence_exports/TOMCAT/OutOfMemory_103099557.html
Apache Tomcat : OutOfMemory Threads,"Any threads a web application starts, a web application should stop. ServletContextListener is your friend. Note Tomcat 7 will warn you if you do this and will also provide a (highly dangerous - use at your own risk) option to terminate the threads.",../data/confluence_exports/TOMCAT/OutOfMemory_103099557.html
Apache Tomcat : OutOfMemory DriverManager,"If you load a java.sql.Driver in your own classloader (or servlets), the driver should be removed before undeploying. Each driver is registered in DriverManager which is loaded in system classloader and references the local driver. Note Tomcat will do this for you if you forget. Enumeration<Driver> drivers = DriverManager.getDrivers();
		ArrayList<Driver> driversToUnload=new ArrayList<Driver>();
		while (drivers.hasMoreElements()) {
			Driver driver = drivers.nextElement();
			if (driver.getClass().getClassLoader().equals(getClass().getClassLoader())) {
				driversToUnload.add(driver);
			}
		}
		for (Driver driver : driversToUnload) {
	            DriverManager.deregisterDriver(driver);
                }",../data/confluence_exports/TOMCAT/OutOfMemory_103099557.html
Apache Tomcat : OutOfMemory ThreadLocal,The lifecycle of a ThreadLocal should match that of a request. There is no guarantee that a thread will ever be used to process a request again so if a ThreadLocal is left on the thread at the end of the request there may be no opportunity for the web application to clean it up. Note Tomcat 7 will do this for you.,../data/confluence_exports/TOMCAT/OutOfMemory_103099557.html
Apache Tomcat : OutOfMemory ContextClassLoader,There are various parts of the Java API that retain a permanent reference to the context class loader. If this happens to be a web application class loader then a memory leak will occur. Tomcat providesworkaroundsfor these where known but there are undoubtedly others.,../data/confluence_exports/TOMCAT/OutOfMemory_103099557.html
Apache Tomcat : OutOfMemory Logging Frameworks,"Most logging frameworks provide a mechanism to release all resources when you have finished with the framework. These should always be used in a container environment.  If you still have a leak then you'll need to debug the root cause. The outline of the process is: You'll need a profiler (I use YourKit), Tomcat and a copy of the app that leaks.Configure Tomcat for use with the profiler. This usually means setting / adding to PATH and CATALINA_OPTS in setenv.(bat|sh)Start Tomcat with the app deployed.Reload the app once.Start up the profiler and connected it to Tomcat.Get a heap dump.Look for instances of WebappClassLoader. If there are more instances than you have apps deployed, you have a leak.If there is a leak, there should be one extra instance of WebappClassLoader.Examine each of the WebappClassLoader objects in turn to find the one where started==false.Trace the GC roots of this object to find out what is holding on to a reference to that object that shouldn't be. That will be the source of the leak.  (In response to[1],[2]) Please remember that a JSP page, even one that simply prints out “OK”, will create a session. This is by design and if you do not want it to create a session you need to explicitly indicate that in your JSP. For example: <%@ page session=""false"" %> This is important in scenarios where you are doing load testing and using custom HTTP clients, because these clients may not be handling sessions correctly and thus end up creating a new session every time they access the page. One known category of misbehaving clients are web bots. To deal with them you can configure aCrawlerSessionManagerValve. It is also possible to limit the number of active sessions by settingmaxActiveSessionsattribute on aManagerelement, e.g. <Context>
  <Manager maxActiveSessions=""500"" />
</Context>",../data/confluence_exports/TOMCAT/OutOfMemory_103099557.html
Apache Tomcat : UsingPhp Prerequisites,Download PHP (this tutorial uses PHP 4.3.5)Download Tomcat (this tutorial uses Tomcat 5.0.19)Define$JAVA_HOMEfor your JDK installationDefine$TOMCAT_HOMEfor your Tomcat installationDefine$PHP_HOMEfor your PHP installation,../data/confluence_exports/TOMCAT/UsingPhp_103100754.html
Apache Tomcat : UsingPhp Patch for PHP configure Script,"There is a patch required to compile PHP 4 to use Tomcat 5.  Prior to version 2.4 of Servlet Specification, the name of the servlet jar file wasservlet.jar.  In version 2.4 of the Servlet Specification, this name was changed toservlet-api.jar.  Tomcat 4 uses the nameservlet.jar, whereas Tomcat 5 and later usesservlet-api.jar. This causes problems with PHP'sconfigurescript.  This patch for PHP's configure script will fix this problem:  --- configure.org       2004-04-07 11:20:24.000000000 +0200
+++ configure   2004-04-07 11:22:50.000000000 +0200
      if test ""$withval"" = ""yes""; then
        SERVLET_CLASSPATH=.
      else
+      if test -f $withval/common/lib/servlet-api.jar; then
+        SERVLET_CLASSPATH=$withval/common/lib/servlet-api.jar
+      fi
+
        if test -f $withval/lib/servlet.jar; then
          SERVLET_CLASSPATH=$withval/lib/servlet.jar
       fi",../data/confluence_exports/TOMCAT/UsingPhp_103100754.html
Apache Tomcat : UsingPhp Patch for sapi/servlet/servlet.java,"enum is now a reserved word with Java 5, thus causing servlet.java to break the make process.  --- servlet.java.orig   2005-09-26 22:25:55.000000000 -0400
+++ servlet.java        2005-09-26 22:26:11.000000000 -0400
@@ -63,12 +63,12 @@
     if (!request.getMethod().equals(""POST"")) {
       result = request.getQueryString();
     } else {
-      Enumeration enum = request.getParameterNames();
+      Enumeration xenum = request.getParameterNames();
       String concat = """";
       result = """";

-      while (enum.hasMoreElements()) {
-        String name  = (String)enum.nextElement();
+      while (xenum.hasMoreElements()) {
+        String name  = (String)xenum.nextElement();
         String value = request.getParameter(name);

         try {",../data/confluence_exports/TOMCAT/UsingPhp_103100754.html
Apache Tomcat : UsingPhp PHP Installation,"Extract the source code to PHP in a work directoryPatch if needed (that is, patch if building PHP to run with Tomcat version 5 or later)Runconfigure, thenmakein the top directory of the PHP sources:./configure --with-servlet=$TOMCAT_HOME --with-java=$JAVA_HOME
 makeA jar file and dynamic library are produced from themake:sapi/servlet/phpsrvlt.jar and libs/libphp4.so.Copy the jar file to your web application's class repository, or, alternately, to Tomcat's common class repository (as is shown here):cp $PHP_HOME/sapi/servlet/phpsrvlt.jar $TOMCAT_HOME/common/libDeclare PHP servlet and servlet-mapping in the web applicationsweb.xmlfile, or in Tomcat's sharedweb.xmlfile:Copy from$PHP_HOME/sapi/servlet/web.xmlthe servlet and servlet-mapping and paste into the file$TOMCAT_HOME/conf/web.xml.Modify your LD_LIBRARY_PATH to include the dynamic library produced in the first step above:LD_LIBRARY_PATH=$PHP_HOME/libs
 export LD_LIBRARY_PATHAs an option, you can put libphp4.so someplace where java is already looking, any place in  System.getProperty(""java.library.path""), such as any of:/usr/lib/jdk1.5.0_04/jre/lib/i386/client:/usr/lib/jdk1.5.0_04/jre/lib/i386:/usr/lib/jdk1.5.0_04/jre/../lib/i386",../data/confluence_exports/TOMCAT/UsingPhp_103100754.html
"Apache Tomcat : UsingPhp Fedora Core 1 Issues with Tomcat 5.5.9, PHP 4.3.11 and jdk1.5.0_03","This may have just been an issue with the particular system I was building, but I was unable to set $JAVA_HOME, $PHP_HOME, $TOMCAT_HOME, or $LD_LIBRARY_PATH at the command line. The workaround was to edit/etc/profileand add the variables there (i.e., and the line  JAVA_HOME=/usr/java/jdk1.5.0_03  and addJAVA_HOMEto theexportvariables).  Ifmakereturns an error wherejavacis not a recognized command, you'll need to patch the Makefile produced by./configure. Look for ""&& javac"" and replace it with the full path to javac (i.e., ""&& /usr/java/jdk1.5.0_03/bin/javac"").Ifmakereturns an error regarding ""enum"" while trying to build phpsrvlt.jar, you'll need to edit$PHP_HOME/sapi/servlet/servlet.javaand replaceenumwithxenum.",../data/confluence_exports/TOMCAT/UsingPhp_103100754.html
Apache Tomcat : UsingPhp Start Tomcat,$TOMCAT_HOME/bin/startup.sh.,../data/confluence_exports/TOMCAT/UsingPhp_103100754.html
Apache Tomcat : UsingPhp Testing,"Verify the following is in your webapp's web.xml (creates the servlet entries and maps .php to that servlet and mentioned in the PHP installation steps above):  <servlet>
        <servlet-name>php</servlet-name>
        <servlet-class>net.php.servlet</servlet-class>
    </servlet>
    <servlet>
        <servlet-name>php-formatter</servlet-name>
        <servlet-class>net.php.formatter</servlet-class>
    </servlet>

    <servlet-mapping>
        <servlet-name>php</servlet-name>
        <url-pattern>*.php</url-pattern>
    </servlet-mapping>
    <servlet-mapping>
        <servlet-name>php-formatter</servlet-name>
        <url-pattern>*.phps</url-pattern>
    </servlet-mapping>  Verify that phpsrvlt.jar is in you WEB-INF/lib directory, or the tomcat common/lib directory (as mentioned above in the PHP installation steps)  Create a file named test.php in the docBase directory of your webapp.  In that file, simply put:  <?php phpinfo(); ?>  Point your browser at the file by navigating tohttp://localhost:8080/test.php  If everything is working as it should, you will see an informational status page produced by PHP.   CategoryFAQ",../data/confluence_exports/TOMCAT/UsingPhp_103100754.html
Apache Tomcat : Class Not Found Issues Preface,"This page discusses the various ways you see Class Not Found errors or very similar errors. It is strongly advised you read the following topics: Classloader HOWTO pages:Tomcat 9.0,Tomcat 8.5,Tomcat 7.0, (historic:Tomcat 6.0).Don'tusepackagelessclassesanddeclareallimported classes!Another answer to a classloader issue If you get aNoClassDefFoundErrorexception, the root cause might be the same as for aClassNotFoundexception.",../data/confluence_exports/TOMCAT/Class-Not-Found-Issues_103098805.html
Apache Tomcat : Class Not Found Issues Questions,Why is jsp:useBean is not working?Why do I get java.lang.NoClassDefFoundError: javax/servlet/Filter?Why do I get java.lang.NoClassDefFoundError: org/xml/sax/InputSource?,../data/confluence_exports/TOMCAT/Class-Not-Found-Issues_103098805.html
Apache Tomcat : Class Not Found Issues Why isjsp:useBeanis not working?,"Make sure: Your bean is packaged in a class.You have fully qualified your class name (e.g.:com.bar.package.MyClass) ORYou have imported your class into your jsp (e.g.:<%@ page import=""com.bar.package.MyClass""%>)",../data/confluence_exports/TOMCAT/Class-Not-Found-Issues_103098805.html
Apache Tomcat : Class Not Found Issues Why do I getjava.lang.NoClassDefFoundError: javax/servlet/Filter?,You probably have servlet-api.jar floating around somewhere it shouldn't be. This really messes up the classloaders since Tomcat's classloaders don't act quite as normal as one expects (see links above). servlet-api.jar should only be found only once in $CATALINA_HOME/lib.,../data/confluence_exports/TOMCAT/Class-Not-Found-Issues_103098805.html
Apache Tomcat : Class Not Found Issues Why do I getjava.lang.NoClassDefFoundError: org/xml/sax/InputSource?,You have conflicting XML api jar files in your classpath. Read the README or RELEASE-NOTES for more information.,../data/confluence_exports/TOMCAT/Class-Not-Found-Issues_103098805.html
Apache Tomcat : TomcatCreateNativeLaunchers Native Tomcat Launchers,Sometimes it is convinient to start Tomcat using a native launcher under Windows & Mac OS X  Clear distinction between other running Tomcat instancesIntegration with desktop application launchersShipping a Tomcat-based product  In my particular use case I was creating native launchers for Apache JSPWiki (seehttps://jspwiki.apache.org),../data/confluence_exports/TOMCAT/TomcatCreateNativeLaunchers_103100503.html
Apache Tomcat : TomcatCreateNativeLaunchers Available Options,What options do you have in the open-source world  for Mac OS and Oracle JDK you can useAppBundler(seehttp://docs.oracle.com/javase/7/docs/technotes/guides/jweb/packagingAppsForMac.html)for Windows you can use Launch4J (seehttp://launch4j.sourceforge.net),../data/confluence_exports/TOMCAT/TomcatCreateNativeLaunchers_103100503.html
Apache Tomcat : TomcatCreateNativeLaunchers Mac OS XAppBundler,"The following snippet  defines an a Ant task definition ""bundleappp""invokes the ""bundleapp"" tasks to create a native Mac OS X launchersets the executable flag of the underyling native launcher (just to make 100% sure the file is exetuable)  <target name=""woas:mac-app-oracle-jdk"" description=""creates an Mac OS X application wrapper for Oracle JDK 1.7+"">
    <taskdef 
        name=""bundleapp"" 
        classname=""com.oracle.appbundler.AppBundlerTask""
        classpath=""${basedir}/src/resources/appbundler/appbundler-1.0.jar"" 
    />
    <bundleapp 
        outputdirectory=""${jspwiki.woas.assembly.dir}""
        name=""woas""
        displayname=""Portable JSPWiki""
        identifier=""org.apache.jspwiki.jspwiki-portable""
        icon=""${basedir}/src/resources/macos/jspwiki.icns""
        shortversion=""${jspwiki.woas.version}""
        applicationCategory=""public.app-category.developer-tools""
        mainclassname=""org.apache.catalina.startup.Bootstrap"">
        <classpath file=""${basedir}/target/unpack/tomcat/${jspwiki.tomcat.distribution}/bin/bootstrap.jar""/>
        <classpath file=""${basedir}/target/unpack/tomcat/${jspwiki.tomcat.distribution}/bin/tomcat-juli.jar""/>
        <option value=""-Xmx96m""/>
        <option value=""-Duser.dir=$APP_ROOT/..""/>
        <option value=""-Dcatalina.home=$APP_ROOT/..""/>
        <option value=""-Dcatalina.base=$APP_ROOT/..""/>
        <option value=""-Djava.io.tmpdir=$APP_ROOT/../temp""/>
    </bundleapp>
    <chmod file=""${jspwiki.woas.assembly.dir}/woas.app/Contents/MacOS/JavaAppLauncher"" perm=""ugo+x""/>
</target>  Some notes along the line  ""APP_ROOT"" is the directory of the Mac OS X launcher and will be expanded accordingly during run-timeThe current working directory is undefined therefore all ""important"" properties must be providedThe two referenced JARs are effectively copied into the native launcherNo JRE is packaged since it assumed that the JRE is available on the target boxIf you need to change the memory settings for an existing native app than you can edit the ""Info.plist"" directlyIf the native app is starting at all you get no error message which is a bit annoying - as work-around you can launch at the command line to see the output, e.g ""./woas.app/Contents/MacOS/JavaAppLauncher""",../data/confluence_exports/TOMCAT/TomcatCreateNativeLaunchers_103100503.html
Apache Tomcat : TomcatCreateNativeLaunchers Windows Launch4J,"Defines an a Ant task definition ""launch4j""Invokes the ""launch4j"" tasks to create a native Mac OS X launcher  <target name=""woas:windows-app"" description=""creates an windows application wrapper"">
    <taskdef 
      name=""launch4j""
      classname=""net.sf.launch4j.ant.Launch4jTask""
      classpath=""${basedir}/src/resources/launch4j/launch4j-3.1.0-beta2.jar:${basedir}/src/resources/launch4j/xstream.jar"" 
    />
    <launch4j>
      <config 
        headerType=""console"" 
        outfile=""${jspwiki.woas.assembly.dir}/woas.exe"" 
        errTitle=""WikiOnAStick"" 
        chdir=""."" 
        icon=""${basedir}/src/resources/windows/jspwiki.ico""
        jar=""${basedir}/src/resources/tomcat/tomcat-launcher-7.0.52.jar""
        >
        <singleInstance mutexName=""org.apache.jspwiki.jspwiki-portable"" />
        <jre minVersion=""1.6.0"" />
        <versionInfo
          fileVersion=""2.1.10.1""
          txtFileVersion=""JSPWiki ${jspwiki.woas.version}""
          fileDescription=""WikiOnAStick""
          copyright=""Apache Software Licence 2.0""
          productVersion=""2.1.10.1""
          txtProductVersion=""JSPWiki ${jspwiki.woas.version}""
          productName=""WikiOnAStick""
          companyName=""Apache Software Foundation""
          internalName=""woas""
          originalFilename=""woas.exe""
        />        
      </config>  
    </launch4j>
  </target>  Some notes along the line  Launch4J allows to set the current working directory of the Tomcat instance therefore all Tomcat properties are bootstrapped for the current working directoryI cheated here by providing a manually packaged ""tomcat-launcher-7.0.52.jar"" which contains ""bootstrap.jar"" and ""tomcat-juli.jar"" - I think there is a way to avoid this but I have to test it on an Windows box",../data/confluence_exports/TOMCAT/TomcatCreateNativeLaunchers_103100503.html
Apache Tomcat : JPMS names Spec JAR module names,SpecificationJava EE 8 JPMS nameJakarta EE 8 JPMS nameJakarta EE 9 JPMS nameAnnotations APIjava.annotationjava.annotationjakarta.annotationEL APIjavax.el.api (auto)jakarta.el.api (auto)jakarta.elJASPIC APIjavax.security.auth.message.api (auto)java.security.auth.messagejakarta.security.auth.messageServlet APIjavax.servlet.api (auto)java.servletjakarta.servletJSP APIjavax.servlet.jsp.api (auto)jakarta.servlet.jsp.api (auto)jakarta.servlet.jspWebSocket (server) APIjavax.websocket.api (auto)jakarta.websocket.api (auto)jakarta.websocket,../data/confluence_exports/TOMCAT/JPMS-names_165222937.html
Apache Tomcat : JPMS names Proposed Module names for Tomcat,SpecificationTomcat 9  JPMS nameTomact 10 JPMS nameAnnotations APIjava.annotationjakarta.annotationEL APIjava.eljakarta.elJASPIC APIjava.security.auth.messagejakarta.security.auth.messageServlet APIjava.servletjakarta.servletJSP APIjava.servlet.jspjakarta.servlet.jspWebSocket (server) APIjava.websocketjakarta.websocket,../data/confluence_exports/TOMCAT/JPMS-names_165222937.html
Apache Tomcat : SmallTomcatTrackUs09 Draft Schedule,"09:00 - 10:00REGISTRATION10:00 - 10:50516: Large Scale Tomcat Deployments, Filip Hanik10:50 - 11:15BREAK11:15 - 12:05455: Becoming a Tomcat super user, Mark Thomas12:05 - 13:30LUNCH13:30 - 14:20Keynote Session14:30 - 15:20440: mod_proxy versus mod_jk. Clustering with HTTP Server as front-end, Jean-Frederic Clere15:20 - 16:00BREAK16:00 - 16:50418: Advanced Reverse Proxy Load Balancing in Apache HTTP Server 2.2, Jim Jagielski17:00 - 17:50PMC track:  mod_jk in depth, Rainer Jung  Seehttp://wiki.apache.org/tomcat/TomcatTrackUs09_Sessionsandhttp://wiki.apache.org/tomcat/TomcatTrackUs09_PMC_Sessionsfor the long description of the sessions.",../data/confluence_exports/TOMCAT/SmallTomcatTrackUs09_103100161.html
Apache Tomcat : Linux Unix Questions,"I have Tomcat x.y.z installed as part of my OS. Is it good to use?When I run ps (on Linux), why do I see my java process a bazillion times!How do I run without an X server and still get graphics?Tomcat dies after I log out!Catalina.log contains : ""SEVERE:StandardServer.await: create[8005] : Throwable occurred: java.net.BindException: The socket name is not available on this system.Examples web application does not start. A ClassNotFoundException occurs.",../data/confluence_exports/TOMCAT/Linux-Unix_103098909.html
Apache Tomcat : Linux Unix I have Tomcat x.y.z installed as part of my OS. Is it good to use?,"Many Linux distributions provide a pre-packaged version of Apache Tomcat. These packages work fine and are easy to install for a normal single-instance case, but they make it more difficult for more specific use cases, and more difficult for people on theTomcat User mailing listto help you. That is because each of these packages distributes the files of Tomcat in different places on the disk, sets different environment variables, sets different links from one directory to the other in the filesystem, etc.. Moreover, some of those packages are notably outdated. So it would be better to install a ""standard"" tomcat downloaded from the websitehttps://tomcat.apache.org/, to some directory like/opt/tomcat, and follow the instructions that are given in the ""RUNNING.txt"" file. This way, everyone here knows what you are talking about and has a good idea of where things are. Several notes: Download a ""binary"" version. There is usually no need to re-compile Tomcat from the source code.Either a ""tar.gz"" or a ""zip"" file is fine. The ""tar.gz"" files use GNU extensions to the tar file format (as mentioned in ""README"" file in the download area). You need a GNU-compatible version oftarto unpack them.Learn how to run Tomcat with separate values ofCATALINA_HOMEandCATALINA_BASE, as explained in ""RUNNING.txt"". This will simplify further upgrades and maintenance.",../data/confluence_exports/TOMCAT/Linux-Unix_103098909.html
"Apache Tomcat : Linux Unix When I run ps (on Linux), why do I see my java process a bazillion times!",Linux implemented threads as processes. Due to other gory details that is beyond the scope of this FAQ - the ps command doesn't work correctly with respect to threads.,../data/confluence_exports/TOMCAT/Linux-Unix_103098909.html
Apache Tomcat : Linux Unix How do I run without an X server and still get graphics?,"You either need to run headless or run an alternate X-server. Some more information can be foundhere,here, orhere. Or if your are using a JVM 1.4 or better, you can use the system propertyjava.awt.headless=true",../data/confluence_exports/TOMCAT/Linux-Unix_103098909.html
Apache Tomcat : Linux Unix Tomcat dies after I log out!,This is a common complaint when using Solaris. Make sure you usenohupand seethis thread,../data/confluence_exports/TOMCAT/Linux-Unix_103098909.html
"Apache Tomcat : Linux Unix Error message: ""SEVERE: StandardServer.await: create[8005]: Throwable occurred: java.net.BindException: The socket name is not available on this system.""","This error message can have 2 causes: Java on AIX isn't supporting IPv6 properly. Fix by inserting-Djava.net.preferIPv4Stack=trueinto JAVA_OPTSYour networking configuration is not correct. If you attempt toping localhostand don't see127.0.0.1you need to look into your/etc/host.conf(most Unixes/Linux) or/etc/netsvc.conf(AIX) file to ensure that something like""hosts = local, bind""is present.",../data/confluence_exports/TOMCAT/Linux-Unix_103098909.html
Apache Tomcat : Linux Unix Examples web application does not start. A ClassNotFoundException occurs.,"Go intowebapps/examples/WEB-INF/classesand check whether the class file mentioned in the error message does exist. If you downloaded a tar.gz file and used a non-GNU version of tar (e.g. on Solaris) it may use wrong (truncated) file names on files that are deep in the hierarchy. This occurs silently: there may be no error or warning during unpacking. One place that is known to suffer from this is the examples web application. The workaround is to download a ""zip"" file instead of a ""tar.gz"" one.thread",../data/confluence_exports/TOMCAT/Linux-Unix_103098909.html
Apache Tomcat : TomcatGridDiscussion Introduction,"The Tomcat Grid manages one or more local or remote Tomcat instances from a centralized  location. This manager application shows the status of each Tomcat instance,  and provides a simple interface to trigger operations on them, individually or  as a group.",../data/confluence_exports/TOMCAT/TomcatGridDiscussion_103100547.html
Apache Tomcat : TomcatGridDiscussion Architectural Overview,"The centralized location of the Tomcat Grid (the primary location) stores the  grid configuration, probably as an XML file. For reliability purposes,  secondary locations can be setup so the grid configuration is replicated after every change.  The primary location runs a Tomcat Web Grid Manager, a web application that runs on a separate Tomcat instance. This manager application shows the state of the all Tomcat instances on the grid, and allows users to trigger operations on these  Tomcat instances.  In addition to the Web Grid Manager, a Command-line (CLI) Grid Manager  application mimics the functionality of the web manager, with an equivalent  (text-only) interface that shows the status of the Tomcat instances, and  provides commands to trigger remote operations on them.  The secondary locations run on dedicated Tomcat instances. They run a copy of the Web Grid Manager but they are shut down all the time unless they are explicitly  started by an operator (when the primary one goes bad). No two managers should  be active at the same time. If activated, the secondary locations can start  producing changes to the local grid configuration, and these changes are  replicated to all other managers (if/when possible).  The example shown in the figure below assumes Machines #1 and #4 run a lighter  web and/or back end applications, so they have enough resources bandwidth to  run the extra Tomcat instance for the manager. In case the primary manager on  Machine #1 goes down (or the whole Machine #1 goes down), there is a secondary  manager (on Machine #4) that can be activated to take over the managing duties.  TomcatGridExample.png!                                                                             The Grid Agents are Java processes installed and running on each machine that listen (on a configured port) for commands from a Grid Manager application (Web or  CLI) and act upon them by interacting with the local Tomcat instances. No  encryption is envisioned on the network channels, since all these machines are  considered to be installed on a secured network segment (at least in prod).  [Maybe we should reconsider this]                                                                             markt: I think this needs to be reconsidered.  vlad: Sure. My take was to use unencrypted connections, since it's fast to implement for me and I don't know well how to implement a secured connection :/ . Maybe we could even offer both options. Anyway, why do you consider we need an secured connection? In my experience, I've never needed them in dev/state/QA/test, and in prod the environment are isolated. Well... I guess there are other scenarios I haven't been exposed to.  All Tomcat instances (including the Web Grid Managers), as well as the Grid  Agents, are installed manually. The primary grid manager and all the agents are started manually too.  Once the Web Grid Manager is started up, machines and instances can be registered in it, so they can become manageable. No centralized  (comprehensive) provisioning is envisioned until later versions of Tomcat Grid  (see below).  The Grid Agents are processes that can also be managed. In particular, status can be obtained (and showed) from them, and basic operations (start/stop/kill) can be triggered on them. These operations are, however, heavily dependent on OS and OS capabilities (configuration, installed tools, etc.) and the infrastructure architecture (fire-walled machines, network VLans, etc).  Collectively, Tomcat instances and Grid Agents are ""services"" since both can be managed.  markt: Managing the agents strikes me as making this significantly more complex. Operating systems have tools to ensure particular services are running and are restarted if they fail. What is the benefit of pulling this into this tool?  vlad: I agree that agents can run as daemons and be up all the time. However, I've seen many ""robust"" programs to have memory leaks and that for one reason or another stop working normally after some time. Maybe after a couple of days, or weeks, or months. Maybe it would be useful to restart them just before a critical operation such as a deployment. I agree that managing agent is more expensive since it requires the development and maintenance of multiple OS-dependent implementations.  Later versions of the Grid include ""collection"" management. This allows to  group subsets of services (Tomcat instances and Grid Agents), so they can be  operated as whole. Each collection can include plain services, or other  collections (recursively).  Below is a general overview of the software modules and their responsibilities.  TomcatGridSoftwareModules.png!  _The modules are:  Core module: Shared logic to be used by all other modules.Core data types, such as ""Machine"", ""Instance"".Share core logic. For example, grid configuration file parsing/update.Defines the Grid Agent functions (as interfaces), but does not implement them. These are used by all Managers modules.Common utility classes.Common exceptions.  Web Manager module: A JEE Web application that includes:Includes a simple Managing web GUI: web pages, navigation logic.Uses the Core module for functions such as:Load Grid Configuration,Interact with grid agents.CLI Manager module: A java command-line program:Command-line interface: command parsing, text output.Uses the Core module for functions such as:Load Grid Configuration,Interact with grid agents.Any other Manager module: Any future module that needs to connect to Grid Agents to manage Tomcat instances.  Grid Agent module: Responds to Managers calls and controls local Tomcat instances.Listen to Manager requests.Implements the Grid Agent interfaces.Includes the high-level interaction with Tomcat intances.Defines and uses the Tomcat Management Primitives (as interfaces), but does not implement them.Receives content (deployables, grid configuration changes) and applies them.Grid Agent Primitives for Linux:Implements the Tomcat Management Primitives for Linux OS.Grid Agent Primitives for Windows:Implements the Tomcat Management Primitives for Windows OS.Grid Agent Primitives for Mac:Implements the Tomcat Management Primitives for Mac OS.Grid Agent Primitives for Other:Implements the Tomcat Management Primitives for Other OS.  The executables themselves are comprised of several modules each that are assembled during the build.  The Grid Web Manager Executable (a WAR) includes:Core moduleWeb Manager moduleThe Grid CLI Manager Executable (a JAR) includes:Core moduleCLI Manager moduleThe Grid Agent Executable (a JAR) includes:Core moduleGrid Agent moduleGrid Agent Primitives for LinuxGrid Agent Primitives for WindowsGrid Agent Primitives for MacGrid Agent Primitives for Other_  Considering all the above, the following phases could be considered as a base line for the road map of the Tomcat Grid.",../data/confluence_exports/TOMCAT/TomcatGridDiscussion_103100547.html
Apache Tomcat : TomcatGridDiscussion Phase 1 - Core Grid Operation,"The first phase includes the most basic features, in order to provide a  functioning and useful first version of the Grid.  In particular, no Tomcat instances or Grid Agents automatic provisioning is  considered, no configuration GUI (only pre-configured XML config files), no WAR  deployments, no command-line interface, no complex grid operations, no  secondary managers, and no collections.  markt: This raises another architectural question. Wouldn't this be more scalable if agents were configured with the location of the primary manager and registered themselves? The manager could persist that registration so an agent would have to be explicitly removed if it was taken off-line permanently.  vlad: Definitively. A basic agent install, and registration could ""summon"" all the necessary software from the primary manager: i.e. Tomcat executables, grid configuration, etc. I thought about something like this as an advanced feature (phase 12), but we can rearrange the priorities if needed.  Included features are:  The Web Grid Manager presents a Web interface that shows information of the whole Grid and present simple buttons to operate the Tomcat instances.The managing logic must be clearly separated from the Web interface logic, since later on, a Command-Line Grid Manager will be included, and will use the same managing logic.The available commands for each instance are:status: retrieves the status of a Tomcat instance through the corresponding Grid Agenttrigger-start: sends a start request to the Tomcat instance using the corresponding Grid Agenttrigger-stop: sends a stop request to the Tomcat instance using the corresponding Grid Agenttrigger-kill: sends a kill request to the Tomcat instance using the corresponding Grid Agentmarkt: A small thing. I think I'd prefer start/stop/kill/vlad: I added these commands on phase 7 and, as I see them, they behave are a little bit different from the trigger ones, specially when we refer to the CLI manager. Thetrigger-start, issues the signal to the Grid Agent and ends. Thestartkeeps on working (and updating the user) until the Tomcat instance is actually up (or fails to start up). On the Web interface the trigger start could show up as a simple icon (omitting its name). On the CLI I see the start command as far more useful than the trigger-start.A simple configuration file lists all the machines and their instances so the Grid knows where each instance resides. [This configuration file is probably in XML format]Grid Agents are installed on each machine and manage all instances in that machine pertaining to the Grid. Grid Agents receive commands from any manager and act accordingly. To manage the instances the Agents use:Shell calls: start an instance, kill an instance.JMX calls to retrieve instance live information.JMX calls to change instance live values, and to request instance shutdown.OS calls for any OS related need.It's assumed that a port will be accessible from each Grid Manager to each machine where the Grid Agents are serving. The firewall, if present must allow active server-type sockets on that port.markt: Another architectural question. Which end opens the connection, does it stay open and which protocol is used? For example, agents connect to Manager viaWebSocket.vlad: I always considered the Grid Agent would would open a server NIO socket, to avoid using ephemereal ports. The Grid Agent is always listening, and the Managers connect when needed. In terms of protocol, it could be a ad-hoc one, specially developed for this tool, or use a well-known standard. I have ad-hoc one that I can use, but I'm open to suggestions.Multiple Grids (and Grid Agents) can be running on the same set (or subset) of machines. If that's the case, Tomcat instances, and Grid Agents run on different ports for each grid. When multiple grids use the same machines they don't interfere with each other and can be operated simultaneously.The status command shows the following information for each instance:MachineService (a unique grid-wide name for each instance)StateThe state of an instance can be:Active: the instance OS process exists, the instance is serving requests, and it looks healthy [enough].Zoetic[for lack of a better word]: the instance OS process exists, but the instance is unresponsive and it doesn't respond to requests for state. It's probably not serving any HTTP requests, does not look healthy, it may be starting, it may be shutting down, it may be overwhelmed. Who knows.Stopped: the instance OS process does not exist, and therefore the instance is not operating at all.Not Available: This is a pseudo state that the manager applications (web and cli) show when a Grid Agent does not respond to requests for status in a timely manner.If possible it would be great to discern different sub cases of the Zoetic state, so to help the user to determine what's going on and tackle the case accordingly:Starting: The Tomcat instance process exists, and the instance is starting. It's not yet serving HTTP requests.Stopping: The Tomcat instance process exists, and the instance is stopping. It's no longer serving HTTP requests.Unresponsive: The Tomcat instance process exists, but the instance health isn't good, it's not responding to HTTP requests, or it's overwhelmed. It's not even responding to requests for status. This state is different from ""Not Available"" since in this case the Grid Agent IS active and responding, but the Tomcat instance itself is unresponsive.On second thoughts, these extra states can actually be discerned today with the current version of Tomcat, since the Grid Agents know all the trigger commands each local instance has received and can deduce (or make up) the sub case. If the Grid Agent is restarted, some kind of persistence of its state might be needed to ""remember"" what was going on before the Grid Agent was shut down, so to make an educated guess.Grid Agents communicate over unsecured TCP sockets, and assume communication security is enforced by the network architecture (segregated segments/VLans).The ""trigger""-type commands just deliver the corresponding signal to the instance's Grid Agent and returns right away, without waiting for the full operation to complete. It's kind of ""fire and forget"". The web user can keep on refreshing the the web interface to find out about the progress of the status of the Tomcat instances.Simple user name/password authentication is implemented to secure the Web interface. [Maybe we'll need to provide more options]",../data/confluence_exports/TOMCAT/TomcatGridDiscussion_103100547.html
Apache Tomcat : TomcatGridDiscussion Phase 2 - Manageable Grid Agents,"This phase revamps the Grid Agents so they become manageable.  Included features are:  As well as the instances, the agents can become unresponsive, or even crash. To address cases like these commands are implemented to manage the Grid Agents as well. The Grid Agents become now manageable services.All grid agents are now also registered in the configuration file under a unique service names. Grid agents names share the same namespace than Tomcat instance names: i.e. the Tomcat instances & Grid Agent names are grid-wide unique. This way commands (such as a trigger-start for example) can distinguish which type of service it needs to act on, and will chose a different logic (program, or script) to execute.All previously defined commands are now available for the Grid Agents:statustrigger-starttrigger-stoptrigger-killThe status command now adds an extra column ""Type"" that indicates the type of service: Tomcat instance, or Grid Agent.The mechanism to manage the grid agents is necessarily OS dependent. For example, in Linux it can be implemented using Bash commands though SSH. Suitable mechanisms must be studied for each OS.",../data/confluence_exports/TOMCAT/TomcatGridDiscussion_103100547.html
Apache Tomcat : TomcatGridDiscussion Phase 3. Secondary Managers,"The functionality for configuration replication is added.  Included features are:  Secondary managers are registered on the grid's configuration file.Every time a configuration change is produced or detected on the configuration of the primary manager, the changes are distributed to all secondary managers.If the primary manager is down, secondary managers can be started, and can start producing changes (based on the local configuration copy). They can also start distributing the new configuration changes.",../data/confluence_exports/TOMCAT/TomcatGridDiscussion_103100547.html
Apache Tomcat : TomcatGridDiscussion Phase 4 - Extended Service & Machine Information,"This phase extends the status information of the whole grid beyond the basic data.  Included features are:  The status command now adds more information for each service (Tomcat instances and Grid Agents):CPU usage (if possible)CPU load (if possible)Heap usage (if possible)Threads (if possible)Started on (if possible)Any other information deemed useful for managing purposes.[Optional] Machine information (same page, or maybe an extra tab) shows per machine:CPU usageCPU load (1 min, 5, min, 15 min)Memory usageFile system space usage for the mount where the ""webapps"" dir is (can this be different per instance?).",../data/confluence_exports/TOMCAT/TomcatGridDiscussion_103100547.html
Apache Tomcat : TomcatGridDiscussion Phase 5 - Command-Line Interface (CLI),"This phase provides a CLI manager interface for environments that cannot use the web interface.  Included features are:  In addition to the Web Grid Manager interface, the Command-Line Grid Manager interface is suitable when the web interface cannot be used. Typical cases are, when no web port is available on the servers (probably fire-walled), when the security policies do not allow remote server operations, etc. This maybe the case on some secured/fire-walled production environments where only text sessions are acceptable.The Command-Line Grid Manager is also suitable for automation (e. g. the weekly full/partial site restart) when unattended operations are scheduled, using cron or equivalent utilities.The Command-Line Grid Manager always leaves a log file per command execution on a directory created for this purpose. Each log file's name includes the time stamp, the command name, and (if possible) the arguments.The implemented commands are:statustrigger-starttrigger-stoptrigger-killThe trigger commands are only executed when necessary. If an instance is already running a trigger-start command will be ignored. Conversely trigger-stop and trigger-kill commands are ignored when the instance is stopped.Return codes must be strategically defined to allow automation. Well defined return codes can provide useful information to the caller program/process (especially for automation), so it can clearly identify the problem and act accordingly.",../data/confluence_exports/TOMCAT/TomcatGridDiscussion_103100547.html
Apache Tomcat : TomcatGridDiscussion Phase 6 - Hooks,"Extending the core operation of the grid services with custom logic.  Included features are:  Hooks are integration points to include extra activities we want to be performed when some events occur on each Tomcat Instance or Grid Agent. A hook program is linked to a hook and may be implemented as a shell script or any other kind of executable program. Hooks can be defined for the following events:pre-trigger-startpost-trigger-startpre-trigger-stoppost-trigger-stoppre-trigger-killpost-trigger-killThe hooks are only executed when the corresponding signal is not ignored. For example, if a trigger-start is issued and the instance is stopped, the corresponding pre-trigger-start and post-trigger-start hooks are executed. If the instance was running, then the command would be ignored and its hooks would also be skipped.Hooks can be useful for many purposes. For example, typical uses are:Prepare an instance configuration.Record instance events.Send emails or other notifications upon restarts.Clear caches & temp dirs before starting an instance.Delay the start of an instance to allow the OS to reclaim resources.Generate thread dumps on specific events.Hooks programs run on the machine where the affected instance runs. Therefore, the hooks programs need to be copied and prepared (manually or automagically) to be executed on all machines of the grid.When hooks programs are registered (maybe uploaded) on the Grid they are automatically distributed behind the scenes to all instances/machines before they are ready to be used.",../data/confluence_exports/TOMCAT/TomcatGridDiscussion_103100547.html
Apache Tomcat : TomcatGridDiscussion Phase 7 - Enhanced Grid Operation,"Beyond the basic trigger operations, there's usually need for more complex ones that provide very common needs.  Included features are:  Non-trigger commands are added to both the Command-Line and Web Grid Managers:start: triggers a start and waits until the operation succeeds or failstop: triggers a stop and waits until the operation succeeds or failkill: triggers a kill and waits until the operation succeeds or failrestart: triggers a stop, waits until it stops, triggers a start, wait until it startskillstart: triggers a kill, waits until it stops, triggers a start, wait until it startsThe new commands operate on both types of services (instances and agents).New hooks are added for the new commands:pre-startpost-startpre-stoppost-stoppre-killpost-killpre-restartpost-restartpre-killstartpost-killstartAll these new commands use the ""trigger"" primitives behind the scenes.The hooks for the non-trigger events are never ignored, so the hook programs are executed even if the related trigger commands are ignored.Automatic trigger-kill operations are now automatically issued for stop and restart operations if configured, when a trigger-stop fails to succeed in the pre-configured time limit. The time limit is now optionally specified in the configuration file on a per-service basis.A restart delay (now optionally specified on a per-service bases on the configuration file) is used when restarting services: it's applied to the restart and killstart commands.The non-trigger commands show an update of the service state periodically (defaults to every 10s, and can be specified on the configuration file), and they keep working until the full operation completes.",../data/confluence_exports/TOMCAT/TomcatGridDiscussion_103100547.html
Apache Tomcat : TomcatGridDiscussion Phase 8 - Simple Deployment,"This phase implements War applications deployment and undeployment to the grid as a whole, or to specific Tomcat instances.  Included features are:  New commands:deploy: deploys a web application (a WAR) to a specific or all grid instancesundeploy: undeploys a web application from a specific or all grid instancesThrough these operations Tomcat instances will be able to run multiple web applications.The status command is revamped so it now lists all war applications deployed on each instance.",../data/confluence_exports/TOMCAT/TomcatGridDiscussion_103100547.html
Apache Tomcat : TomcatGridDiscussion Phase 9 - Application Version Management,"The application versioning is like a deployment functionality on steroids.  The Application Version Management may interfere with the simple Deployment as described before, since it manages web applications in a different manner. It needs to be studied if both modes are compatible and can work at the same time, or if both are mutually exclusive. If the latter, the user will need to choose which mode to use when setting up the grid.  When the a new version of the application (a Release) needs to be deployed the deployment happens in an orderly manner. The grid also keeps track of which version is live, which ones are not live but still on the grid, and also provide rollback capabilities.  Included features are:  A Release includes one or more deployables. Deployables are WARs, JARs, etc. that will be part of an application we want to deploy on the grid.Maybe all deployables are deployed to all instances, maybe each deployable goes to a subset of instances. This will need to be specified on the configuration file.Releases are first registered into the Grid (maybe even uploaded) under a client-provided unique Version ID. If no Version ID is provided, the system generates it. Behind the scenes, each release deployable is transferred to the corresponding machines automatically during registration. Since this operation may take a while the release will show the states of loading, ready, or removing.Releases must be deployed to the grid using the Version ID. Since all deployables are already distributed to all machines, a deployment now corresponds to local copy (or sym link) of each deployable to the Tomcat's webapps dir. The deployment automatically registers which Version ID is now deployed on every instance. The deployment operation can be sent to the whole grid or to a single instance.No two versions are deployed at the same time on a Tomcat instance. When a version is deployed to an instance, the existing one is first unlinked from the instance. It's not actually removed from the grid or the file system, so a rollback operation can be performed quickly if needed.New commands are implemented:register: loads a new application version into the grid under a unique Version IDderegister: removes a non-live application version from the gridversions: list all loaded and live Version IDs and where they are deployeddeploy: deploys all version's deployables to the corresponding instances, unlinking the old onesrollback: undeploys the current version from all Tomcat instances, and restores the previous versiondeploystop: deploys all version's deployables to the corresponding instances, but keep the Tomcat instances downundeploy: undeploys a version (or a single deployable) from the grid or a subset of the gridAs shown above the deploy and undeploy commands are revamped.The deploy, rollback, deploystop, and undeploy command may use, behind the scenes, many low-level ""link"" and ""unlink"" tasks. These tasks add/remove a deployable to/from an instance correspondingly.Each deploy inspects the current state of the grid and saves a Rollback Plan. The rollback commands executes the Rollback Plan. Only one Rollback Plan is saved at any given time.The Rollback (if saved) is shown on the Web and CLI interfaces.The status operation now also shows for each instance:Deployables (the list of war applications deployed in the instance)Version IDsDeployed at (date & time)New hooks are added for the new commands:pre-register-releasepost-register-releasepre-register-deployablepost-register-deployablepre-deregister-releasepost-deregister-releasepre-deregister-deployablepost-deregister-deployablepre-deploypost-deploypre-rollbackpost-rollbackpre-deploystoppost-deploystoppre-undeploypost-undeployunlink(unlink: low-level operation that removes a deployable from an instance)post-unlinkpre-link(link: low-level operation that adds a deployable to an instance)post-link",../data/confluence_exports/TOMCAT/TomcatGridDiscussion_103100547.html
Apache Tomcat : TomcatGridDiscussion Phase 10 - Collections,"Collections are groups and sub-groups of services (instances and/or agents) that are managed together. Essentially, instead of issuing a command on a single service, you can now issue it onto a collection. The commands will now affect all services in the collection and will probably take longer to complete, since multiple operations are now needed to complete the whole command.  Collections may group identical Tomcat instances or maybe Tomcat instances that do not serve the same purpose. For example, one subgroup of Tomcat instances may run the customer facing site (like an HTTP cluster), other group may run the back end site (maybe processing JMS queues), other group may be dedicated to serving or connecting to integration points.  Included features are:  The following commands can now be issued on collections in addition to plain services:statustrigger-starttrigger-stoptrigger-killstartstopkillrestartkillstartdeployrollbackdeploystopundeployHooks are modified to provide information of the collection they are affecting.When a hook runs on a collection, it runs on the machine where the manager application (web or cli) runs, not remotely on the machine of any other instance. This is because in this case the execution is not tied to a specific instance, but to a collection.Services defined in a collection can be managed in sequential or parallel modes. For example, a restart command on a sequential collection will restart the second service only, when the first one has fully completed. Once the second completes, it will restart the third one, and so on. A parallel collection would issue a restart on all services simultaneously.Collections are defined in the configuration file and are of a recursive nature: a collection can include plain services, other collections, or both. For sequential mode, each ""sub-collection"" is treated as a single element so it's considered fully complete when all its included services and collections complete.[To be analyzed and defined if it's useful or not] Collections editing through the Web interface. This can be useful to graphically update collections when machines/instances are added/removed.",../data/confluence_exports/TOMCAT/TomcatGridDiscussion_103100547.html
Apache Tomcat : TomcatGridDiscussion Phase 11 - Instance Configuration,"The Web interface adds functionality to specify Tomcat instances configuration from the centralized location.                                                                             The CLI interface does not offer this functionality. [to be discussed]                                                                             Included features are:  Using the Web interface the user can change instance's configuration remotely. This operation allows the instances to be changed remotely, for example to:Add libraries (typically JDBC drivers, MQ drivers, JSF, etc.)Set JVM parameters (memory settings, GC behavior, JVM tweaking, etc.)Prepare (add/change/remove) JDBC data sourcesSet context parametersSet JNDI entriesView required WAR resources, and set resources values accordinglyChanging listenersOther instance configurations",../data/confluence_exports/TOMCAT/TomcatGridDiscussion_103100547.html
Apache Tomcat : TomcatGridDiscussion Phase 12 - Instance Provisioning,"This functionality removes the necessity of a manual setup of all machines and Tomcat instances of the Grid. After installing the first Tomcat instance and deploying the Web Manager, provisioning operations to local or remote machines can be performed through the web interface.  Because of its nature, the implementation of the provisioning operations is heavily OS dependent.  In addition it may not be possible to install, configure, and run the Grid Agents remotely because of fire-walled machines. If that's the case, the Grid Agents will need to be manually setup. Once the Grid Agents are running, the rest of the provisioning can be performed through the Web Manager, using the Grid Agents.  Included features are:  The provisioning operation will automate the following tasks:Login into a machineInstalling the Grid AgentsConfiguring & running the Grid AgentInstalling the Tomcat instancesConfiguring the Tomcat instancesConfiguring the environment (shell variables, other)Using the Web and Command-Line interfaces the user can provision the Grid. Typical operations can be:Adding a new machine to the Grid.Removing a machine from the Grid.Adding a new instance to a machine.Removing an instance from a machine.Once new machine is registered, the machine's Agent is installed and executed.To deregister a machine all instances must have been removed first.If a machine is deregistered, the Agent is stopped and optionally uninstalled. Maybe we'll leave it there, so it will be easier in the future to re-provision the machine.Once a new instance is created the following operations are performed:Registering the machines on the grid configuration fileStandard instance's directory tree is copiedAll the instance extra configuration (libraries, JDBC data sources, etc.) are performedNo deployments are installed yet.To remove an instance, all deployables must have been undeployed first.Once an instance is removed:The instance is removed from the configuration file and any collection that included itAll deployments are removed from itThe whole directory tree for it is removed on the remote machineThe provisioning operations require remote access to the new machine, and therefore some kind of connections needs to be setup. For example, an SSH connection could be used if the user provides the user name/password credentials or if if an ssh key exchange had been previously setup between the machines.",../data/confluence_exports/TOMCAT/TomcatGridDiscussion_103100547.html
Apache Tomcat : TomcatGridDiscussion Phase 13 - Additional Commands,[To be described],../data/confluence_exports/TOMCAT/TomcatGridDiscussion_103100547.html
Apache Tomcat : SupportAndTraining Example company name,"Use this example as a basis for your entry. New entries should be added just above this example.   {{http://www.kippdata.de/site/themes/kippdata/img/elements/kippdata_logo.gif  Kippdata offers aselection of trainings for Apache Tomcat. We focus on single day trainings like „Apache Tomcat Best Practices“, „Java Memory Sizing and Garbage Collection Tuning“. Kippdata is located in Bonn, Germany.  Kippdata bietet eineAuswahl an Schulungen für Apache Tomcatan. Wir haben uns auf Ein-Tages-Schulungen zu Themen wie „Apache Tomcat Best Practices“, „Java Memory Sizing and Garbage Collection Tuning“ spezialisiert. Kippdata ist in Bonn, Deutschland, ansässig.  {{https://www.mulesoft.com/sites/default/files/3C_mulesoft_logo_updated.svg  MuleSoft provides comprehensiveTomcat Trainingfor users of Apache Tomcat. MuleSoft offers a FREE pre-recorded training course to help users to get started with Apache Tomcat and also intensive instructor led training courses on advanced topics such asTomcat performance tuning,configuration management. The training is designed by Tomcat experts and delivered by experienced instructors.  {{http://www.nobleprog.com/sites/all/themes/nexus_subtheme/logo.png  NobleProg provides comprehensiveOnline Instructor-led Tomcat Trainingall around the world. NobleProg also provides intensiveTomcat Classroom Courses in the UK. The training can be bespoken as users want with hands-on exercises. The course can be conducted even if there is only one person by experienced Tomcat instructors.  {{http://www.roguewave.com/CMSPages/GetAzureFile.aspx?path=~\roguewave\media\assets\logos\rw-logo.png&hash=a8e0402f2ee46c3ac9de1b48128e0d1465ef716dde369d3cd448811ab1c55206  Rogue WaveRoguewave Tomcat Support/OpenLogicRoguewave Tomcat Supportoffers a comprehensive week-long instructor-led training program for developers and admins. Rogue WaveOpenLogicalso provides Architecture and Design Consulting, and a Developer Support contract to assist in the development of web applications.  http://www.developintelligence.com/img/logo.jpg  DevelopIntelligence provides customized on-siteApache training, withopen source training coursesonApache Tomcat Training,ActiveMQ,Maven, and more.  {{http://www.savoirfairelinux.com/image/image_gallery?img_id=53315&t=1287776986392  Savoir-faire Linux provides 24/7 support, consulting, development and training services on Apache Tomcat. We provideTomcat trainingin our offices in Montreal, Quebec City and Ottawa, Canada. For private, on-site and custom courses, contact training@savoirfairelinux.com or call 1-877-735-4689.    !Intertechdelivers training and consulting for Tomcatin our complete line-up of open source-related training and consulting.  For training, students can attend courses virtually, in-person at Intertech's facility, or we can come to your location.  {{https://www.mechsoft.com.tr/images/logo/mechsoft.jpg  MechSoft supports the development of open source software in Turkey and the world. We always aim to use open source applications and tools while developing our enterprise applications. Besides, our highly talented developers represent MechSoft and Turkey at well-known open source foundation.  MechSoft helps companies to adopt open source solutions in their IT infrastructure.  {{http://www.learncomputer.com/wp-content/themes/learncomputer/img/learncomputer-logo.png  LearnComputer offers instructor-led online and onsiteTomcat trainingcourses for companies and public. We also offer beginner to advanced courses in Android, Apache, Hadoop, PHP, MySQL, Linux, Java and Networking.    GFU Cyrus AG offers extensiveseminars and inhouse training on Apache Tomcat, from basic principles to advanced and intensive courses. Architecture, installation and configuration – in the cologne training centre you obtain an overview of the possibilities of the application server.  Die GFU Cyrus AG bietet umfassendeSeminare und Inhouse-Schulungen zu Apache Tomcat, vom Grundlagen-Seminar bis zu Aufbau- und Intensivkursen. Architektur, Installation, Konfiguration – im Kölner Schulungszentrum erhalten Sie einen umfassenden Überblick über die Möglichkeiten des Anwendungsservers.  {{http://www.accelebrate.com/assets/img/logo_large.png  Accelebrate offers privateApache Tomcat trainingat client sites in the US, Canada, and worldwide, as well as online.  In addition, the firm offersApache httpd coursesandApache Maven classes.  {{http://www.webagesolutions.com/img/web-age-solutions.png  Web Age Solutions offersApache Tomcat training and mentoringon-site in the US, Canada and worldwide, at a training center, in a Live Virtual Class, or through our subscription-based video library.",../data/confluence_exports/TOMCAT/SupportAndTraining_103100272.html
Apache Tomcat : SupportAndTraining Example company name,Use this example as a basis for your entry. New entries should be added just above this example.,../data/confluence_exports/TOMCAT/SupportAndTraining_103100272.html
Apache Tomcat : Logging Preface,"This FAQ section provides help with logging-related issues. As you read these questions, please keep in mind that Tomcat's internal logging is separate from your own webapp's logging. You would typically be concerned only with your own webapp's logging. You would modify Tomcat's internal logging settings if you are debugging a possible issue or running into other problems. It is anticipated that Tomcat's out-of-the-box logging configuration will be fine for the vast majority of users and environments.",../data/confluence_exports/TOMCAT/Logging_103098922.html
Apache Tomcat : Logging Questions,"Does Tomcat have built-in logging capabilities, and if so how do I use them?What role does commons-logging play in logging?What role does JULI and log4j play in logging?How do I configure commons-logging for use with Tomcat?How should I log in my own webapps?Where does System.out go?How do I rotate catalina.out?Where are the logs when running Tomcat as a Windows service?How do I customize the location of the tomcat logging.properties file?Since java.logging is the default commons-logging implementation in Tomcat, why is it not working in my Linux distribution?",../data/confluence_exports/TOMCAT/Logging_103098922.html
"Apache Tomcat : Logging Does Tomcat have built-in logging capabilities, and if so how do I use them?","The Servlet Specification requires Servlet Containers like Tomcat to provide at least a rudimentary implementation of theServletContext#logmethod. Tomcat provides a much richer implementation than required by the Spec, as follows: Prior to Tomcat 5.5, Tomcat provided a Logger element that you could configure and extend according to your needs.Starting with Tomcat 5.5, Logger was removed andApache Commons-LoggingLogis used everywhere in Tomcat. Read the Commons-Logging documentation if you'd like to know how to better use and configure Tomcat's internal logging. See alsohttps://tomcat.apache.org/tomcat-9.0-doc/logging.htmlIn Tomcat 7 (and also 6), the logging code is based on a set of classes interacting with thejava.util.logging API(JUL), which comes with Java since version 1.4. The Tomcat startup script configures the JVM to use a web-application-aware implementation of theJUL LogManager. This Tomcat logging infrastructure is called JULI, and one can still distinguish its Apache Commons Logging heritage, but the complex configuration has been edited out and the package name changed. Web applications can get logging service by using the Servlet API logging (which not recommended), the JUL interface (which ultimately goes to JULI) or any other preferred interface for which they furnish the jar files and the appropriate configuration (see the respective descriptions forLog4J,SLF4J,logbackorApache Commons Loggingfor example). To additionally log information about requests going to the web application, ""Valves"" can be configured in the server.xml file, as described in detailhere. For example, inside the <Engine> tag: <Valve className=""org.apache.catalina.valves.AccessLogValve""
	directory=""logs""  prefix=""localhost_access_log."" suffix="".log"" pattern=""common"" resolveHosts=""false""/> This will produce a log file for each day, such as logs/localhost_access_log.2008-03-10.log, containing the files requested, IP address of the requester, and similar information. 128.34.123.121 - - [10/Mar/2008:15:55:57 -0500] ""GET /upload/ClickPoints.jsp HTTP/1.1"" 200 2725 In addition, Tomcat does not swallow the System.out and System.err JVM output streams. You may use these streams for elementary logging if you wish, but a more robust approach such as commons-logging orLog4Jis recommended for production applications.",../data/confluence_exports/TOMCAT/Logging_103098922.html
Apache Tomcat : Logging What role does commons-logging play in logging?,"Tomcat wants to support multiple logging implementations, so it uses commons-logging. In case that's unclear, think of it like this. You are a Tomcat developer. The car you drive when logging is the commons-logging car. The engine of that car is either JULI or log4j. Without one of these engines, the car goes no where. However regardless of whether you use JULI or log4j, the steering wheel, break, gas pedal, etc. are the same. Related FAQ:What role does JULI and log4j play in logging?",../data/confluence_exports/TOMCAT/Logging_103098922.html
Apache Tomcat : Logging What role does JULI and log4j play in logging?,"First see:What role does commons-logging play in logging? Note in addition that in your own applications you could log directly with JULI or log4j. But once you choose one, you can't easily switch to the other later. If you use commons-logging you can.",../data/confluence_exports/TOMCAT/Logging_103098922.html
Apache Tomcat : Logging How do I configure commons-logging for use with Tomcat?,"You need to specify a commons-logging configuration file and, if you wish, a logging implementation that supports commons-logging. JDK 1.4 (and later) java.util.Logging and Log4j are the two most commonly used logging toolkits for Tomcat. Tomcat 5.5 and Tomcat 6.0 use java.logging as default implementation for commons-logging. So thisshouldwork by default, but sometimes it doesn't (see#Q9). If you supply an external logging toolkit such as Log4J, it needs to be located in the $CATALINA_HOME/common/lib directory (for Tomcat 5.0 and earlier). Tomcat 5.5 and later uses commons-logging while bootstrapping so some people suggest adding Log4j to the bootstrap classpath by using the scripts in $CATALINA_HOME/bin (seeNeed for it to be in bootstrap classpath?). A better approach apparently working is: Put log4j.jar in the $CATALINA_HOME/common/lib directoryPut thefullcommons-logging.jar in the $CATALINA_HOME/common/lib directory, even if you see thereducedAPI version there, named commons-logging-api.jar Through some classloading voodoo during bootstrapping, if you have the full commons-logging.jar file in your common/lib directory, it replaces the classes from the commons-logging-api.jar file and will reinitialize the logging system and attempt to locate log4j or whatever other logging system you may be using. (seethis thread). The above recipe is for Tomcat 5.5. For Tomcat 7 — seeDocumentation. See also the following mailing list discussions: A log4j 1.x exampleLogging ConfigurationExample with JSVC and running on port 80.Tomcat and Log4j Configuration (and Velocity), addressing and solving the bootstrap commons-logging.jar problem",../data/confluence_exports/TOMCAT/Logging_103098922.html
Apache Tomcat : Logging How should I log in my own webapps?,"While you can use System.out and System.err to log, we strongly recommend using a toolkit like Log4J or JDK 1.4's java.util.logging package. With these toolkits, you have significantly more functionality. For example, sending emails, logging to a database, controlling at runtime the logging level of different classes, inspecting the logs with a graphical viewer, etc. We also recommend that you separate your logging from Tomcat's internal logging. That means you should bundle your logging toolkit with your webapp. If you are using Log4J, for example, place the Log4J jar in the WEB-INF/lib directory of your webapp and the Log4J configuration file in the WEB-INF/classes directory of your webapp. This way different web applications can have different logging configurations and you don't need to worry about them interfering with each other.",../data/confluence_exports/TOMCAT/Logging_103098922.html
Apache Tomcat : Logging Where does System.out go?,"System.out and System.err are both redirected to CATALINA_BASE/logs/catalina.out when using Tomcat's startup scripts (bin/startup.sh/.bat or bin/catalina.sh/.bat). Any code that writes to System.out or System.err will end up writing to that file. If your webapp uses System.out and/or System.err a lot, you can suppress this via the 'swallowOutput' attribute in your <Context> configuration element and send to different log files (configured elsewhere: see the documentation for configuring logging).",../data/confluence_exports/TOMCAT/Logging_103098922.html
Apache Tomcat : Logging How do I rotate catalina.out?,"CATALINA_BASE/logs/catalina.out does not rotate. But it should not be an issue because nothing should be printing to standard output since you are using a logging package, right? If you really must rotate catalina.out, here are some techniques you can use: If you are using jsvc 1.0.4 or later (fromApache Commons Daemonproject) to launch Tomcat, you can send SIGUSR1 signal to jsvc to get it to re-open its log files (Jira Ticket). You can couple this with 'logrotate' or your favorite log-rotation utility (including good-old 'mv') to re-name catalina.out at intervals and then get jsvc to re-open the original (catalina.out) file and continue writing to it.Use 'logrotate' with the 'copytruncate' option. This allows you to externally rotate catalina.out without changing anything within Tomcat.Modify bin/catalina.sh (or bin/catalina.bat) to pipe output from the JVM into a piped-logger such ascronologor Apache httpd'srotatelogs(note that the previous reference is for Apache httpd documentation and *is not applicable to Tomcat* – it merely illustrates the concept).See also the patch inBug 53930, ""Allow capture of catalina stdout/stderr to a command instead of just a file"". References to mailing list discussions: tomcat-users thread from 2003tomcat-users thread from 2009tomcat-users thread from 2011tomcat-users thread from 2012",../data/confluence_exports/TOMCAT/Logging_103098922.html
Apache Tomcat : Logging Where are the logs when running Tomcat as a Windows service?,See these mailing list archive threads: Where are the Tomcat logs when running as a Windows service?,../data/confluence_exports/TOMCAT/Logging_103098922.html
Apache Tomcat : Logging How do I customize the location of the tomcat logging.properties file?,"Set the following property when starting tomcat: java.util.logging.config.file Example:-Djava.util.logging.config.file=/etc/tomcat/logging.properties For another example of how to set this look in catalina.sh for Tomcat 6.0.16 on lines 182-185. The statements look like this: # Set juli LogManager if it is present
if [ -r ""$CATALINA_BASE""/conf/logging.properties ]; then
  JAVA_OPTS=""$JAVA_OPTS ""-Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager"" ""-Djava.util.logging.config.file=""$CATALINA_BASE/conf/logging.properties""
fi Projects such as JPackage that repackage Tomcat for Linux typically move the configuration to a directory dictated by the FHS standard (http://www.pathname.com/fhs/), and therefore use the java.util.logging.config.file property to set the location of the logging.properties file in the Tomcat startup script. On Fedora the startup script in typically located in /etc/rc.d/init.d/ and on Gentoo linux it is located in /etc/init.d/. On RedHat the startup script for Tomcat 5.5 is /etc/init.d/tomcat5 but eventually the real startup script is /usr/bin/dtomcat5.",../data/confluence_exports/TOMCAT/Logging_103098922.html
"Apache Tomcat : Logging Since java.logging is the default commons-logging implementation in Tomcat, why is it not working in my Linux distribution?","Yes, if you read Tomcat logging documentation, it says java.util.logging should work by default. But many Linux distribution repackage Tomcat and sometimes it does NOT work by default. Here are some things you can check: tomcat-juli.jar should be in your $CATALINA_HOME/bin directorytomcat startup script should run java with-Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManagertomcat startup script should run java with-Djava.util.logging.config.file=<some_path>/logging.propertiesobviously, the logging.properties file must exist in the directory specified in the tomcat script at point #3 If you don't know where to look for your Tomcat startup script, see the previousHow do I customize the location of the tomcat logging.properties file? In RHEL5 (RedHat Enterprise Server 5) the Tomcat 5.5 rpm installation does not include the tomcat-juli.jar file. This is what I made: look for what Tomcat version you got installed with:yum list installed tomcat5 Since I had the 5.5.23, I downloaded the Tomcat Binaries 5.5.23 fromhttps://archive.apache.org/dist/tomcat/, then: tar xf apache-tomcat-5.5.23.tar.gz
cd apache-tomcat-5.5.23/bin
cp tomcat-juli.jar /usr/share/tomcat5/bin/ Restart Tomcat... and it's working!",../data/confluence_exports/TOMCAT/Logging_103098922.html
Apache Tomcat : TomcatTrackNA10 PMC Sessions mod_jk / mod_proxy and others: Front-ends of Tomcat Clusters.,"By Jean-Frederic Clere:  Tomcat is often used as a cluster and/or is the back-end server of a front-end reserve proxy. Several front-end can be used, mod_jk, mod_proxy, mod_serf, Traffic Server and mod_cluster, quick presentation of each of them, featuring: loadbalancing, failover, QoS, performances etc.Jean-Frederic has spent more than 20 years writing client/server software. He is committer in APR, Jakarta, Httpd and Tomcat and he likes complex projects where different languages and machines are involved. Born in France, Jean-Frederic lived in Barcelona (Spain) for 14 years. Since May 2006 he lives in Neuchatel (Switzerland) where he works forRedHatin the JBoss division.",../data/confluence_exports/TOMCAT/TomcatTrackNA10-PMC-Sessions_103100629.html
Apache Tomcat : TomcatTrackNA10 PMC Sessions New memory leak prevention and detection features in Tomcat 7,"By Mark Thomas:  It is all too easy to trigger a memory leak within a web application, particularly inPermGenwhen the application is reloaded. These leaks are often the unintended and unexpected consequences of using a library or even parts of the standard Java APIs. This session will explore the various ways these leaks can be triggered, how to diagnose them when they occur and the techniques that can be used to avoid them. The session will be based around the successful memory leak prevention code recently added to Apache Tomcat codebase although the causes, process of diagnosis and the remedies are widely applicable to any J2EE application server or, in fact, any other environment that provides a 'reload' mechanism that uses multiple class loaders.Mark has been using and developing Tomcat for over six years. He first got involved in the development of Tomcat when he needed better control over the SSL configuration than was available at the time. After fixing that first bug, he started working his way through the remaining Tomcat bugs and is still going. Along the way Mark has become a Tomcat committer and PMC member, volunteered to be the Tomcat 4 & 7 release manager, created the Tomcat security pages, become a member of the ASF and joined the Apache Security Committee. He also helps maintain the ASF's Bugzilla instances. Mark has a MEng in Electronic and Electrical Engineering from the University of Birmingham, UK and is currently a Senior Software Engineer with theSpringSourceDivision of VMWare.",../data/confluence_exports/TOMCAT/TomcatTrackNA10-PMC-Sessions_103100629.html
Apache Tomcat : TomcatTrackNA10 PMC Sessions Becoming a Tomcat 7 super user,"By Mark Thomas:  Have you ever wanted to to do more than just download Tomcat and install some web applications? Do you want to learn more about Tomcat internals and how everything fits together? Do you want to be able to apply patches for bugs to your Tomcat installation without having to wait for the next release? If you answered yes to any of the above questions then this presentation is for you. Starting with how the Tomcat subversion repository is structured, this presentation will show you how to build each of the major Tomcat versions from source, how to use your local build to generate binary patches for specific bugs and how to extend Tomcat functionality for your environment. After an overview of the Tomcat request processing architecture, a request will be examined as it passes through Tomcat using remote debugging. Key classes and decision points will be highlighted to provide attendees with potential starting points when debugging their own issues. To finish the presentation, and to give a practical example of how the knowledge gained may be put to use, a current bug will be examined, the root cause identified and a patch to fix it generated.",../data/confluence_exports/TOMCAT/TomcatTrackNA10-PMC-Sessions_103100629.html
Apache Tomcat : TomcatTrackNA10 PMC Sessions Quickstart guide to embedding Tomcat 7,"By Mark Thomas:  Tomcat has always been embeddable but with Tomcat 7 it has been much simpler. This presentation will show you how you can embed Tomcat in your application in just five - may need to edit this figure- lines of code. The remainder of the presentation will demonstrate how this embedded instance can be configured and will cover standard web.xml configuration, how to modify settings normally configured in server.xml and integrating with custom components such as a custom Realm.",../data/confluence_exports/TOMCAT/TomcatTrackNA10-PMC-Sessions_103100629.html
Apache Tomcat : TomcatTrackNA10 PMC Sessions Securing and managing your Tomcat installations,"By Mladen Turk:  Using Apache Tomcat as production application server requires some basic knowledge of using multi-tier application servers to host business logic and data access services. This talk addresses each component and technology separately and shows you how to secure your application server in each case. It focuses on security measures that needs to be considered when securing the associated communication channels that connect Web server to the application server and the application server to the database server. It also gives a glimpse overview on the threats and countermeasures needed to secure the Tomcat in production environments.Mladen Turk is a Principal Software Engineer at JBoss, a division of Red Hat (Switzerland), where he is responsible for Native Integration, Enterprise Web Services and Multiplatform technologies. He is member of JBoss Application Server team and gives more then 20 years of experience in client/server technologies. Mladen is currently acting as Apache Tomcat PMC chair, and beside Tomcat, he actively contributes to APR, Httpd,TrafficServerand Commons projects.",../data/confluence_exports/TOMCAT/TomcatTrackNA10-PMC-Sessions_103100629.html
Apache Tomcat : TomcatTrackNA10 PMC Sessions Running Tomcat Stand-alone on Port 80,"By Jason Brittain [jason d0t brittain a+ gmail do+ com]:                                                                             On non-Windows operating systems, Tomcat can only answer web requests on port 80 or port 443 when Tomcat is integrated with a solution that allows receiving requests on these default privileged port numbers. Because using the default port numbers is preferable or even required for many reasons, Tomcat administrators must research the solutions and choose a way to configure it to work on their servers.There are a number of solutions for making Tomcat answer port 80 requests. We'll list the popular solutions, discuss the pros and cons of each solution, and discuss which solutions are usually preferable, and for which reasons.Jason is a co-author of Tomcat: The Definitive Guide, now in its second edition, and has been using Tomcat and contributing to the Apache Tomcat project for over ten years. As a Debian Java committer, Jason works on the Debian and Ubuntu Tomcat 6 package. Jason is a Software Architect atMuleSoft, working on the Tcat Server product.",../data/confluence_exports/TOMCAT/TomcatTrackNA10-PMC-Sessions_103100629.html
Apache Tomcat : TomcatTrackNA10 PMC Sessions Introduction to Apache Tomcat 7,"By Tim Funk:  Apache Tomcat 7 is the latest version of Tomcat products. In this talk, we will overview the new features in Tomcat 7. We will highlight Servlet 3.0 features and how they apply to Tomcat, deprecations which are now gone versus Tomcat 6, and mention features still in common with Tomcat 6.Tim Funk has been a Tomcat user for over 7 years and committer since 2003. Tim currently resides in Pennsylvania and works for Armstrong World Industries performing a range of roles, enjoying the role of developer the most. Tim has a Masters of Software Engineering from Penn State.",../data/confluence_exports/TOMCAT/TomcatTrackNA10-PMC-Sessions_103100629.html
Apache Tomcat : TomcatTrackNA10 PMC Sessions Tomcat 7 - Implementing a Servlet Container,"By Damodar Chetty:  The servlet specification forms the requirements document that a servlet container, such as Tomcat, actually implements. To truly appreciate what a servlet container does, it is therefore critically important to first start with the servlet specification.In this session we'll take a long hard look both at the specification as well as at how Tomcat stays true to it.We will consider the conceptual underpinnings and implementations of aspects such as deployment descriptors, the servlet life cycle, session management, listeners, and filters.Damodar Chetty is the author of the recently published, Tomcat 6 Developer's Guide, and is a veteran software engineer with over two decades in the field. He has graduate degrees from the University of Minnesota (Computer Engineering) and the University of Goa (MBA), and an undergraduate degree in electrical engineering form the University of Bombay.",../data/confluence_exports/TOMCAT/TomcatTrackNA10-PMC-Sessions_103100629.html
Apache Tomcat : TomcatTrackNA10 PMC Sessions Tomcat Vital Component Deep Dive,"By Damodar Chetty:  In this session we'll take an up close and personal look at the most vital components that comprise Tomcat– the connector subsystem, virtual host, the context, the wrapper, and the session manager.We will explore the implementation of the JIO and NIO connectors, look at alternative virtual hosting techniques and its context deployment responsibilities, delve into protocol handlers and the class loading mechanics associated with web application contexts, get familiar with request dispatching and the filter decoration aspects of wrappers, and immerse ourselves in the intricacies of state management with sessions.Damodar Chetty is the author of the recently published, Tomcat 6 Developer's Guide, and is a veteran software engineer with over two decades in the field. He has graduate degrees from the University of Minnesota (Computer Engineering) and the University of Goa (MBA), and an undergraduate degree in electrical engineering form the University of Bombay.",../data/confluence_exports/TOMCAT/TomcatTrackNA10-PMC-Sessions_103100629.html
Apache Tomcat : ReleaseProcess Introduction,This is written primarily for Apache Tomcat release managers although it may also be of interest to anyone looking to validate and/or replicate the release process. This page uses Tomcat 9 as an example but the same process applies to later versions as well.,../data/confluence_exports/TOMCAT/ReleaseProcess_103100084.html
Apache Tomcat : ReleaseProcess Pre-requisites,"A git client installed and on your pathApache Ant installed and on your path (see BUILDING.txt in the root of the code repository for version requirements)The latest release of the minimum Java version that the Tomcat version runs on installed and on your pathOperating systemWindows just worksLinux also requires wine (standard package manager version should be fine)MacOS also requires wine (see below)GnuPG installedA public key that is part of the Apache web of trustA reasonable internet connection (you will need to upload ~100MB) The above can be accomplished on a Windows 10 Virtual Machine fromMicrosoft's Edge Development resources. Download + unpack the archive for your VM software and launch the VM. Login, open a PowerShell.exe window as Administrator, and installedChocolatey: PS C:\Users\IEUser> Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1')) Once Chocolatey is installed, you can install all the above prerequisites at once: PS C:\Users\ISUser> choco install git svn adoptopenjdk11 ant gnupg sed xsltproc This command will run for a while, and ask you repeatedly if it's okay to run ""chocolateyInstall.ps1"", which you will have to do to proceed. Once the above command has completed, you have all the software prerequisites installed and on your PATH. Well, once you quit PowerShell and launch a new CMD.EXE or PowerShell window, of course",../data/confluence_exports/TOMCAT/ReleaseProcess_103100084.html
Apache Tomcat : ReleaseProcess wine on MacOS,This is for 11.x.x (Big Sur) You will need to installhomebrewif you haven't already. Then install wine-crossover fromhttps://github.com/Gcenx/homebrew-wine Configure a 32-bit wine environment using: WINEARCH=win32 WINEPREFIX=~/.wine32 winecfg Then before you start the release ensure the following environment variables are set: export WINEARCH=win32export WINEPREFIX=~/.wine32,../data/confluence_exports/TOMCAT/ReleaseProcess_103100084.html
Apache Tomcat : ReleaseProcess Preliminary checks,"Check that the version numbers have been incremented after the previous release, as expected. (e.g.https://github.com/apache/tomcat/commit/cec14f9c33af9da271c9681bf6b343c61b6d055a)Check that the changelog file mentions your login name as release manager for this release (e.g. ""Tomcat 9.0.94 (markt)"").Check whether theKEYSfile differs fromhttps://dist.apache.org/repos/dist/release/tomcat/tomcat-9/KEYSone. The latter one will be replaced after you do a release. Check that theKEYSfile contains your public key.Check that the full build still works (ant release, preferably). Check that Buildbot builds (https://tomcat.apache.org/ci.html#Buildbot) are green.Prepare your build environment:Add a build.properties file (or, better yet, keep this file in your ~/build.properties if you have a dedicated build machine) with the following configuration (adjust paths for your environment)execute.validate=true

execute.test.bio=true
execute.test.nio=true
execute.test.apr=true

test.haltonfailure=true

gpg.exec=C:/Program Files (x86)/GNU/GnuPG/gpg2.exe
#gpg.exec=/usr/bin/gpg

base.path=C:/temp/libs
#base.path=~/tomcat-libs
# Enable the following if the DigiCert ONE magic is all set up, including ~/.digicertone/pkcs11properties.cfg
codesigning.storepass=apikey|keystorepath|keystorepassword
do.codesigning=true",../data/confluence_exports/TOMCAT/ReleaseProcess_103100084.html
Apache Tomcat : ReleaseProcess Build the release + create git tag,"The aim is to create a copy of the current trunk but without the ""-dev"" appended to the end of the version number. All artifacts required for repeatable builds will be included as well. Perform an git clone and switch to the correct branchgit clonehttps://github.com/apache/tomcat/c/releases/asf-tomcatorgit clone -b 9.0.x git clonehttps://github.com/apache/tomcat/c/releases/asf-tomcatcd/c/releases/asf-tomcatgit checkout 9.0.xgit pull(Hint: usingant pre-releasemay save you the following step, plus steps 2-3 in the Maven release process.)Edit ""build.properties.default"" and change the lines after ""# ----- Reproducible builds -----"" to a new value. (NOTE: this is done by 'ant pre-release')Note that the value ofant.tstamp.nowproperty is in seconds (unlike the value returned bySystem.currentTimeMillis()method which is milliseconds, seebug 65527for how this happened for Tomcat8.5.70).The value can be printed in a Bash shell with the following command:date +%sTo print seconds since epoch, and date and time in human-readable format in UTC time zone:date -u '+%s %Y-%m-%d %H:%M:%S %Z'Edit ""build.properties.default"" and change the line version.dev=-dev to version.dev= (NOTE: this is done by 'ant pre-release')This can be done withsed -i.bak ""s/^version.dev=.*/version.dev=/"" build.properties.defaultEdit ""webapps/docs/changelog.xml"" and replace rtext=""in development"" with rtext=""Release in progress"" for the version being released (NOTE: this is done by 'ant pre-release')Remember that notepad.exe will remove UTF-8 Byte-Order Marks (BOMs). Use write.exe or Notepad++ instead.Build the releaseant releaseCommit these changesgit add-A (to pick-up the repeatable build artifacts in addition to the content-changes from above)git commit -a -m ""Tag 9.0.94""git tag 9.0.94git push origin 9.0.94Check the diff mailed to the dev list Notes: GPG should be configured to use your Apache code signing key by defaultI always ensuredc:/temp/libswas empty so that the build had to download all the dependenciesThis does not include the signing of the Windows installer, which must be done usinghttps://one.digicert.com/andhttps://infra.apache.org/digicert-use.htmlwhich is automated during the build process once the Tomcat PMC key is accessible by using jsign. (Working! Try settingdo.codesigning=truein build.properties).The logs for the Windows signing are in ~/.signingmanager/logs on LinuxIf you get an error signing the installer/uninstaller ""Cannot load keystore ~/.digicertone/pkcs11properties.cfg"" then you probably don't have your environment variables (orcodesigning.storepassproperty) set properly.",../data/confluence_exports/TOMCAT/ReleaseProcess_103100084.html
Apache Tomcat : ReleaseProcess Upload the release,"Upload the contents ofTOMCAT_9_0_XX/output/releasetohttps://dist.apache.org/repos/dist/dev/tomcat/tomcat-9/ svn checkouthttps://dist.apache.org/repos/dist/dev/tomcat/tomcat-9cp -r TOMCAT_9_0_XX/output/release/* tomcat-9cd tomcat-9 && svn add v* && svn commit -m ""Upload vX.Y.Z for voting""",../data/confluence_exports/TOMCAT/ReleaseProcess_103100084.html
Apache Tomcat : ReleaseProcess Generate the Maven artifacts,Seehttps://github.com/apache/tomcat/blob/9.0.x/res/maven/README.txtsteps 1 to 3 for the release (not step 4 until the vote passes !),../data/confluence_exports/TOMCAT/ReleaseProcess_103100084.html
Apache Tomcat : ReleaseProcess Call a vote,E.g.http://markmail.org/message/gvmbwocspnwb2dfeIncrement the version number ready for the next tag (e.g.https://svn.apache.org/viewvc?view=revision&revision=1500094),../data/confluence_exports/TOMCAT/ReleaseProcess_103100084.html
Apache Tomcat : ReleaseProcess Cleaning Up,Reset to the 9.0.x branch (git reset HEAD~ && git checkout build.properties.default && git checkout webapps/docs/changelog.xml && rm -f build.properties.release res/maven/mvn.properties.release) I found it simplest to keep this clone for tagging to ensure no other edits found their way into the tag. To get a clean copy of the release (e.g. for testing): mkdir /c/releases/TOMCAT_9_0_94git archive 9.0.94 | tar -x -C /c/releases/TOMCAT_9_0_94/ I'm using Git Bash for the above. Adjust as necessary for you choice of tools.,../data/confluence_exports/TOMCAT/ReleaseProcess_103100084.html
Apache Tomcat : ReleaseProcess If the vote passes,"Send a recap ""[VOTE][RESULT]"" email to the vote thread on the dev list with the vote counts.svn mv https://dist.apache.org/repos/dist/dev/tomcat/tomcat-9/v9.0.XX https://dist.apache.org/repos/dist/release/tomcat/tomcat-9/v9.0.XXsvn checkout --depth immediates https://dist.apache.org/repos/dist/release/tomcat/tomcat-9/and update theKEYSfile there to be the same as the one used for release. (The download page has links pointing to this file).Release the Maven artifacts (https://github.com/apache/tomcat/blob/9.0.x/res/maven/README.txtstep 4)Wait for the release artifacts to appear in these locations:https://downloads.apache.org/tomcat/tomcat-[v]/https://dlcdn.apache.org/tomcat/tomcat-[v]/Update the website (e.g.https://svn.apache.org/viewvc?view=revision&revision=1901819)Note:onlymodify thexdocs/ files, then useant docsto generate the HTML.Note: the index and oldnews pages are sorted by date. Thus the new announcement pops up to the top of the page.Note: usingtools/update-version.sh oldver newver releasedatefrom the 'site' project may help you with almost all of the website updates below.Update the build.properties.default file to ensure the correct release is listed.Updatedownloadpage (update ""[define v]"" macro for the new version)Updatewhichversionpage (the current version number).Updatedoap_Tomcat.rdffile (version number and release date).Update docs/tomcat-[v]-doc/changelog.html to set the release date to the date the files were copied to the release area in yyyy-MM-dd format (""beta, 2007-02-28"" for betas).Updatemigrationpage (versions in ""configuration file differences"" comparison form).Notable changes that change behaviour and may require action when upgrading from the previous release may be added to the ""noteable changes"" section on themigrationpage.Move old announcement to the top of theoldnewspage.Add announcement for the new release to the top of the front page (index).Update the javadocs (seehttps://svn.apache.org/repos/asf/tomcat/site/trunk/README.txt)Announce the release (e.g.http://markmail.org/message/xyantb3ozzmucdjt) to users@t.a.o, cc dev@t.a.o, announce@t.a.o, announce@a.oAdd the version number in Bugzilla (https://bz.apache.org/bugzilla/→ Administration → Products /versions)Drop the artifacts for the previous release fromhttps://dist.apache.org/repos/dist/release/tomcat/tomcat-[v]/Add release data (version and date) to reporter.apache.org -https://reporter.apache.org/addrelease.html?tomcat",../data/confluence_exports/TOMCAT/ReleaseProcess_103100084.html
Apache Tomcat : ReleaseProcess If the vote does not pass,"Update the release date in the changelog to ""not released""Drop the artifacts fromhttps://dist.apache.org/repos/dist/dev/tomcat/tomcat-9/Drop the maven artifacts from the staged maven repository",../data/confluence_exports/TOMCAT/ReleaseProcess_103100084.html
Apache Tomcat : EditDefaultJSPPage Editing the default JSP home page loaded by Tomcat,"The contents of the default Tomcat home page comes from the ROOT webapp servlet calledorg.apache.jsp.index_jsp. The page that you see in$CATALINA_HOME/webapps/ROOT/index.jsphas been precompiled into a class file (org.apache.jsp.index_jsp.class) stored in a JAR file (catalina-root.jar) in the ROOT webapp'sWEB-INF/libdirectory. Because of this servlet, Tomcat will not look at the contents of the ROOT web application'sindex.jspfile if you change it.  The easiest way to change the contents of theindex.jsppage is to remove thisindex_jspservlet from the ROOT webapp. Once you remove theindex_jspservlet and restart Tomcat, Tomcat will see theindex.jspfile in the ROOT directory and compile it on the fly into a class file. You now will be able to edit the ROOT/index.jsp file and have those changes take effect immediately by reloading thehttp://localhost:8080/page.  To remove theindex_jspservlet, edit the ROOT web application's configuration file,$CATALINA_HOME/webapps/ROOT/WEB-INF/web.xml. Comment out the definition of the servlet and the servlet mapping, so that section of the file will look like this (note the commented out section):  <!-- JSPC servlet mappings start -->

  <!-- Commenting out so I can change the index.jsp page
    <servlet>
        <servlet-name>org.apache.jsp.index_jsp</servlet-name>
        <servlet-class>org.apache.jsp.index_jsp</servlet-class>
    </servlet>

    <servlet-mapping>
        <servlet-name>org.apache.jsp.index_jsp</servlet-name>
        <url-pattern>/index.jsp</url-pattern>
    </servlet-mapping>
  -->
<!-- JSPC servlet mappings end -->  Once you disable theindex_jspservlet and restart Tomcat, how does Tomcat know to compile theindex.jsppage in the ROOT web app's directory? Easy. First, when you request the default page of a web application, Tomcat (like every servlet container) will look for a welcome file. The default welcome files are defined at the bottom of$CATALINA_HOME/conf/web.xml. This web.xml file acts as a global web.xml file used for all web applications installed in Tomcat. The default welcome file list includesindex.jsp, which means Tomcat will try to load that file (if found) in order to display it. Second, the$CATALINA_HOME/conf/web.xmlconfiguration file also defines a servlet called simplyjsp. This section of theweb.xmlfile:  <!-- The mapping for the JSP servlet -->
    <servlet-mapping>
        <servlet-name>jsp</servlet-name>
        <url-pattern>*.jsp</url-pattern>
    </servlet-mapping>

    <servlet-mapping>
        <servlet-name>jsp</servlet-name>
        <url-pattern>*.jspx</url-pattern>
    </servlet-mapping>  maps all.jspand.jspxpages to the jsp servlet. The jsp servlet performs the work of compiling the source JSP file into a servlet and then executing the servlet. The JSP servlet, by default, will check the JSP source page every time it is requested to see if it was modified since the last time it was compiled. If the page changed within 4 seconds of the last time it was compiled, the servlet will recompile the source JSP page before running it. The behavior of the jsp servlet is quite configurable. You can see all its options defined in the$CATALINA_HOME/conf/web.xmlconfiguration file.",../data/confluence_exports/TOMCAT/EditDefaultJSPPage_103098737.html
Apache Tomcat : JASPIC TCK Tomcat,None required (clean 9.0.x build).,../data/confluence_exports/TOMCAT/JASPIC-TCK_103090002.html
Apache Tomcat : JASPIC TCK Test Suite,Download latest nightly build https://download.eclipse.org/ee4j/jakartaee-tck/8.0.1/nightly/jaspictck-1.1_latest.zip Extract to JASPIC_TCK_HOME  Edit $JASPIC_HOME/bin/ts.jte You'll need to set the following properties (adjust the paths and values for your environment) harness.executeMode=2 jaspic.home=/path/to/tomcat orb.port=8080 sigTestClasspath=${jaspic.home}/lib/jaspic-api.jar:${JAVA_HOME}/lib/jce.jar:${JAVA_HOME}/lib/rt.jar:${JAVA_HOME}/../Classes/classes.jar  set JAVA_HOME cd $JSP_TCK_HOME/bin ant gui Accept the defaults and then run the tests  A default 9.0.x build with the above configuration triggers 106 test failures. Everything fails. Configuration is not yet correct.It appears you have to install the RI to run the TCK.,../data/confluence_exports/TOMCAT/JASPIC-TCK_103090002.html
Apache Tomcat : Removing unpackWARs Background,"Prior to Tomcat 8 unpackWARs=false did not result in the web application running entirely from the WAR. For performance reasons, any JARs in WEB-INF/lib were unpacked to the work directory and accessed from there. Tomcat 8 introduced a new resources implementation that did not perform this unpacking. This had a significant (e.g. 3x to 10x slower application start) performance impact -Bug 57251. One of the options suggested in the discussion on that bug was the removal of the unpackWARs feature.",../data/confluence_exports/TOMCAT/Removing-unpackWARs_61320779.html
Apache Tomcat : Removing unpackWARs Simpler deployment while Tomcat is stopped,"If unpackWARs=true and a WAR is updated while Tomcat is stopped, Tomcat will not realise that the unpacked directory structure is from an older version of the WAR and will continue to use it. The current work-arounds are: ensure that the exploded directory is deleted when the WAR is replaceddeploy an exploded directory rather than a WARuse unpackWARs=false The last of these work-arounds is often viewed as the simplest and would not be available if the unpackWARs option was removed. Work is in progress to handle this use case within the existing automatic deployment code so WARs updated while Tomcat is shut-down result in the old directory being removed and the new WAR expanded in its place.",../data/confluence_exports/TOMCAT/Removing-unpackWARs_61320779.html
Apache Tomcat : Removing unpackWARs Security - make appBase read-only,"Note that this discussions assumes a pre-existing application or Tomcat vulnerability that permits writing files into Tomcat's appBase. If the appBase is writeable by the Tomcat user and automatic deployment is enabled then it makes it easier for an attacker to deploy a malicious application by placing it in the appBase. Using unpackWARs=false and a read-only appBase defeats this particular attack vector in Tomcat 8. In Tomcat 7 the protection is provides is less than complete since the location where the JARs are copied to remains writable. The removal of unpackWARs would effectively require the deployment of web applications as exploded directories if the appBase was to remain read-only to the Tomcat user. An alternative to removing uppackWARs in this case might be to move the location of the unpacked WAR files: the appBase is still the *source* of all WAR data, but unpacked WAR files would be unpacked elsewhere -- such as into the work directory, etc. This would allow a read-only appBase and still allow unpackWARs=true. From a security point of view, the work directory itself is still vulnerable, but it would not be possible (given a pre-existing application or container vulnerability) for an attacker to deploy a completely new WAR onto the container.",../data/confluence_exports/TOMCAT/Removing-unpackWARs_61320779.html
Apache Tomcat : Removing unpackWARs Actually read-only filesystem,"If the filesystem is actually read-only (or effectively so, where the effective Tomcat user has no file-write rights whatsoever), then unpackWARs must be false in order to deploy. If all JSPs are pre-compiled and logs are not written to disk, Tomcat should be able to run on a read-only filesystem. Deploying an exploded WAR file with pre-compiled JSPs would also meet this requirement.",../data/confluence_exports/TOMCAT/Removing-unpackWARs_61320779.html
Apache Tomcat : Removing unpackWARs Testing - configuration with getRealPath() returning null,"When unpackWARs=""false"" the SevletContext.getRealPath() method always returns null. This configuration can be used to test that a web application is programmed correctly and can function without relying on getRealPath() method.",../data/confluence_exports/TOMCAT/Removing-unpackWARs_61320779.html
Apache Tomcat : Removing unpackWARs Other use cases,Additional use cases welcome. Either directly on this page or via the users mailing list.,../data/confluence_exports/TOMCAT/Removing-unpackWARs_61320779.html
Apache Tomcat : Other Operating Systems Preface,"This section of the FAQ deals with running Tomcat on platforms not supported by the built-in startup/shutdown scripts. These include OS/2, z/OS, OpenVMS (Alpha/I64), and others.",../data/confluence_exports/TOMCAT/Other-Operating-Systems_103099019.html
Apache Tomcat : Other Operating Systems Questions,Where are the scripts for my operating system? OS/2:NetBeans.orgEComStationz/OS:JZOSGNU/Linux:From pander@users.sourceforge.netOpenVMS-Alpha:Hewlett Packard (meg.garrison@hp.com)OpenVMS-I64:See the HowTo,../data/confluence_exports/TOMCAT/Other-Operating-Systems_103099019.html
Apache Tomcat : TomcatCon 2021-02 Committer availability (all times UTC),Committer18 Feb 202109101112131415161718192021222300010203040506070809101112marktremmschultz KeyAvailableMaybeUnavailable,../data/confluence_exports/TOMCAT/TomcatCon-2021-02_173083312.html
Apache Tomcat : TomcatCon 2021-02 Session Ideas,"TitlePresenterAbtractTLS Configuration - traditional CAmarktStep by step guide to configuring Tomcat for TLS from creating the CSR to editing server.xmlTLS Configuration - LetsEncryptschultzBest practises for using LetsEncrypt with TomcatSecuring your reverse proxymarktAJP vs HTTP(s), securing AJP, safely proxying mixed HTTP/HTTPSLocking-down Apache TomcatschultzCovers tools Tomcat provides as well as others you might want to consider. Originally presented at Apache Road Show 2019 in Fairfax, VA. There is some overlap with topics above, but that stuff could easily be edited-out, or switched to averybrief intro with a mention that later talks would cover details.",../data/confluence_exports/TOMCAT/TomcatCon-2021-02_173083312.html
Apache Tomcat : Common Native Build Environment Hardware,Known working configurations include: VMware virtual machine60GB HDD2 processors2GB RAMNo USBNo soundBridged networkingOthers TBD,../data/confluence_exports/TOMCAT/Common-Native-Build-Environment_65872531.html
Apache Tomcat : Common Native Build Environment Operating System,Known working configurations include: Windows 10Static IP address1600x1200 screen resolutionUTC timezoneInstall VMware ToolsApply all updatesOthers TBD,../data/confluence_exports/TOMCAT/Common-Native-Build-Environment_65872531.html
Apache Tomcat : Common Native Build Environment Build tools,"Git for windows (http://git-scm.com/) 2.24.0.windows.2Mladen's Custom Microsoft Compiler Toolkit CompilationYou can either download a binary distribution of CMSC from github or build one from source using the following stepsCheckouthttps://github.com/mturk/cmsc(I used 132baba36d88279e7a9950e7e5e8456ce757b78a, latest tag should also be fine)Download(sha256: 3d2f97178e72cabe0a972b14c4d0aae80f7ed99ec73be9970cc95f42a31bfdb6) and install the Windows Server 2003 R2 Platform SDKNote: This isn't the official Microsoft download. It appears it is only available via MSDN subscription at the moment. The hash matches the version from MSDN.no documentationNo samplesDownloadand install the Windows 7 SDKno documentationNo samplesComplete set of developer toolsDownloadand install the Windows DDK7.1.0Build environmentsToolsCompile as per <cmsc-root>/tools/README.txtInstall Perl as per <cmsc-root>/tools/README.txt7-Zip 19.00 64-bitAdopt OpenJDK jdk8u242-b08 64-bit",../data/confluence_exports/TOMCAT/Common-Native-Build-Environment_65872531.html
Apache Tomcat : Character Encoding Questions,WhyWhat is the default character encoding of the request or response body?Why does everything have to be this way?HowHow do I change how GET parameters are interpreted?How do I change how POST parameters are interpreted?What can you recommend to just make everything work? (How to use UTF-8 everywhere).How can I test if my configuration will work correctly?How can I send higher characters in HTTP headers?How to configure the BASIC authentication scheme to use UTF-8TroubleshootingI'm having a problem with character encoding in Tomcat 5,../data/confluence_exports/TOMCAT/Character-Encoding_103098774.html
Apache Tomcat : Character Encoding What is the default character encoding of the request or response body?,"If a character encoding is not specified, the Servlet specification requires that an encoding of ISO-8859-1 is used. The character encoding for the body of an HTTP message (requestorresponse) is specified in theContent-Typeheader field. An example of such a header isContent-Type: text/html; charset=ISO-8859-1which explicitly states that the default (ISO-8859-1) is being used. References:HTTP 1.1 Specification, Section 3.7.1 The above general rules apply to Servlets. The behaviour of JSP pages is further specified by the JSP specification. The request character encoding handling is the same, but response character encoding behaves a bit differently. See chapter ""JSP.4.2 Response Character Encoding"". For JSP pages in standard syntax the default response charset is the usualISO-8859-1, but for the ones in XML syntax it isUTF-8.",../data/confluence_exports/TOMCAT/Character-Encoding_103098774.html
Apache Tomcat : Character Encoding Why does everything have to be this way?,"Everything covered in this page comes down to practical interpretation of a number of specifications. When working with Java servlets, the Java Servlet Specification is the primary reference, but the servlet spec itself relies on older specifications such as HTTP for its foundation. Here are a couple of references before we cover exactly where these items are located in them. A more detailed list can be found on theSpecificationspage. Java Servlet Specification 4.0HTTP 1.1 Protocol: Message Syntax and Routing,HTTP 1.1 Protocol: Semantics and Content…URI SyntaxARPA Internet Text MessagesHTML 4,HTML 5",../data/confluence_exports/TOMCAT/Character-Encoding_103098774.html
Apache Tomcat : Character Encoding Default encoding for request and response bodies,See 'Default Encoding for POST' below.,../data/confluence_exports/TOMCAT/Character-Encoding_103098774.html
Apache Tomcat : Character Encoding Default encoding for GET,"The character set for HTTP query strings (that's the technical term for 'GET parameters') can be found in sections 2 and 2.1 the ""URI Syntax"" specification. The character set is defined to beUS-ASCII. Any character that does not map to US-ASCII must be encoded in some way. Section 2.1 of the URI Syntax specification says that characters outside of US-ASCII must be encoded using%escape sequences: each character is encoded as a literal%followed by the two hexadecimal codes which indicate its character code. Thus,a(US-ASCII character code 97 = 0x61) is equivalent to%61. Although the URI specification does not mandate a default encoding for percent-encoded octets, it recommends UTF-8 especially for new URI schemes, and most modern user agents have settled on UTF-8 for percent-encoding URI characters. Some notes about the character encoding of URIs: ISO-8859-1 and ASCII are compatible for character codes 0x20 to 0x7E, so they are often used interchangeably.Modern browsers encoding URIs using UTF-8. Some browsers appear to use the encoding of the current page to encode URIs for links.HTML 4.0recommends the use of UTF-8 to encode the query string.When in doubt, use POST for any data you think might have problems surviving a trip through the query string.",../data/confluence_exports/TOMCAT/Character-Encoding_103098774.html
Apache Tomcat : Character Encoding Default Encoding for POST,"Older versions of the HTTP/1.1 specification (e.g.RFC 2616) indicated thatISO-8859-1is the default charset for text-based HTTP request and response bodies if no charset is indicated. AlthoughRFC 7231removed this default, the servlet specification continues to follow suit. Thus the servlet specification indicates that if aPOSTrequest does not indicate an encoding, it must be processed asISO-8859-1, except forapplication/x-www-form-urlencoded, which by default should be interpreted as {{`}}US-ASCII` (as it by definition should contain only characters within the ASCII range to begin with). Some notes about the character encoding of a POST request: RFC 2616 Section 3.4.1 stated that recipients of an HTTP messagemustrespect the character encoding specified by the sender in theContent-Typeheader if the encoding is supported. A missing character allows the recipient to ""guess"" what encoding is appropriate.Most web browsers todaydo notspecify the character set of a request, even when it is something other than ISO-8859-1. This seems to be in violation of the HTTP specification. Most web browsers appear to send a request body using the encoding of the page used to generate the POST (for instance, the <form> element came from a page with a specific encoding... it isthatencoding which is used to submit the POST data for that form).",../data/confluence_exports/TOMCAT/Character-Encoding_103098774.html
Apache Tomcat : Character Encoding Percent Encoding forapplication/x-www-form-urlencoded,"TheHTML 4.01specification indicated that percent-encoding of any non alphanumeric characters ofapplication/x-www-form-urlencoded(the default content type for HTML form submissions) should be performed usingUS-ASCIIbyte sequences. HoweverHTML 5changed this to use UTF-8 byte sequences, matching the modern percent encoding for URLs. Modern browsers therefore percent-encode UTF-8 sequences when submitting forms usingapplication/x-www-form-urlencoded. The servlet specification, however, requires servlet containers to interpret percent-encoded sequences inapplication/x-www-form-urlencodedasISO-8859-1, which in a default configuration will result in corrupted content because of the charset mismatch. See below for how this can be reconfigured in Tomcat.",../data/confluence_exports/TOMCAT/Character-Encoding_103098774.html
Apache Tomcat : Character Encoding HTTP Headers,Section 3.1 of the ARPA Internet Text Messages spec states that headers are always in US-ASCII encoding. Anything outside of that needs to be encoded. See the section above regarding query strings in URIs.,../data/confluence_exports/TOMCAT/Character-Encoding_103098774.html
Apache Tomcat : Character Encoding How do I change how GET parameters are interpreted?,"Tomcat will use ISO-8859-1 as the default character encoding of the entire URL, including the query string (""GET parameters"") (though see Tomcat 8 notice below). There are two ways to specify how GET parameters are interpreted: Set theURIEncodingattribute on the <Connector> element in server.xml to something specific (e.g.URIEncoding=""UTF-8"").Set theuseBodyEncodingForURIattribute on the <Connector> element in server.xml totrue. This will cause the Connector to use the request body's encoding for GET parameters. In Tomcat 8 starting with 8.0.0 (8.0.0-RC3, to be specific), the default value ofURIEncodingattribute on the <Connector> element depends on ""strict servlet compliance"" setting. The default value (strict compliance is off) ofURIEncodingis nowUTF-8. If ""strict servlet compliance"" is enabled, the default value isISO-8859-1. References:Tomcat 7 HTTP Connector,Tomcat 7 AJP Connector,Tomcat 8.5 HTTP Connector,Tomcat 8.5 AJP Connector",../data/confluence_exports/TOMCAT/Character-Encoding_103098774.html
Apache Tomcat : Character Encoding How do I change how POST parameters are interpreted?,"POSTrequests should specify the encoding of the parameters and values they send. Since many clients fail to set an explicit encoding, the default used isUS-ASCIIforapplication/x-www-form-urlencodedandISO-8859-1for all other content types. In addition, the servlet specification requires that percent-encoded sequences ofapplication/x-www-form-urlencodedbe interpreted asISO-8859-1by default which, as explained above, does not match the HTML 5 specification and modern user agent practice of using UTF-8 to percent encode characters. Nevertheless the servlet specification requires the servlet container's interpretation of percent-encoded sequences ofapplication/x-www-form-urlencodedto follow any configured character encoding. Thus appropriate intepretation ofapplication/x-www-form-urlencodedbyte sequences can be achieved by setting the request character encoding toUTF-8. The container-agnostic approach for specifying the request character encoding for applications using Servlet 4.0 or later (which would correspond to Tomcat 9.0 and later) is to set the<request-character-encoding>element in the web applicationweb.xmlfile: <request-character-encoding>UTF-8</request-character-encoding> Note: If you are using the Eclipse integrated development environment, as of Eclipse Enterprise Java Developers 2019-03 M1 (4.11.0 M1) the IDE does not recognize the<request-character-encoding>setting and will temporarily freeze the IDE and generate errors with any edit of web application files. You can track the latest status of this problem atEclipse Bug 543377. Otherwise one can employ ajavax.servlet.Filter. Writing such a filter is trivial. 6.x, 7.x::Tomcat already comes with such an example filter. Please take a look atwebapps/examples/WEB-INF/classes/filters/SetCharacterEncodingFilter.java. 5.5.36+, 6.0.36+, 7.0.20+, 8.x and later::Since Tomcat 7.0.20, 6.0.36 and 5.5.36 the filter became first-class citizen and was moved from the examples into core Tomcat and is available to any web application without the need to compile and bundle it separately, although this will not allow the web application to be deployed in non-Tomcat servlet containers that do not have this filter available, if the servlet is defined in the web application's ownweb-xmlfile. See documentation for the list offiltersprovided by Tomcat. The class name isorg.apache.catalina.filters.SetCharacterEncodingFilter. It is also possible to define such a filter in the Tomcat installation configuration fileconf/web.xml, which would set the request character encoding across all web applications without the need for anyweb.xmlmodifications. In fact the latest Tomcat versions come with sections inconf/web.xmlthat already configure a filter to set the request character encoding toUTF-8. Simply editconf/web.xmland uncomment both the definition and the mapping of the filter namedsetCharacterEncodingFilter. Note: The request encoding setting is effective only if it is done earlier than parameters are parsed. Once parsing happens, there is no way back. Parameters parsing is triggered by the first method that asks for parameter name or value. Make sure that the filter is positioned before any other filters that ask for request parameters. The positioning depends on the order offilter-mappingdeclarations in the WEB-INF/web.xml file, though since Servlet 3.0 specification there are additional options to control the order. To check the actual order you can throw an Exception from your page and check its stack trace for filter names. Tomcat 9.x and later: do not use a<filter>at all and instead specify<request-character-encoding>in your application's web.xml file.",../data/confluence_exports/TOMCAT/Character-Encoding_103098774.html
Apache Tomcat : Character Encoding What can you recommend to just make everything work? (How to use UTF-8 everywhere).,"UsingUTF-8as your character encoding for everything is a safe bet. This should work for pretty much every situation. In order to completely switch to using UTF-8, you need to make the following changes: SetURIEncoding=""UTF-8""on your <Connector> inserver.xml. References:Tomcat 7 HTTP Connector,Tomcat 7 AJP Connector,Tomcat 8.5 HTTP Connector,Tomcat 8.5 AJP Connector.Set thedefault request character encodingeither in the Tomcatconf/web.xmlfile or in the web appweb.xmlfile; either by setting<request-character-encoding>(for applications using Servlet 4.0 / Tomcat 9.x+) or by using a character encoding filter.Change all your JSPs to include charset name in their contentType. For example, use<%@page contentType=""text/html; charset=UTF-8"" %>for the usual JSP pages and<jsp:directive.page contentType=""text/html; charset=UTF-8"" />for the pages in XML syntax (aka JSP Documents).Change all your servlets to set the content type for responses and to include charset name in the content type to be UTF-8. Useresponse.setContentType(""text/html; charset=UTF-8"")orresponse.setCharacterEncoding(""UTF-8"").Change any content-generation libraries you use (Velocity, Freemarker, etc.) to use UTF-8 and to specify UTF-8 in the content type of the responses that they generate.Disable any valves or filters that may read request parameters before your character encoding filter or jsp page has a chance to set the encoding to UTF-8. For more information seehttps://www.mail-archive.com/users@tomcat.apache.org/msg21117.html.",../data/confluence_exports/TOMCAT/Character-Encoding_103098774.html
Apache Tomcat : Character Encoding How can I test if my configuration will work correctly?,"The following sample JSP should work on a clean Tomcat install for any input. If you set the URIEncoding=""UTF-8"" on the connector, it will also work with method=""GET"". <%@ page contentType=""text/html; charset=UTF-8"" %>
<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.01 Transitional//EN"">
<html>
   <head>
     <title>Character encoding test page</title>
   </head>
   <body>
     <p>Data posted to this form was:
     <%
       request.setCharacterEncoding(""UTF-8"");
       out.print(request.getParameter(""mydata""));
     %>

     </p>
     <form method=""POST"" action=""index.jsp"">
       <input type=""text"" name=""mydata"">
       <input type=""submit"" value=""Submit"" />
       <input type=""reset"" value=""Reset"" />
     </form>
   </body>
</html>",../data/confluence_exports/TOMCAT/Character-Encoding_103098774.html
Apache Tomcat : Character Encoding How can I send higher characters in my HTTP headers?,You have to encode them in some way before you insert them into a header. Using url-encoding (%+ high byte number + low byte number) would be a good idea.,../data/confluence_exports/TOMCAT/Character-Encoding_103098774.html
Apache Tomcat : Character Encoding How to configure the BASIC authentication scheme to use UTF-8,"If a web application is configured to use the BASIC authentication scheme (e.g. configured with<auth-method>BASIC</auth-method>in its web.xml file), it means that an instance ofBasicAuthenticatorwill be automatically created and inserted into the chain of Valves for this web application (this Context), unless any other Authenticator valve has already been explicitly configured. To enable support for UTF-8 in a BasicAuthenticator, you can configure it explicitly, by inserting the following line into the Context configuration file of your web application (usually META-INF/context.xml): <Valve className=""org.apache.catalina.authenticator.BasicAuthenticator"" charset=""UTF-8"" /> If you do so, the BasicAuthenticator will append ""charset=UTF-8"" to the value of WWW-Authenticate header that it sends and will interpret the values sent by clients as UTF-8. See also: Configuration Reference (Tomcat 9):Valves (Authentication),Context (Defining a context).Bug 61280Bug 66174Specifications(RFC 7617)",../data/confluence_exports/TOMCAT/Character-Encoding_103098774.html
Apache Tomcat : Character Encoding I'm having a problem with character encoding in Tomcat 5,"In Tomcat 5 - there have been issues reported with respect to character encoding (usually of the the form ""request.setCharacterEncoding(String) doesn't work""). Odds are, its not a bug. Before filing a bug report, see these bug reports as well as any bug reports linked to these bug reports: 2392925360252312523522666245572434525848",../data/confluence_exports/TOMCAT/Character-Encoding_103098774.html
Apache Tomcat : Windows Preface,This page addresses various issues related to running Tomcat on a Windows platform. Please see theUsefulLinksfor more links related to Windows.,../data/confluence_exports/TOMCAT/Windows_103099101.html
Apache Tomcat : Windows Questions,"Why do I get Out of Environment Space?When I start up tomcat (or when it is running), I get the error java.lang.IllegalMonitorStateException: current thread not ownerCan I turn off case sensitivity?Can I use NTLM authentication?I want to redeploy web applications, how do I prevent resources from getting locked?Can I use UNC paths?Why can't Tomcat see my mapped drive when running as a service?Why aren't access logs showing up in Tomcat on Vista?Why do I get a ""HTTP/1.x 400 No Host matches server name"" error when I change the ""webapps"" folder in Tomcat on Vista?How do I add or customize a Windows Service for Tomcat?What are tomcat9w.exe/tomcat9.exe (or tomcat7w.exe/tomcat7.exe etc..) ?",../data/confluence_exports/TOMCAT/Windows_103099101.html
Apache Tomcat : Windows Why do I get Out of Environment Space?,"Check the Tomcat README, andthis link",../data/confluence_exports/TOMCAT/Windows_103099101.html
"Apache Tomcat : Windows When I start up tomcat (or when it is running), I get the error java.lang.IllegalMonitorStateException: current thread not owner",That weird issue was observed many years ago and now is a history. See theTomcat Bug Report #13723andSun Bug Parade report #4776385for the answer.,../data/confluence_exports/TOMCAT/Windows_103099101.html
Apache Tomcat : Windows Can I turn off case sensitivity?,"It ispossiblein Tomcat 6 and earlier, but not recommended.",../data/confluence_exports/TOMCAT/Windows_103099101.html
Apache Tomcat : Windows Can I use NTLM authentication?,"Yes. Waffle/JNA(obsolete)Tomcat SPNEGO(obsolete)SPNEGO SFJespa(commercial)Tomcat IIS ConnectorSamba JCIFs(obsolete, no NTLMv2)",../data/confluence_exports/TOMCAT/Windows_103099101.html
"Apache Tomcat : Windows I want to redeploy web applications, how do I prevent resources from getting locked?","Most locking issues will occur with JARs from /WEB-INF/lib, and are usually caused by access through URLs. Tomcat has mechanisms to allow avoiding locking. Since Tomcat 5.0, a mechanism exists to prevent locking when accessing resources using the getResource method of the URLClassLoader. Many applications, such as Xerces, do not set the use of caching to false before opening the URL connection to a JAR file, and that causes locking. In Tomcat 5.5, this mechanism is disabled by default (as it has a non negligible influence on startup times, and is often useless), and can be enabled using theantiJARLockingattribute of theContextelement. If getResource call occurs, resources inside the JARs will be extracted to the work directory of the web application. There is an alternative to this since Tomcat 6.0.24: you can configure aJreMemoryLeakPreventionListenerin yourserver.xmland it will set the URL connection caching to be off by default. There is another lock prevention mechanism in Tomcat 5.5 (antiResourceLockingattribute), which will cause the web application files to be copied to the temp folder and run from this location. This has a larger impact on web application startup times, but obviously prevents locking on all resources of the web application. This also allows more flexible management operations as none of the web application resources will be locked, even while the web application is running (as a special note, when making changes to JSPs without reloading the application, the changes have to be duplicated to the path where the web application resources have been copied in the temp folder).",../data/confluence_exports/TOMCAT/Windows_103099101.html
Apache Tomcat : Windows Can I use UNC paths?,Yes. Make sure that the user that Tomcat is running as is able to access the path. This is particularly important when running Tomcat as a service since the local service account willnothave the necessary permissions.,../data/confluence_exports/TOMCAT/Windows_103099101.html
Apache Tomcat : Windows Why can't Tomcat see my mapped drive when running as a service?,The mapped drives are part of a user's profile and they are not used when running as a service. You should be OK with UNC paths.,../data/confluence_exports/TOMCAT/Windows_103099101.html
Apache Tomcat : Windows Why aren't access logs showing up in Tomcat on Vista?,"By default, the Tomcat Windows Service installer attempts to place Tomcat inside the ""Program Files"" folder. Default Vista folder permissions cause various logging functions (though mysteriously not every log function) to fail silently. It is best to change Tomcat's install folder to something like ""C:\Tomcat"". This issue can be hard to spot because by default the access logs are not enabled and the example webapps work just fine.",../data/confluence_exports/TOMCAT/Windows_103099101.html
"Apache Tomcat : Windows Why do I get a ""HTTP/1.x 400 No Host matches server name"" error when I change the ""webapps"" folder in Tomcat on Vista?","By default, the Tomcat Windows Service installer attempts to place Tomcat inside the ""Program Files"" folder. Default Vista folder permissions conflict with the contents of the ""webapps"" folder, can case a blank page to come up when attempting to access the webapp. By using a HTTP Header inspector like ""Live HTTP Headers"" you can see a slightly more descriptive error message. It is best to change Tomcat's install folder to something like ""C:\Tomcat"". This issue can be hard to spot because by default the example webapps work just fine.",../data/confluence_exports/TOMCAT/Windows_103099101.html
Apache Tomcat : Windows How do I add or customize a Windows Service for Tomcat?,"Tomcat uses the Apache Commons Daemon. You can read its documentation athttps://commons.apache.org/proper/commons-daemon/procrun.htmlAs a short example, you can create a new Windows Service with the full version number in its name like this: bin\tomcat6.exe //IS//tomcat6026 --DisplayName ""Apache Tomcat 6.0.26"" See also theservice.batfile that comes in the*-windows-<arch>.zip distributions of Tomcat.",../data/confluence_exports/TOMCAT/Windows_103099101.html
Apache Tomcat : Windows What are tomcat9w.exe/tomcat9.exe (or tomcat7w.exe/tomcat7.exe etc..)?,"Questions on this topic come regularly at various levels. So this is a longish explanation meant basically for real Tomcat/Windows beginners. Apologies in advance for any shortcuts and approximations. You can sort this out by yourself according to your own needs. Java is a programming language designed to be ""compile once, run anywhere"". The idea is that when you compile a java program, the java compiler creates a ""java bytecode"" version of your java program, and this bytecode version can be run by any Java Virtual Machine (a JVM) which runs on any platform (iow under any operating system) where such a JVM has been ported. Microsoft Windows is such a platform, and there exists a JVM which runs on it. Tomcat is a java application, and when it runs on a Windows platform, what really runs as a Windows process is the JVM, which in turn executes the bytecode of the compiled Tomcat application. Because the JVM has to run on many different platforms, it cannot be too specific for each platform. For example under Windows, the JVM is not very good at running as a Windows Service. Windows Services are supposed to respond in specific ways to ""signals"" (or ""messages"") sent by the Windows Services Manager, and the Windows JVM does not really implement the code needed to do that. To make the JVM really capable to respond to such Windows signals/messages, one solution is to run the JVM inside of a ""wrapper"" program which is written in such a way that it does respond properly to Windows signals/messages, passes these signals to the JVM in a way which the JVM understands, and returns appropriate messages to the Windows Services Manager to indicate that the JVM started or stopped properly. The Apache ""procrun"" software project provides such a wrapper program. Its original documentation can be found here:https://commons.apache.org/proper/commons-daemon/procrun.html. But this documentation is more of the ""reference"" type : good for someone who already knows what they are looking for, but not very good as an introduction; and it doesn't explain what tomcat7w.exe (or tomcat6w.exe or tomcat5w.exe) and tomcat7.exe (or tomcat6.exe or tomcat5.exe) really are. From there this FAQ entry. tomcat7.exe is in fact a renamed copy of the ""prunsrv"" program from the procrun project. This is the ""wrapper"" mentioned previously. When you install Tomcat as a Windows Service, what you are really installing is the prunsrv program, renamed as tomcat7.exe. This is the program that Windows knows as being the Tomcat Service, and it will send to this program the messages telling the Tomcat Service to start or stop. In turn, tomcat7.exe will run the JVM, and it will translate for the JVM these Windows messages. And the JVM will run Tomcat. Through this subterfuge, Windows will see a Windows Service application named ""Apache Tomcat"", which runs as a Windows Service and which responds properly to Windows Service messages. So what is tomcat7w.exe then ? It is a renamed version of the procrun ""prunmgr"" program. This is in fact a simple graphical Windows Registry editor, which is able to set and modify specific keys and values in the Registry; and these specific keys/values are the ones which are read by tomcat7.exe (the JVM wrapper program mentioned earlier). When tomcat7.exe launches a JVM, it can pass to this JVM a number of ""command-line options"" (things such as ""-Xmx"", ""-Dxxx"" and so on). To know which command-line options to pass, it reads specific keys/values in the Windows Registry. And these keys/values are the ones which you can set/modify via the tomcat7w.exe program. One more thing: because the tomcat7.exe wrapper program actually ""runs"" the JVM, it must match the type of JVM that it runs, in terms of 32bit/64bit version. If you try to start a 64-bit JVM with a 32-bit tomcat7.exe, it won't work, and vice-versa. This is why there are several versions of the tomcat7.exe program: one 32-bit version and one 64-bit version. The 64-bit version is for the widely used ""AMD64""/""x86-64"" architecture (x64). If you have a 64-bit processor it is likely that you want to use this one. (Some time ago there was another 64-bit version provided as well: the one for the ""Intel Itanium"" architecture ""IA-64"" (i64). It was discontinued). You must install and use the correct one matching the JVM that you are using. The Tomcat Service installer for Windows normally bundles all three versions of the service wrapper and selects one for you automatically, according to the JRE instance that you selected during installation. The ZIP distributions of Tomcat contain only one version of the program, so you have to select the correct distributive to download (*-windows-x86.zip or *-windows-x64.zip).",../data/confluence_exports/TOMCAT/Windows_103099101.html
Apache Tomcat : Clustering Preface,This page discusses cluster- and clustering-related questions. Please make sure to read the Clustering HowTo page in the main Tomcat documentation bundle as well.,../data/confluence_exports/TOMCAT/Clustering_103098820.html
Apache Tomcat : Clustering Questions,"Can I configure a cluster at the Engine level?Show me a simple cluster configuration example.How do I turn on transport logging?How do I use JMX to monitor the cluster?Can I pause the message sending?Can I add more senders (pooled mode)?What happens when I pull the network cable?On my windows laptop without network my cluster doesn't work.The cluster doesn't work under Linux with two nodes on two boxes.I get ""localhost"" rather than ""eth0"" or another interface when using tcpListenAddress=""auto"".",../data/confluence_exports/TOMCAT/Clustering_103098820.html
Apache Tomcat : Clustering Can I configure a cluster at the Engine level?,"Yes, beginning with Tomcat 5.5.10 you can configure clusters at both the Engine and Host levels. This helps support clustering for web hosting companies.",../data/confluence_exports/TOMCAT/Clustering_103098820.html
Apache Tomcat : Clustering Show me a simple cluster configuration example.,"For Tomcat 5.5.10 and later: <Cluster className=""org.apache.catalina.cluster.tcp.SimpleTcpCluster"" defaultMode=""true"" />",../data/confluence_exports/TOMCAT/Clustering_103098820.html
Apache Tomcat : Clustering How do I turn on transport logging?,"(FIXME: The text below needs an update, The logging categories areorg.apache.catalina.haandorg.apache.catalina.tribesnowadays. What else is needed? There no ""clusterLog"" attribute.) Use ""org.apache.catalina.cluster"" as logger category and switch to info, debug or trace as log level.Configure the clusterLog attribute (logging category) to get and send and receive message log.",../data/confluence_exports/TOMCAT/Clustering_103098820.html
Apache Tomcat : Clustering How do I use JMX to monitor the cluster?,"Since Java 5 you can use the jconsole application to look inside the runnnig cluster: please see the JMX configuration section in the Clustering HowTo document. In fastasyncmode replication mode you can get more information with sender attributes doProcessingStats=""true"" and queueDoStats=""true"". Finally, with the new JMX remote Ant task you can access the state and call operations.",../data/confluence_exports/TOMCAT/Clustering_103098820.html
Apache Tomcat : Clustering Can I pause the message sending?,"Yes, the async senders buffer the messages, but make sure the membership ping is active. With fastasyncqueue mode you can limit the max queue size.",../data/confluence_exports/TOMCAT/Clustering_103098820.html
Apache Tomcat : Clustering Can I add more senders (pooled mode)?,"Yes, with sender attribute maxPoolSocketLimit=""40"" you can have more than the default 25 sockets to transfer more parallel messages.",../data/confluence_exports/TOMCAT/Clustering_103098820.html
Apache Tomcat : Clustering What happens when I pull the network cable?,"The other members will remove the instance from the cluster, but when you insert the cable again, the Tomcat instance might have completely flipped out. This is because the OS might start using 100% of the CPU when a multicast message is sent. There has not yet been a good solution for this, I will let you know when I have come up with one. (pero: I test this and I works correct with java 5 and exists when you use the cluster with JDK 1.4.x)",../data/confluence_exports/TOMCAT/Clustering_103098820.html
Apache Tomcat : Clustering On my windows laptop without network my cluster doesn't work.,"The Membership attribute mcastBindAddress=""127.0.0.1"" must be set!",../data/confluence_exports/TOMCAT/Clustering_103098820.html
Apache Tomcat : Clustering The cluster doesn't work under Linux with two nodes on two boxes.,Check the the following: Is your network interface enabled for multicast? ifconfig eth0 MULTICASTExists a multicast route to your network interface? route add -host 228.0.0.4 dev eth0Is your firewall active? Then check that multicast port is on your UDP open list and the receiver TCP port is also for both machines open!,../data/confluence_exports/TOMCAT/Clustering_103098820.html
"Apache Tomcat : Clustering I get ""localhost"" rather than ""eth0"" or another interface when using tcpListenAddress=""auto"".","Change /etc/hosts so that the localhost domain resolves to the actual IP address of the NIC, eth0. Please see Bugzilla for more.",../data/confluence_exports/TOMCAT/Clustering_103098820.html
Apache Tomcat : Tomcat and IIS Howto Permissions,The official documentation lacks a few important bits around file permissions  that must be set if the application pool identity the connector is running under  does not have global permissions to the servers file system:  The directory with your isapi_redirect.dll and associated .properties files  must be at least read only to the application pool identity accountIf you have logging enabled  for the filter then the application pool  identity must have read & write access to the log fileIf you use the registry for the configuration then the application pool  identity must have read access to the keys If you miss any of the above then the filter simply will not work and you  will not receive any form of error messages to tell you it has not  worked.,../data/confluence_exports/TOMCAT/Tomcat-and-IIS-Howto_103100461.html
Apache Tomcat : Tomcat and IIS Howto Windows 2008 / IIS 7 Step by Step,"The following steps detail setting up IIS to function correctly with the  connector, it assumes you have already completed the .properties or registry  configuration:  Allow ISAPI Filter to run:  In the IIS Manager console select the computer nodeOpen the ""ISAPI and CGI"" restrictions panelClick the ""Add"" buttonUnder ""ISAPI or CGI path"" enter the full path to the Connector ISAPI  dllUnder ""Description"" add something meaningful such as ""Tomcat""Check ""Allow extension path to execute""Click ""Ok""  Enable ISAPI Filter for individual  sites:  In the IIS Manager console open the computer node and select the website to  add the ISAPI filter toRight click on the website node and add a new virtual directory pointing to  the directory containing the dll and associated .properties filesOpen the ""ISAPI Filters"" panelClick the ""Add"" buttonUnder ""Filter Name"" add something meaningful such as ""Tomcat""Under ""Executable"" enter the full path to the Connector ISAPI dllClick ""Ok""  If you are using the website for hosting other technologies (such as asp.net)  the order of execution for the filters may have to be changed for everything to  work as expected. In order to do this click ""View Ordered List"" in the ""ISAPI  Filters"" panel for a website and then use the ""Move Up"" and ""Move Down"" buttons  to reorganise the execution order. If you would like asp.net to handle cases  such as 404's then the Tomcat filter should appear BEFORE those for .net. If you  are using specialist authentication filters (such as CASiteMinder) then these  should appear before Tomcat so that requests are correctly authenticated prior  to being received by the connector.",../data/confluence_exports/TOMCAT/Tomcat-and-IIS-Howto_103100461.html
Apache Tomcat : Tomcat and IIS Howto Setup,"Configuring Tomcat and IIS is documented atthe IIS Howto page. The instructions there work fine, but there is asetup programwhich allows you to skip steps 1 through 7 in the ""Configuring the ISAPI Redirector"" section of this page. Also, step 8 can be done using a VBScript file like this:  'Taken from http://msdn.microsoft.com/library/en-us/iissdk/html/8fcd5343-07cb-49e9-a206-0c65a988dcca.asp?frame=true and
' http://msdn.microsoft.com/library/default.asp?url=/library/en-us/iissdk/html/425ff52d-9998-44a9-95dd-b46b2e390db8.asp
' or Google the Microsoft web site for ""Creating Sites and Virtual Directories Using ADSI"" and ""Enabling ISAPI Filters Using ADSI""
' with http://www.google.com/search?q=%22Creating+Sites+and+Virtual+Directories+Using+ADSI%22+site%3Amsdn.microsoft.com
' and http://www.google.com/search?q=%22Enabling+ISAPI+Filters+Using+ADSI%22+site%3Amsdn.microsoft.com

Option Explicit
Dim IIsPath
Dim ConnectorPath

Dim FiltersObj
Dim FilterObj
Dim LoadOrder
Dim Name 'of the filter and the virtual directory
Dim DLLName

'Double-check this path
ConnectorPath = ""C:\Program Files\Apache Software Foundation\Jakarta Isapi Redirector\bin\""

'Configure the first (default) web site on a machine. Change to ""2"" for the second web site, etc.
IIsPath = ""IIS://LocalHost/W3SVC/1/""

'=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
'SHOULDN'T NEED TO CHANGE ANYTHING BELOW THIS LINE
'=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Name = ""jakarta""
DLLName = ""isapi_redirect.dll""

'Configure the filter order (add it at the bottom of the list)
Set FiltersObj = GetObject(IIsPath & ""Filters"")
LoadOrder = FiltersObj.FilterLoadOrder
If LoadOrder <> """" Then
  LoadOrder = LoadOrder & "",""
End If
LoadOrder = LoadOrder & Name
FiltersObj.FilterLoadOrder = LoadOrder
FiltersObj.SetInfo

'Actually create the filter
'ToDo: set priority. This might be possible with the FilterFlags property, but the IIS doc says:
' ""Because this property is internally configured by IIS, you should consider it to be read-only. Do not configure this property.""
Set FilterObj = FiltersObj.Create(""IIsFilter"", Name)
FilterObj.FilterPath = ConnectorPath & DLLName
FilterObj.SetInfo  Save this script to a file with a VBS extension. Open it in Notepad and verify that theConnectorPathvariable is correct for your connector installation. Then open a command prompt and run it with the cscript interpreter:cscript filename.vbs. Restart IIS, and test the connector as mentioned in the Howto (go tohttp://localhost/servlets-examples/).  Corrections on the above script and suggestions on how to get it into the Setup program are welcome.  You still need to do the other steps on theIIS Howto page, such as adding your contexts, etc.   Added bySkyBristolon 4/11/2006  The setup file included here and the script to add the ISAPI filter work just fine in IIS6, but there is one additional step needed. You must also allow access to the filter through the Web Service Extensions part of IIS. To do this graphically, do the following:  1) open the IIS Manager and go to the Web Service Extensions item under the appropriate server 2) Click the ""Add a new Web service extension"" item under ""Tasks"" 3) Give it a name like ""jakarta"" 4) Browse to the isapi_redirect.dll file you installed above as a ""Required File"" 5) Check the box to enable this extension  Added by Tony.Britton on 12/11/2009:  To add the Jakarta WSE programmatically, run the following line at a command prompt:  (Change the File Path/File Name as appropriate)  cscript.exe //NOLOGO iisext.vbs /AddFile ""C:\Program Files\Apache Software Foundation\Jakarta Isapi Redirector\bin\isapi_redirect.dll"" 1 jakarta 1 jakarta   Added byMaiPiuon 12 May 2006  IIS and Tomcat: you found also in Other links see hereUsefulLinks. Little notice: Using the present help, I made working IIS 6.0 , Jk, Tomcat 5.0 on Windows2003 server... (but in link before you found works also with jk2!)   Added byJohnJameson 12 April 2011:  I have used the .net basedBonCodeJK connector rather than the ISAPI redirector to connect IIS6 and IIS7 to Apache Tomcat 6. This worked very easily. There is GUI installer in the distribution package that worked for me. There are also videos on how to do the setup.",../data/confluence_exports/TOMCAT/Tomcat-and-IIS-Howto_103100461.html
Apache Tomcat : Ciphers BIO/NIO/NIO2 with JSSE Results (Default),Java 6Java 7Java 8Java 9Java 10Tomcat 7CBAAATomcat 8N/ABAAATomcat 8.5N/ABAAATomcat 9N/AN/AAAA Note: These results were obtained using the JCE Unlimited Strength Jurisdiction Policy Files Note: The Java 6 results are capped at C because Java 6 does not support TLS 1.1 or 1.2. Note: The Java 7 results are capped at B because Java 7 does not support AEAD ciphers. The equivalent OpenSSL cipher configurations used to obtain the above results are: Java 6HIGH:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!DHEJava 7HIGH:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!kRSA:!DHEJava 8HIGH:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!kRSAJava 9HIGH:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!kRSA Note: kRSA ciphers are not excluded in Java 6 since they are likely to be the only ones left Note: In Java 7 and earlier DHE ciphers use insecure DH keys with no means to configure longer keys which is why DHE ciphers are excluded in those Java versions.,../data/confluence_exports/TOMCAT/Ciphers_118166026.html
Apache Tomcat : Ciphers NIO/NIO2 with JSSE+OpenSSL Results (Default),Java 6Java 7Java 8Java 9Java 10Tomcat 8.5N/AAAAATomcat 9N/AN/AAAA The OpenSSL cipher configuration used wasHIGH:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!kRSA. Up-to-date selection of secure cipher suites in OpenSSL format is available atMozilla wiki.,../data/confluence_exports/TOMCAT/Ciphers_118166026.html
Apache Tomcat : Ciphers APR with OpenSSL Results (Default),Java 6Java 7Java 8Java 9Java 10Tomcat 7AAAAATomcat 8N/AAAAATomcat 8.5N/AAAAATomcat 9N/AN/AAAA The OpenSSL cipher configuration used wasHIGH:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!kRSA. Up-to-date selection of secure cipher suites in OpenSSL format is available atMozilla wiki.,../data/confluence_exports/TOMCAT/Ciphers_118166026.html
Apache Tomcat : Ciphers Environment,"The results above were generated with: Java 6, 64-bit, update 45Java 7, 64-bit, update 80Java 8, 64-bit, update 172Java 9, 9.0.4Apache Tomcat 7.0.88-dev, r1737253.Apache Tomcat 8.0.53-dev, r1737224.Apache Tomcat 8.5.32-dev, r1737241.Apache Tomcat 9.0.9-dev, r1737193.tc-native 1.2.16",../data/confluence_exports/TOMCAT/Ciphers_118166026.html
Apache Tomcat : FDA Validation Preface,"This page discusses using Tomcat in anFDAvalidatedenvironment, i.e. one where21 CFR Part 11regulations apply. Please note that although this page mentions specific companies, we do not explicitly endorse or sell anyone's services. Tomcat and Apache are not-for-profit organizations. This page is also far from a complete listing of vendors and support options. It is meant as a demonstration showing that these options do exist and that running Tomcat in a validated environment is both feasible and reasonable.",../data/confluence_exports/TOMCAT/FDA-Validation_103098885.html
Apache Tomcat : FDA Validation Questions,Can Tomcat be used in a validated environment?Has anyone actually done it?Is Tomcat itself validated?What kind of support is there around validating Tomcat?fully and accuratelyHow do I know I have a validated release? How do I know no one has tampered with the release package?What about security? I'm concerned about attacks.,../data/confluence_exports/TOMCAT/FDA-Validation_103098885.html
Apache Tomcat : FDA Validation Can Tomcat be used in a validated environment?,"Yes. There's nothing in Tomcat's design or implementation that prevent it from being used in a validated environment. The same validation procedures and guidelines that apply to most software packages apply to Tomcat as well. Being an open-source application does not preclude Tomcat validation. In fact, it helps in at least one key aspect: the source code itself can be audited, as can the commit and change logs for the software.",../data/confluence_exports/TOMCAT/FDA-Validation_103098885.html
Apache Tomcat : FDA Validation Has anyone actually done it?,"Yes. As shown inthis user mailing list archive, Merck and other large companies are using Tomcat in a validated environment. In addition, there is at least one application provider (Interchange Digital) whose application runs on Tomcat that has deployed said package in numerous pharma data centers.",../data/confluence_exports/TOMCAT/FDA-Validation_103098885.html
Apache Tomcat : FDA Validation Is Tomcat itself validated?,"Yes. Tomcat itself is validated to the extent it can be. TomcatimplementsseveralJava EE Specifications, most important of them are the Servlet Specification and the Java Server Pages (JSP) Specification. Each of these specifications has a Technology Compatibility Kit (TCK), which is a collection of tests to certify a given product meets the Specification fully and accurately. TheApache Software Foundationis licensed to run these TCKs. They are run against every major Tomcat release.No Tomcat release is pronounced stable unless it has passed both of these TCKs with 100% compliance. Therefore, every stable Tomcat release is validated to the extent of Tomcat's core functionality. Furthermore, any company of individual mayapplyto obtains and use these TCKs themselves. That way, you can re-validated Tomcat including any custom patches you have implemented. However, we cannot validate your application's use of Tomcat. You're on your own there.",../data/confluence_exports/TOMCAT/FDA-Validation_103098885.html
Apache Tomcat : FDA Validation What kind of support is there around validating Tomcat?,"Several kinds. They include: There are numerous smallervendorsand several large ones, including IBM, HP, Sun, and Novell, who offer Tomcat consulting and support services, including application auditing, environment assessments, and risk analysis.There are numerous vendors in addition to the above consultants, likeSpringSource(formerly Covalent) andJBoss, who offer 24/7/365 enterprise-level support for Tomcat.The Tomcatmailing listsare extremely active and contain members of many of the above organizations, including contractors available for hire.",../data/confluence_exports/TOMCAT/FDA-Validation_103098885.html
Apache Tomcat : FDA Validation How do I know I have a validated release? How do I know no one has tampered with the release package?,"All Tomcat releases are signed using the Release Manager'sPGPkey. The key is also available in the KEYS file that ships with every Tomcat release. The same KEYS file is also available in the Tomcat Git repository (here). The PGP signatures are available on all the Tomcat download pages, and can (and should!) be used to verify the release really is the signed distribution. As for tampering: every Tomcat release is also digested using the SHA-512 algorithm as specified inRFC6234. The SHA-512 digest is included in all the download pages. Users runsha512sumon their local machine to verify that the digest of what they downloaded is the same as that published in the Apache download pages. That way, users are assured the distribution has not been modified since the Release Manager signed it.",../data/confluence_exports/TOMCAT/FDA-Validation_103098885.html
Apache Tomcat : FDA Validation What about security? I'm concerned about attacks.,There's no need to be. See thesecurity pageof this FAQ for more information.,../data/confluence_exports/TOMCAT/FDA-Validation_103098885.html
Apache Tomcat : Building the Tomcat Native Connector binaries for Windows Building,"While tcnative itself needs to be builtlast, we unpack it first because there are some patches in the tcnative source distribution that will need to be applied to both APR and OpenSSL. Obtain the tcnative source from one of: the win32-src.zip source bundle for the version you wish to build;https://svn.apache.org/repos/asf/tomcat/native/ E.g.: To build the latest 1.2.x development build from trunk c:cd \svn co https://svn.apache.org/repos/asf/tomcat/native/trunk/ tomcat-native-1.2.xcd tomcat-native-1.2.x\native\srclib\apr  Unpack APR 1.7.0 source distribution in this directory (C:\tomcat-native-1.2.x\native\srclib\apr). Apply the apr-enable-ipv6.patch. Note that the patch will apply but depending on exactly which revision you are working with you may need to skip the first part of the patch and an offset will probably be required. git apply apr-enable-ipv6.patchgit apply win-ipv6.patchThen build c:\cmsc\setenv.bat /x86nmake -f NMAKEmakefile BUILD_CPU=x86 APR_DECLARE_STATIC=1c:\cmsc\setenv.bat /x64nmake -f NMAKEmakefile BUILD_CPU=x64 APR_DECLARE_STATIC=1cd ..\openssl",../data/confluence_exports/TOMCAT/Building-the-Tomcat-Native-Connector-binaries-for-Windows_61320769.html
Apache Tomcat : Building the Tomcat Native Connector binaries for Windows OpenSSL 1.1.1 and later,Unpack the OpenSSL 1.1.1k source distribution in this directory (C:\tomcat-native-1.2.x\native\srclib\openssl). Apply openssl-msvcrt-1.1.1.patch. Note that you may need to skip and/or use an offset to get the patch to apply. c:\cmsc\setenv.bat /x86perl Configure no-shared VC-WIN32nmakemkdir out32-x86copy libssl.lib out32-x86\copy libcrypto.lib out32-x86\copy apps\openssl.exe out32-x86\nmake cleanc:\cmsc\setenv.bat /x64perl Configure no-shared VC-WIN64Anmakemkdir out32-x64copy libssl.lib out32-x64\copy libcrypto.lib out32-x64\copy apps\openssl.exe out32-x64\,../data/confluence_exports/TOMCAT/Building-the-Tomcat-Native-Connector-binaries-for-Windows_61320769.html
Apache Tomcat : Building the Tomcat Native Connector binaries for Windows Tomcat Native,"Keeping the various libraries in versioned directories saves having to rebuild them next time if the version remains unchanged. cd ..set OPENSSL_VER=1.1.1kset APR_VER=1.7.0 mkdir \deps-x86\apr-%APR_VER%\includemkdir \deps-x86\apr-%APR_VER%\libmkdir \deps-x86\openssl-%OPENSSL_VER%\includemkdir \deps-x86\openssl-%OPENSSL_VER%\libxcopy /E \deps-x86\apr-%APR_VER% \deps-x64\apr-%APR_VER%\xcopy /E \deps-x86\openssl-%OPENSSL_VER% \deps-x64\openssl-%OPENSSL_VER%\ xcopy /E apr\include \deps-x86\apr-%APR_VER%\include\xcopy /E apr\include \deps-x64\apr-%APR_VER%\include\copy apr\WIN7_X86_LIB_RELEASE\apr-1.lib \deps-x86\apr-%APR_VER%\libcopy apr\WIN7_X64_LIB_RELEASE\apr-1.lib \deps-x64\apr-%APR_VER%\lib xcopy /E openssl\include\openssl \deps-x86\openssl-%OPENSSL_VER%\include\openssl\
 xcopy /E openssl\include\openssl \deps-x64\openssl-%OPENSSL_VER%\include\openssl\ copy openssl\out32-x86\*.lib \deps-x86\openssl-%OPENSSL_VER%\lib\copy openssl\out32-x64\*.lib \deps-x64\openssl-%OPENSSL_VER%\lib\ copy openssl\out32-x86\openssl.exe \deps-x86\openssl-%OPENSSL_VER%\copy openssl\out32-x64\openssl.exe \deps-x64\openssl-%OPENSSL_VER%\ cd ..set JAVA_HOME=\java\adopt-8.0.242.09-x64 c:\cmsc\setenv.bat /x86nmake -f NMAKEMakefile WITH_APR=C:\deps-x86\apr-%APR_VER% WITH_OPENSSL=C:\deps-x86\openssl-%OPENSSL_VER% APR_DECLARE_STATIC=1 OPENSSL_NEW_LIBS=1 ENABLE_OCSP=1move WIN7_X86_DLL_RELEASE WIN7_X86_OCSP_DLL_RELEASEnmake -f NMAKEMakefile WITH_APR=C:\deps-x86\apr-%APR_VER% WITH_OPENSSL=C:\deps-x86\openssl-%OPENSSL_VER% APR_DECLARE_STATIC=1 OPENSSL_NEW_LIBS=1 c:\cmsc\setenv.bat /x64
 nmake -f NMAKEMakefile WITH_APR=C:\deps-x64\apr-%APR_VER% WITH_OPENSSL=C:\deps-x64\openssl-%OPENSSL_VER% APR_DECLARE_STATIC=1 OPENSSL_NEW_LIBS=1 ENABLE_OCSP=1
 move WIN7_X64_DLL_RELEASE WIN7_X64_OCSP_DLL_RELEASE
 nmake -f NMAKEMakefile WITH_APR=C:\deps-x64\apr-%APR_VER% WITH_OPENSSL=C:\deps-x64\openssl-%OPENSSL_VER% APR_DECLARE_STATIC=1 OPENSSL_NEW_LIBS=1  Tomcat Native Connector DLLs may then be found in C:\tomcat-native-1.2.x\native\WIN7_*_[OCSP_]DLL_RELEASE Construct the binary distributions set VER=1.2.24 mkdir tomcat-native-%VER%-openssl-%OPENSSL_VER%-win32-bincopy LICENSE.bin.win tomcat-native-%VER%-openssl-%OPENSSL_VER%-win32-bin\LICENSEcopy NOTICE.bin.win tomcat-native-%VER%-openssl-%OPENSSL_VER%-win32-bin\NOTICEcopy ..\README.txt tomcat-native-%VER%-openssl-%OPENSSL_VER%-win32-bin\copy srclib\VERSIONS tomcat-native-%VER%-openssl-%OPENSSL_VER%-win32-bin\ mkdir tomcat-native-%VER%-openssl-%OPENSSL_VER%-win32-bin\binmkdir tomcat-native-%VER%-openssl-%OPENSSL_VER%-win32-bin\bin\x64copy C:\deps-x86\openssl-%OPENSSL_VER%\openssl.exe tomcat-native-%VER%-openssl-%OPENSSL_VER%-win32-bin\binxcopy /E tomcat-native-%VER%-openssl-%OPENSSL_VER%-win32-bin tomcat-native-%VER%-openssl-%OPENSSL_VER%-ocsp-win32-bin\copy WIN7_X86_DLL_RELEASE\tcnative-1.dll tomcat-native-%VER%-openssl-%OPENSSL_VER%-win32-bin\bincopy WIN7_X86_OCSP_DLL_RELEASE\tcnative-1.dll tomcat-native-%VER%-openssl-%OPENSSL_VER%-ocsp-win32-bin\bincopy WIN7_X86_DLL_RELEASE\tcnative-1-src.pdb tomcat-native-%VER%-openssl-%OPENSSL_VER%-win32-bin\bincopy WIN7_X86_OCSP_DLL_RELEASE\tcnative-1-src.pdb tomcat-native-%VER%-openssl-%OPENSSL_VER%-ocsp-win32-bin\bincopy WIN7_X64_DLL_RELEASE\tcnative-1.dll tomcat-native-%VER%-openssl-%OPENSSL_VER%-win32-bin\bin\x64copy WIN7_X64_OCSP_DLL_RELEASE\tcnative-1.dll tomcat-native-%VER%-openssl-%OPENSSL_VER%-ocsp-win32-bin\bin\x64copy WIN7_X64_DLL_RELEASE\tcnative-1-src.pdb tomcat-native-%VER%-openssl-%OPENSSL_VER%-win32-bin\bin\x64copy WIN7_X64_OCSP_DLL_RELEASE\tcnative-1-src.pdb tomcat-native-%VER%-openssl-%OPENSSL_VER%-ocsp-win32-bin\bin\x64set PATH=%PATH%;%JAVA_HOME%\bincd tomcat-native-%VER%-openssl-%OPENSSL_VER%-win32-binjar -cMf ..\tomcat-native-%VER%-openssl-%OPENSSL_VER%-win32-bin.zip *cd ..\tomcat-native-%VER%-openssl-%OPENSSL_VER%-ocsp-win32-binjar -cMf ..\tomcat-native-%VER%-openssl-%OPENSSL_VER%-ocsp-win32-bin.zip *  The Windows binary distributions may then be found in C:\tomcat-native-1.2.x\native\ These need to be signed and hashed before uploading for the release vote. Beware of typos in the name and contents of hash files for OCSP binaries (""ocsp"" vs ""oscp""). Such typos happened. A correct example: tomcat-native-1.1.32-ocsp-win32-bin.zip.md5 0b0e1e4c77b9b7051fc2c751b70d2880 *tomcat-native-1.1.32-ocsp-win32-bin.zip tomcat-native-1.1.32-ocsp-win32-bin.zip.sha1 b47f96dd3153d002a529e881b6b8f524cd6e321c *tomcat-native-1.1.32-ocsp-win32-bin.zip",../data/confluence_exports/TOMCAT/Building-the-Tomcat-Native-Connector-binaries-for-Windows_61320769.html
Apache Tomcat : Tomcat User mailing list Preface,"The Tomcat user mailing list is for all your questions about how to use Tomcat. It is a high volume list! For those who can't handle that kind of traffic, you can also get it in digest form. If you post to tomcat-user, use good etiquette, ask good questions, and use Best Practices, and also please read the docs, faqs, readme, and search the archives before asking a question. Here are some links about how to use mailing lists and standard etiquette in using mailing lists. These links are not Tomcat specific! Please do not contact the author of these documents with questions. Netiquette Guidelines (RFC 1855)How To Ask Questions The Smart Way",../data/confluence_exports/TOMCAT/Tomcat-User-mailing-list_103099074.html
Apache Tomcat : Tomcat User mailing list Questions,How do I subscribe?Why won't people answer my question?How do I unsubscribe?Should I post to users or dev?,../data/confluence_exports/TOMCAT/Tomcat-User-mailing-list_103099074.html
Apache Tomcat : Tomcat User mailing list How do I subscribe?,"See themailing list page. If you are unable to subscribe, it could be due to your e-mail system mangling your e-mail address. If you send an e-mail to the list owner to subscribe you to the list, be prepared to wait until the owner has an opportunity to add you to the list.",../data/confluence_exports/TOMCAT/Tomcat-User-mailing-list_103099074.html
Apache Tomcat : Tomcat User mailing list Why won't people answer my question?,"It could be one of any of the following answers: No one knows the answer.Everyone is ignoring you.The person who knows the answer hasn't gotten around to it yet.The person who knows the answer hasn't gotten around to it yet. (So please don't post the same question multiple times) In other words - you get what you pay for. If you phrase you question intelligently and not make your question annoying, your quality of support will probably better than any commercial help support. In particular, some readers on the tomcat-user list have been known to automatically ignore a message which meets any of the following criteria: (You have been warned) ""help"" in the subject line (we know you need help by writing in the first place)plz or please in the subject line. We already know you need help.ALL CAPS IN THE SUBJECT LINE. (Sorry for anyone emailing from a mainframe)Do not place !! (more than 2!) in the subject line. The number of exclamation points has no relation to anyone's interest in answering the question. In fact - it has a higher chance at being ignored.Your e-mail has a return receipt. You don't need to know I got the message.Select posters who have consistently displayed no desire to research before posting a question.",../data/confluence_exports/TOMCAT/Tomcat-User-mailing-list_103099074.html
Apache Tomcat : Tomcat User mailing list How do I unsubscribe?,"See themailing list page. If you are unable to unsubscribe, it could be due to your e-mail system mangling your e-mail address or you have subscribed under an alias. In that case, maybethis threadwill help. If you send an e-mail to the list owner to remove you from the list, be prepared to continue receiving more e-mails from the lists until the owner has an opportunity to remove you from the list.",../data/confluence_exports/TOMCAT/Tomcat-User-mailing-list_103099074.html
Apache Tomcat : Tomcat User mailing list Should I post to users or dev?,"tomcat-user is for user based questions for tomcat. That means anyone who is developing any type of software to be used with tomcat. tomcat-dev is the forum to discuss changes to the Tomcat's source code. If your question is not related to changing the source code as maintained by the tomcat committers, then it does not belong here, it belongs on the users list. If you wish to extend tomcat using your own Valve, Realm, or anything else ... those discussions belong in tomcat-user. You might be using Tomcat's internal API but unless your changes are going back into the official source tree, the question belongs on the user list.",../data/confluence_exports/TOMCAT/Tomcat-User-mailing-list_103099074.html
Apache Tomcat : Monitoring Heap and other Memory Information,"You will certainly want to inspect your JVM's memory usage. Here are some JMX beans and attributes that can be used to do so. JMX Bean:java.lang:type=MemoryAttribute:HeapMemoryUsage The attribute value is ajavax.management.openmbean.CompositeDatawhich contains 4 keys: committed, init, max, and used. The 'used' key is probably the most useful (or a combination of 'used' / 'max' to get a memory-usage metric as a ratio). JMX Bean:java.lang:type=MemoryPool,name=CMS Perm GenAttribute:Usage Similar to theHeapMemoryUsageMXBean described above, this one will give you information about the ""PermGen"" heap generation. Depending upon your garbage collection and other memory settings, you might have different MXBeans underjava.lang:type=MemoryPoolwith different names. You should inspect each one to determine if they would be useful for you to inspect.",../data/confluence_exports/TOMCAT/Monitoring_103099000.html
Apache Tomcat : Monitoring Tomcat Information,"Version Warning These JMX bean names are accurate for the current version of Tomcat 7 (7.0.37 at the time of this writing). If you are using a different version of Tomcat, you may have to adjust the names of the beans identified on this page.",../data/confluence_exports/TOMCAT/Monitoring_103099000.html
Apache Tomcat : Monitoring Thread Usage,"JMX Bean:Catalina:type=Executor,name=[executor name]Attributes:poolSize,activeCount This is the number of threads currently in the executor's thread pool. Obviously, this is only useful if you are using an <Executor> (which youareusing, of course, right?).",../data/confluence_exports/TOMCAT/Monitoring_103099000.html
Apache Tomcat : Monitoring Not using an Executor,"JMX Bean:Catalina: type=ThreadPool,name=""[depends]""Attributes:maxThreads,connectionCount This information is largely useless in Tomcat 7, as an Executor is always used and the data can be found there, while theThreadPoolhas only initial configuration information: the real-time data is available from the Executor's MBean.",../data/confluence_exports/TOMCAT/Monitoring_103099000.html
Apache Tomcat : Monitoring Request Throughput,"JMX Bean:Catalina:type=GlobalRequestProcessor,name=""[depends]""Attributes:bytesSent,bytesReceived,errorCount,maxTime,requestCountOperations:resetCounters",../data/confluence_exports/TOMCAT/Monitoring_103099000.html
Apache Tomcat : Monitoring Sessions,"JMX Bean:Catalina:type=Manager,context=[context name],host=[hostname]Attributes:activeSessions,sessionCounter,expiredSessions",../data/confluence_exports/TOMCAT/Monitoring_103099000.html
Apache Tomcat : Monitoring JNDI DataSource,"JMX Bean:Catalina:type=DataSource,context=[context name],host=[hostname],class=javax.sql.DataSource,name=""[JNDI name]""Attributes:numActive,numIdle  Another way to watch a Tomcat application is to use an external monitoring tool. MoSKito, is an open source, multi-purpose, non-invasive, interval-based monitoring system kit that collects, stores and provides instant analysis of a Tomcat application’s performance and behavior data. JavaMelodycan monitor your JavaEE/Tomcat application from dev to production. It is open-source and easy: get thefirst viewof your application inabout 2 minutesfrom now. Other plug-in-based monitoring software like Nagios or Icinga may need some help interacting with Tomcat's JMXProxyServlet. tools/check_jmxproxy.pl is a Perl script that can be used with these tools to monitor Tomcat via the JMXProxyServlet.  It is possible to write custom MBeans that expose your Tomcat application internals through JMX. User-defined MBeans need to be registered with Tomcat MBean server. Once registered, user can access MBean and observe user-defined attributes and call user-defined operations, e.g. JMX Bean:Counter:type=CounterManagerAttributes:currentTime,currentCountOperations:resetCounter,resetCounter(int) Example Application Exposing Internals Using JMX",../data/confluence_exports/TOMCAT/Monitoring_103099000.html
Apache Tomcat : TomcatOnOpenVMS Running Tomcat on OpenVMS,"HP offers an Apache based web server named ""Secure Web Server"" (SWS). It includes an Apache module CSWS_JAVA (Tomcat). The current version of this module is CSWS_JAVA Version 7.0-29 for OpenVMS Integrity Server (based on Apache Tomcat 7.0.29).  At the time of writing, it is unknown to me whether HP plans to create newer versions of Tomcat for SWS.  These notes show how to install and run Tomcat on OpenVMS, independent of HP's Secure Web Server.  I hope these notes are of use to someone out there.",../data/confluence_exports/TOMCAT/TomcatOnOpenVMS_103100599.html
Apache Tomcat : TomcatOnOpenVMS THE ENVIRONMENT,"OpenVMS for Integrity Servers Version 8.3 is the minimum version for which this should work. Newer versions are recommended, as there are 8.3-1H1 or 8.4 from HP or 8.4-1H1 from VSI.  Java 6 (at the time of writing there is no newer Java version available from HP or VSI), the HP kit is named J2SE™ Development Kit (JDK) 6.0-4, which is based on Java SE 6 Update 37.  Patches, aka ECOs, for OpenVMS as listed inhttp://h18012.www1.hp.com/java/download/ivms/1.6.0/jdk6.0_patches.html.  Some disk space (~20MB is the minimum for the installation) on an ODS5 disk.  A recent archive tool for zip archives or compressed tar files.  Scripts, aka VMS DCL command procedures, as shown below.",../data/confluence_exports/TOMCAT/TomcatOnOpenVMS_103100599.html
Apache Tomcat : TomcatOnOpenVMS THE INSTALLATION,"Ensure that Java 6 is installed on OpenVMS and that at least all the ECOs as described above are installed. It is recommended to have the operating system patched to the current level. At least the C-run-time environment should have the current patch level.  Step 1: Download Tomcat  Download Tomcat from the Tomcat home page, and get the binary version, either the zip or tar.gz file. I downloadedapache-tomcat-7.0.64.tar.gz.  Step 2: Extract Tomcat archive  Extract the files into a directory on an ODS5 disk. Verify that your unzip or tar utility creates case sensitive file names. The result of extracting will be a sub-directoryapache-tomcat-7.0.64, which from the VMS command line will be shown asapache-tomcat-7^.0^.64.DIR;1(For convenience you can rename the directory file, to avoid the escape character'^', for example toapache-tomcat-7064.DIR;1.)  It is recommended to use the directory filename as is and to set the VMS process' parse style for commands to accept ODS5 file specifications:$ set process/parse_style=extended/case_lookup=blind.                                                                             It is also recommended to make the sub-directory files deletable:$ set protection=o:rwed [.apache-tomcat-7^.0^.64...]*.dir                                                                             Step 2: Add the OpenVMS specific scripts  Extract the scripts shown below into the appropriate directories.  Step 3: Test the installation                                                                             Run the version script:$ @[.apache-tomcat-7^.0^.64.bin]version. The result should look like:                                                                             Server version: Apache Tomcat/7.0.64
Server built:   Aug 19 2015 17:18:06 UTC
Server number:  7.0.64.0
OS Name:        OpenVMS
OS Version:     V8.4
Architecture:   ia64
JVM Version:    1.6.0-6
JVM Vendor:     Hewlett-Packard Company
$                                                                             Start Tomcat:@[.apache-tomcat-7^.0^.64.bin]startup.                                                                             The supplied startup script runs Tomcat in a spawned OpenVMS sub-process of your current interactive process. That is, there will be some asynchronous output shown in your terminal which is produced by VMS and/or Tomcat.  However, you can enter any DCL command while Tomcat is running - including the command to stop it.  The output may look like:  %DCL-S-SPAWNED, process BECKER_39053 spawned
$ 
Oct 16, 2015 2:56:17 PM org.apache.catalina.startup.VersionLoggerListener log
INFO: Server version:        Apache Tomcat/7.0.64
Oct 16, 2015 2:56:17 PM org.apache.catalina.startup.VersionLoggerListener log
INFO: Server built:          Aug 19 2015 17:18:06 UTC
Oct 16, 2015 2:56:17 PM org.apache.catalina.startup.VersionLoggerListener log
INFO: Server number:         7.0.64.0  
...
Oct 16, 2015 2:56:26 PM org.apache.catalina.startup.Catalina start
INFO: Server startup in 8339 ms  Now open a browser window, and enterhttp://YOUR_VMS_NODE:8080- the default Tomcat page should open.                                                                             Stop Tomcat:@[.apache-tomcat-7^.0^.64.bin]shutdown.                                                                             Similar to the startup script, the supplied shutdown script runs in a spawned OpenVMS sub-process. That is, there will be some asynchronous output shown in your terminal which is produced by VMS and/or Tomcat.  However you can enter any DCL command while Tomcat is shutting down.  The output may look like:  %DCL-S-SPAWNED, process BECKER_18865 spawned
$ 
Oct 16, 2015 3:00:50 PM org.apache.catalina.core.StandardServer await
INFO: A valid shutdown command was received via the shutdown port. Stopping the Server instance.
...  Step 4: After the installation  Now you are ready to deploy your web application and configure Tomcat for your needs.  Whether you remove specific files, which are not required for OpenVMS is up to you as well as removing the default Tomcat web application.",../data/confluence_exports/TOMCAT/TomcatOnOpenVMS_103100599.html
Apache Tomcat : TomcatOnOpenVMS OpenVMS SCRIPTS,"Save this file ascatalina.cominto the bin directory of your installation.  Contents of the OpenVMS specific catalina server script  $! OpenVMS Control Script for the CATALINA Server
$! catalina.com, 8-Apr-2014, hb
$
$ default = f$environment(""default"")
$ this_file = f$environment(""procedure"")
$ on control_y then -
        $ goto common_exit
$ on error then -
        $ goto common_exit
$ set def 'f$parse(this_file,,,""device"")''f$parse(this_file,,,""directory"")'
$ set def [-]
$ set protection=(SYSTEM:RWED,OWNER:RWED,GROUP:RE,WORLD)/default
$ config_setup = f$search(""[.bin]java$config_setup.com"")
$ if config_setup .eqs. """"
$ then
$     write sys$output ""%version-e-fnf, java$config_setup.com not found.""
$     goto common_exit
$ endif
$ if f$trnlnm(""JAVA$CANCEL_CURRENT"") .nes. """"
$ then
$     ! just avoid the %SYSTEM-F-NOLOGNAM from @JAVA$CANCEL_CURRENT
$     deassign := deassign/nolog
$     @JAVA$CANCEL_CURRENT
$ endif
$ @sys$startup:java$60_setup
$ define := define/nolog
$ @'config_setup
$ LOGGING_CONFIG=""""""-Djava.util.logging.config.file=./conf/logging.properties""""""
$ options=""""
$ options= options +-
        """"""-Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager"""" ""
$ options= options +-
        """"""-Djava.util.logging.config.file=./conf/logging.properties"""" ""
$ libs=""./bin/bootstrap.jar:"" +-
        ""./bin/tomcat-juli.jar""
$ define/job/nolog classpath ""''libs'""
$ spawn/nowait/input=nl: java 'LOGGING_CONFIG 'options -
        ""-Dcatalina.base=./"" ""-Dcatalina.home=./"" -
        ""-Djava.io.tmpdir=./temp"" -
        ""org.apache.catalina.startup.Bootstrap"" 'p1'
$
$ common_exit:
$ set default 'default  Save this file asstartup.cominto the bin directory of your installation.  Contents of the OpenVMS specific startup script  $! OpenVMS Start Script for the CATALINA Server
$! startup.com, 17-Jun-2013, hb
$
$ this_file = f$environment(""procedure"")
$ parse = f$getjpi(0,""parse_style_perm"")
$ case = f$getjpi(0,""case_lookup_perm"")
$ on control_y then -
              $ goto common_exit
$ on error then -
              $ goto common_exit
$ set process/parse=extended/case=blind
$ @'f$parse(this_file,,,""device"")''f$parse(this_file,,,""directory"")'catalina ""start""
$ common_exit:
$ set process/parse='parse/case='case  Save this file asshutdown.cominto the bin directory of your installation.  Contents of the OpenVMS specific shutdown script  $! OpenVMS Stop Script for the CATALINA Server
$! shutdown.com, 17-Jun-2013, hb
$
$ this_file = f$environment(""procedure"")
$ parse = f$getjpi(0,""parse_style_perm"")
$ case = f$getjpi(0,""case_lookup_perm"")
$ on control_y then -
              $ goto common_exit
$ on error then -
              $ goto common_exit
$ set process/parse=extended/case=blind
$ @'f$parse(this_file,,,""device"")''f$parse(this_file,,,""directory"")'catalina ""stop""
$ common_exit:
$ set process/parse='parse/case='case  Save this file asversion.cominto the bin directory of your installation.  Contents of the OpenVMS specific version script  $! OpenVMS Version Script for the CATALINA Server
$! version.com, 8-Apr-2014, hb
$
$ default = f$environment(""default"")
$ this_file = f$environment(""procedure"")
$ parse = f$getjpi(0,""parse_style_perm"")
$ case = f$getjpi(0,""case_lookup_perm"")
$ on control_y then -
        $ goto common_exit
$ on error then -
        $ goto common_exit
$ set process/parse=extended/case=blind
$ set def 'f$parse(this_file,,,""device"")''f$parse(this_file,,,""directory"")'
$ set def [-]
$ config_setup = f$search(""[.bin]java$config_setup.com"")
$ if config_setup .eqs. """"
$ then
$     write sys$output ""%version-e-fnf, java$config_setup.com not found.""
$     goto common_exit
$ endif
$ if f$trnlnm(""JAVA$CANCEL_CURRENT"") .nes. """"
$ then
$     ! just avoid the %SYSTEM-F-NOLOGNAM from @JAVA$CANCEL_CURRENT
$     deassign := deassign/nolog
$     @JAVA$CANCEL_CURRENT
$ endif
$ @sys$startup:java$60_setup
$ define := define/nolog
$ @'config_setup
$ java -classpath ./lib/catalina.jar org.apache.catalina.util.ServerInfo
$ common_exit:
$ set default 'default
$ set process/parse='parse/case='case  Save this file asjava$config_setup.cominto the bin directory of your installation.  Contents of the OpenVMS specific Java setup script  $! JAVA$CONFIG_SETUP.COM
$!
$! --NOTE:
$! --       THIS FILE WAS ORIGINALLY GENERATED BY
$! --       SYS$COMMON:[JAVA$60.COM]JAVA$CONFIG_WIZARD.COM;1
$! --       ON  7-MAR-2013 14:41:12.27
$! --       IT HAS BEEN MANUALLY EDITTED SINCE THEN.
$! --
$! --       For more complete description of the logicals shown,
$! --       please read the release notes.
$!
$! The following are useful in all environments.
$!
$ SET PROCESS/PARSE_STYLE=EXTENDED
$ DEFINE/JOB DECC$ARGV_PARSE_STYLE TRUE
$ DEFINE/JOB DECC$EFS_CASE_PRESERVE TRUE
$ DEFINE/JOB JAVA$DAEMONIZE_MAIN_THREAD TRUE
$ DEFINE/JOB JAVA$DISABLE_CMDFILE_WHITESPACE_PARSING TRUE
$ DEFINE/JOB DECC$ENABLE_GETENV_CACHE TRUE
$ DEFINE/JOB DECC$FILE_PERMISSION_UNIX TRUE
$!
$! Omitting logicals useful in debugging environment.
$!
$! Omitting logicals useful in mimicing UNIX file system behavior
$!
$! Omitting logicals useful in heavy child processes { runtime.exec() } behavior
$!
$! Logicals useful for supporting file sharing:
$!
$ DEFINE/JOB DECC$FILE_SHARING TRUE  ! ask for C-RTL support for file sharing
$!
$! Omitting logicals useful for bypassing secondary stat() call.
$!
$! Logicals useful to set up caching of file/directory information
$!
$ DEFINE/JOB JAVA$CACHING_INTERVAL 60  ! to disable, undefine logical or set value to 0
$ DEFINE/JOB JAVA$CACHING_DIRECTORY TRUE ! enable special case -- don't look in directory if directory known to be absent
$!
$! Logicals useful for supporting ODS-5 operation only.
$! 
$! NOTE:
$!    The value of JAVA$FILENAME_CONTROLS generated below is different than the one set in JAVA$60_SETUP.COM
$!    To make this new value be in effect, be sure to run JAVA$CONFIG_SETUP.COM after you run JAVA$60_SETUP.COM.  
$ DEFINE/JOB DECC$EFS_CHARSET  TRUE  ! 
$ DEFINE/JOB DECC$EFS_CASE_SPECIAL  FALSE  ! 
$ DEFINE/JOB JAVA$FILENAME_CONTROLS 8 ! only enable the mixed UNIX/VMS filespec mapping
$ DEFINE/JOB JAVA$READDIR_CASE_DISABLE  TRUE  ! turns off auto. class/source case-sensitive name extraction
$!
$!    See: SYS$COMMON:[JAVA$60.COM]JAVA$FILENAME_CONTROLS.COM for explanation of bits enabled above.
$!
$! Omitting logicals useful supporting PC-style keyboard on workstation.
$!   CategoryFAQ",../data/confluence_exports/TOMCAT/TomcatOnOpenVMS_103100599.html
Apache Tomcat : Git migration Resolved Issues,"Review board. Tomcat gas been removed from (technically hidden in) the ASF ReviewBoard instanceConfig diffs in migration guide. Use gitweb. e.g.:https://gitbox.apache.org/repos/asf?p=camel.git;a=blobdiff;f=pom.xml;hb=camel-2.20.1;hpb=camel-2.19.0Branch names. master, tc8.5, tc8.0, tc7.0 etcPRs. Those against apache/tomcat will remain. Currently no other open PRs. Any opened between now and migration will be closed and asked to be made against apache/tomcat.Merge strategy. Commit to master then cherry-pick to branches for older versions as required.CI Systems. Leave them pointing at svn. Migrate to git. Once we are happy with the git repo update the CI systems to use it.Java source code for tomcat-native. Pull in the source code from a specific tag/hash as part fo the build process.Mail format. See recent tomcat-training emails on dev@ for examples. For further tweaks, infra have indicated patches tohttps://github.com/apache/infrastructure-puppet/blob/deployment/modules/gitbox/files/asfgit/git_multimail.pyare welcome.Make old github repos read-only. Ensure information from PRs is available on dev@ list, in BZ or similar. Copy across anything missing. Then delete old repos.Developer process. git worktreehttps://spin.atomicobject.com/2016/06/26/parallelize-development-git-worktrees/https://blog.github.com/2015-07-29-git-2-5-including-multiple-worktrees-and-triangular-workflows/Documentation updateUpdate post migration and release ASAPWebsite updateUpdate post migrationRename to source.htmlEdit svn.html to link to source.html",../data/confluence_exports/TOMCAT/Git-migration_74690230.html
Apache Tomcat : HowTo FasterStartUp General,"Before we continue to specific tips and tricks, the general advice is that if Tomcat hangs or is not responsive, you have to perform diagnostics. That is totake several thread dumpsto see what Tomcat is really doing. SeeTroubleshooting and Diagnosticspage for details.",../data/confluence_exports/TOMCAT/HowTo-FasterStartUp_103099266.html
Apache Tomcat : HowTo FasterStartUp JAR scanning,"TheServlet 3.0 specification(chapter 8) introduced support for several ""plugability features"". Those exist to simplify a structure of a web application and to simplify plugging of additional frameworks. Unfortunately, these features require scanning of JAR and class files, which may take noticeable time. Conformance to the specification requires that the scanning were performed by default, but you can configure your own web application in several ways to avoid it (see below). It is also possible to configure which JARs Tomcat should skip.  For further talk, the features that require scanning are:  Introduced by Servlet 3.0:  SCI (javax.servlet.ServletContainerInitializer)Web fragments (META-INF/web-fragment.xml)Resources of a web application bundled in jar files (META-INF/resources/*)Annotations that define components of a web application (@WebServletetc.)Annotations that define components for 3-rd party libraries initialized by an SCI (arbitrary annotations that are defined in@HandlesTypesannotation on a SCI class)  Older features, introduced by earlier specifications:  TLD scanning, (Discovery of tag libraries. Scans for Tag Library Descriptor files,META-INF/**/*.tld).  Among the scans the annotation scanning is the slowest. That is because each class file (except ones in ignored JARs) has to be read and parsed looking for annotations in it.  An example of a container-provided SCI that triggers annotation scanning is theWebSocketAPI implementation which is included with standard distribution in all versions of Tomcat 8 and with Tomcat 7 starting with 7.0.47. An SCI class declared there triggers scanning forWebSocketendpoints (the classes annotated with@ServerEndpointor implementingServerApplicationConfiginterface or extending the abstractEndpointclass). If you do not need support forWebSockets, you may remove theWebSocketAPI andWebSocketimplementation JARs from Tomcat (websocket-api.jarandtomcat7-websocket.jarortomcat-websocket.jar).  A note on TLD scanning: In Tomcat 7 and earlier the TLD scanning happens twice,  first, at startup time, to discover listeners declared in tld files (done byTldConfigclass),second, by JSP engine when generating java code for a JSP page (done byTldLocationsCache).  The second scanning is more noticeable, because it prints a diagnostic message about scanned JARs that contained no TLDs. In Tomcat 8 the TLD scanning happens only once at startup time (inJasperInitializer).",../data/confluence_exports/TOMCAT/HowTo-FasterStartUp_103099266.html
Apache Tomcat : HowTo FasterStartUp Configure your web application,"See chapter inTomcat 7 migration guide.  There are two options that can be specified in yourWEB-INF/web.xmlfile:  Setmetadata-complete=""true""attribute on the<web-app>element. 2. Add an empty<absolute-ordering />element.  Settingmetadata-complete=""true""disables scanning your web application and its libraries for classes that use annotations to define components of a web application (Servlets etc.). Themetadata-completeoption is not enough to disable all of annotation scanning. If there is a SCI with a@HandlesTypesannotation, Tomcat has to scan your application for classes that use annotations or interfaces specified in that annotation.  The<absolute-ordering>element specifies which web fragment JARs (according to the names in theirWEB-INF/web-fragment.xmlfiles) have to be scanned for SCIs, fragments and annotations. An empty<absolute-ordering/>element configures that none are to be scanned.  In Tomcat 7 theabsolute-orderingoption affects discovery both of SCIs provided by web application and ones provided by the container (i.e. by the libraries in$CATALINA_HOME/lib). In Tomcat 8 the option affects the web application ones only, while the container-provided SCIs are always discovered, regardless ofabsolute-ordering. In such case theabsolute-orderingoption alone does not prevent scanning for annotations, but the list of JARs to be scanned will be empty, and thus the scanning will complete quickly. The classes inWEB-INF/classesare always scanned regardless ofabsolute-ordering.  Scanning for web application resources and TLD scanning are not affected by these options.",../data/confluence_exports/TOMCAT/HowTo-FasterStartUp_103099266.html
Apache Tomcat : HowTo FasterStartUp Remove unnecessary JARs,"Remove any JAR files you do not need. When searching for classes every JAR file needs to be examined to find the needed class. If the jar file is not there - there is nothing to search.  Notethat a web application should never have its own copy of Servlet API or Tomcat classes. All those are provided by the container (Tomcat) and should never be present in the web application. If you are using Apache Maven, such dependencies should be configured with<scope>provided</scope>. See also astackoverflow page.",../data/confluence_exports/TOMCAT/HowTo-FasterStartUp_103099266.html
Apache Tomcat : HowTo FasterStartUp Exclude JARs from scanning,In Tomcat 7 JAR files can be excluded from scanning by listing their names or name patterns in asystem property. Those are usually configured in theconf/catalina.propertiesfile.  In Tomcat 8 there are several options available. You can use asystem propertyor configure a<JarScanFilter>elementin thecontext fileof your web application.,../data/confluence_exports/TOMCAT/HowTo-FasterStartUp_103099266.html
Apache Tomcat : HowTo FasterStartUp DisableWebSocketsupport,"There exists an attribute onContextelement,containerSciFilter. It can be used to disable container-provided features that are plugged into Tomcat via SCI API: WebSocket support (in Tomcat 7 and later), JSP support (in Tomcat 8 and later).  The class names to filter can be detected by looking intoMETA-INF/services/javax.servlet.ServletContainerInitializerfiles in Tomcat JARs. For WebSocket support the name isorg.apache.tomcat.websocket.server.WsSci, for JSP support the name isorg.apache.jasper.servlet.JasperInitializer. e.g.:  <Context containerSciFilter=""WsSci"" />  The impact of disabling WebSocket support will depend on how many JARs were being scanned for WebSocket annotations and whether any other SCIs trigger annotation scans. Generally, it is the first SCI scan that has the biggest performance impact. The impact of additional scans is minimal.  References:Bug 55855,Tomcat 8 Context documentation",../data/confluence_exports/TOMCAT/HowTo-FasterStartUp_103099266.html
Apache Tomcat : HowTo FasterStartUp Entropy Source,"Tomcat 7+ heavily relies on SecureRandom class to provide random values for its session ids and in other places. Depending on your JRE it can cause delays during startup if entropy source that is used to initialize SecureRandom is short of entropy. You will see warning in the logs when this happens, e.g.:  <DATE> org.apache.catalina.util.SessionIdGenerator createSecureRandom
INFO: Creation of SecureRandom instance for session ID generation using [SHA1PRNG] took [5172] milliseconds.  There is a way to configure JRE to use a non-blocking entropy source by setting the following system property:-Djava.security.egd=file:/dev/./urandom  Note the ""/./"" characters in the value. They are needed to work around knownOracle JRE bug #6202721. See alsoJDK Enhancement Proposal 123. It is known that implementation ofSecureRandomwas improved in Java 8 onwards.  Also note that replacing the blocking entropy source (/dev/random) with a non-blocking one actually reduces security because you are getting less-random data. If you have a problem generating entropy on your server (which is common), consider looking into entropy-generating hardware products such as ""EntropyKey"".",../data/confluence_exports/TOMCAT/HowTo-FasterStartUp_103099266.html
Apache Tomcat : HowTo FasterStartUp Starting several web applications in parallel,With Tomcat 7.0.23+ you can configure it to start several web applications in parallel. This is disabled by default but can be enabled by setting thestartStopThreadsattribute of aHostto a value greater than one.,../data/confluence_exports/TOMCAT/HowTo-FasterStartUp_103099266.html
Apache Tomcat : HowTo FasterStartUp Memory,Tweak memory parameters - Google is your friend.,../data/confluence_exports/TOMCAT/HowTo-FasterStartUp_103099266.html
Apache Tomcat : HowTo FasterStartUp Config,Trim the config files as much as possible. XML parsing is not cheap. The less there is to parse - the faster things will go.,../data/confluence_exports/TOMCAT/HowTo-FasterStartUp_103099266.html
Apache Tomcat : HowTo FasterStartUp Web application,Remove any web applications that you do not need. (So remove the all the web applications installed with tomcat) 2.  Make sure your code is not doing slow things. (Use a profiler)   CategoryFAQ,../data/confluence_exports/TOMCAT/HowTo-FasterStartUp_103099266.html
Apache Tomcat : Example Application Exposing Internals Using JMX Description,"Application contains:  Web deployment descriptor (e.g. WEB-INF/web.xml) that describes the application (optional)JSP views: home.jsp and WEB-INF/views/view.jspA counter (e.g. example.jmx.web.MyCounter) that can increment, get current count, reset counter and reset to a specific value.A servlet (e.g. example.jmx.web.CounterServlet) that calls counter and forwards to a view (e.g. WEB-INF/views/view.jsp)A servlet listener (e.g. example.jmx.web.RegisterMBeansListener), that registers the MBean to JMX MBeanServerAn MBean interface (e.g. example.jmx.mbean.CounterMBean) that defines JMX attributes and operations, e.g. name, resetCounter, getCurrentCount, getCurrentTimeAn MBean implementation (e.g. example.jmx.mbean.Counter) that implements MBean interface",../data/confluence_exports/TOMCAT/Example-Application-Exposing-Internals-Using-JMX_103098747.html
Apache Tomcat : Example Application Exposing Internals Using JMX example.jmx.web.MyCounter,"package example.jmx.web;

import java.util.concurrent.atomic.AtomicInteger;

public class MyCounter {

    private static AtomicInteger counter = new AtomicInteger(0);
    
    public static int incrementAndGet() {
        return counter.incrementAndGet();
    }
    
    public static int getCurrentCount() {
        return counter.get();
    }
    
    public static void resetCounter() {
        resetCounter(0);
    }
    
    public static void resetCounter(int start) {
        counter = new AtomicInteger(start);
    }

}",../data/confluence_exports/TOMCAT/Example-Application-Exposing-Internals-Using-JMX_103098747.html
Apache Tomcat : Example Application Exposing Internals Using JMX example.jmx.web.CounterServlet servlet,"package example.jmx.web;

import java.io.IOException;

import javax.servlet.ServletException;
import javax.servlet.annotation.WebServlet;
import javax.servlet.http.HttpServlet;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;

@WebServlet(""/count"")
public class CounterServlet extends HttpServlet {

    private static final long serialVersionUID = 886395215542306826L;
    
    @Override
    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {

        request.setAttribute(""COUNT"", MyCounter.incrementAndGet());
        request.getRequestDispatcher(""/WEB-INF/views/view.jsp"").forward(request, response); 
    }

}",../data/confluence_exports/TOMCAT/Example-Application-Exposing-Internals-Using-JMX_103098747.html
Apache Tomcat : Example Application Exposing Internals Using JMX example.jmx.web.RegisterMBeansListener web listener,"package example.jmx.web;

import java.lang.management.ManagementFactory;

import javax.management.InstanceAlreadyExistsException;
import javax.management.InstanceNotFoundException;
import javax.management.MBeanRegistrationException;
import javax.management.MBeanServer;
import javax.management.MalformedObjectNameException;
import javax.management.NotCompliantMBeanException;
import javax.management.ObjectName;
import javax.servlet.ServletContextEvent;
import javax.servlet.ServletContextListener;
import javax.servlet.annotation.WebListener;

import example.jmx.mbean.CounterMBean;
import example.jmx.mbean.Counter;

@WebListener
public class RegisterMBeansListener implements ServletContextListener {

    private ObjectName objectName;

    public RegisterMBeansListener() {
    }

    @Override
    public void contextInitialized(ServletContextEvent sce) {
        System.out.println(""Registering MBean..."");
        final MBeanServer server = ManagementFactory.getPlatformMBeanServer();
        try {
            objectName = new ObjectName(""JmxExampleApp:type=Counter"");
            final CounterMBean mbean = new Counter();
            server.registerMBean(mbean, objectName);
            System.out.println(""MBean registered: "" + objectName);
        } catch (MalformedObjectNameException mone) {
            mone.printStackTrace();
        } catch (InstanceAlreadyExistsException iaee) {
            iaee.printStackTrace();
        } catch (MBeanRegistrationException mbre) {
            mbre.printStackTrace();
        } catch (NotCompliantMBeanException ncmbe) {
            ncmbe.printStackTrace();
        }
    }

    @Override
    public void contextDestroyed(ServletContextEvent sce) {
        System.out.println(""Unregistering MBean..."");
        final MBeanServer server = ManagementFactory.getPlatformMBeanServer();
        try {
            objectName = new ObjectName(""JmxExampleApp:type=Counter"");
            server.unregisterMBean(objectName);
            System.out.println(""MBean unregistered: "" + objectName);
        } catch (MalformedObjectNameException mone) {
            mone.printStackTrace();
        } catch (MBeanRegistrationException mbre) {
            mbre.printStackTrace();
        } catch (InstanceNotFoundException infe) {
            infe.printStackTrace();
        } 
    } 
  
}",../data/confluence_exports/TOMCAT/Example-Application-Exposing-Internals-Using-JMX_103098747.html
Apache Tomcat : Example Application Exposing Internals Using JMX example.jmx.mbean.CounterMBean interface,"package example.jmx.mbean;

import java.util.Date;

public interface CounterMBean {
    void resetCounter();
    void resetCounter(int start);
    int getCurrentCount();
    Date getCurrentTime();
    long getCurrentTimeMilis();
}",../data/confluence_exports/TOMCAT/Example-Application-Exposing-Internals-Using-JMX_103098747.html
Apache Tomcat : Example Application Exposing Internals Using JMX example.jmx.mbean.Counter implementation,"package example.jmx.mbean;

import java.util.Date;

import example.jmx.web.MyCounter;

public class Counter implements CounterMBean {

    private String name;
    
    public Counter() {
        this.name = ""InitialName"";
    }
    
    public String getName() {
        return name;
    }
    public void setName(String name) {
        this.name = name;
    }
    
    @Override
    public void resetCounter() {
        MyCounter.resetCounter();
    }

    @Override
    public void resetCounter(int start) {
        MyCounter.resetCounter(start);
    }
    
    @Override
    public int getCurrentCount() {
        return MyCounter.getCurrentCount();
    }
    
    @Override
    public Date getCurrentTime() {
        return new Date();
    }
    
    @Override
    public long getCurrentTimeMilis() {
        return System.currentTimeMillis();
    }

}",../data/confluence_exports/TOMCAT/Example-Application-Exposing-Internals-Using-JMX_103098747.html
Apache Tomcat : Example Application Exposing Internals Using JMX Download,DownloadJmxExample.war(with source code).,../data/confluence_exports/TOMCAT/Example-Application-Exposing-Internals-Using-JMX_103098747.html
Apache Tomcat : Specifications Overview,"This section provides a list of API specifications which are implemented by Apache Tomcat. The specifications are developed and maintained by theJava Community Process (JCP). The members of the JCP are coming from software industry, other organizations like the Apache Software Foundation (ASF), educational institutions but include also individual (personal) members. Each specifications starts its life as a so-called Java Specification Request JSR. The JSRs are also known by the unique number they receive once the specification process starts. On the web site of the JCP you can find an overview page for each spec, and a separate download page. The download page lists various stages of each spec reflecting the development process of JCP specs. Examples are ""Early Draft Review"", ""Public Final Draft"" and ""Final Release"". You would like to make sure that you always access the latest documents. Different Tomcat versions implementdifferentversions of the specifications (seemain site,wiki).",../data/confluence_exports/TOMCAT/Specifications_103100166.html
Apache Tomcat : Specifications Java Servlet Specifications,"Servlet 4.0 specification is JSR 369. Servlet 3.1 specification is JSR 340. Servlet 3.0 specification is JSR 315. Servlet 2.5 is a maintenance release of Servlet 2.4. Both are JSR 154. Spec versions:Servlet 4.0Main page:JSR369Java.net project:servlet-specAlso see Java EE Platform Specification project,javaee-specStable:Final ReleaseDate:05 Sept, 2017Download Page:OverviewDirect DownloadOnline Javadoc:Java EE 8(A javaee.github.io link. Not available at docs.oracle.com?)Minimum Tomcat version:9.0.0 Spec versions:Servlet 3.1Main page:JSR340Java.net project:servlet-specStable:Final ReleaseDate:28 May, 2013Download Page:OverviewDirect DownloadOnline Javadoc:Java EE 7Minimum Tomcat version:8.0.0 Spec versions:Servlet 3.0Main page:JSR315Stable:Final ReleaseMaintenance Release (Version 3.0 Rev a)Date:10 Dec, 20096 Feb, 2011Download Page:OverviewDirect DownloadOverviewChange LogDirect Download - PDFOnline Javadoc:Java EE 6Minimum Tomcat version:7.0.07.0.25 Spec versions:Servlet 2.5Main page:JSR154Stable:Maintenance ReleaseMaintenance Release 2Date:11 May, 200611 Sep, 2007Download Page:OverviewDirect Download - Javadoc, classesOverviewDirect Download - PDFOnline Javadoc:Java EE 5Minimum Tomcat version:6.0.06.0.44 (bug 57703) Spec versions:Servlet 2.4Main page:JSR154Stable:Final ReleaseDate:24 Nov, 2003Download Page:OverviewDirect DownloadOnline Javadoc:Java EE 1.4Minimum Tomcat version:5.0.0",../data/confluence_exports/TOMCAT/Specifications_103100166.html
Apache Tomcat : Specifications JavaServer Pages and Expression Language Specifications,"JSP 2.3 is the second maintenance release of JSP 2.1 (JSR 245). Expression Language 3.0 is JSR 341. JSP 2.2 is the first maintenance release of JSP 2.1 (JSR 245). JSP 2.1 is JSR 245. JSP 2.0 is JSR 152. Expression Language was covered by JSP 2.0 and JSP 2.1 specifications, but became a separate document starting with JSP 2.2. Specification:JSP 2.3EL 3.0Main page:JSR245JSR341Java.net project:jsp-spec-public?el-specStable:Maintenance Release 2The naming is according to JSR 245. The title page of theJSP specification document says ""Maintenace Release 3""Final ReleaseDate:12 Jun, 201322 May, 2013Download Page:OverviewDirect DownloadOverviewDirect DownloadOnline Javadoc:Java EE 7,Java EE 8Minimum Tomcat version:8.0.0 Spec versions:JSP 2.2, EL 2.2Main page:JSR245Stable:Maintenance ReleaseThe naming is according to JSR 245. The title page of theJSP specification document says ""Maintenace Release 2""Date:10 Dec, 2009Download Page:OverviewDirect Download - JSP 2.2Direct Download - EL 2.2Online Javadoc:Java EE 6Minimum Tomcat version:7.0.0 Spec versions:JSP 2.1Main page:JSR245Stable:Final ReleaseDate:11 May, 2006Download Page:OverviewDirect DownloadOnline Javadoc:Java EE 5Minimum Tomcat version:6.0.0 Spec versions:JSP 2.0Main page:JSR152Stable:Final ReleaseDate:24 November, 2003Download Page:OverviewDirect DownloadOnline Javadoc:Java EE 1.4Minimum Tomcat version:5.0.0",../data/confluence_exports/TOMCAT/Specifications_103100166.html
Apache Tomcat : Specifications Java API for WebSocket,"Java API for WebSocket is JSR 356. An implementation is to be included in Tomcat 8. See alsoBug 51181. Since Tomcat 7.0.47 this implementation is included with Tomcat 7, but it is only available if Tomcat is run with Java 7 or later. Spec versions:Java API for WebSocket 1.0Java API for WebSocket 1.1Main page:JSR356JSR356Java.net project:websocket-specStable:Final ReleaseMaintenance ReleaseDate:22 May, 20135 August, 2014Download Page:OverviewDirect DownloadOverviewDirect DownloadOnline Javadoc:Java EE 7Minimum Tomcat version:7.0.47, 8.0.07.0.56, 8.0.13",../data/confluence_exports/TOMCAT/Specifications_103100166.html
Apache Tomcat : Specifications Java Authentication Service Provider Interface for Containers Specification,"JASPIC 1.1 is the second maintenance release, Maintenance Release B, of JASPIC 1.0 (JSR 196). Spec versions:JASPIC 1.0JASPIC 1.1Main page:JSR196Java.net project:jaspic-specStable:Final ReleaseMaintenance Release BDate:10 Oct, 200712 Jun, 2013Download Page:OverviewDirect DownloadOverviewDirect DownloadOnline Javadoc:Java EE 7Minimum Tomcat version:8.5.0",../data/confluence_exports/TOMCAT/Specifications_103100166.html
Apache Tomcat : Specifications See Also,Tomcat VersionsWhich Tomcat Version Do I Want  The following are specifications for the web protocols supported by Tomcat.,../data/confluence_exports/TOMCAT/Specifications_103100166.html
"Apache Tomcat : Specifications HTTP, HTTP/2","HTTP 0.9The Original HTTP as defined in 1991at W3.orgHTTP/1.0RFC 1945(May 1996) - Hypertext Transfer Protocol -- HTTP/1.0HTTP/1.1RFC 2068(January 1997) - Hypertext Transfer Protocol -- HTTP/1.1 - obsolete, replaced by 2616RFC 2616(June 1999) - Hypertext Transfer Protocol -- HTTP/1.1 - obsolete, replaced by 7230...7235RFC 7230(June 2014) - Hypertext Transfer Protocol (HTTP/1.1): Message Syntax and Routing - obsolete, replaced by 9110, 9112RFC 7231(June 2014) - Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content - obsolete, replaced by 9110RFC 7232(June 2014) - Hypertext Transfer Protocol (HTTP/1.1): Conditional Requests - obsolete, replaced by 9110RFC 7233(June 2014) - Hypertext Transfer Protocol (HTTP/1.1): Range Requests - obsolete, replaced by 9110RFC 7234(June 2014) - Hypertext Transfer Protocol (HTTP/1.1): Caching - obsolete, replaced by 9111RFC 7235(June 2014) - Hypertext Transfer Protocol (HTTP/1.1): Authentication - obsolete, replaced by 9110RFC 9110(June 2022) - HTTP SemanticsRFC 9111(June 2022) - HTTP CachingRFC 9112(June 2022) - HTTP/1.1HTTP/2HTTP/2 home page, maintained by the IETF HTTPbis Working GroupRFC 7540(May 2015) - Hypertext Transfer Protocol Version 2 (HTTP/2) - obsolete, replaced by 9113RFC 7541(May 2015) - HPACK: Header Compression for HTTP/2RFC 9113(June 2022) - HTTP/2HTTP/3RFC 9114(June 2022) - HTTP/3Not yet implemented by Apache Tomcat. (As of July 2022)",../data/confluence_exports/TOMCAT/Specifications_103100166.html
Apache Tomcat : Specifications HTTP - Related Specifications,"BASIC and DIGEST authentication methodsRFC 2068(January 1997) - Hypertext Transfer Protocol -- HTTP/1.1 - obsolete, replaced by 2616RFC 2069(January 1997) - An Extension to HTTP : Digest Access Authentication - obsolete, replaced by 2617.The authentication as a whole and the BASIC authentication method were defined in RFC 2068 ch.11. The DIGEST authentication method was defined in RFC 2069.RFC 2617(June 1999) - HTTP Authentication: Basic and Digest Access Authentication - obsolete,It covers BASIC and DIGEST authentication methodsRFC 7616(September 2015) - HTTP Digest Access AuthenticationRFC 7617(September 2015) - The 'Basic' HTTP Authentication SchemeSee alsoRFC 7235(obsolete),RFC 7615(obsolete),RFC 9110.RFC 6265""HTTP State Management Mechanism""The specification about cookies. It is implemented byorg.apache.tomcat.util.http.Rfc6265CookieProcessorthat is available since Tomcat 8.0.15 and is the default one starting with Tomcat 8.5.0. See also ""Cookies"" page in ""Development and Development Issues / Archive"" on this wiki.Obsolete specifications:RFC2109,RFC 2965.RFC 6266""Use of the Content-Disposition Header Field in the Hypertext Transfer Protocol (HTTP)""Content-Disposition header is used by file uploads. See alsoBug 59115draft-thomson-hybi-http-timeout-03""Hypertext Transfer Protocol (HTTP) Keep-Alive Header"". A draft of specification.Support for sending a Keep-Alive response header was added in Tomcat 8.5.48, 9.0.29 —Bug 63835. This feature can be turned off via an attribute on HTTP/1.1Connector.RFC 9218(June 2022)""Extensible Prioritization Scheme for HTTP""Defines prioritization scheme and priority signals. To be used in HTTP/2 as a replacement for the original specification of stream priority signals (defined byRFC 7540) that was later deprecated byRFC 9113(section 5.3.2.).Not yet implemented by Apache Tomcat. (As of December 6th 2022)Seediscussion.",../data/confluence_exports/TOMCAT/Specifications_103100166.html
Apache Tomcat : Specifications AJP,The AJP protocol specification lives on theApache Tomcat Connectorweb site. AJP/1.3AJP Protocol Reference - AJPv13,../data/confluence_exports/TOMCAT/Specifications_103100166.html
Apache Tomcat : Specifications WebSocket,WebSocket protocol is specified byRFC 6455.,../data/confluence_exports/TOMCAT/Specifications_103100166.html
Apache Tomcat : Memory Preface,"This page discusses various memory issues. In a nutshell - if your computer has less than 128MB of ram - you will probably have trouble. Anyhow, also read the following threads for other memory related issues: java.lang.OutOfMemoryError during deployMemory requirementsMemory Mgmt TomcatTomcat Out of memoryTracking memory usage over time Also look atYourKit, or maybe you IDE has a profiling tool in it, or other profiling tools are available. (The following tools were recommended by many people in the past, but now seem to be discontinued by their vendors: JProbe by Quest Software — the company was acquired by Dell, OptimizeIt by Borland). This is not an endorsement for them, I just notice other people like them.",../data/confluence_exports/TOMCAT/Memory_103098949.html
Apache Tomcat : Memory Questions,How do I adjust memory settings?Why do I get OutOfMemoryError errors?How much memory is Tomcat/webapp/??? using?,../data/confluence_exports/TOMCAT/Memory_103098949.html
Apache Tomcat : Memory How do I adjust memory settings?,"First look atjava -Xto determine what parameters to set. Then you can set them via the environment variableCATALINA_OPTS(usingJAVA_OPTSalso works, but is not recommended). This variable is usually set in a filebin/setenv.shorbin/setenv.batthat you may need to create by yourselves. Thesetenvfile is documented in RUNNING.txt in your version of Tomcat. The environment variables are described in a comment at the top of catalina.bat or catalina.sh files.",../data/confluence_exports/TOMCAT/Memory_103098949.html
Apache Tomcat : Memory Why do I getOutOfMemoryErrorerrors?,"Many reasons. You're out of memory. Simple as that - add more to your heap.You're out of memory. You have code which is hanging onto object references and the garbage collector can't do its job. Get a profiler to debug this one.You ran out of file descriptors. If you are on a *nix system, it has been observed that anOutOfMemoryErrorcan be thrown if you run out of file descriptors. This can occur if your threshold is too low. Theulimitprogram can help you out here. You also may need to account for socket connections too when thinking about these thresholds. Google is your friend for getting more information about this topic.You have too many threads running. Some OS's have a limit to the number of threads which may be executed by a single process. (Which is what the JVM is.) Refer to your OS docs for more information on how to raise this threshold.If you have a lot of servlets or JSP's, you may need to increase your permanent generation. By default, it is 64MB. Quadrupling it to be-XX:MaxPermSize=256mmight be a good start.Your OS limits the amount of memory your process may take. OK, this one is grasping at straws.The JVM has a bug. This has been known to happen with JVM1.2.? and using EJB's with another servlet engine.Not actually a reason - but on your particular platform, look at thejava -Xoptions. They may be VERY helpful.Your classloaders are not being garbage collected.You run out of process memory (non java/GC memory), for example when using java.util.zip classes or JNI classes allocating process memory. SeeInstantiating Inflater/Deflater causes OutOfMemoryError; finalizers not called promptly enough See alsoOutOfMemoryandMemoryLeakProtection.",../data/confluence_exports/TOMCAT/Memory_103098949.html
Apache Tomcat : Memory How much memory is Tomcat/webapp/??? using?,"To find out how much memory Tomcat is using, you might be able to use theRuntimeclass provided by the JDK.You can't find out how much memory a webapp is using. The JVM doesn't give us these detail.You can't find out how much memory a ??? is using. The JVM doesn't give us these detail.That being said, a memory profiling tool might prove the above statements wrong - but you probably don't want to use them in a production environment.",../data/confluence_exports/TOMCAT/Memory_103098949.html
Apache Tomcat : Jakarta EE Release Numbering *** DRAFT *** DRAFT *** DRAFT ***,"Jakarta EE 9 will be, as far as Tomcat is concerned, identical to Java EE 8 with one notable exception. The package names for many of the Jakarta EE packages will change from javax.* to jakarta.* Jakarta EE 10 will follow relatively quickly after Jakarta EE 9 and will include API changes, new features etc. Tomcat major versions have, historically, tracked releases of the Java Servlet specification (and associated Java EE specification). The expectation is that future Tomcat versions will continue to track releases of the Jakarta Servlet specification  (and associated Jakarta EE specification). The Tomcat community wishes to implement Jakarta EE 9 but recognises that many users will wish to remain on Java EE 8 while taking advantage of new features introduced in Tomcat 10 onwards. The Tomcat community has therefore drafted the following release plan:",../data/confluence_exports/TOMCAT/Jakarta-EE-Release-Numbering_135863433.html
Apache Tomcat : Jakarta EE Release Numbering Current status,7.0.x : Continues to support Java EE 68.5.x : Continues to support Java EE 79.0.x : Continues to support Java EE 810.0.x : Jakarta EE 9 development,../data/confluence_exports/TOMCAT/Jakarta-EE-Release-Numbering_135863433.html
Apache Tomcat : Jakarta EE Release Numbering Step 1: Announce EOL for 7.0.x,"Announce EOL for Tomcat 7 (as 31 March 2021?) That gives us: 7 : Continues to support Java EE 68 : Continues to support Java EE 79 : Continues to support Java EE 810.0.0-Mx (master) development branch for Jakarta EE 9 support Note the 10.0.0-Mx will be Milestone releases Step 2: Releases between now and Jakarta EE 9 release Apply Tomcat API changes planned for Tomcat 10 to 10.0.0-Mx Apply Jakarta EE 9 package rename changes. Other changes applied to 10.0.0-Mx and then back-ported to 9.0.x, 8.5.x and 7.0.x Monthly milestone releases of 10.0.0-Mx alongside 9.0.x and 8.5.x",../data/confluence_exports/TOMCAT/Jakarta-EE-Release-Numbering_135863433.html
Apache Tomcat : Jakarta EE Release Numbering Step 3: Jakarta EE 9 release,Once Jakarta EE 9 is released and Tomcat 10.0.0-Mx passes the TCK Release 10.0.0 Create a 10.0.x branch from master. 10.0.x will be supported until there is a stable 10.1.0 release. Create a 9.10.x branch (from master and revert the package rename commits) 9.n will be supported until there is a stable 9.(n+1) release. Latest 9.n aims to be identical to latest Tomcat n except it won't include the javax → jakarta package rename. That gives us: 7.0.x : Continues to support Java EE 68.5.x : Continues to support Java EE 79.0.x : Continues to support Java EE 89.10.x: Continues to support Java EE 8 with Tomcat API identical to latest Tomcat 1010.0.x: Continues to support Jakarta EE 9. Critical / important fixes only. Will be EOL'd as soon as Jakarta EE 10 is stable.10.1-0-Mx (master): Development branch for Jakarta EE 10 support,../data/confluence_exports/TOMCAT/Jakarta-EE-Release-Numbering_135863433.html
Apache Tomcat : Jakarta EE Release Numbering Step 4 : Jakarta EE 10 release,It is assumed that Tomcat 7 EOL has passed by this point. 10.0.x will also be EOL. That gives us: 8.5.x : Continues to support Java EE 79.0.x : Continues to support Java EE 89.10.x: Continues to support Java EE 8 with Tomcat API identical to latest Tomcat 10.1.x10.1.x (master): Supports Jakarta EE 10,../data/confluence_exports/TOMCAT/Jakarta-EE-Release-Numbering_135863433.html
Apache Tomcat : Jakarta EE Release Numbering Step 5 Jakarta EE 11 development starts,Announce EOL for Tomcat 8. Create 10.1.x branch from master. That gives us: 8.5.x : Continues to support Java EE 79.0.x : Continues to support Java EE 89.10.x : Continues to support Java EE 8 with Tomcat API identical to latest Tomcat 10.0.x10.1.x : Supports Jakarta EE 1011.0.x (master): Development of Jakarta EE 11 support,../data/confluence_exports/TOMCAT/Jakarta-EE-Release-Numbering_135863433.html
Apache Tomcat : Jakarta EE Release Numbering Step 6 : Jakarta EE 11 release,9.10.x is now EOL. Create a 9.11.x branch (from master and revert the package rename commits) That gives us: 8.5.x : Continues to support Java EE 7 (likely to be EOL soon if not already EOL)9.0.x : Continues to support Java EE 89.11.x : Continues to support Java EE 8 with Tomcat API identical to latest Tomcat 11.0.x10.1.x : Supports Jakarta EE 1011.0.x : (master): Supports of Jakarta EE 11,../data/confluence_exports/TOMCAT/Jakarta-EE-Release-Numbering_135863433.html
Apache Tomcat : Jakarta EE Release Numbering Steady state,The steady state should be repeating the pattern of steps 5 and 6 with the following versions: 9.N: Continues to support Java EE 8 with Tomcat API identical to latest Tomcat N(N-2): Supports Jakarta EE (N-2)(N-1): Supports Jakarta EE (N-1)N (master): Development of Jakarta EE N support,../data/confluence_exports/TOMCAT/Jakarta-EE-Release-Numbering_135863433.html
Apache Tomcat : Logging Tutorial Configuring Loggers,"Suppose we wanted to configure myLogger via a logging.properties file, rather than programmatically as we did earlier, and simply set its Level to SEVERE.We would put the following in the configuration file:   com.example.myapp.CriticalComponent.level = SEVERE   That's how Loggers are configured.  We specified the name of the  Logger followed by a period and the name of the  property we wanted to set, which is 'level' in this case.  If we had several classes with corresponding loggers, and we did not want to set the  logging level for each class individually, we could set the logging level on the root Logger like this:   .level = SEVERE   This will set the level on all loggers to SEVERE, unless the Logger uses a non-default level (Either programmatically or via configuration).  So if you wished to turn logging off all together simply configure the root  logger like this and make sure that there are no child loggers that override this setting:   .level = OFF   On the flip side to log everything set it like this:   .level = ALL",../data/confluence_exports/TOMCAT/Logging-Tutorial_103099495.html
Apache Tomcat : Logging Tutorial Configuring Handlers,"Suppose that we also wanted to add a Handler to the Logger named  com.example.myapp.CriticalComponent.  We could define our handler like this:  handlers = org.apache.juli.FileHandler  And configure it like this:org.apache.juli.FileHandler.level = WARNING org.apache.juli.FileHandler.directory = /var/log/tomcat/org.apache.juli.FileHandler.prefix = mywebapp  Then assign it to our Logger like this:  com.example.myapp.CriticalComponent.handlers = org.apache.juli.FileHandler  Now we've done the same thing declaratively in our logging.properties file that we did programmatically earlier.  What if we wanted to make the Handler we just configured the root Handler?  We could configure it as the root Handler like this:.handlers = org.apache.juli.FileHandler.  Now all Tomcat Loggers that do not have a Handler assigned to them directly, either programmatically or declaratively via the logging.properties file,  will delegate to this Handler.",../data/confluence_exports/TOMCAT/Logging-Tutorial_103099495.html
Apache Tomcat : Logging Tutorial Configuring Context Loggers,"So now you've had a look at the example logging.properties file in the official documentation again, and you understand most of it.  Then you get to these lines:   org.apache.catalina.core.ContainerBase.[Catalina].[localhost].[/manager].level = INFO org.apache.catalina.core.ContainerBase.[Catalina].[localhost].[/manager].handlers = \3manager.org.apache.juli.FileHandler                                                                        These don't follow the same syntax we've been using thus far to configure Loggers and Handlers.  These are for configuring Context loggers.  What's a context logger?  First you need to know a little about theServletContext.  For a description of what a Servlet Context is see:   http://java.sun.com/j2ee/1.4/docs/api/javax/servlet/ServletContext.html   You'll notice specifically that it mentions that there is oneServletContextper webapp, and that thisServletContextcan be used to perform logging.  Looking down a little further in theServletContext.html document we see that theServletContexthas a log method that takes a String.  If a developer working on the manager application were to use theServletContext.log method, without the corresponding configuration lines shown above, output would go to standard out, and end up on the console or catalina.out.  However now that theServletContextLogger has been configured for the manager application,ServletContext.log(""..."") statements made within the /manager application will end up in manager.dd.mm.yyy.log.   CategoryFAQ",../data/confluence_exports/TOMCAT/Logging-Tutorial_103099495.html
Apache Tomcat : Miscellaneous Preface,This section contains various miscellaneous questions that are asked frequently enough to be listed here.,../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous Questions,"I am unable to compile my JSP!I can't get servlets to work under /servlet/*!Why is the invoker evil?How to I get Tomcat's version number?Tomcat eats 100% of the CPU!How do I get a customized error page?Should I use the LE version (of Tomcat 4)?How do I configure Tomcat to NOT to store the sessions during shutdown?Is there a DTD for server.xml?How do I change the welcome file? ( I want to show index.jsp instead of index.html)How do I enable/disable directory listings?How do I use symbolic links with jars?How do I change the name of the file in the download Save-As dialog from a servlet? (or jsp)Is tag pooling broken? It doesn't call release!How do I disable tag pooling?Why do I get java.lang.IllegalStateException ?How do I make a scheduled event on Tomcat?What is Element ""web-app"" does not allow ""servlet"" here?How do I open a file for reading in my webapp?Can I run Tomcat with the JRE, or do I need the full JDK?Is Tomcat an EJB server? Can I use EJBs with Tomcat?Can I access Tomcat's JNDI provider from outside Tomcat?Who uses Tomcat in production?I'm getting java.lang.ThreadDeath exceptions when reloading my webapp.Help! Even though I run shutdown.sh (or shutdown.bat), Tomcat does not stop!How do I debug JSP errors in the Admin web application?What order do webapps start (or How can I change startup order)?What's the difference between a Valve and Filter?How do I set system properties at startup?",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous I am unable to compile my JSP!,"Are you seeing this? org.apache.jasper.JasperException: Unable to compile class for JSP

      An error occurred at line: -1 in the jsp file: null

      Generated servlet error:
          [javac] Since fork is true, ignoring compiler setting.
          [javac] Compiling 1 source file
          [javac] Since fork is true, ignoring compiler setting. If so, here are some solutions: Environment variable issueEnvironment issueorAnt issueOthers have had success by added JAVA_HOME/bin to their PATH.[1][2]and make sure you are using the full JDK and not the JRE. In the case of the Environment issues, it is typical that on Windows, the startup scripts work fine and the service does not. The service uses registry values to look for java and other ""stuff"".",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous I can't get servlets to work under /servlet/*!,"Using /servlet/ to map servlets is evil, absolutely evil. Even more evil thanthis. That being said, here are some threads that may answer this: (SECURITY) Apache Tomcat 4.x JSP source disclosurevulnerabilityCannot Run Servlets, only JSP's, Part IITomcat configuration problem: JSPs work, servlets don't",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous Why is the invoker evil?,"This is opinions of the writer (YMMV) Quickie about the invoker: The invoker is a dynamic servlet which allows run-time loading of other servlets based on class name. This servlet is the one that allowshttp://localhost/servlet/com.foo.MyClass?more=cowbell, wherecom.foo.MyClassis some class which can be loaded as a servlet but was never explicitly declared in a config file. Evil because: Security risk ... see links aboveConfiguration hiding - There is NO way to determine which servlets are used vs which are not used. In web.xml, every servlet is declared and mapped. In that one file you instantly have a road map to how the webapp works.Back doors. Servlets whicharemapped can be alternately called via the invoker by class name. Since the URL is different, all security constraints might be ignored since the URL pattern is VERY different.Back doors. Bad programmers make it easier to do bad things.Back doors. It may be common to use common 3rd party jars in a shared area. If that shared jar has servlets in them and that servlet has a hole in it, bad things happen.Configuration hiding - it's important enough to say twice. Explicit declaration while a PITA, will be more helpful in the maintenance scheme of your webapp. For another explanation of the invoker servlet, why it's evil, and what to do about it, seeJavaRanch FAQ.",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous How to I get Tomcat's version number?,javax.servlet.ServletContext.getServerInfo(); Starting with Tomcat 5.0.28 - there is now a version.sh (or version.bat) program in the bin directory. It will print the version number of Tomcat to Standard output.,../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous Tomcat eats 100% of the CPU!,"Odds are, it might be the garbage collector going wacky. In 1.4.1 JVMs there is a memory leak which could hampering the garbage collector.More informationOtherwise - get a memory profiler and/or tweak your memory settings.",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous How do I get a customized error page?,"In yourWEB-INF/web.xmlfile: <error-page>
        <error-code>404</error-code>
        <location>/error/404.html</location>
    </error-page> You may also catch error 500's as well as other specific exceptions or exceptions which extend Throwable. For more information, see the Servlet Specification for all the gory details of how you can play with this.",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous Should I use the LE version (of Tomcat 4)?,"No. It was an experiment, it failed. (YMMV) The original purpose of LE ""light"" edition of Tomcat 4.0, 4.1 was because jdk1.4 provides a LOT of standard functionality that Tomcat uses that was not in jdk1.3. So to save some space two distributions were made to save some bandwidth. Since then, it has been an exercise in confusion. (Flame me if you disagree)More info",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous How do I configure Tomcat to NOT to store the sessions during shutdown?,"Look at the Session manager componentand tweak accordingly. One way is to tell Tomcat to persist sessions to a path which does not exist. (So Tomcat will not store, or be able to load the sessions).",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous Is there a DTD for server.xml?,No! Nor can one accurately exist.Here's why,../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous How do I change the welcome file? ( I want to show index.jsp instead of index.html),This is done in web.xml by changing welcome-files-list.More detail.,../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous How do I enable/disable directory listings?,"To make a global change, this is done in CATALINA_BASE/conf/web.xml by changing the listings property for the default servlet. If you want to enable it for an individual webapp, then you need to add something similar to the following to yourWEB-INF/web.xmlfile (for your individual app): <servlet>
       <servlet-name>listing</servlet-name>
       <servlet-class>org.apache.catalina.servlets.DefaultServlet</servlet-class>
       <init-param>
           <param-name>debug</param-name>
           <param-value>0</param-value>
       </init-param>
       <init-param>
           <param-name>listings</param-name>
           <param-value>true</param-value>
       </init-param>
       <load-on-startup>1</load-on-startup>
   </servlet>

   <servlet-mapping>
       <servlet-name>listing</servlet-name>
       <url-pattern>/</url-pattern>
   </servlet-mapping>",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous How do I use symbolic links with jars?,You'll need to tweak with server.xml anddo this. (Don't worry - its really easy!) Update: you should be worried – this feature has its drawbacks.,../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous How do I change the name of the file in the download Save-As dialog from a servlet? (or jsp),Use the Content-Disposition header (as defined byRFC6266).,../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous Is tag pooling broken? It doesn't call release!,"It is not broken, your tag probably is. Many bug reports have been filed about this.Bug 16001has all the gory details. There is also a nicedescription of the life cycle. Note, that this issue does not affect theSimpleTagtags and tag files of JSP 2.0 and later specifications, because those do not use tag pooling.",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous How do I disable tag pooling?,"To disable tag pooling: See $CATALINA_BASE/conf/web.xml file and set enablePooling=false for theJspServletdeclaration. Note:This option, as well as most other configuration options ofJspServlet, affects code generation and compilation of JSP pages. It will not alter the behavior of JSPs that have already been compiled.",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous Why do I getjava.lang.IllegalStateException?,"These are the most common reasons how you can get anjava.lang.IllegalStateException: CallingsetBufferSizeand content has been written.The response has been committed and you do any of the following:CallingServletResponse.reset()orServletResponse.resetBuffer()Calling eitherHttpServletResponse.sendError()orHttpServletResponse.sendRedirect().CallingRequestDispatcher.forward()which includes performing a jsp:forwardCallingRequestDispatcher.forward()which includes performing a jsp:forward Remember that if you callforward()orsendRedirect(), any following lines of code will still execute. For example: {
  ...
  response.sendRedirect(""foo.jsp"");
  // At this point, you should probably have a return statement otherwise
  // the following code will run
  System.out.println(""After redirect! By the way ..."");
  System.out.println(""Use commons-logging or log4j, not System.out"");
  System.out.println(""System.out is a bad practice!"");

  response.sendRedirect(""bar.jsp""); /* This will throw an error! */

}",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous How do I make a scheduled event on Tomcat?,"Tomcat does not support this directly. Its not part of the Servlet or JSP Specification. If you do need this functionality seeApache TomEEwhich includes support for the@Scheduleannotation andScheduleExpression API, or seethis threadorthis link.",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
"Apache Tomcat : Miscellaneous What is Element ""web-app"" does not allow ""servlet"" here?",Your web.xml is not well formed or it is not conforming to the DTD as defined by the servlet spec. Use an XML validator to ensure your web.xml file is ok.,../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous How do open a file for reading in my webapp?,UseServletContext.getResourceAsStream(),../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
"Apache Tomcat : Miscellaneous Can I run Tomcat with the JRE, or do I need the full JDK?",Tomcat 4 requires the full JDK. Tomcat 5.5 onwards will work with a JRE or a JDK.,../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous Is Tomcat an EJB server? Can I use EJBs with Tomcat?,"Tomcat is not an EJB server. Tomcat is not a full JavaEE server. Tomcat is a Servlet container. Tomcat does support those parts of the JavaEE specification that are required for Servlets, such as a subset of JNDI functionality. Furthermore, you can connect to remote JavaEE servers, or run Tomcat embedded inside a full JavaEE server. Apache TomEEis a Java EE certified distribution built from Tomcat and contains support for EJB, CDI, JSF, JPA, BeanValidation and Transactions. The goal of TomEE is to maintain all Tomcat functionality and identity, only adding what is necessary to pass that Java EE 6 Web Profile TCK. A second distribution, called TomEE Plus, adds JAX-RS, JAX-WS, JMS and support for the JavaEE Connector Architecture.",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous Can I access Tomcat's JNDI provider from outside Tomcat?,Not at this time.,../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous Who uses Tomcat in production?,Numerous organizations across various industries all over the world. There is aPoweredBypage in this wiki.,../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous I'm gettingjava.lang.ThreadDeathexceptions when reloading my webapp.,SeeBugzilla discussionof this.,../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
"Apache Tomcat : Miscellaneous Help! Even though I run shutdown.sh (or shutdown.bat), Tomcat does not stop!","Most likely, a non-daemon thread is running. JVM's do not shutdown until there are 0 non-daemon threads. You will need to perform athread dumpto determine the code which started the thread.",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous How do I debug JSP errors in the Admin web application?,"The admin web application that was shiped with Tomcat's binary distribution of Tomcat 5.5 and earlier, contained pre-compiled JSPs and mappings for them. In order to debug these JSPs, you need to get the source versions, place them in the admin webapp directory, and disable the JSPC-generated web.xml servlet mappings. Here's how to do it from scratch for a clean Tomcat installation: Download Tomcat's base and admin webapp distributions (binaries), unzip to a directory of your choice. We'll use c:\temp in this example.Edit c:\temp\server\webapps\admin\WEB-INF\web.xml to remove the JSPC-generated servlet mappings. These are marked in the web.xml file with comments indicating the beginning and end of the JSPC section. You can simply comment out all these servlet-mappping elements. Be careful not to comment out other servlet mappings such as the Struts dispatcher servlet.Open c:\temp\server\webapps\admin\WEB-INF\lib\catalina-admin.jar with a zip file program likeWinZip. Remove the files named *_jsp.class, as these are the compiled JSPs. Do not remove the other class files, the ones under the org/apache/ paths.Download a Tomcat source distribution and unzip it to a directory of your choice, but not the same directory that you used above. We'll use c:\src in this example.Copy everythingexceptthe WEB-INF/lib directory from c:\src\container\webapps\admin to c:\temp\server\webapps\admin. Now you will have the uncompiled admin webapp JSPs.If you haven't already, define an admin user in %CATALINA_HOME%\conf\tomcat-users.xml.Start Tomcat, navigate tohttp://localhost:8080(or your actual server:port if you have modified the default values), and access the admin web application. It will run slower because Tomcat is now compiling the JSPs on-demand the first time you access them, but should otherwise appear normal.",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous What order do webapps start (or how can I change startup order)?,"There is no expected startup order. Neither the Servlet spec nor Tomcat define one. You can't rely on the apps starting in any particular order. While the above statement is certainly true, there is a potential ""workaround"":If you actually have two (or more) apps depending on each other, you may decide to start multiple services in you server.xml: <Service name=""Webapps1"">
  <Connector .../>

  <Engine ...>
     <Host appbase=""webapps1"" ...>
       ...        
     </Host>
  </Engine>
</Service>
<Service name=""Webapps2"">
  <Connector .../>

  <Engine ...>
     <Host appbase=""webapps2"" ...>
       ...        
     </Host>
  </Engine>
</Service> I.e. you split the regular ""/webapps"" directory into ""/webapps1"" and ""/webapps2"", whereas everything in the former is deployed and started before the later. The drawback is that you need separate ports for the services.",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous What's the difference between a Valve and Filter?,"A Filter lives in the webapp space and its behavior is defined by the Servlet spec. It will work across all Servlet containers. A Valve can do everything a Filter can do but is Tomcat specific, and potentially not portable across Tomcat versions. A Valve can be executed earlier in the lifecycle of a request, access Tomcat internals, and do other ""interesting"" decorations to your webapp in the chance you cannot change your deployed webapp. If you can achieve the desired functionality with a Filter, it is the preferred way to go.",../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : Miscellaneous How do I set system properties at startup?,Create asetenv.batorsetenv.shfile. It is documented in RUNNING.txt. Example (windows): bin/setenv.batSET CATALINA_OPTS='-DpropName=propValue' Example (UNIX): bin/setenv.shexport CATALINA_OPTS='-DpropName=propValue' Windows service users — use service configuration application (e.g.tomcat9w.exe).,../data/confluence_exports/TOMCAT/Miscellaneous_103098964.html
Apache Tomcat : ClusteringCloud How to use Tomcat clustering in the cloud.,"The load-balancer and the sticky (or not sticky) logic is provided by the cloud itself, basically you have to expose a service and configure a route. Cloud configuration depends on the cloud providers, documentation for more cloud providers will be added to this wiki. The Tomcat clustering for the cloud uses Kubernetes, so you have to configure your nodes to use Kubernetes, all cloud providers support Kubernetes. Kubernetes uses Docker so you have to create a Docker image to use Tomcat in the cloud. There are 2 ways to organize your images: use a standalone Tomcat and add your webapps to it or prepare your webapps as a micro service and have one image per webapp. Each image will be started as a pod on Kubernetes, you can scale up and down by changing the number of pods running your webapp or your Tomcat. Hanging or dying pods are restarted by Kubernetes. 1 - ""Full"" tomcat configuration: In server.xml use the following: <Cluster className=""org.apache.catalina.ha.tcp.SimpleTcpCluster"">
<Channel className=""org.apache.catalina.tribes.group.GroupChannel"">
<Membership className=""org.apache.catalina.tribes.membership.cloud.CloudMembershipService""/>
</Channel>
</Cluster>  2 - Example: There is an example to use it with OpenShift inhttps://github.com/jfclere/tomcat-kubernetes",../data/confluence_exports/TOMCAT/ClusteringCloud_103098633.html
Apache Tomcat : Servlet TCK 5.0 setenv.[sh|bat],Set the following system properties -Dorg.apache.catalina.STRICT_SERVLET_COMPLIANCE=true,../data/confluence_exports/TOMCAT/Servlet-TCK-5.0_158863856.html
Apache Tomcat : Servlet TCK 5.0 context.xml,"Make the following changes: <Context crossContext=""true"" resourceOnlyServlets=""jsp""> <CookieProcessor className=""org.apache.tomcat.util.http.LegacyCookieProcessor"" alwaysAddExpires=""true"" forwardSlashIsSeparator=""false"" /> ... </Context>",../data/confluence_exports/TOMCAT/Servlet-TCK-5.0_158863856.html
Apache Tomcat : Servlet TCK 5.0 server.xml,"Enable h2c on port 8080, and add some trailer headers<Connector ... allowedTrailerHeaders=""myTrailer, myTrailer2"" > <UpgradeProtocol className=""org.apache.coyote.http2.Http2Protocol"" /> </Connector> Enable TLS on port 8443 <Connector port=""8443"" protocol=""HTTP/1.1"" SSLEnabled=""true""> <SSLHostConfig protocols=""TLSv1.2"" truststoreFile=""conf/cacerts.jks""> <Certificate certificateKeystoreFile=""conf/clientcert.jks"" certificateKeystorePassword=""changeit"" type=""RSA"" /> </SSLHostConfig> </Connector> Note: Set protocols=""TLSv1.2"" to disable TLSv1.3 since the TCK requires post-handshake authentication and the Java 11 client does not support that.",../data/confluence_exports/TOMCAT/Servlet-TCK-5.0_158863856.html
Apache Tomcat : Servlet TCK 5.0 tomcat-users.xml,"Make the following changes: <user username=""CN=CTS, OU=Java Software, O=Sun Microsystems Inc., L=Burlington, ST=MA, C=US"" roles=""Administrator""/><user username=""j2ee"" password=""j2ee"" roles=""Administrator,Employee"" /><user username=""javajoe"" password=""javajoe"" roles=""VP,Manager"" />",../data/confluence_exports/TOMCAT/Servlet-TCK-5.0_158863856.html
Apache Tomcat : Servlet TCK 5.0 web.xml,Remove the sections setting the default character encoding for requests and responses to UTF-8.,../data/confluence_exports/TOMCAT/Servlet-TCK-5.0_158863856.html
Apache Tomcat : Servlet TCK 5.0 Test Suite,"Download the Jakarta Servlet 5.0.0 TCK https://download.eclipse.org/ee4j/jakartaee-tck/jakartaee9/promoted/servlet-tck-5.0.0.zip Extract to SERVLET_TCK_HOME cd $SERVLET_TCK_HOME/bin/certificates Convert cts_cert to a truststore doing: ""keytool -import -alias cts -file cts_cert -storetype JKS -keystore cacerts.jks"" password should be ""changeit"" Add $SERVLET_TCK_HOME/bin/certificates/cacerts.jks and $SERVLET_TCK_HOME/bin/certificates/clientcert.jks in the Tomcat conf folder  Edit $SERVLET_TCK_HOME/bin/ts.jte You'll need to set the following properties (adjust the paths and values for your environment)  web.home=/path/to/tomcat servlet.classes=${web.home}/lib/servlet-api.jar:${web.home}/lib/annotations-api.jar webServerHost=localhost webServerPort=8080 securedWebServicePort=8443command.testExecute += -Djavax.net.ssl.trustStore=${ts.home}/bin/certificates/cacerts.jks command.textExecute -= -Djava.endorsed.dirs=${endorsedlib.dir} (Java 11 only) set JAVA_HOME cd $SERVLET_TCK_HOME/bin ant gui Accept the defaults and then run the tests  A default 10.0.x build (as of 10.0.0-M10) with the above configuration and the 5.0.0 TCK triggers 1 test failure with the following JREs: Adopt OpenJDK 8u275 b01Adopt OpenJDK 11.0.9 b11 (TCK and Tomcat) 1 Expected failures 1 x default context path test as Tomcat configuration always overrides this Fixed TCK bugs PR 338Incorrect major version (1 failure),Using LF rather an CRLF (15 failures)Strange /j_security_check test (2 failures)Missing annotation marker in Java 8 signature tests (1 failure)Re-do Java 11 signature test based on Java 8Fix regression in error page tests (1 failure)Java 11 issues with HTTP/2 client",../data/confluence_exports/TOMCAT/Servlet-TCK-5.0_158863856.html
Apache Tomcat : TomcatGridScreenshots Screenshots,alpha1,../data/confluence_exports/TOMCAT/TomcatGridScreenshots_103100565.html
Apache Tomcat : TomcatGridScreenshotsAlpha1 The Web Manager dashboard page shows the status of the grid,TomcatGridWebManager.png!,../data/confluence_exports/TOMCAT/TomcatGridScreenshotsAlpha1_103100566.html
Apache Tomcat : TomcatGridScreenshotsAlpha1 The CLI can also show the status of the grid as,TomcatGridCliManager1.png!,../data/confluence_exports/TOMCAT/TomcatGridScreenshotsAlpha1_103100566.html
Apache Tomcat : TomcatGridScreenshotsAlpha1 A few operations using the CLI Manager,TomcatGridCliManager2.png!,../data/confluence_exports/TOMCAT/TomcatGridScreenshotsAlpha1_103100566.html
Apache Tomcat : HowTo SSLCiphers Using Java implementation,"For BIO and NIO connectors the attribute that specifies the list of ciphers is calledciphersand multiple values are separated by a comma (,). For the list of possible values see the list of cipher suite names for your version of Java, e.g.  Oracle Java 6Oracle Java 7  See thread ""Default SSL ciphers supported by Tomcat 6"" from October 2009herefor a short program that displays available ciphers in your particular JVM.  Sample configuration:  ciphers=""SSL_RSA_WITH_RC4_128_MD5,
           SSL_RSA_WITH_RC4_128_SHA,
           TLS_ECDHE_ECDSA_WITH_RC4_128_SHA,
           TLS_ECDHE_RSA_WITH_RC4_128_SHA,
           TLS_ECDH_ECDSA_WITH_RC4_128_SHA,
           TLS_ECDH_RSA_WITH_RC4_128_SHA""",../data/confluence_exports/TOMCAT/HowTo-SSLCiphers_103099295.html
Apache Tomcat : HowTo SSLCiphers Using OpenSSL implementation (APR connector),"For APR connector the attribute that specifies the list of ciphers is calledSSLCipherSuiteand multiple values are separated by a colon (:). Generally, it is configured in the same way asSSLCipherSuitedirective ofmod_ssl of Apache HTTPD server. For the list of possible values seeOpenSSL documentation, or runopenssl.exe ciphers -v.  Sample configurations:  a)  SSLCipherSuite=""RSA:!EXP:!NULL:+HIGH:+MEDIUM:-LOW""  b)  SSLCipherSuite=""RC4-SHA""   CategoryFAQ",../data/confluence_exports/TOMCAT/HowTo-SSLCiphers_103099295.html
Apache Tomcat : JNDI HowTo Lotus Domino,"<Realm
   className=""org.apache.catalina.realm.JNDIRealm"" 
   debug=""99""
   connectionURL=""ldap://ldap.company.com:389""
   roleSearch=""(member={0})"" 
   roleName=""cn""
   userSearch=""(cn={0})"" />",../data/confluence_exports/TOMCAT/JNDI-HowTo_103099448.html
Apache Tomcat : JNDI HowTo Oracle Internet Directory (OID),"<Realm
   className=""org.apache.catalina.realm.JNDIRealm"" 
   debug=""99""
   connectionURL=""ldap://ldap.company.com:389""
   roleBase=""cn=Groups,dc=company,dc=com""
   roleSearch=""(uniquemember={0})"" 
   roleName=""cn""
   userBase=""cn=Users,dc=company,dc=com""
   userSearch=""(uid={0})"" />",../data/confluence_exports/TOMCAT/JNDI-HowTo_103099448.html
Apache Tomcat : JNDI HowTo Microsoft Active Directory,"<Realm
   className=""org.apache.catalina.realm.JNDIRealm"" 
   debug=""99""
   connectionURL=""ldap://ldap.company.com:3268""
   authentication=""simple""
   referrals=""follow""
   connectionName=""cn=LDAPUser,ou=Service Accounts,dc=company,dc=com""
   connectionPassword=""VerySecretPassword"" 
   userSearch=""(sAMAccountName={0})""
   userBase=""dc=company,dc=com"" 
   userSubtree=""true""
   roleSearch=""(member={0})"" 
   roleName=""cn"" 
   roleSubtree=""true""
   roleBase=""dc=company,dc=com"" /> The port number (3268) connects to the global catalog. This is important because 389 might throw errors when getting referrals. (It sends a referral to ldap://company.com:389.) With this setting we don't have to change anything in /etc/hosts. For a reference on startTLS, see Self:JNDI_startTLs_HowTo",../data/confluence_exports/TOMCAT/JNDI-HowTo_103099448.html
Apache Tomcat : HowTo How do I add a question to this page?,"Anyone may edit this page to add their own content. That is why this page is part of a Wiki and not a hardcoded static file in the FAQ. However,do notadd questions without answers to this page. If you have a question about how to do something in Tomcat which has not been addressed yet, ask thetomcat-user list. Once you've figured out how to fix your problem, come back and update the Wiki to allow the rest of us to benefit from what you've learned!",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I contribute to Tomcat's documentation?,"Download the source bundle or grab the source files from TomcatGit repository(atGitHub). The docs are in thewebapps/docssubdirectory. They are in XML format and get processed into the HTML documentation as part of the Tomcat release. Edit the documentation XML file(s) as you wish. The xdocs format is self-explanatory: use normal HTML markup, and add<section>or<subsection>tags as you see fit. Look at the existing docs as examples. Make sure you use valid XML markup. If you're interested in previewing your changes, you will need to follow the directions for building Tomcat yourself. The docs will be generated in theoutput/build/webapps/docsdirectory. Open a Bugzilla enhancementitem with the explanation of your enhancements, and attach agit diffordiff -uformat of your patch, or create a Pull Requestat GitHub. We will evaluate and commit your patch as needed. Note, that the Tomcat web site is updated with every release, so that documentation changes will not be visible until next Tomcat release. It is possible to view documentation for unreleased versions of Tomcat 7, Tomcat 8.5 and Tomcat 9 that is published by ASF Buildbot. See links on thebuildbotpage on Apache Tomcat web site.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I set up and run Tomcat on Macintosh OS X?,SeeTomcatOnMacOS,../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I set up and run Tomcat on Solaris 10?,SeeTomcatOnSolaris10,../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I set up and run Tomcat on OpenVMS?,SeeTomcatOnOpenVMS,../data/confluence_exports/TOMCAT/HowTo_103099265.html
"Apache Tomcat : HowTo How do I set up another tomcat service on Windows, sharing the same Tomcat Home ?","To install another Tomcat service using separate Home (binaries) and Base (configuration) paths you can use theservice.batscript provided by Apache Tomcat. If your installation of Apache Tomcat does not have aservice.batscript (in thebindirectory), you can get one from a zip distributive for that version. To install the service: Set environment variablesCATALINA_HOME,CATALINA_BASE andJAVA_HOME(orJRE_HOME) as usual, as documented inRUNNING.txtfile.Call theservice.batscript to install the service, as shown in theWindows Service How-Toin Tomcat documentation.service.bat install NewServiceName--rename",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I install Tomcat as a service under Unix?,"Create a shell program to start Tomcat automatically. Each UNIX varies in how it starts up automatic services, but there are two main variants: BSD::In a typical BSD system, there are a series of start up scripts in/etcstarting withrc.. Look for, or create, a file called/etc/rc.localand enter the appropriate instructions to start up Tomcat there as a shell script. System V::In a typical UNIX System V setup, there is a directory containing startup scripts, and other directories which contain links to these startup scripts. Create the appropriate startup script for your setup, then create the appropriate links. For more information on each, check your system documentation. It also makes a lot of sense to use theJavaServiceWrapper.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How to run Tomcat without root privileges?,"The best way is to use jsvc, available as part of theApache Commons Daemonproject.  Other way is to put Apache httpd with mod_jk before your Tomcat servers, and use ports >=1024 in the Tomcat(s). However, if httpd is not needed for some other reason, this is the most inefficient approach.  An other method is to use SetUID scripts (assuming you have the capability) to do this. Here's how I do it. Create a file calledfoo.cwith this content (replace ""/path/startupscript"" with the tomcat startup script): foo.c#include <unistd.h>
#include <stdlib.h>
int main( int argc, char *argv[] ) {
  if ( setuid( 0 ) != 0 ) {
    perror( ""setuid() error"" );
    return 1;
  }
  printf( ""Starting ${APPLICATION}\n"" );
  execl( ""/bin/sh"", ""sh"", ""/path/startupscript"", 0 );
  return 0;
} Run the following as root (replacing tmp with whatever you want the startup script to be and replacing XXXXX with whatever group you want to be able to start and stop tomcat: gcc tmp.c -o tmp
chown root:XXXXX tmp
chmod ugo-rwx tmp
chmod u+rwxs,g+rx tmp Now members of the tomcat group should be able to start and stop tomcat. One caveat though, you need to ensure that that your tomcat startup script is not writable by anyone other than root, otherwise your users will be able to insert commands into the script and have them run as root (very big security hole).  An other way is to use Iptables to redirect Port 80 and 443 to user ports (>1024) /sbin/iptables -A FORWARD -p tcp --destination-port 443 -j ACCEPT
/sbin/iptables -t nat -A PREROUTING -j REDIRECT -p tcp --destination-port 443 --to-ports 8443
/sbin/iptables -A FORWARD -p tcp --destination-port 80 -j ACCEPT
/sbin/iptables -t nat -A PREROUTING -j REDIRECT -p tcp --destination-port 80 --to-ports 8080
/sbin/iptables-save or /etc/init.d/iptables save If you'd like ""localhost:443"" to also redirect to ""localhost:8443"", you'll need this command as well: /sbin/iptables -t nat -A OUTPUT -p tcp -o lo -destination-port 443 -j REDIRECT --to-ports 8443 Also note that if you have existing rules defined in your chains, you will need to make sure that the rules above are not ruled-out by another rule when using -A to add the above rules. For example, if you have an existing FORWARD rule that says ""-j REJECT"" than adding the FORWARD rule after it will have no effect.  BSD-based Unix systems such as Mac OS X use a tool similar to iptables, called ipfw (for Internet Protocol Fire Wall). This tool is similar in that it watches all network packets go by, and can apply rules to affect those packets, such as ""port-forwarding"" from port 80 to some other port such as Tomcat's default 8080. The syntax of the rules is different than iptables, but the same idea. For more info, google and read the man page. Here is one possible rule to do the port-forwarding: sudo ipfw add 100 fwd 127.0.0.1,8080 tcp from any to any 80 in   Yet another way is to use authbind package (part of Debian- and CentOS based distributions) which allows a program that would normally require superuser privileges to access privileged network services to run as a non-privileged user.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How to create native launchers for Tomcat,SeeTomcatCreateNativeLaunchers,../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I rotate catalina.out?,"Honestly, the first question is ""why are you rotating catalina.out""? Tomcat logs very little to catalina.out so the usual culprit is web applications that stupidly send output to System.out or System.err. If that's the case, what you ought to do is set swallowOutput=""true"" on the application's <Context> configuration. That will send the output to a file configured (default) by conf/logging.properties. Once you've done that, get the application fixed to use a real logger, or at least use ServletContext.log(). If you've decided that you still absolutely positively need to rotate catalina.out, there is something that you have to understand: catalina.out is created by your shell's output redirection, just like when you type ""ls -l > dir_listing.txt"". So rotating the file needs to be done carefully. You can't just re-name the file or you'll find that Tomcat will continue logging to the file under the new name. You also can't delete catalina.out and re-create it, or you'll never get anything logged to catalina.out after that, unless you restart Tomcat. There are really only two ways to properly rotate catalina.out, and they both have downsides.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo Rotate catalina.out using logrotate (or similar),"To use a tool likelogrotate, you'll want to use the ""copytruncate"" configuration option. This will copy catalina.out to another file (like catalina.out.datestamp) and then truncates catalina.out to zero-bytes. There is a major downside to this if catalina.out is seeing a lot of action: some log messages written to the log file during the copy/truncate procedure may be lost.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo Rotate catalina.out using rotatelogs or chronolog (or similar),"To use a tool like Apache httpd'srotatelogsorchronolog, you'll have to modify Tomcat's catalina.sh (or catalina.bat) script to change the output redirection from a redirect to a pipe. The existing code in catalina.sh looks like this: eval ""\""$_RUNJAVA\"""" ""\""$LOGGING_CONFIG\"""" $LOGGING_MANAGER $JAVA_OPTS $CATALINA_OPTS \
      -Djava.endorsed.dirs=""\""$JAVA_ENDORSED_DIRS\"""" -classpath ""\""$CLASSPATH\"""" \
      -Djava.security.manager \
      -Djava.security.policy==""\""$CATALINA_BASE/conf/catalina.policy\"""" \
      -Dcatalina.base=""\""$CATALINA_BASE\"""" \
      -Dcatalina.home=""\""$CATALINA_HOME\"""" \
      -Djava.io.tmpdir=""\""$CATALINA_TMPDIR\"""" \
      org.apache.catalina.startup.Bootstrap ""$@"" start \
      >> ""$CATALINA_OUT"" 2>&1 ""&"" You'll need to change that to something which looks more like this: eval ""\""$_RUNJAVA\"""" ""\""$LOGGING_CONFIG\"""" $LOGGING_MANAGER $JAVA_OPTS $CATALINA_OPTS \
      -Djava.endorsed.dirs=""\""$JAVA_ENDORSED_DIRS\"""" -classpath ""\""$CLASSPATH\"""" \
      -Djava.security.manager \
      -Djava.security.policy==""\""$CATALINA_BASE/conf/catalina.policy\"""" \
      -Dcatalina.base=""\""$CATALINA_BASE\"""" \
      -Dcatalina.home=""\""$CATALINA_HOME\"""" \
      -Djava.io.tmpdir=""\""$CATALINA_TMPDIR\"""" \
      org.apache.catalina.startup.Bootstrap ""$@"" start \
      | ""$PATH_TO_CHRONOLOG"" $CATALINA_BASE/logs/catalina.out.%Y-%m-%d This will be somewhat similar for catalina.bat, but the actual launch command will look different. Also note that there are currently two places in catalina.sh (and catalina.bat) where Tomcat is launched, depending upon whether you are using a security manager or not. You should read the whole catalina.sh (or catalina.bat) file to make sure you have handled every case where Tomcat is launched.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I set up multiple sites sharing the same war application/war file?,SeeCreateVirtualHosts,../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I log requests ?,SeeAccessLogValve,../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo Can I set Java system properties differently for each webapp?,"No. If you can edit Tomcat's startup scripts (or better create asetenv.shfile), you can add ""-D"" options to Java. But there is no way in Java to have different values of system properties for different classes in the same JVM. There are some other methods available, like usingServletContext.getContextPath()to get the context name of your web application and locate some resources accordingly, or to define<context-param>elements inWEB-INF/web.xmlfile of your web application and then set the values for them in Tomcat context file (META-INF/context.xml). Seehttps://tomcat.apache.org/tomcat-9.0-doc/config/context.html.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I configure Tomcat Connectors?,"On the Tomcat FAQ, there is a list of Other Resources which should have information pointing you to the relevant pages. Each connector has its own configuration, and its own set up. Check them for more information. In particular, here are a number of locations for Tomcat Connectors: Tomcat Connectors DocumentationTomcat Connectors FAQConfiguring Tomcat Connectors for ApacheConnectors How ToAJP Connector in Tomcat 9 Configuration Reference The followingexcellentarticle was written by Mladen Turk. He is a Developer and Consultant for JBoss Inc in Europe, where he is responsible for native integration. He is a long time commiter for Jakarta Tomcat Connectors, Apache Httpd and Apache Portable Runtime projects. Fronting Tomcat with Apache or IIS - Best Practiceshttps://people.apache.org/~mturk/docs/article/ftwai.html Over the time several different connectors have been built, and some connector projects have been abandoned (so beware of old documentation).",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I configure Tomcat to work with IIS and NTLM?,SeeTomcatNTLM,../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo Setting up SSL,"Threads from thetomcat-user list Using VeriSign: https://marc.info/?l=tomcat-user&m=106285452711698&w=2https://marc.info/?l=tomcat-user&m=107584265122914&w=2 Using OpenSSL: https://marc.info/?l=tomcat-user&m=106293430225790&w=2https://marc.info/?l=tomcat-user&m=106453566416102&w=2https://marc.info/?l=tomcat-user&m=106621232531781&w=2 A description of ""what SSL is all about anyway"": https://marc.info/?l=tomcat-user&m=106692394104667&w=2",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo HowTo SSL Client Authentication with Fallback to FORM Authentication,SeeSSLWithFORMFallback,../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I restrict the list of SSL ciphers used for HTTPS,SeeHowTo SSLCiphers.,../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I make Tomcat startup faster?,SeeHowTo FasterStartUp,../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I override the default home page loaded by Tomcat?,"After successfully installing Tomcat, you usually test it by loadinghttp://localhost:8080. It is quite easy to override that page. Inside$TOMCAT_HOME/conf/web.xmlthere is a section called<welcome-file-list>and it looks like this: <welcome-file-list>
        <welcome-file>index.html</welcome-file>
        <welcome-file>index.htm</welcome-file>
        <welcome-file>index.jsp</welcome-file>
    </welcome-file-list> The default servlet attempts to load theindex.*files in the order listed. You may easily override theindex.jspfile by creating an index.html file at$TOMCAT_HOME/webapps/ROOT. It's somewhat common for that file to contain a new static home page or a redirect to a servlet's main page. A redirect would look like: <html>

<head>
<meta http-equiv=""refresh"" content=""0;URL=http://mydomain.com/some/path/to/servlet/homepage/"">
</head>

<body>
</body>

</html> This change takes effect immediately and does not require a restart of Tomcat.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I enable Server Side Includes (SSI)?,Seehttps://tomcat.apache.org/tomcat-7.0-doc/ssi-howto.html,../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo Tomcat 5.5,"If you install Tomcat 5.5 binaries, the Administration web app is not bundled with it; this describes how to add the Administration web app to your Tomcat 5.5 installation. (Tomcat 4.1 comes with the Administration web app as part of the binary). The following refers to a Tomcat 5.5 set up on Windows 2000, so your path names will be different on *nix platforms. In this example, Tomcat 5.5.17 in installed inc:\Program Files\Apache Software Foundation\Tomcat 5.5(this is myCATALINA_HOME). Unzip or untar (be careful to use GNU tar) the file containing the administration web app files (eg.apache-tomcat-5.5.17-admin.zip) to a temporary directory, eg.c:\temp.Copyc:\temp\apache-tomcat-5.5.17\conf\Catalina\localhost\admin.xmlto the directoryc:\Program Files\Apache Software Foundation\Tomcat 5.5\conf\Catalina\localhost.Copy the entire directory treec:\temp\apache-tomcat-5.5.17\server\webapps\admin to the directoryc:\Program Files\Apache Software Foundation\Tomcat 5.5\server\webapps. This is an overlay, so \server\webapps is just pointing you to the \server\webapps, and the admin directory with its contents will be the only thing you see added there. Add a line to yourc:\Program Files\Apache Software Foundation\Tomcat 5.5\conf\tomcat-users.xmlfile so that you have a user who hasadminrole. For example, add this line just before the last line (containing</tomcat-users>) of the file:<user username=""admin"" password=""makesomethingup"" roles=""admin,manager""/>Restart Tomcat.Now when you visithttp://localhost:8080/adminyou should see a page that asks for a user name and password. If you still see the ""no longer loaded"" error message in your browser, you must either force a full reload of the web page (in Firefox, hold down Shift key while clicking on the Reload button) or just restart your browser completely.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo Tomcat 6.0 and later,"Development of Administration web app was ceased and it is no longer provided for Tomcat 6.0 and later versions. An alternative is to use 3-rd party applications, such as PSI Probe. SeeAddOnspage for links.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I add JARs or classes to the common classloader without adding them to $CATALINA_HOME/lib?,"Either a) Configure Tomcat to run with separate$CATALINA_BASEand$CATALINA_HOMEdirectories (as documented inRUNNING.txt), and place your JARs and classes into$CATALINA_BASE/lib, or b) Edit the filecatalina.propertiesunder$CATALINA_BASE/conf; there is a property calledcommon.loaderto which you can add additional paths to find JARs or classes for the common classloader.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I authenticate Manager access via JNDI to Active Directory for multiple Tomcat instances?,"ADS insists that the CN of every group be unique, but the Manager app. always uses the group CN=manager. The default can be changed, but it's hard to find and you have to do it over every time you upgrade. Instead, pick an attribute other than the common name – for example, ""description"" – that doesn't have to be unique, name it as theRoleNameattribute of theRealm(in server.xml, which you'll be editing anyway), and set that attribute to ""manager"" in each group you create. Create an OU for each Tomcat instance's groups and give that OU's DN as theRoleBasein that instance's server.xml. Create a uniquely-named group in each instance's OU with the chosen attribute (""description"" for example) set to ""manager"".",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo Where and how do I set memory related settings to improve Tomcat performance?,"When your web application is using large memory as this memory size default setting can be too small. One way to address this problem is to set a larger heap size. If you start Tomcat by using the standardscript files(such asCATALINA_HOME/bin/catalina.batorcatalina.sh), this can be done by settingCATALINA_OPTSenvironment variable. The recommended way to do so is to create asetenv.batorsetenv.shfile, — readRUNNING.txtfor details. Let say you want to increase it to 256 MB (as you required but make sure you have enough amount of physical memory/RAM and for 32bit system, use no more than 1.0-1.1 GB heap space size ). Set theCATALINA_OPTSto the value of-Xms256m -Xmx256m. In some cases it is better to set slightly lower size for-Xms. Forsetenv.batuse the following line: set CATALINA_OPTS=-Xms256m -Xmx256m Forsetenv.shuse the following: CATALINA_OPTS='-Xms256m -Xmx256m' There are other parameters that can be added, depending on your application and requirements, e.g:'-XX:MaxPermSize'. For other parameters, look at the following pages: FAQ/MemoryOutOfMemory If you are running Tomcat as aWindows service, then environment variables andsetenv.batscript have no effect. The relevant settings for the service wrapper application are stored in the Windows registry. They can be edited via Configuration application (tomcat<N>w.exe). See ""Java"" tab in the configuration dialog. The{{-Xms}} and-Xmxoptions are configured in fields named ""Initial memory pool"" and ""Maximum memory pool"". Other options can be added to ""Java Options"" field as if they were specified on the command line of java executable.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I make my web application be the Tomcat default application?,"Congratulations. You have created and tested a first web application (traditionally called ""mywebapp""), users can access it via the URL ""http://myhost.company.com/mywebapp"". You are very proud and satisfied. But now, how do you change the setup, so that ""mywebapp"" gets called when the user enters the URL ""http://myhost.company.com"" ? The pages and code of your ""mywebapp"" application currently reside in (CATALINA_BASE)/webapps/mywebapp/. In a standard Tomcat installation, you will notice that under the same directory (CATALINA_BASE)/webapps/, there is a directory calledROOT(the capitals are important, even under Windows). That is the residence of thecurrentTomcat default application, the one that is called right now when a user calls up ""http://myhost.company.com[:port]"". The trick is to put your application in its place. First stop Tomcat.Then before you replace the current default application, it may be a good idea to make a copy of it somewhere else.Then delete everything under the ROOT directory, and move everything that was previously under the (CATALINA_BASE)/webapps/mywebapp/ directory, toward this (CATALINA_BASE)/webapps/ROOT directory. In other words, what was previously .../mywebapp/WEB-INF should now be .../ROOT/WEB-INF (and not .../ROOT/mywebapp/WEB-INF). Just by doing this, you have already made you webapp into the Tomcatdefault webapp. Restart Tomcat and you're done.Call up ""http://myhost.company.com/"" and enjoy. Addendum 1: If you are deploying your application as a war file.. The above instructions relate to the situation where you are ""manually"" deploying your application as a directory-and-files structure under the /webapps directory. If instead you are using the ""war"" method to deploy your application, the principle is about the same : delete the ROOT directoryname your war file ""ROOT.war"" (capitals mandatory)drop the ROOT.war file directly in the /webapps directory.Tomcat will automatically deploy it. For more information about this topic in general, consult this page :""Configuration Reference / Context"" Addendum 2: If for some reason you want another method.. If, for some reason, you do not want to deploy your application under the CATALINA_BASE/webapps/ROOT subdirectory, or you do not want to name your war-file ""ROOT.war"", then read on. But you should first read this :""Configuration Reference / Context""and make sure you understand the implications. The method described above is the simple method. The two methods below are more complex, and the second one has definite implications on the way you manage and run your Tomcat. Method 2.1 Place your war file outside of CATALINA_BASE/webapps (itmustbe outside to prevent double deployment).Place a context file named ROOT.xml in CATALINA_BASE/conf/<engine name>/<host name>. The single <Context> element in this context file MUST have adocBaseattribute pointing to the location of your war file. The path element should not be set - it is derived from the name of the .xml file, in this case ROOT.xml. See the Context Container above for details. Method 2.2 If you really know what you are doing.. leave your war file in CATALINA_BASE/webapps, under its original nameturn off autoDeployanddeployOnStartup in your Host element in the server.xml file.explicitly defineallapplication Contexts in server.xml, specifying both path and docBase. You must do this, because you have disabled all the Tomcat auto-deploy mechanisms, and Tomcat will not deploy your applications anymore unless it finds their Context in the server.xml. Note that this last method also implies that in order to make any change to any application, you will have to stop and restart Tomcat.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I set up Tomcat virtual hosts in a development environment?,SeeTomcatDevelopmentVirtualHosts,../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I get started with a simple cluster?,SeeClusteringOverview,../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do call tomcat ant tasks to deploy webapps?,SeeAntDeploy,../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I load a properties file?,"Here are the three most popular ways:: Use a classloader'sgetResource()method to get an url to the properties file and load it into the Properties. The properties file must be located within the webapp classpath (i.e. eitherWEB-INF/classes/...or in a jar inWEB-INF/lib/). A challenge is to get the classloader when you are in a static initializer: public class Config {
     private static java.util.Properties prop = new java.util.Properties();
     private static loadProperties() {
          // get class loader
          ClassLoader loader = Config.class.getClassLoader();
          if(loader==null)
            loader = ClassLoader.getSystemClassLoader();

          // assuming you want to load application.properties located in WEB-INF/classes/conf/
          String propFile = ""conf/application.properties"";
          java.net.URL url = loader.getResource(propFile);
          try{prop.load(url.openStream());}catch(Exception e){System.err.println(""Could not load configuration file: "" + propFile);}
     }

     //....
     // add your methods here. prop is filled with the content of conf/application.properties

     // load the properties when class is accessed
     static {
        loadProperties();
     }
  } This method even works in a standalone java application. So it is my preferred way. Use aResourceBundle. See the Java docs for the specifics of how theResourceBundleclass works. Using this method, the properties file must go into theWEB-INF/classesdirectory or in a jar file contained in theWEB-INF/libdirectory.Another way is to use the methodgetResourceAsStream()from theServletContextclass. This allows you update the file without having to reload the webapp as required by the first method. Here is an example code snippet, without any error trapping: // Assuming you are in a Servlet extending HttpServlet
// This will look for a file called ""/more/cowbell.properties"" relative
// to your servlet Root Context
InputStream is = getServletContext().getResourceAsStream(""/more/cowbell.properties"");
Properties  p  = new Properties();
p.load(is);
is.close();",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I share sessions across web apps?,"You cannot share sessions directly across web apps, as that would be a violation of the Servlet Specification. There are workarounds, including using a singleton class loaded from the common classloader repository to hold shared information, or putting some of this shared information in a database or another data store. Some of these approaches have been discussed on thetomcat-user mailing list, whose archives you should search for more information. Sharing sessions across containers for clustering or replication purposes is a different matter altogether.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How can I access members of a custom Realm or Principal?,"When you create a custom subclass ofRealmBaseorGenericPrincipaland attempt to use those classes in your webapp code, you'll probably have problems withClassCastException. This is because the instance returned byrequest.getUserPrincipal()is of a class loaded by the server's classloader, and you are trying to access it through you webapp's classloader. While the classes maybe otherwise exactly the same, different (sibling) classloaders makes them different classes. This assumes you created aMyPrincipalclass, and put in Tomcat's server/classes (or lib) directory, as well as in your webapp's webinf/classes (or lib) directory. Normally, you would put custom realm and principal classes in the server directory because they depend on other classes there. Here's what you would like to do, but it throwsClassCastException: MyPrincipal p = request.getUserPrincipal();
String emailAddress = p.getEmailAddress(); Here are 4 ways you might get around the classloader boundary: 1)Reflection Principal p = request.getUserPrincipal();
String emailAddress = p.getClass().getMethod(""getEmailAddress"", null).invoke(p, null); 2)Move classes to a common classloader You could put your custom classes in a classloader that is common to both the server and your webapp - e.g., either the ""common"" or bootstrap classloaders. To do this, however, you would also need to move the classes that your custom classes depend on up to the common classloader, and that seems like a bad idea, because there a many of them and they a core server classes. 3)Common Interfaces Rather than move the implementing custom classes up, you could define interfaces for your customs classes, and put the interfaces in the common directory. You're code would look like this: public interface MyPrincipalInterface extends java.security.Principal {
  public String getEmailAddress();
}

public class MyPrincipal implements MyPrincipalInterface {
...
  public String getEmailAddress() {
    return emailAddress;
  }
}

public class MyServlet implements Servlet {
  protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {
    MyPrincipalInterface p = (MyPrincipalInterface)request.getUserPrincipal();
    String emailAddress = p.getEmailAddress();
...
} Notice that this method gives you pretty much the webapp code you wanted in the first place 4)Serializing / Deserializing You might want to try serializing the response ofrequest.getUserPrincipal()and deserialize it to an instance of webapp's MyPrincipal.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I get direct access to a Tomcat Realm?,"Credit: This code is from a post by Yoav Shapirahttps://www.yoavshapira.com/in the user list Sometimes access directly into the Tomcat realm object is needed; to do, this the following code can be used. Be aware, however, that by using this, your application is relying on a Tomcat extension and is therefore non-standard. Note that in order for this to work the Context of the web application in question needs to have its privileged attribute set to ""true"", otherwise web apps do not have access to the Tomcat classes. Server server = ServerFactory.getServer();
//Note, this assumes the Container is ""Catalina""
Service service = server.findService(""Catalina"");
Engine engine = (Engine) service.getContainer();
Host host = (Host) engine.findChild(engine.getDefaultHost());
//Note, this assumes your context is ""myContext""
Context context = (Context) host.findChild(""myContext"");
Realm realm = context.getRealm(); Warning:The above recipe on how to obtain aContextfor a web application is a bit obsolete and does not work in Tomcat 7 and later (as Server is no longer a singleton). There are other ways to achieve that. An easy one is to add aValveorListenerto a context, as those classes have access to Tomcat internals. There may be other ways mentioned in the archives of theusers mailing list.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I redirect System.out and System.err to my web page?,"I have met a situation where I needed to redirect a portion of standard ouput (System.out, STDOUT) and standard error (System.err, STDERR) to my web page instead of a log file. An example of such an application is a compiler research platform that our resarch team is putting online for anybody to be able to quickly compile-test their programs on line. Naturally, the compilers dump some of their stuff to STDERR or STDOUT and they are not web application.jar. Thus, I needed badly these streams related to the compiler output to be redirected to my web editor interface. Having found no easy instructions on how to do that lead me writing up this quick HOWTO. The HOWTO is based on Servlets, but similar arrangements can be done for JSPs. The below example shows the essentials, with most non-essentials removed. public class WebEditor extends HttpServlet
{
 ...
        public void doGet
        (
                HttpServletRequest poHTTPRequest,
                HttpServletResponse poHTTPResponse
        )
        throws IOException, ServletException
        {
                poHTTPResponse.setContentType(""text/html"");

                ServletOutputStream out = poHTTPResponse.getOutputStream();

                out.println(""<html>"");
                out.println(""<body>"");
                out.println(""<head>"");
                out.println(""<title>WebEditor Test $Revision: 1.6 $</title>"");
                out.println(""</head>"");
                out.println(""<body>"");
                out.println(""<h3>WebEditor Test $Revision: 1.6 $</h3>"");
                out.println(""<hr />"");

                // Backup the streams
                PrintStream oStdOutBackup = System.out;
                PrintStream oStdErrBackup = System.err;

                try {

                  // Redired STDOUT and STDERR to the ServletOutputStream
                  System.setOut(new PrintStream(out));
                  System.setErr(new PrintStream(out));

                  try {
                        // ... call compiler here that produces
                        // tons of STDOUT/STDERR messages ...
                  } catch(Exception e) {
                        out.println(e);
                  }

                } finally {

                  // Restore original STDOUT and STDERR
                  System.setOut(oStdOutBackup);
                  System.setErr(oStdErrBackup);

                }

                out.println(""<hr />"");
                out.println(""</body>"");
                out.println(""</html>"");
        }
} A few caveats arise, as for instance while theSystem.outandSystem.errare redirected as per above, no logging of these is done to files. You will need more legwork to do to make the additional logging. It is important to backup and restore the original streams as the above example does. The servlet should not be used to process several requests in parallel (some synchronization should be added to the above code to prevent that). Also, notice the use ofgetOutputStream(): when this method is called, thegetWriter()method can no longer be used in the same response object. Corrections and comments are most welcome!",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I connect to a Websphere MQ (MQ Series) server using JMS and JNDI?,"Basically, this works just as described inhttps://tomcat.apache.org/tomcat-9.0-doc/jndi-resources-howto.html:Within your application, you are using the standard JNDI and JMS API calls. In web.xml (the container independent application descriptor), you specify resource references (stub resources). And in context.xml (the container specific application descriptor), you are actually configuring the JMS connection. More to the point. Here's some example code, which might be added to a Servlet. The example is sending a message to an MQ server: import javax.jms.Queue;
    import javax.jms.QueueConnection;
    import javax.jms.QueueConnectionFactory;
    import javax.jms.QueueSender;
    import javax.jms.QueueSession;
    import javax.jms.Session;
    import javax.jms.TextMessage;
    import javax.naming.Context;
    import javax.naming.InitialContext;

    Context ctx = (Context) new InitialContext().lookup(""java:comp/env"");
    QueueConnectionFactory qcf = (QueueConnectionFactory) ctx.lookup(""jms/MyQCF"");
    QueueConnection qc = qcf.createQueueConnection();
    Queue q = (Queue) ctx.lookup(""jms/MyQ"");
    QueueSession qs = qc.createQueueSession(false, Session.AUTO_ACKNOWLEDGE);
    TextMessage tm = qs.createTextMessage();
    tm.setText(""Hi, there!"");
    QueueSender sender = qc.createSender();
    sender.send(tm);
    sender.close();
    qs.close();
    qc.close(); Note the following: I have intentionally omitted proper resource handling. For example, one ought to ensure that qc.close() is always called by using a try { .. } finally { ..} block.The code contains absolutely no references to com.ibm.mq*.jar.There are only two items, which need configuration: ""jms/MyQCF"", and ""jms/MyQ"". We'll find them again in web.xml, and context.xml. We have now written the code. Additionally, our web application needs the following files, and directories: +--META-INF
    |  +--- context.xml
    +--WEB-INF
       +--- web.xml
       +--- lib
            +--- com.ibm.mq.jar
            +--- com.ibm.mqjms.jar
            +--- connector.jar
            +--- dhbcore.jar
            +--- geronimo-j2ee-management_1.0_spec-1.0.jar
            +--- geronimo-jms_1.1_spec-1.0.jar The application descriptor web.xml looks just the same as usual, with the exception of the following lines: <resource-env-ref>
    <resource-env-ref-name>jms/MyQCF</resource-env-ref-name>
    <resource-env-ref-type>javax.jms.QueueConnectionFactory</resource-env-ref-type>
  </resource-env-ref>

  <resource-env-ref>
    <resource-env-ref-name>jms/MyQ</resource-env-ref-name>
    <resource-env-ref-type>javax.jms.Queue</resource-env-ref-type>
  </resource-env-ref> This is simply telling, that the items ""jms/MyQCF"", and ""jms/MyQ"" exist, and are instances of QueueConnectionFactory, and Queue, respectively. The actual configuration is in context.xml: <Resource
      name=""jms/MyQCF""
      auth=""Container""
      type=""com.ibm.mq.jms.MQQueueConnectionFactory""
      factory=""com.ibm.mq.jms.MQQueueConnectionFactoryFactory""
      description=""JMS Queue Connection Factory for sending messages""
      HOST=""<mymqserver>""
      PORT=""1414""
      CHAN=""<mychannel>""
      TRAN=""1""
      QMGR=""<myqueuemanager>""/>
   <Resource
      name=""jms/MyQ""
      auth=""Container""
      type=""com.ibm.mq.jms.MQQueue""
      factory=""com.ibm.mq.jms.MQQueueFactory""
      description=""JMS Queue for receiving messages from Dialog""
      QU=""<myqueue>""/> Basically, you just have to enter your values for <myqserver> (the WebSphere MQ servers host name), <mychannel> (the channel name), <myqueuemanager> (the queue manager name), and <myqueue> (the queue name). Both these values, the associated names (HOST, PORT, CHAN, ...), and their collection is truly MQ specific. For example, with ActiveMQ, you typically have a broker URL, and a broker name, rather than HOST, PORT, CHAN, ... The main thing to know (and the reason why I am writing this, because it took me some hours to find out): How do I know the property names, their meaning, and possible values? Well, there is an excellent manual, called ""WebSphere MQ Using Java"". It should be easy to find by entering the title into Google. The manual contains a section, called ""Administering JMS objects"", which describes the objects being configured in JNDI. But the most important part is the subsection on ""Properties"", which contains all the required details.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I use DataSources with Tomcat?,SeeUsingDataSources,../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I use Hibernate and database connection pooling with Tomcat?,SeeTomcatHibernate,../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I use DataSourceRealms for authentication and authorization?,SeeTomcatDataSourceRealms,../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo Tomcat crashed! What do I do now?,"These steps are in no particular order ... Read the Tomcat FAQRead the Tomcat RELEASE NOTES - there is something about Linux in itFirst look at the stack traces. I hope a stack trace was produced before the failure aborted the JVM process. After you get a few stack traces, see if a pattern appears. Trace back to source code if needed.Patch (orunpatch!) the operating system as needed.Patch (orunpatch!) the JVM (Java Virtual Machine).Linux Problem? - read the RELEASE NOTES!Look at commercial vendor support for other servlet engines. Sometimes the problem is universal regardless of servlet engine and may be a JVM/OS/application code issueSearch Google for web pages - maybe someone else had this problem. I'll bet they did.Search Google news groupsIf the JVM is from a commercial vendor, (eg: IBM, HP) check their release notes and news groupsUsing a database? Make sure JDBC type 4 drivers are used. Check their release notes.Tweak JVM memory parameters. Setting memory too high can be as bad as having memory too low. If your memory settings are set too high, Java 1.3 JVMs may freeze while waiting for the entire garbage collection to finish. Also if the JVM has too much memory, if may be starving other resources on the machine which are needed which may be causing unforeseen exceptions. In a nutshell, throwing more memory doesn't always solve the problem!Turn off the Java JIT compiler. See the Java Docs on how to do this.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo I'm encountering classloader problems when using JNI under Tomcat,"The important thing to know about using JNI under Tomcat is that one cannot place the native libraries OR their JNI interfaces under the WEB-INF/lib or WEB-INF/classes directories of a web application and expect to be able to reload the webapp without restarting the server. The class that calls System.loadLibrary(String) must be loaded by a classloader that is not affected by reloading the web application itself. Thus, if you have JNI code that follows the convention of including a static initilaizer like this: class FooWrapper {
    static {
        System.loadLibrary(""foo"");
    }

    native void doFoo();
} then both this class and the shared library should be placed in the$CATALINA_HOME/shared/libdirectory. Note that under Windows, you'll also need to make sure that the library is in thejava.library.path. Either add%CATALINA_HOME%\shared\libto your Windows PATH environment variable, or place the DLL files in another location that is currently on thejava.library.path. There may be a similar requirement for UNIX based system (I haven't checked), in which case you'd also have to add$CATALINA_HOME/shared/libto the PATH environment variable. (Note: I'm not the original author of this entry.) The symptom of this problem that I encountered looked something like this - java.lang.UnsatisfiedLinkError: Native Library WEB-INF/lib/libfoo.so already loaded in another classloader
        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1525) If theUnsatisfiedLinkErroris intermittent, it may be related to Tomcat's default session manager. It restored previous sessions at startup. One of those objects may load the JNI library. Try stopping the Tomcat JVM, deleting the SESSIONS.ser file, then starting Tomcat. You may consider changing the session persistence manager at this time. Note that Tomcat 6.0.14 the $CATALINA_HOME/shared/lib directory does not exist. You will need to add this and you will need to edit $CATALINA_HOME/conf/catalina.properties so that the shared.loader line looks like this shared.loader=$CATALINA_HOME/shared/lib",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I debug a Tomcat application?,"There is nothing magical about debugging a Tomcat application. All you need is an IDE and two environment variables. If you have not already done so begin by creating a new Tomcat context for your application. Navigate toTOMCAT_HOME\conf\Catalina\localhostand create a new file, say, myapp.xml. This will become part of your url, so to access your app you'll have to typehttp://localhost:8080/myapp.Enter the following in myapp.xml: <Context docBase=""c:/workspace/myapp/WebRoot"" /> This assumes you have a web application containing WEB-INF inc:/workspace/myapp/WebRootCreate two environment variables: C:\>set JPDA_ADDRESS=1044
C:\>set JPDA_TRANSPORT=dt_socket Now, you can launch Tomcat with these debug options: TOMCAT_HOME\bin\>catalina jpda start Use your IDE to connect to Tomcat through port 1044 See also:FAQ/Developing",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I debug a Tomcat application when Tomcat is run as a Windows service ?,"You can debug the tomcat service by editing the service parameters as follows. Launch a command promptSet the proper CATALINA_HOME environment variable: pointing to tomcat homeRun the following command: %CATALINA_HOME%\bin\tomcat6w.exe //ES//tomcat6 Select the Java tab in the properties dialog box,Add the following two lines to the Java Options text box: -Xdebug
-Xrunjdwp:transport=dt_socket,address=127.0.0.1:1044,server=y,suspend=n If you want to allow remote debugging, replace 127.0.0.1 by your server IP address. Click on ""Apply"" and close the dialog by clicking on ""OK""Restart the Apache Tomcat serviceUse your IDE to connect to Tomcat through port 1044 For IntelliJ IDEA you choose a remote debug target and set transport to ""socket"" and mode to ""attach"" , then you specify the host (127.0.0.1) and port (1044) See also:FAQ/Developing",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I check whether Tomcat is UP or DOWN? There is no status command,"Unfortunately, theorg.apache.catalina.util.ServerInfoclass does not determine if Tomcat is UP or DOWN. It is possible to do an HTTP GET on the root url but this is not accurate. In my case I sometimes use a regular Apache HTTPd to display a maintainence message while upgrading, etc. and using that method would give false positives. The correct way to do determine status is to parse the admin port from server.xml and see if we can connect to it. If we can then the Tomcat is UP otherwise it is DOWN. Here is my code to do this. Consider it public domain and use it as you see fit. Tomcat makes a note of this connection with something like this on the console. May 1, 2007 5:10:35 PM org.apache.catalina.core.StandardServer await
WARNING: StandardServer.await: Invalid command '' received Ideally this should be incorporated intoorg.apache.catalina.util.ServerInfoby some committer. In addition to the shutdown command they should add commands like status (UP or DOWN) and uptime in the await method oforg.apache.catalina.core.StandardServer import java.io.File;
import java.io.IOException;
import java.io.OutputStream;
import java.net.Socket;
import javax.xml.parsers.DocumentBuilderFactory;
import javax.xml.parsers.ParserConfigurationException;
import org.w3c.dom.Document;
import org.w3c.dom.Element;
import org.xml.sax.SAXException;

/**
 * Check to see if Tomcat is UP/DOWN.
 *
 * This parses the server.xml file for the Tomcat admin port and see if
 * we can connect to it. If we can, then the Tomcat is UP otherwise it
 * is DOWN
 *
 * It is invoked as follows:
 *    java -Dcatalina.base=c:/tomcat-6.0.10 CatalinaStatus
 *
 * It can also (optionally) shutdown the Tomcat by adding the shutdown
 * command line parameter as follows:
 *
 *    java -Dcatalina.base=c:/tomcat-6.0.10 CatalinaStatus shutdown
 *
 * @author Shiraz Kanga <skanga at yahoo.com>
 */
public class CatalinaStatus
{
  /**
   * Pathname to the server configuration file.
   */
  protected static String configFile = ""conf/server.xml"";
  protected static String serverShutdown;
  protected static int serverPort;

  /**
   * The application main program.
   *
   * @param args Command line arguments
   */
  public static void main (String args[])
  {
    Document configDom = getXmlDom (configFile ());
    parseDocument (configDom);
    // System.out.println (""Catalina.serverPort: "" + serverPort);
    // System.out.println (""Catalina.serverShutdown: "" + serverShutdown);

    // Stop the existing server
    try
    {
      Socket localSocket = new Socket (""127.0.0.1"", serverPort);
      System.err.println (""Server status:  UP"");
      if ((args.length > 0) && (args[0].equalsIgnoreCase (""shutdown"")))
      {
        System.out.println (""Tomcat shutdown initiated"" );
        doShutdown (localSocket);
      }

      localSocket.close ();
    }
    catch (IOException e)
    {
      System.err.println (""Server status:  DOWN"");
      System.exit(1);
    }
  }

  /**
   * Return a File object representing our configuration file.
   */
  protected static File configFile ()
  {
    File confFile = new File (configFile);
    if (!confFile.isAbsolute())
      confFile = new File (System.getProperty (""catalina.base""), configFile);
    return (confFile);
  }

  /**
   * Parses an XML file and returns a DOM document.
   */
  public static Document getXmlDom (File fileName)
  {
    try
    {
      // Create a builder factory
      DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance ();

      // Create the builder and parse the file
      Document doc = factory.newDocumentBuilder ().parse (fileName);
      return doc;
    }
    catch (SAXException e)
    {
      // A parsing error occurred; the xml input is not valid
      e.printStackTrace ();
    }
    catch (ParserConfigurationException e)
    {
      e.printStackTrace ();
    }
    catch (IOException e)
    {
      e.printStackTrace ();
    }
    return null;
  }

  /**
   * Extract the server port & shutdown command from the DOM
   */
  private static void parseDocument (Document configDom)
  {
    //get the root element which is Server Eg: <Server port=""8005"" shutdown=""SHUTDOWN"">

    Element docEle = configDom.getDocumentElement ();
    serverPort = Integer.parseInt (docEle.getAttribute (""port""));
    serverShutdown = docEle.getAttribute (""shutdown"");
  }

  /**
   * Send the shutdown command to the server
   */
  private static void doShutdown (Socket localSocket)
  {
    try
    {
      OutputStream outStream = localSocket.getOutputStream ();

      for (int i = 0; i < serverShutdown.length (); i++)
        outStream.write (serverShutdown.charAt (i));
      outStream.flush ();
      outStream.close ();
    }
    catch (IOException e)
    {
      System.out.println (""ERROR: I/O Exception during server shutdown."");
      e.printStackTrace ();
    }
  }
}",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I obtain a thread dump of my running webapp ?,"You can only get a thread dump of the entire JVM, not just your webapp. This shouldn't be a big deal, but should be made clear: you are getting a dump of all JVM threads, not just those ""for your application"", whatever that means. Getting a thread dump depends a lot on your environment. Please choose the section below that matches your environment best. The more universal and convenient options are presented first, while the more difficult ones or those for specific setups are provided later. Generally, you should start at the top of the list and work your way down until you find a technique that works for you.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo If you are running Oracle (Sun) JDK,"Oracle JDK (not the JRE) (formerly Sun JDK) since version 1.6 (and since 1.4 on *nix systems) ships with a program calledjstack(orjstack.exeon Microsoft Windows) which will give you a thread dump on standard output. Redirect the output into a file and you have your thread dump. You will need the process id (""pid"") of the process to dump. Use of the programjps(jps.exeon Microsoft Windows) can help you determine the pid of a specific Java process. SeeTools pagein JDK documentation for usage reference.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo If you are running on *NIX,"Send a SIGQUIT to the process. The thread dump will be sent to stdout which is likely to be redirected to CATALINA_BASE/logs/catalina.out. To send a SIGQUIT, usekill -3 <pid>from the command line.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo If you are running on Microsoft Windows,"You can try to use SendSignal, developed specifically for this purpose. Make sure you read the comments for certain sitautions (e.g. running as a service, RDP connections, etc.).http://www.latenighthacking.com/projects/2003/sendSignal/",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo If you are running Tomcat as a service on Microsoft Windows,"Tomcat service has a monitoring application with it. When it is running it puts an icon in the Windows system tray area. Right-click the icon, a menu will appear. Select ""Thread Dump"" command from the menu. It will cause thread dump to be printed to stdout. The service captures stdout into a log file (logs/tomcatNN-stdout.DATE.log). If the monitoring application is not running, you can start it manually. The command is Tomcat9w.exe //MS// or Tomcat9w.exe //MS//servicename If you installed Tomcat with an ""exe"" installer, ""Apache Tomcatversionservicename"" group in the Windows menu has shortcut ""Monitor Tomcat"" that starts the monitoring application. For details, seeWindows service pagein Tomcat documentation.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo If you have Tomcat running in a console,"*NIX: Press CRTL-\ Microsoft Windows: press CRTL-BREAK This will produce a thread dump on standard output, but may not be possible to capture to a file.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo Using Java code,"If you cannot use any of the above methods, it is also possible to obtain a thread dump programmatically, with a Servlet or JSP page that runs within Tomcat. For example, you can usejava.lang.Thread.getAllStackTraces()method. Tomcat Manager web application starting with Tomcat 7.0.58 / 8.0.0 supports a command that outputs a thread dump. (Tomcat 9 documentation,BZ 57261) StuckThreadDetectionValvevalve logs stacktraces of request processing threads that are busy for longer than configured time limit. It is available starting with Tomcat 6.0.36 / 7.0.14. (Tomcat 9 documentation)",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I read a Java thread dump ?,"Java thread dumps are just text files, so you can read them with any text editor. There are some tools that can make your life easier, especially if you need to look at more than one thread dump at once. One such tool is the Thread Dump Viewer (TDV), which you can find here:https://sourceforge.net/projects/tdv/. It is a bit old (last release: 2007) but it can be somewhat helpful.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I obtain a heap dump?,SeeGetting a Heap Dumpon the help pages ofEclipse Memory Analysis Tool.,../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : HowTo How do I add my own custom MBean to monitor my application within Tomcat 6?,"First of all, you can readthis great tutorialfrom Christopher Blunck ( chris@wxnet.org ). I will just add my comments and improvements. 1. Start your Tomcat and check that you have access tohttp://localhost:8080/manager/jmxproxy/. It means that JMX is enabled on your Tomcat configuration (if not, check if the following line is in your /conf/server.xml file:<Listener className=""org.apache.catalina.mbeans.ServerLifecycleListener"" />Otherwise, check the Tomcat documentation to activate it). Let this page opened to check further if your custom MBean is detected by Tomcat. 2. Build your custom MBean by following the Christopher Blunck's example: ServerMBean.java: package org.wxnet.mbeans;

  public interface ServerMBean {
    public void setLoggingLevel(int level);
    public long getUptime();
  } Server.java: package org.wxnet.mbeans;

  import javax.management.MBeanServer;
  import javax.management.MBeanServerFactory;
  import javax.management.ObjectName;

  import java.util.ArrayList;

  public class Server implements ServerMBean {
    private int _logLevel = 1;
    private long _startTime = 0L;

    public Server() {
      MBeanServer server = getServer();

      ObjectName name = null;
      try {
        name = new ObjectName(""Application:Name=Server,Type=Server"");
        server.registerMBean(this, name);
      } catch (Exception e) {
        e.printStackTrace();
      }

      _startTime = System.currentTimeMillis();
    }


    private MBeanServer getServer() {
      MBeanServer mbserver = null;

      ArrayList mbservers = MBeanServerFactory.findMBeanServer(null);

      if (mbservers.size() > 0) {
        mbserver = (MBeanServer) mbservers.get(0);
      }

      if (mbserver != null) {
        System.out.println(""Found our MBean server"");
      } else {
        mbserver = MBeanServerFactory.createMBeanServer();
      }

      return mbserver;
    }


    // interface method implementations
    public void setLoggingLevel(int level) { _logLevel = level; }
    public long getUptime() { return System.currentTimeMills() - _startTime; }
  } In this implementation, firstly notice theObjectNamerepresenting the MBean (in the constructor):name = new ObjectName(""Application:Name=Server,Type=Server"");Do not hesitate to change the domain name (the first parameter) by your own to easily find your MBean reference in thehttp://localhost:8080/manager/jmxproxypage. Secondly, take a look at your MBean constructor: First step is to get a reference to the Tomcat's MBeanServer withMBeanServer server = getServer();.ThegetServer()method returns the first MBean server in the list of MBean servers registered in JVM, which is the one used by Tomcat. In my application architecture, I placed the 2 MBeans files (the interface and its implementation) in a particular package (I don't think its compulsary but definitely more aesthetic). Compile those one in a jar archive and place it in the Tomcat's library folder (/lib). 3. Build yourContextListener: According to theTomcat's documentation, a Listener is aa component that performs actions when specific events occur, usually Tomcatstartingor Tomcat stopping.. We need to instantiate and load our MBean at Tomcat's start. So we build a ContextListener.java file which is placed wherever you want in your project architecture: package org.bonitasoft.context;

/**
 * @author Christophe Havard
 *
 */

import javax.servlet.ServletContext;
import javax.servlet.ServletContextEvent;
import javax.servlet.ServletContextListener;

import org.bonitasoft.mbeans.Server;

public final class ContextListener  implements ServletContextListener {

  public void contextInitialized(ServletContextEvent event) {
    Server mbean = new Server();
  }

  public void contextDestroyed(ServletContextEvent event) { }

} Take a look especially at the contextInitialized method. It just instantiates our custom MBean. Don't forget that the MBean register itself in the Tomcat's MBeanServer in its constructor. DO NOT FORGET to change thepackageline according to your application architecture. Then, you have to modify your WEB-INF/web.xml file to make Tomcat execute your ContextListener. <?xml version=""1.0"" encoding=""ISO-8859-1""?>
<!DOCTYPE web-app
    PUBLIC ""-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN""
    ""http://java.sun.com/dtd/web-app_2_3.dtd"">

<web-app>
  <display-name>My Web Application</display-name>
 '''''bla bla bla...'''''
  <listener>
    <listener-class>org.bonitasoft.context.ContextListener</listener-class>
  </listener>
</web-app> In his tutorial, Christopher Blunck suggests to compile the ContextListener.java file in a jar archive and then place it into our WEB-INF/lib folder. In my own experiments, I never found any difference without doing this. 4. Thembeans-descriptor.xmlfile: The only entry in the Tomcat documentation about custom MBean is about this file. It says ""You may also add MBean descriptions for custom components in a mbeans-descriptor.xml file, located in the same package as the class files it describes."". Unfortunately, instead of reading this file, Tomcat applied its own templates to replace my MBeans attributes and operations descriptions... I really didn't figure out what is the correct way of using and placing this file. So I don't use it. 5. The configuration should be over. You should have done the following operations: Build your MBean,Compile it and place the .jar archive in the Tomcat's /lib folder,Build your ContextListener.java,Add a reference to your ContextListener inside your WEB-INF/web.xml file You can try to run your project. Open thehttp://localhost:8080/manager/jmxproxypage and find your custom MBean (with a simple ctrl+f). You can see its domain, name, type and its attributes and methods. You can now use this MBean in your application by getting a reference to the Tomcat's MBean server: MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
//call operations with invoke(...) and attributes with getAttributes(...) Do not hesitate to check the ManagementFactory class javadoc.",../data/confluence_exports/TOMCAT/HowTo_103099265.html
Apache Tomcat : Memory Related Bugs Preface,"This page discusses various memory issues. In a nutshell - if your computer has less than 128MB of ram - you will probably have trouble. Anyhow, also read the following threads for other memory related issues: Memory requirementsjava.lang.OutOfMemoryError during deployMemory Mgmt Tomcat",../data/confluence_exports/TOMCAT/Memory-Related-Bugs_103098761.html
Apache Tomcat : Memory Related Bugs JSP Recompilation,"If your application uses JSPs which are frequently recompiled at runtime, e.g. headers that change value hourly, please make sure to read the JSP HOW-TO page and RELEASE NOTES documents. You may wish to tune the JSP compiler configuration to prevent memory leaks. Of course, these are documents you should have read by now anyways..",../data/confluence_exports/TOMCAT/Memory-Related-Bugs_103098761.html
Apache Tomcat : Memory Related Bugs How do I adjust memory settings?,"First look atjava -Xto determine what parameters to set. Then you can set them via the environment variableCATALINA_OPTS. Read a comment at the top of the filescatalina.batorcatalina.shfor more information onCATALINA_OPTS. Note that there also exists environment variableJAVA_OPTS, but itshould notbe used to set the memory settings. The difference is thatJAVA_OPTSsettings are used for all Tomcat management commands. When you stop Apache Tomcat (usingshutdown.shorcatalina.sh stopcommand), a small short-lived Java process is created that notifies the main Java process of Tomcat that it has to shut down. This small Java process does not need the same memory settings as the main process. The small process uses the settings fromJAVA_OPTS. The main process uses the settings from bothJAVA_OPTSandCATALINA_OPTS. If you launch Tomcat in any other way that does not usecatalina.batorcatalina.shto prepare a command line for Java (e.g. you launch java directly from within an IDE, or you use a Service Wrapper to run Tomcat as a service on Windows), see their options on how to provide settings for the java executable. E.g. Apache Commons Daemon service wrapper for Windows has a GUI configuration dialog thatcan be openedby runningtomcat9w.exeor a similar command.",../data/confluence_exports/TOMCAT/Memory-Related-Bugs_103098761.html
Apache Tomcat : Memory Related Bugs Why do I getOutOfMemoryErrorerrors?,"Many reasons. You're out of memory. Simple as that - add more to your heap.You're out of memory. You have code which is hanging onto object references and the garbage collector can't do its job. Get a profiler to debug this one.You ran out of file descriptors. If you are on a *nix system, it has been observed that anOutOfMemoryErrorcan be thrown if you run out of file descriptors. This can occur if your threshold is too low. Theulimitprogram can help you out here. You also may need to account for socket connections too when thinking about these thresholds. Google is your friend for getting more information about this topic.You have too many threads running. Some OS's have a limit to the number of threads which may be executed by a single process. (Which is what the JVM is.) Refer to your OS docs for more information on how to raise this threshold.If you have a lot of servlets or JSP's, you may need to increase your permanent generation. By default, it is 64MB. Doubling it to be-XX:MaxPermSize=256mmight be a good start.Your OS limits the amount of memory your process may take. OK, this one is grasping at straws.Not actually a reason - but on your particular platform, look at thejava -Xoptions. They may be VERY helpful.",../data/confluence_exports/TOMCAT/Memory-Related-Bugs_103098761.html
Apache Tomcat : Memory Related Bugs How much memory is Tomcat/webapp/??? using?,"To find out how much memory Tomcat is using, you might be able to use theRuntimeclass provided by the JDK.You can't find out how much memory a webapp is using. The JVM doesn't give us these detail.You can't find out how much memory a ??? is using. The JVM doesn't give us these detail.That being said, a memory profiling tool might prove the above statements wrong - but you probably don't want to use them in a production environment.",../data/confluence_exports/TOMCAT/Memory-Related-Bugs_103098761.html
Apache Tomcat : Password Why are plain text passwords in the config files?,"Because there is no good way to ""secure"" them. When Tomcat needs to connect to a database, it needs the original password. While the password could be encoded, there still needs to be a mechanism to decode it. And since the source to Tomcat is freely available, the attacker would know the decoding method. So at best, the password is obscured - but not really protected. Please see the user and dev list archives for flame wars about this topic. That said, any configuration file that does contain a password needs to be appropriately secured. That meanslimiting accessto the file so that it could be read only by the user that Tomcat process runs as and root (or the administrator on Windows). InThe Cathedral and the Bazaar, Eric S. Raymond recounts a story where his fetchmail users asked for encrypted passwords in the .fetchmailrc file (which is almost identical to the situation posed here with server.xml). He refusedusing the same arguments posed here: encrypting or otherwise obfuscating the password in server.xml does not provide any real security: only ""security by obscurity"" which isn't actually secure. Auditors do not like this answer. In order to please auditors, feel free to do any of the following. Please be aware, that all of the following are ""security by obscurity"" and are not making the Tomcat more secure. But it may allow you to pass an auditors checklist .... Use properties replacement so that in the xml config you have ${db.password} and in conf/catalina.properties you put the password there.Since server.xml is an XML file — you can use XML entities. For example: ""woot"" becomes ""&#119;&#111;&#111;&#116;"" which is a way to obscure the password. You may even go through an extra layer of indirection by converting ${db.password} into XML entities so that the property replacement above is also performed. (But remember, while ""clever, not more secure)XML entities can be read from an external file. That is, add the following text at the top of server.xml just after the XML declaration (<?xml ...?>) and before the<Server>element (line wraps can be removed):<!DOCTYPE Server [
  <!ENTITY resources SYSTEM ""resources.txt"">
]> Now, whenever you write&resources;in the text below, it will be replaced by the content of the file ""resources.txt"". The file path is relative to the conf directory.Write your own datasource implementation which wraps your datasource and obscure your brains out (XORandROT13are great candidates for this since their strength matches the protection you'll actually get). See the docs on how to do this.Write your ownjavax.naming.spi.ObjectFactoryimplementation that creates and configures your datasource.Write your ownorg.apache.tomcat.util.IntrospectionUtils.PropertySourceimplementation to 'decrypt' passwords that are 'encrypted' in catalina.properties and referenced via ${...} in server.xml. You will need to set thesystem propertyorg.apache.tomcat.util.digester.PROPERTY_SOURCEto point to your PropertySource implementation.An example of a project that provides such custom PropertySource:Vault for Apache Tomcat. A cultural reference: It is turtles all the way down(Wikipedia)",../data/confluence_exports/TOMCAT/Password_103099026.html
TIKA : Troubleshooting Tika Wrong Content Extracted,"Make sure you're passing Tika the source file you meant to pass, and it hasn't been corrupted in the transfer processMake sure Tika is able to correctly detect your file's type, seeContent Incorrectly DetectedMake sure Tika used the parser you meant it to, seeWrong Parser UsedMake sure you're actually using the version of Tika you meant to use! SeeIdentifying your Tika VersionProblems with a PDF? SeePDF Text Problems",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika No Content Extracted,"Make sure Tika is able to correctly detect your file's type, seeContent Incorrectly DetectedMake sure Tika has the parser for your format, and its dependencies, available and working. SeeParsers MissingMake sure you're actually using the version of Tika you meant to use! SeeIdentifying your Tika Version",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Wrong Parser Used,"Make sure Tika is able to correctly detect your file's type, seeContent Incorrectly DetectedMake sure the parser you wanted to use is available to Tika. SeeIdentifying what Parsers your Tika install supports,Parsers MissingandIdentifying is any Parsers failed to be loaded",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Content Incorrectly Detected,"Tika detects content types based on mime magic, format (normally container) specific detectors, content type hints and filename hints. Things to check: Does Tika know about your type? SeeIdentifying what Mime Types your Tika install supportsIf the mime type isn't listed there, seeMime Type MissingDoes Tika have all its detectors? SeeIdentifying what Detectors your Tika install supportsandDetectors MissingIs your file a different version of the format? Check the first few hundred bytes in a hex editor, and compare to the built-in mime type",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Parsers Missing,"In order for a Parser to be loaded by Apache Tika, it needs: The parser class to be on the classpathat runtimeAnd all of its dependenciesFor most parsers, that means thetika-parsersjar and dependenciesOne of:a Tika Config which explicitly lists the parser classa Tika Config (eg default one) which usesDefaultParseranda service file for the parserandno exclusion of that parser or parser's type To check what parsers you have, seeIdentifying what Parsers your Tika install supports To check if any parsers were defined but failed to load seeIdentifying if any Parsers failed to be loaded To create a service file for auto-loading, seethe quickstart guide",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Detectors Missing,"In order for a Detector to be loaded by Apache Tika, it needs: The detector class to be on the classpathat runtimeAnd all of its dependenciesFor most detectors, that means thetika-parsersjar and dependencies (the container detectors are generally stored along with the parsers)One of:a Tika Config which explicitly lists the detector classa Tika Config (eg default one) which usesDefaultDetectoranda service file for the detector To check what detectors you have, seeIdentifying what Detectors your Tika install supports To check if any detectors were defined but failed to load seeIdentifying if any Detectors failed to be loaded",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Mime Type Missing,"If Tika doesn't out of the box, you need to add a custom mimetypes file. Seethe quick guidefor howIf you have written a custom mimetypes file, it needs to be present on your classpath at runtime with theexactname oforg/apache/tika/mime/custom-mimetypes.xml. Double check you added it to your classpath, it has exactly that name (no typos, no prefix directories, no suffixes etc), and useIdentifying what Mime Types your Tika install supportsto see if you've loaded it or not",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Tika App,java -jar tika-app-blah.jar --version,../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Tika Server,Go tohttp://localhost:9998/version,../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Tika Facade,"// Get your Tika object, eg
Tika tika = new Tika();
// Call toString() to get the version
String version = tika.toString();",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Tika Java classes,"// Get your Tika Config, eg
TikaConfig config = TikaConfig.getDefaultConfig();
// Go via the Tika Facade
String version = (new Tika(config)).toString();",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Tika App,java -jar tika-app-blah.jar --list-supported-types,../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Tika Server,Go tohttp://localhost:9998/mime-types,../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Tika Facade,"This is not directly possible from the Tika Facade class. Instead, follow theTika Java classesroute below",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Tika Java classes,"// Get your Tika Config, eg
TikaConfig config = TikaConfig.getDefaultConfig();
// Get the registry
MediaTypeRegistry registry = config.getMediaTypeRegistry();
// List
for (MediaType type : registry.getTypes()) {
   String typeStr = type.toString();
}",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Tika App,java -jar tika-app-blah.jar --list-parsers,../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Tika Server,Go tohttp://localhost:9998/parsers,../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Tika Facade,"// Get your Tika object, eg
Tika tika = new Tika();
// Get the root parser
CompositeParser parser = (CompositeParser)parser.getParser();
// Fetch the types it supports
for (MediaType type : parser.getSupportedTypes(new ParseContext())) {
   String typeStr = type.toString();
}
// Fetch the parsers that make it up (note - may need to recurse if any are a CompositeParser too)
for (Parser p : parser.getAllComponentParsers()) {
   String parserName = p.getClass().getName();
}",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Tika Java classes,"// Get your Tika Config, eg
TikaConfig config = TikaConfig.getDefaultConfig();
// Get the root parser
CompositeParser parser = (CompositeParser)parser.getParser();
// Fetch the types it supports
for (MediaType type : parser.getSupportedTypes(new ParseContext())) {
   String typeStr = type.toString();
}
// Fetch the parsers that make it up (note - may need to recurse if any are a CompositeParser too)
for (Parser p : parser.getAllComponentParsers()) {
   String parserName = p.getClass().getName();
   if (p instanceof CompositeParser) {
      // Check child ones too
   }
}",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Tika App,java -jar tika-app-blah.jar --list-detectors,../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Tika Server,Go tohttp://localhost:9998/detectors,../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Tika Facade,"// Get your Tika object, eg
Tika tika = new Tika();
// Get the root detector
CompositeDetector detector = (CompositeDetector)parser.getDetector();
// Fetch the detectors that make it up (note - may need to recurse if any are a CompositeDetector too)
for (Detector d : parser.getDetectors()) {
   String detectorName = d.getClass().getName();
}",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Tika Java classes,"// Get your Tika Config, eg
TikaConfig config = TikaConfig.getDefaultConfig();
// Get the root detector
CompositeDetector detector = (CompositeDetector)parser.getDetector();
// Fetch the detectors that make it up (note - may need to recurse if any are a CompositeDetector too)
for (Detector d : parser.getDetectors()) {
   String detectorName = d.getClass().getName();
   if (d instanceof CompositeDetector) {
      // Check child ones too
   }
} d",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Identifying if any Parsers failed to be loaded,"When staring your JVM, if you pass in-Dorg.apache.tika.service.error.warn=truethen you'll get warnings logged if any Parsers or Detectors couldn't be loaded. With the default logging configuration, you'll see things like this printed to your standard output of the JVM: WARNING: Unable to load org.apache.tika.parser.microsoft.OfficeParser
java.lang.NoClassDefFoundError: org/apache/poi/poifs/filesystem/DirectoryEntry
	at java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2585)
	at java.lang.Class.getConstructor0(Class.java:2885)
	at java.lang.Class.newInstance(Class.java:350)
	at org.apache.tika.config.ServiceLoader.loadStaticServiceProviders(ServiceLoader.java:315)
	at org.apache.tika.parser.DefaultParser.getDefaultParsers(DefaultParser.java:52)
	at org.apache.tika.parser.DefaultParser.<init>(DefaultParser.java:61)
	at org.apache.tika.parser.DefaultParser.<init>(DefaultParser.java:66)
	at org.apache.tika.config.TikaConfig.getDefaultParser(TikaConfig.java:76)
	at org.apache.tika.config.TikaConfig.<init>(TikaConfig.java:182)
	at org.apache.tika.config.TikaConfig.getDefaultConfig(TikaConfig.java:291)
	at org.apache.tika.Tika.<init>(Tika.java:115)
	at org.apache.tika.cli.TikaCLI.version(TikaCLI.java:629)
	at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:365)
	at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:134)
Caused by: java.lang.ClassNotFoundException: org.apache.poi.poifs.filesystem.DirectoryEntry
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	... 15 more In this case, the error is telling us that we're missing the Apache POI jars which are a required dependency of Tika Parsers, and of the org.apache.tika.parser.microsoft.OfficeParser parser. TODO describe how to use aServiceLoader.LoadErrorHandler.ERROR to trigger an exception",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika Identifying if any Detectors failed to be loaded,"When staring your JVM, if you pass in-Dorg.apache.tika.service.error.warn=truethen you'll get warnings logged if any Parsers or Detectors couldn't be loaded. With the default logging configuration, you'll see things like this printed to your standard output of the JVM: WARNING: Unable to load org.apache.tika.parser.microsoft.POIFSContainerDetector
java.lang.NoClassDefFoundError: org/apache/poi/poifs/filesystem/DirectoryEntry
	at java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2585)
	at java.lang.Class.getConstructor0(Class.java:2885)
	at java.lang.Class.newInstance(Class.java:350)
	at org.apache.tika.config.ServiceLoader.loadStaticServiceProviders(ServiceLoader.java:315)
	at org.apache.tika.detect.DefaultDetector.getDefaultDetectors(DefaultDetector.java:55)
	at org.apache.tika.detect.DefaultDetector.<init>(DefaultDetector.java:66)
	at org.apache.tika.config.TikaConfig.getDefaultDetector(TikaConfig.java:71)
	at org.apache.tika.config.TikaConfig.<init>(TikaConfig.java:183)
	at org.apache.tika.config.TikaConfig.getDefaultConfig(TikaConfig.java:291)
	at org.apache.tika.Tika.<init>(Tika.java:115)
	at org.apache.tika.cli.TikaCLI.version(TikaCLI.java:629)
	at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:365)
	at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:134)
Caused by: java.lang.ClassNotFoundException: org.apache.poi.poifs.filesystem.DirectoryEntry
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	... 14 more In this case, the error is telling us that we're missing the Apache POI jars which are a required dependency of Tika Parsers, and of the org.apache.tika.parser.microsoft.POIFSContainerDetector detector. TODO describe how to use aServiceLoader.LoadErrorHandler.ERROR to trigger an exception",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : Troubleshooting Tika PDF Text Problems,"If Tika isn't extracting the right text from a PDF, and/or is giving errors, the first thing to do is identify if this is a Tika issue, or an issue with the underlying Apache PDFBox library used, or an issue with the PDF itself. To check, grab the latestApache PDFBox pdfbox-app jarand use theExtractText command line toolon your problematic PDF: java -jar pdfbox-app.X.Y.jar ExtractText problematicPDF.pdf If PDFBox reports that there are unmapped Unicode characters or other problems, there may be a problem with the PDF itself.  Try opening it in, for example, Adobe Reader and ""saving as text"" or copying and pasting the text. If the ""saved text"" is just as errorful as what Tika was extracting, there's a problem with the PDF file itself. If the ""saved text"" is in good shape, then there may be a problem in PDFBox.  In which case, pleasefile an Apache PDFBox bug reportand attach at least one failing file to the bug. When that gets fixed, Tika will pick up the new release and will get the fix. If PDFBox ExtractText works fine, it may* be a Tika bug. Pleasereport an Apache Tika bug, attach at least one failing file, and mention that PDFBox ExtractText doesn't have the issue. *PDFBox'sExtractTextdoes not pull text from Annotations or Acroforms, so it is possible that a problem not encountered by PDFBox'sExtractTextreveals a bug in Annotations or Acroforms; might be a bug in Tika, too. When in doubt, ask. See also:PDFParser notes.",../data/confluence_exports/TIKA/Troubleshooting-Tika_109454101.html
TIKA : TikaParseContext General,"The following uses apply to several parsers:  Handling embedded files1a. EmbeddedDocumentExtractor – for handling embedded files, the user can specify a custom EmbeddedDocumentExtractor.  1b. Parser – if the user fails to pass in an EmbeddedDocumentExtractor, the parsers will look for a Parser.class in theParseContext, and Tika will build a ParsingEmbeddedDocumentExtractor based on that Parser automatically.1c. NOTE: As of Tika 1.15, if the user doesn't specify an EmbeddedDocumentExtractor.class or a Parser.class, a ParsingEmbeddedDocumentExtractor will be automatically added with an AutoDetectParser.  Before Tika 1.15, if a user failed to pass in an EmbeddedDocumentExtractor or a Parser, Tika would skip embedded files.  2. XMLParsing – Users can send in their own XMLReader (StAX), SAXParser (SAX), SAXParserFactory (SAX) or DocumentBuilder (DOM).  Parsers that use XML parsing will use these resources for XML parsing.  3. PasswordProvider – If you know the password to password protected files, you can send in a PasswordProvider via the ParseContext.  4. ExecutorService – For parsers that use an ExecutorService, users can pass in their own ExecutorService.",../data/confluence_exports/TIKA/TikaParseContext_109454097.html
TIKA : TikaParseContext Parser Specific,HtmlParser  2. TesseractOcrParser  3. PDFParser  4. Microsoft Parser (as of Tika 1.15),../data/confluence_exports/TIKA/TikaParseContext_109454097.html
TIKA : 3rd party parser plugins Microsoft TNEF / LZFU,"This is a MS compression format used for compressed RTF, email attachments (like WINMAIL.DAT) and more. The parser is available from a github fork of theJTNEF project.  (Tika 0.10 includes a TNEF parser as standard now, which may be sufficient)  Install instructions:  git clone http``://github.com/jukka/jtnef.git jtnefcd jtnefmvn packagecp target/jtnef-*.jar $SOMEWHERE_ON_CLASS_PATH",../data/confluence_exports/TIKA/3rd-party-parser-plugins_109454034.html
TIKA : 3rd party parser plugins Microsoft Project,"This parser extracts metadata and content from Microsoft Project (MPP and MPX) files  It builds on top ofMPXJ, which is available under the LGPL  Installation instructions:  git clone git``://git.code.sf.net/p/mpxj/mpxjcd !mpxjmvn packagecp target/mpxj-*SNAPSHOT.jar $SOMEWHERE_ON_CLASS_PATHgit clone http``://github.com/Gagravarr/MPXJ-Tikacd !MPXJ-Tikamvn packagecp target/mpxj-tika-*SNAPSHOT.jar $SOMEWHERE_ON_CLASS_PATH",../data/confluence_exports/TIKA/3rd-party-parser-plugins_109454034.html
TIKA : 3rd party parser plugins Ogg Vorbis and FLAC,"This parser extracts metadata from Ogg Vorbis and FLAC audio files.  The library and parser are available under the Apache License, so this is now included as part of Tika.",../data/confluence_exports/TIKA/3rd-party-parser-plugins_109454034.html
TIKA : 3rd party parser plugins Your plugin,<Your description here>  Install instructions:  <Your instructions here>,../data/confluence_exports/TIKA/3rd-party-parser-plugins_109454034.html
TIKA : SMTWithApacheJoshua Introduction,"The page provides detail on how to useApache Joshua (Incubating)to undertake statistical machine translation (STM) via theTika.translateAPI. This work is the result of development that has taken place both throughTIKA-1343 Create a Tika Translator implementation that uses JoshuaDecoderand via close work within the Joshua community. The benefits of using this approach for achieving language translation through Tika are as follows; It's free! As opposed to several other translation services currently available via Tika, STM via Joshua is free. You build the language models, you set up and manage the infrastructure and you have 100% control over the resulting translationYou are not restricted under some usage ceiling. As there is no paid service, you can use this method completely unrestricted.The language model generation and quality are completely transparent. Nowadays a large issue with the use of statistical models (or more generally any models utilized within learning processes) is typically not shared and hence it is difficult to fully quantify or justify the results you get. For example, if we were to use Google Translate, we have absolutely no insight into how the translations are undertaken, what accuracy they achieve, etc. The method and work which is proposed here address this concern entirely. Everything is 100% transparent. The downsides of using this approach are as follows; Joshua, the underlying STM toolkit is quite a complex piece of software. This should by no means be a surprise... after all STM is an extremely difficult and active research area. Some of the world's largest companies e.g. Google, Yahoo!, Bing, IBM, etc are investing large sums of money and significant resources trying to address the issues. The fact that we have STM available via Tika is a huge step towards building the STM open source community.Depending upon your translation requirements, you may be required to build your own language models. This however depends on which models are available via the Joshua community. If you do need to build your own models/language packs, this is not exactly a trivial process however you can find loads of help on this topic over on theJoshua mailing lists.Depending on the availability of good hardware, you may encounter performance issues. The loading of large language models, STM tasks generally, and building new language packs tend to benefit from powerful machines with lots of RAM. If this is not available then you may encounter issues. With the above in mind, let us continue with configuring and provisioning Tika for STM with Joshua.",../data/confluence_exports/TIKA/SMTWithApacheJoshua_109454071.html
TIKA : SMTWithApacheJoshua Step 1: Retrieve the Joshua Language Pack,"In this example, we will be using aSpanish-to-English n-gram language model packwhich was generated on October 6th 2016 and built usingBerkeleyLM. For more detail on the language pack itself and how it was produced, see theLanguage Pack Details. Grab the language pack and set it up as follows $ cd /usr/local
$ wget ""http://cs.jhu.edu/~post/files/apache-joshua-es-en-2016-10-06.tgz""
$ tar -zxvf apache-joshua-es-en-2016-10-06.tgz
$ cd apache-joshua-es-en-2016-10-06 To run the language pack, invoke the command ./joshua [OPTIONS ...] The Joshua decoder will start running, accepting input from STDIN and writing to STDOUT. Joshua expects its input in the form of a single sentence per line. Each sentence should first be piped throughprepare.sh, which normalizes and tokenizes the input for the language pack's source language. Here we have apassage.txtfor you to try. If you run out of memory then please increase it within thejoshuascript. cat passage.txt | prepare.sh | joshua > output.txt It takes some time (sometimes as much as a minute) to load all of the models into memory, which means there is high latency from startup until the first translation. To reduce this time, Joshua can also be run in server mode, implementing either a direct TCP-IP interface, or implementing a Google-translate style RESTful API. To run Joshua as a TCP-IP server, add the option joshua -server-port 5674 You can then connect directly to the socket using nc or telnet: cat passage.txt | prepare.sh | nc localhost 5674 > output.txt Take a look atoutput.txtand you will see your translated passage... pretty cool eh? You can set the RESTful interface by also passing '-server-type http': joshua -server-port 5674 -server-type http The RESTful interface is used when running the browser demo (seeapache-joshua-es-en-2016-10-06/web/index.html) or when using theJoshua Translation Engine.",../data/confluence_exports/TIKA/SMTWithApacheJoshua_109454071.html
TIKA : SMTWithApacheJoshua Step 2: Using the Joshua Translation Engine,"In this step we establish a translation engine server written in Python that provides translations of documents http requests as responses to http requests. It provides a very convenient way for establishing a remotely accessible translation service which can be used in a RESTful manner which is exactly what Tika does when configured to use theJoshuaNetworkTranslatorimplementation. Lets check out the Joshua Translation Engine source and start the service $ cd /usr/local
$ git clone https://github.com/joshua-decoder/joshua_translation_engine.git
$ cd joshua_translation_engine
$ python app.py -b /usr/local/joshua_resources/apache-joshua-es-en-2016-10-06 -s es -t en -p 5674
...
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) The Python application can also print us basic help instructions to explain the above parameters usage: app.py [options]
Specify at least one bundle and source and target languages for each bundle. The order of -s and -t correspond to the order of -b options

Start a translation engine server.

optional arguments:
  -h, --help            show this help message and exit
  -b BUNDLE_DIR [BUNDLE_DIR ...], --bundle-dir BUNDLE_DIR [BUNDLE_DIR ...]
                        path to directory generated by Joshua's run_bundler.py
                        script
  -s SOURCE_LANG [SOURCE_LANG ...], --source-lang SOURCE_LANG [SOURCE_LANG ...]
                        the two-character language code of the input text.
  -t TARGET_LANG [TARGET_LANG ...], --target-lang TARGET_LANG [TARGET_LANG ...]
                        the two-character language code of the output text
  -p [PORT [PORT ...]], --port [PORT [PORT ...]]
                        the TCP port(s). Either specify just one port, and the
                        rest of the bundles will start on automatically
                        incremented port numbers, or specify one port number
                        per bundle. Omitting this option defaults to setting
                        the first port number to 56748. Once the above is stable you can then progress with configuring Tika!",../data/confluence_exports/TIKA/SMTWithApacheJoshua_109454071.html
TIKA : SMTWithApacheJoshua Step 3: Configure and Provision Apache Tika,"So now let us grab the Tika source, configure, compile and deploy it such that we can utilize Joshua's STM functionality. $ cd /usr/local
$ git clone https://github.com/apache/tika.git
$ cd tika
$ mvn clean install -DskipTests
$ java -jar tika-server/target/tika-server-1.15-SNAPSHOT.jar
...

Oct 25, 2016 10:16:25 AM org.apache.tika.server.TikaServerCli main
INFO: Starting Apache Tika 1.15-SNAPSHOT server
Oct 25, 2016 10:16:26 AM org.apache.cxf.endpoint.ServerImpl initDestination
INFO: Setting the server's publish address to be http://localhost:9998/
Oct 25, 2016 10:16:26 AM org.slf4j.impl.JCLLoggerAdapter info
INFO: jetty-8.y.z-SNAPSHOT
Oct 25, 2016 10:16:26 AM org.slf4j.impl.JCLLoggerAdapter info
INFO: Started SelectChannelConnector@localhost:9998
Oct 25, 2016 10:16:26 AM org.apache.tika.server.TikaServerCli main
INFO: Started We can now utilize theTika Translate REST APIto undertake language translation for us. curl http://localhost:9998/translate/all/org.apache.tika.language.translate.JoshuaNetworkTranslator/spanish/english -X PUT -T passage.txt The response you receive should be the translated text for the passage you posted. Pretty neat eh?",../data/confluence_exports/TIKA/SMTWithApacheJoshua_109454071.html
TIKA : SMTWithApacheJoshua Contact/Assistance,"If you have issues with the Tika components of this document, you canget help from the Tika community mailing lists. If you have issues with the Joshua components of this document, you canget help from the Joshua community mailing lists.",../data/confluence_exports/TIKA/SMTWithApacheJoshua_109454071.html
TIKA : SMTWithApacheJoshua Details,"The language pack being used in this example was trained on the following bitexts, with a 4-gram language model built over the target side of the bitext. Many of the parallel documents were sources from OPUS, A Collection of Multilingual Parallel Corpora with Tools and Interfaces (http://opus.lingfil.uu.se/). Europarl version 7. Proceedings of the European Parliament.http://www.statmt.org/europarl/DGT: A collection of translation memories provided by the JRC.https://ec.europa.eu/jrc/en/language-technologies/dgt-translation-memoryEMEA. PDF documents from the European Medicines Agency.http://www.emea.europa.eu/Global Voices. Parallel news stories.https://globalvoices.org/JRC-Aquis: legislative text of the European Union from 1950 on.https://ec.europa.eu/jrc/en/language-technologies/jrc-acquisNews Commentary v10. News commentaries provided by WMT.http://www.casmacat.eu/corpus/news-commentaryTatoeba.http://tatoeba.orgFisher & Callhome Spanish. Translations of conversational Spanish.https://catalog.ldc.upenn.edu/ldc2014t23",../data/confluence_exports/TIKA/SMTWithApacheJoshua_109454071.html
TIKA : SMTWithApacheJoshua Benchmark,Test setBLEU scoreMeteor scoreDescriptionNewstest201327.4633.39NewsFisher dev237.1336.75Conversational speechCallhome evltest32.4433.39Conversational speechGlobal Voices36.3336.40News,../data/confluence_exports/TIKA/SMTWithApacheJoshua_109454071.html
TIKA : SMTWithApacheJoshua Configuration,"# MERT optimized configuration
# decoder /export/projects/mpost/language-packs/es-en/5/tune/model/run-joshua.sh
# BLEU 0.2655 on dev /export/projects/mpost/language-packs/es-en/5/data/tune/corpus.es
# We were before running iteration 4
# finished Tue Sep 27 12:03:42 EDT 2016
# This file is a template for the Joshua pipeline; variables enclosed
# in <angle-brackets> are substituted by the pipeline script as
# appropriate.  This file also serves to document Joshua's many
# parameters.

# These are the grammar file specifications.  Joshua supports an
# arbitrary number of grammar files, each specified on its own line
# using the following format:
#
#   tm = TYPE OWNER LIMIT FILE
# 
# TYPE is ""packed"", ""thrax"", or ""samt"".  The latter denotes the format
# used in Zollmann and Venugopal's SAMT decoder
# (http://www.cs.cmu.edu/~zollmann/samt/).
# 
# OWNER is the ""owner"" of the rules in the grammar; this is used to
# determine which set of phrasal features apply to the grammar's
# rules.  Having different owners allows different features to be
# applied to different grammars, and for grammars to share features
# across files.
#
# LIMIT is the maximum input span permitted for the application of
# grammar rules found in the grammar file.  A value of -1 implies no limit.
#
# FILE is the grammar file (or directory when using packed grammars).
# The file can be compressed with gzip, which is determined by the
# presence or absence of a "".gz"" file extension.
#
# By a convention defined by Chiang (2007), the grammars are split
# into two files: the main translation grammar containing all the
# learned translation rules, and a glue grammar which supports
# monotonic concatenation of hierarchical phrases. The glue grammar's
# main distinction from the regular grammar is that the span limit
# does not apply to it.  

tm = phrase -owner pt -maxspan 0 -path model/grammar.gz.packed

# This symbol is used over unknown words in the source language

default-non-terminal = X

# This is the goal nonterminal, used to determine when a complete
# parse is found.  It should correspond to the root-level rules in the
# glue grammar.

goal-symbol = GOAL

# Language model config.
#
# Multiple language models are supported.  For each language model,
# create one of the following lines:
#
# feature-function = LanguageModel -lm_type TYPE -lm_order ORDER -lm_file FILE
# feature-function = StateMinimizingLanguageModel -lm_order ORDER -lm_file FILE
#
# - TYPE is one of ""kenlm"" or ""berkeleylm""
# - ORDER is the order of the language model (default 5)
# - FILE is the path to the LM file. This can be binarized if appropriate to the type
#   (e.g., KenLM has a compiled format)
#
# A state-minimizing LM collapses left-state. Currently only KenLM supports this.
#
# For each LM, add a weight lm_INDEX below, where indexing starts from 0.



# The suffix _OOV is appended to unknown source-language words if this
# is set to true.

mark-oovs = false

# The search algorithm: ""cky"" for hierarchical / phrase-based decoding, 
# ""stack"" for phrase-based decoding
search = stack

# The pop-limit for decoding.  This determines how many hypotheses are
# considered over each span of the input.

pop-limit = 100

# How many hypotheses to output

top-n = 1

# Whether those hypotheses should be distinct strings

use-unique-nbest = true

# This is the default format of the ouput printed to STDOUT.  The variables that can be
# substituted are:
#
# %i: the sentence number (0-indexed)
# %s: the translated sentence
# %t: the derivation tree
# %f: the feature string
# %c: the model cost

output-format = %S

# When printing the trees (%t in 'output-format'), this controls whether the alignments
# are also printed.

include-align-index = false

# And these are the feature functions to activate.
feature-function = OOVPenalty
feature-function = WordPenalty

## Model weights #####################################################

# For each langage model line listed above, create a weight in the
# following format: the keyword ""lm"", a 0-based index, and the weight.
# lm_INDEX WEIGHT


# The phrasal weights correspond to weights stored with each of the
# grammar rules.  The format is
#
#   tm_OWNER_COLUMN WEIGHT
#
# where COLUMN denotes the 0-based order of the parameter in the
# grammar file and WEIGHT is the corresponding weight.  In the future,
# we plan to add a sparse feature representation which will simplify
# this.

# The wordpenalty feature counts the number of words in each hypothesis.


# This feature counts the number of unknown words in the hypothesis.


# This feature weights paths through an input lattice.  It is only activated
# when decoding lattices.
feature-function = LanguageModel -lm_order 4 -lm_file model/lm.gz -lm_type berkeleylm
feature-function = Distortion
feature-function = PhrasePenalty



lowercase = -project-case

lm_0 0.242132004556722
WordPenalty -0.111308832033767
OOVPenalty 0.0101534888932218
tm_pt_2 0.0241130425384253
PhrasePenalty -0.0240605834315291
tm_pt_0 0.0262269656358665
tm_pt_1 0.0535319307753204
Distortion 0.121100027216756
tm_pt_3 0.191275104853519
tm_pt_4 0.119489193983075
tm_pt_5 0.076608826081799",../data/confluence_exports/TIKA/SMTWithApacheJoshua_109454071.html
TIKA : TikaBatchOverview The Need,"William Palmerdocumentswhat many integrators of Tika face – Tika works very well on most documents, but it can run into problems. As Nick Burchnoteson slides 47-55, even if Tika fails catastrophically on a small percentage of documents, a small percentage of a lot of documents is still a lot of documents, and ""you need to plan for failures"".  Some types of catastrophic failures include:  Permanent hangs (runaway parsers)Out-of-Memory ErrorsMemory leaks  Running Tika efficiently and making it robust against these problems are non-trivial issues, and it will be helpful to have a framework that the community can use, fix and improve so that each integrator doesn't have to reinvent these solutions.",../data/confluence_exports/TIKA/TikaBatchOverview_109454082.html
TIKA : TikaBatchOverview The Basic Design,"The basic design of the tika-batch module is intended for conventional processing – if anything can be reused/modified for hadoop, please contribute!  The overall design is a producer/consumer design pattern.  The producer and consumers share an ArrayBlockingQueue.  Given a wide range of use cases for Tika, it would be great if the batch process were highly configurable.  The current implementation includes an xml config file and relies on builders.  This will allow developers to add their own components into the current framework as long as they also include builders.  Tika-batch has far more code than I originally envisioned, but multi-threading, multi-processing, robust logging and configurability are common culprits for code-bloat.  Any input into code-pruning is welcome!",../data/confluence_exports/TIKA/TikaBatchOverview_109454082.html
TIKA : TikaBatchOverview BatchProcess,"A BatchProcess manages a single process for: a ResourceCrawler, ResourceConsumers, an Interrupter and a StatusReporter.  Each ResourceConsumer runs in its own thread, and the user can specify the number of consumer threads.",../data/confluence_exports/TIKA/TikaBatchOverview_109454082.html
TIKA : TikaBatchOverview ResourceCrawler,"Generically, a ResourceCrawler adds potential files for processing onto the queue.  The initial implementation of tika-batch offers two: one crawls a file system directory, and one reads a list of files to be processed in a file system directory.  Some other implementations of a ResourceCrawler might include a directory listener or a database exporter.  Anything else?",../data/confluence_exports/TIKA/TikaBatchOverview_109454082.html
TIKA : TikaBatchOverview ResourceConsumers,"A ResourceConsumer pulls a resource from the queue and consumes it. A resource should be a lightweight pointer to a file resource (not the actual bytes!), and it returns an InputStream and a Metadata object.",../data/confluence_exports/TIKA/TikaBatchOverview_109454082.html
TIKA : TikaBatchOverview Interrupter,An interrupter runs in a separate thread and allows users to ask the BatchProcess to shut down gracefully.,../data/confluence_exports/TIKA/TikaBatchOverview_109454082.html
TIKA : TikaBatchOverview StatusReporter,"A StatusReporter runs in a separate thread.  It has visibility into the crawler and the consumers, and it periodically reports on how many files have been processed, how many exceptions, and so on.",../data/confluence_exports/TIKA/TikaBatchOverview_109454082.html
TIKA : TikaBatchOverview StaleChecker,"This is an inner class within BatchProcess that periodically checks for stale consumers.  It will cause the BatchProcess to shut down if it finds a stale consumer.  In earlier versions of the code, the user could specify the maximum number of stale threads before BatchProcess shutdown, however, in practice, a single stale consumer can tie up a huge amount of resources.  For now, BatchProcess will shutdown if it finds one stale consumer.",../data/confluence_exports/TIKA/TikaBatchOverview_109454082.html
TIKA : TikaBatchOverview ProcessDriver,"This initiates the BatchProcess and monitors it to make sure that it is still alive when it should be.  If the BatchProcess sends a restart signal via stderr or a restart exit code to the ProcessDriver, the ProcessDriver will restart the BatchProcess.",../data/confluence_exports/TIKA/TikaBatchOverview_109454082.html
"TIKA : TikaBatchOverview File System (FS) Batch, Step 1","The initial use case for tika-batch is to process a directory of files recursively and generate an output file for each input file.  The output directory has the same structure/hierarchy as the input folder, and each output file has a file suffix appended to it depending on the ContentHandler ("".xml"", "".txt"", "".json"", etc).",../data/confluence_exports/TIKA/TikaBatchOverview_109454082.html
TIKA : TikaBatchOverview FS Resource Crawlers,"For FSBatch, the directory crawler starts with a root directory and crawls all files.  Bells and whistles include:  The directory crawler can start with a root directory and a file list, and it will ""crawl"" all of the files on the list relative to the root directory.  This is very useful for testing or for processing a subset of documents.The directory crawler uses a Tika DocumentSelector to determine whether or not to add a file to the queue.  The only metadata available to the directory crawler at this point in the processing is the file name and the length of the file in bytes.  The user can specify a regex for files to include (based on filename) and a regex for files to exclude (based on filename); the user can also specify a max bytes limit.The directory crawler also has the idea of a start directory.  This is a child directory of the root directory.  This allows users another way to process a subset of a directory structure.  I've also added a strawman driver that runs multiple threads but kicks off a single app.jar process for every file.",../data/confluence_exports/TIKA/TikaBatchOverview_109454082.html
TIKA : TikaBatchOverview FS Resource Consumers,"The tika-batch package includes an abstract ResourceConsumer class that handles much of the multi-threading burden.  Concrete classes of resource consumer only have to implement processFileResource(FileResource fileResource).  Concrete classes should also handle all exceptions that they want to handle and make appropriate calls to incrementHandledExceptions().  There are two consumers currently.  One handles traditional Tika processing with the ToTextContentHandler, the ToHtmlContentHandler or the ToXMLContentHandler.  The user can specify a write limit and whether or not to process documents recursively.The other offers handling by the (to be added) RecursiveParserWrapper.  For each input file, the output is a json-formatted list of Metadata items, with a special key for the content.  FSResourceConsumers rely on a ContentHandlerFactory to get the user-specified handler and an OutputStreamFactory to get the FileOutputStream to write to.",../data/confluence_exports/TIKA/TikaBatchOverview_109454082.html
TIKA : TikaBatchOverview ContentHandlerFactory,This is a simple class that builds a handler for the three basic types mentioned above the ToXXXContentHandler and optionally specifies a write limit.,../data/confluence_exports/TIKA/TikaBatchOverview_109454082.html
TIKA : TikaBatchOverview OutputStreamFactory,"This calculates the output (target) file location and name, builds the requisite parent directories and returns the OutputStream to the consumer.  If a target file exists, this will do one of three things:  Skip it – return a null OutputStream. The consumers know to avoid parsing a file if the returned OutputStream is null.Rename the file – add e.g. (1) to the end of the file name (before the suffix) until there is a new file.Overwrite the existing file.",../data/confluence_exports/TIKA/TikaBatchOverview_109454082.html
TIKA : TikaBatchOverview USAGE,SeeTikaBatchUsage.,../data/confluence_exports/TIKA/TikaBatchOverview_109454082.html
TIKA : TikaBatchOverview TODO,Any design recommendations at this point?  SeeTIKA-1330.,../data/confluence_exports/TIKA/TikaBatchOverview_109454082.html
TIKA : TikaBatchOverview Tika Batch Hadoop,SeeTikaInHadoop.,../data/confluence_exports/TIKA/TikaBatchOverview_109454082.html
TIKA : Release Process for tika-docker DockerHub Repository and Access,To be able to release the Apache Tika Docker image on DockerHub you will need to have access to theapache/tikarepository. This is controlled by the ASF Infra team and can be requested through aINFRA JIRA ticket. Make sure to tag the ticket with theDockerlabel.,../data/confluence_exports/TIKA/Release-Process-for-tika-docker_177051223.html
TIKA : Release Process for tika-docker tika-docker repo,Thisrepositorycontains the Dockerfiles used to create the minimal and full images for Apache Tika. Its also containers helper examples and configurations.,../data/confluence_exports/TIKA/Release-Process-for-tika-docker_177051223.html
TIKA : Release Process for tika-docker Image Types,"There are two image types: Minimal - containing just Apache Tika and it's base dependencies (i.e. Java)Full - containing Apache Tika, it's dependencies, as well as Tesseract and GDAL. The Dockerfile for each image is in the correspondingly named directory, and are the only assets used to public the images.",../data/confluence_exports/TIKA/Release-Process-for-tika-docker_177051223.html
TIKA : Release Process for tika-docker Docker Compose Files,There are a number of Docker Compose files to allow users to quickly test certain scenarios: Recognising and Captioning Video and Images with TensorFlow REST (see here)Enriching Academic PDF Parsing with Grobid REST (see here)OCR of PDF or Images with Tesseract including a Custom Configuration (see here)Named Entity Recognition (see here) These different scenarios use the corresponding configuration in thesample-configsdirectory. Neither these Docker Compose YML files or the Sample Configurations are used for publishing Apache Tika's Docker image. They are only used to provide examples for complex configurations. An example of using these is providedhere.,../data/confluence_exports/TIKA/Release-Process-for-tika-docker_177051223.html
TIKA : Release Process for tika-docker docker-tool.sh,"This shell file is a helper script used to simplify the building, testing and publication of the images. It provides the following options: build - to build a minimal and full image of the passed in versiontest - to verify the built image can start and the version number be received backpublish - to publish the image on DockerHub (only for those who have access to the DockerHub repo)latest - to tag the supplied version built locally aslateston DockerHub.",../data/confluence_exports/TIKA/Release-Process-for-tika-docker_177051223.html
TIKA : Release Process for tika-docker republish-images.sh,This shell file was used to republish the older images when the Dockerfile was updated. It is redundant now but kept in the repo incase something similar needs done in the future.,../data/confluence_exports/TIKA/Release-Process-for-tika-docker_177051223.html
TIKA : Release Process for tika-docker Release Process,"Update theREADME.md'sAvailable TagssectionUpdate the TAG version in.env to be X.Y.Z.Q+1Update the version in.travis.yml to be X.Y.Z.Q+1 X.Y.ZUpdate CHANGES.md to include this release, changes and release dateTest the release as in the example belowCommit the changesTo release a new version of Apache Tika on DockerHub, you can follow the below steps (replacing2.5.0with the version number you wish to publish).  As of 2.5.0, we started having to version our docker images even when based on the same Tika version.  So, Docker tags might be 2.5.0.1 for Tika version 2.5.0.  The first version in the commandlines is the Docker version, and the second version in the build command is the Tika version. $ git clone https://github.com/apache/tika-docker && cd tika-docker
$ ./docker-tool.sh build 2.5.0.1 2.5.0
$ ./docker-tool.sh test 2.5.0.1

# If you see the test passed, you can then publish it and tag it as latest:
$ ./docker-tool.sh publish 2.5.0.1
$ ./docker-tool.sh latest 2.5.0.1 6. If everything worked, tag the last commit git tag -a 2.5.0.1 -m ""New release for 2.5.0.1""git push  --tags",../data/confluence_exports/TIKA/Release-Process-for-tika-docker_177051223.html
TIKA : CTAKESParser Example Request,"curl -T Vose-2013-American_Journal_of_Hematology.pdf -H ""Content-Disposition: attachment;filename=Vose-2013-American_Journal_of_Hematology.pdf"" http://localhost:9998/rmeta",../data/confluence_exports/TIKA/CTAKESParser_109454041.html
TIKA : CTAKESParser Example Response,"And the output should be (much omitted from below): [
    {
        ""Content-Type"": ""application/pdf"",
        ""Creation-Date"": ""2013-11-20T13:24:11Z"",
        ""Last-Modified"": ""2013-11-22T14:13:25Z"",
        ""Last-Save-Date"": ""2013-11-22T14:13:25Z"",
        ""WPS-ARTICLEDOI"": ""10.1002/ajh.23615"",
        ""WPS-JOURNALDOI"": ""10.1002/(ISSN)1096-8652"",
        ""WPS-PROCLEVEL"": ""2"",
        ""X-Parsed-By"": [
            ""org.apache.tika.parser.CompositeParser"",
            ""org.apache.tika.parser.ctakes.CTAKESParser"",
            ""org.apache.tika.parser.DefaultParser"",
            ""org.apache.tika.parser.pdf.PDFParser""
        ],
        ""X-TIKA:content"": ""\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJW-AJH#130002 1083..1088\n\n\nContinuing medical education activity\nin American Journal of Hematology\n\nCME Author: Julie M. Vose, M.D., M.B.A.\nCME Editor: Ayalew Tefferi, M.D.\n\nCME Information: Mantle Cell Lymphoma: 2013 Update on\nDiagnosis, Risk-Stratification andClinical Management\nIf you wish to receive credit for this activity, please refer\n\nto the website: www.wileyhealthlearning.com\n\nAccreditation and Designation Statement:\n\nBlackwell Futura Media Services is accredited by the\nAccreditation Council for Continuing Medical Education to\nprovide continuing medical education for physicians.\nBlackwell Futura Media Services designates this journal-\n\nbased CME for a maximum of 1 AMA PRA Category 1\nCreditTM. Physicians should only claim credit commensu-\nrate with the extent of their participation in the activity.\n\nEducational Objectives\n\nUpon completion of this educational activity, participants\nwill be better able to:\n1. Identify the histologic types and prognostic models\n\nused for mantle cell lymphoma\n2. Explain the different options for initial therapy for man-\n\ntle cell lymphoma\n3. Explain the different options for treatment of recurrent\n\nmantle cell lymphoma\n\nActivity Disclosures\n\nNo commercial support has been accepted related to the\ndevelopment or publication of this activity.\nAuthor: Julie M. Vose, M.D., M.B.A., discloses research\n\ngrant support from Allos Therapeautics/Spectrum, Bristol-\nMyers Squibb, Celgene, Genentech, GlaxoSmithKline, Incyte\nCorp., Janssen Biotech, Millennium, Onyx Pharmaceuticals,\nPharmacyclics, Sanofi-Aventis US, Inc., and US Biotest, Inc.\nCME \n"",
        ""X-TIKA:parse_time_millis"": ""366163"",
        ""access_permission:assemble_document"": ""true"",
        ""access_permission:can_modify"": ""true"",
        ""access_permission:can_print"": ""true"",
        ""access_permission:can_print_degraded"": ""true"",
        ""access_permission:extract_content"": ""true"",
        ""access_permission:extract_for_accessibility"": ""true"",
        ""access_permission:fill_in_form"": ""true"",
        ""access_permission:modify_annotations"": ""true"",
        ""created"": ""Wed Nov 20 05:24:11 PST 2013"",
        ""ctakes:AnatomicalSiteMention"": [
            ""Cell:189:193:C0007634,C1269647"",
            ""Media:432:437:C0162867"",
            ""Media:593:598:C0162867"",
            ""cell:967:971:C0007634,C1269647"",
            ""cell:1045:1049:C0007634,C1269647"",
            ""cell:1124:1128:C0007634,C1269647"",
            ""Media:2134:2139:C0162867"",
            ""cell:3716:3720:C0007634,C1269647"",
            ""cell:3839:3843:C0007634,C1269647"",
            ""lymph nodes:3920:3931:C1269047,C0024204"",
            ""spleen:3933:3939:C1278932,C0037993"",
           ],
        ""ctakes:DateAnnotation"": [
            ""October 2013:8713:8725:"",
            ""1/2:35241:35244:"",
            ""1/2:39154:39157:""
        ],
        ""ctakes:DiseaseDisorderMention"": [
            ""Mantle Cell Lymphoma:182:202:C0334634,C0334634,C0334634,C0334634"",
            ""Lymphoma:194:202:C0024299,C0024299,C0024299,C0024299,C0024299,C0024299,C0024299,C0024299,C0024299"",
            ""ards:1673:1677:C0035222,C0035222,C0035222,C0035222,C0035222,C0035222"",
            ""dis:1955:1958:C0012634"",
            ""HEMATOLOGICAL MALIGNANCIES:3645:3671:C0376545,C0376545,C0376545"",
         ],
        ""ctakes:FractionAnnotation"": [
            "".1088:19:24:"",
            ""2013.:8781:8786:"",
            ""10.1002:8857:8864:"",
            "".2:11412:11414:"",
            ""11.:11849:11852:"",
         ],
        ""ctakes:MeasurementAnnotation"": [
            ""106 L:18964:18969:"",
            ""175 mg:23907:23913:""
        ],
        ""ctakes:MedicationMention"": [
            ""duration:3987:3995:C2926735"",
            ""Bendamustine:5472:5484:C0525079,C0525079"",
            ""bortezomib:5609:5619:C1176309,C1176309"",
            ""Tyrosine:5735:5743:C0041485,C0041485"",
            ""formalin:12875:12883:C0949307,C0949307"",
            ""paraffin:12891:12899:C0030415"",
        ],
        ""ctakes:ProcedureMention"": [
            ""therapy:1025:1032:C0087111,C0087111,C0087111,C0087111"",
            ""treatment:1095:1104:C0087111,C0087111,C1533734,C0087111,C1533734,C0087111"",
         ],
        ""ctakes:RomanNumeralAnnotation"": [
            ""M:127:128:"",
            ""M:161:162:"",
            ""D:163:164:"",
         ],
        ""ctakes:SignSymptomMention"": [
            ""education:43:52:C0013658,C0013658,C0013658,C0424927,C0013658,C0013658,C0013658,C0013658"",
            ""M:127:128:C0024554,C0024554,C0024554,C0024554"",
        ],
        ""ctakes:schema"": ""coveredText:start:end:ontologyConceptArr"",
        ""date"": ""2013-11-22T14:13:25Z"",
        ""dc:format"": ""application/pdf; version=1.5"",
        ""dc:title"": ""JW-AJH#130002 1083..1088"",
        ""dcterms:created"": ""2013-11-20T13:24:11Z"",
        ""dcterms:modified"": ""2013-11-22T14:13:25Z"",
        ""meta:creation-date"": ""2013-11-20T13:24:11Z"",
        ""meta:save-date"": ""2013-11-22T14:13:25Z"",
        ""modified"": ""2013-11-22T14:13:25Z"",
        ""pdf:PDFVersion"": ""1.5"",
        ""pdf:encrypted"": ""false"",
        ""producer"": ""PDFlib PLOP 2.0.0p6 (SunOS)/Adobe LiveCycle PDFG ES"",
        ""resourceName"": ""Vose-2013-American_Journal_of_Hematology.pdf"",
        ""title"": ""JW-AJH#130002 1083..1088"",
        ""xmp:CreatorTool"": ""Arbortext Advanced Print Publisher 9.0.114/W"",
        ""xmpTPg:NPages"": ""7""
    }
]",../data/confluence_exports/TIKA/CTAKESParser_109454041.html
TIKA : RecursiveMetadata Setting up Recursive Parsing,"public static void main(String[] args) throws Exception {
       Parser parser = new RecursiveMetadataParser(new AutoDetectParser());
       ParseContext context = new ParseContext();
       context.set(Parser.class, parser);  The example starts by setting up recursive parsing. If you are parsing text files, word documents, etc. then you'll never notice if recursive parsing is enable or not. If you are parsing containers like zip files and tar.gz files, the only way to get the text for the files contained by the containers is to enable recursive parsing.  The way to enable recursive parsing is to create aParseContextand add a parser to it as shown on the linecontext.set(Parser.class, parser). This is the parser that will be used to parse any nested documents.  In this case the parser is aRecursiveMetadataParserthat is a wrapper around anAutoDetectParser. TheRecursiveMetadataparser is part of Jukka's example and more details are given below.",../data/confluence_exports/TIKA/RecursiveMetadata_109454069.html
TIKA : RecursiveMetadata Parsing a File,"ContentHandler handler = new DefaultHandler();
       Metadata metadata = new Metadata();

       InputStream stream = TikaInputStream.get(new File(args[0]));
       try {
           parser.parse(stream, handler, metadata, context);
       } finally {
           stream.close();
       }  The rest of the main function parses a file. The parser used to parse the root document is the same parser that was added to theParseContextas the parser to use for nested documents.  Looking at the Tika API (http://tika.apache.org/0.7/api/), I don't see aDefaultHandlerclass or aTikaInputStream. In the place ofDefaultHandleryou could useBodyContentHandler, and in the place ofTikaInputStreamyou could useFileInputStream.",../data/confluence_exports/TIKA/RecursiveMetadata_109454069.html
TIKA : RecursiveMetadata RecursiveMetadataParserConstructor,"private static class RecursiveMetadataParser extends ParserDecorator {
       public RecursiveMetadataParser(Parser parser) {
           super(parser);
       }  TheRecursiveMetadataParserextendsParserDecorator. All the constructor has to do is let theParserDecoratorsuperclass know which parser object is being decorated.",../data/confluence_exports/TIKA/RecursiveMetadata_109454069.html
TIKA : RecursiveMetadata RecursiveMetadataParserparse,"@Override
       public void parse(
               InputStream stream, ContentHandler ignore,
               Metadata metadata, ParseContext context)
               throws IOException, SAXException, TikaException {
           ContentHandler content = new BodyContentHandler();
           super.parse(stream, content, metadata, context);

           System.out.println(""----"");
           System.out.println(metadata);
           System.out.println(""----"");
           System.out.println(content.toString());
       }

   }  The parse method is where you get access to the metadata and the body text. When the parser set inParseContextis used to parse a nested document, a new Metadata object is created and passed to the parse method. Since the example put aRecursiveMetadataParserin theParseContext,RecursiveMetadataParser's parse method is called. Before callingsuper.parse, the metadata object is empty. Aftersuper.parsereturns, the metadata object contains all of the metadata the decorated parser found andSystem.out.println(metadata)prints all of the metadata to standard output.  By creating a newBodyContentHandlerand passing that tosuper.parse, the text for each document is captured without mixing it with text from other documents.   When using the code above, if you have a container format that contains another container, you may wish to keep track of where in the stack you are. To do that, you'd want code something like:  private static class RecursiveTrackingMetadataParser extends ParserDecorator {
        private String location;
        private int unknownCount = 0;

        public RecursiveTrackingMetadataParser(Parser parser, String location) {
            super(parser);
            this.location = location;
            if (! this.location.endsWith(""/"")) {
               this.location += ""/"";
            }
        }

        @Override
        public void parse(
                InputStream stream, ContentHandler ignore,
                Metadata metadata, ParseContext context)
                throws IOException, SAXException, TikaException {
            // Work out what this thing is
            String objectName = null;
            if (metadata.get(TikaMetadataKeys.RESOURCE_NAME_KEY) != null) {
               objectName = metadata.get(TikaMetadataKeys.RESOURCE_NAME_KEY);
            } else if (metadata.get(TikaMetadataKeys.EMBEDDED_RELATIONSHIP_ID) != null) {
               objectName = metadata.get(TikaMetadataKeys.EMBEDDED_RELATIONSHIP_ID);
            } else {
               objectName = ""embedded-"" + (++unknownCount);
            }
            String objectLocation = this.location + objectName;

            // Fetch the contents, and recurse if possible
            ContentHandler content = new BodyContentHandler();
            Parser preContextParser = context.get(Parser.class);
            context.set(Parser.class, new RecursiveTrackingMetadataParser(getWrappedParser(), objectLocation));
            super.parse(stream, content, metadata, context);
            context.set(Parser.class, preContextParser);

            // Report what this one is
            System.out.println(""----"");
            System.out.println(""Resource is "" + objectLocation);
            System.out.println(""----"");
            System.out.println(metadata);
            System.out.println(""----"");
            System.out.println(content.toString());
        }
    }   The great thing aboutAutoDetectParseris that it can parse and extract text from almost anything. In particular, it can parse zip, tar, tar.bz2, and other archives that contain documents. If you have a zip file with 100 text files in it, using Jukka's example code you can get the text and metadata for each file nested inside of the zip file. What you might not expect is that you also get metadata and body text for the zip file itself.  Maybe this doesn't surprise you at all. My first reaction when I saw both metadata AND text for the zip file itself was ""What text could a zip file possibly have?"" My naive assumption was that a zip file wouldn't contain any text, and my assumption was wrong.  I was thinking that a zip, tar, or other archive file was simply a container for other files, and so didn't have any text of its own. Tika looks at archives differently; Tika sees an archive as being like a directory in a file system, and the text for an archive is a list of the contents of the archive.  If you have a zip file that contains 100 text files, after using the code on this page to get the text and metadata for each file, you will get the text and metadata for 101 files: 100 text files, and 1 zip file. The text for the zip file will list the names for each of the 100 text files it contains.  If you aren't interested in seeing text and metadata for the zip file itself, you'll want to take a look atmetadata.get(Metadata.CONTENT_TYPE))for each file Tika parses so you can skip the archives themselves. For a zip file, the content type is ""application/zip"".   ARecursiveParserWrapperthat is based on Jukka and Nick's example above was added to Tika as of 1.7.  The wrapper returns a list of Metadata objects – the first contains the metadata+content for the container document and the rest contain the metadata+content for each embedded document. The content of each document is stored in ""X-TIKA:content"", and the embedded document's location in the container document is stored in ""X-TIKA:embedded_resource_path"" (e.g. ""embedded-1/embed1.zip/embed2.zip/embed3.pdf"").  A downside to the wrapper is that it breaks the Tika goal of streaming output – the wrapper caches all metadata+content in memory.  This wrapper must be used with care.  As of Tika 1.7, a JSONified view of this output was integrated into tika-app (the -J option) and tika-server (""/rmeta"").  This format serves as the basis for the upcoming tika-eval module that will help with comparisons of the output of different versions of Tika or other content extractors.",../data/confluence_exports/TIKA/RecursiveMetadata_109454069.html
TIKA : TikaEvalAndStructuralComponents Background,"File formats often contain structural or stylistic elements, and Apache Tika attempts to normalize and represent some of these features in its XHTML output.  As of Tika 1.20, users can get counts of common XHTML tags (in Profile mode) and/or comparison counts of common XHTML tags (in Compare mode).  Users can also count ""tag exceptions"" – cases where the structure tags violate XML/XHTML requirements, e.g.<b><i></b></i>.   1) Simply counting structure tags offers only a rudimentary insight into the structure of a single extract or as a comparison between two extracts of the same source file.  One might want to apply a more advanced tree-based similarity/distance metric between two extracts – ourJIRAis open and committers are standing by.  2) If one one tool's extracts have more<p>elements than do another tool's that doesn't necessarily tell you that one extract is better than another.For example, one tool  – Tool A – might add<p>elements for every new line in a PDF:  <p>The quick brown fox</p>  <p>jumped over the lazy dog</p>  Another tool – Tool B – might apply heuristics to reconstruct logical paragraphs, such as<p>The quick brown fox jumped over the lazy dog. </p>  3) The tika-eval module is currently usingTagSoupon *.html files.TagSoupis designed to silently fix structural problems in the HTML so no exceptions will likely be thrown for non-compliant HTML.  We may alter this behavior in the future.  Tool A would have more<p>tags, but Tool B is probably capturing better information about the structure of the document.   While there are obvious limitations, the tag capability will allow users to identify:  If one tool is completely skipping or capturing fewer structural elements.  One could imagine a regression in Tika that dropped footnotes in DOCX, and this would quickly show that there were fewer<div>elements in the output from the more recent version; or one could quickly see that one parser is extracting URLs (<a>) elements from PDFs, but another isn't.Structural problems in Tika's XHTML output – there is an XML exception flag in the database if an error was thrown parsing Tika's output   If you are using Tika to generate .json files, follow the directions onTikaEvalfor how to create a directory of extracts,butdon't include the-toption:java -jar tika-app.X.Y.jar -J -i input_dir -o extracts.  This has the effect of storing the content that is extracted as XHTML, and it sets a metadata value ofToXMLContentHandlerfor the keyX-TIKA:content_handler.  When tika-eval finds that value set in the metadata, it parses the XHTML with a SAXParser to count the structure tags and extract the text.   As of Tika 1.21, the tika-eval module includes the following structural components:  <a><b><div><i><img><li><ol><p><table><td><title><tr><u><ul>  NOTE:For most parsers, aside from the HTML parser, Tika injects the file's ""title"" metadata component into the<title>tag.  For example, if the ""title"" metadata element in a PDF is ""Tika 2.0 – NextGen Extraction"", that will be injected into the XHTML as both a metadata itemdc:title, and in the content within<title>tags.  The HTML parser passes through the source HTML page's<title>contents.  Given how much Tika relies on<div>tags, we may at some point want to include theclassinformation so that we can tell instantly if there's a change in e.g.<div class=""slide-notes"">elements.  For now, we've chosen not to include the<h*>tags because, while the HTML parser passes those through, the other parsers do not tend to generate header tags.   While we added structural component extraction in Tika 1.20, we only added report writing in 1.21-SNAPSHOT.  Users of Tika 1.20 can use the H2 database directly for reporting, generate their own reports via a custom XML reporting config file or use the updated report configuration files from 1.21-SNAPSHOT available:for profile modeandfor comparison mode.",../data/confluence_exports/TIKA/TikaEvalAndStructuralComponents_109454085.html
TIKA : TikaEvalAndStructuralComponents Profile Mode,"As of Tika 1.21-SNAPSHOT, there are reports for:  Tags by mime – each row is a mime type, and there's a single column for the sum of the tags of each tag typeTag exceptions by mime – each row is a mime type, and there's a count for the number of XHTML extract files that triggered a parse exception in tika-evalTag exception details – this lists every XHTML extract file that triggered a parse exception in tika-eval",../data/confluence_exports/TIKA/TikaEvalAndStructuralComponents_109454085.html
TIKA : TikaEvalAndStructuralComponents Comparison Mode,"As of Tika 1.21-SNAPSHOT, there are reports for:  Tag comparisons by mime – each row is a mime type pair (mime type as identified by tool A and mime type as identified by tool B), and there are pairs of columns for each tag, e.g.tags_div_acontains the sum of thedivtags in extracts from tool A andtags_div_bcontains the sum of the div tags in extracts from tool B.Tag exceptions by mime pair – each row is a mime type pair (mime_type_aandmime_type_b) and there's a count for the number of XHTML extract files that triggered a parse exception in tika-evalTag exception details A – this lists every XHTML extract file from tool A that triggered a parse exception in tika-evalTag exception details B – this lists every XHTML extract file from tool B that triggered a parse exception in tika-eval",../data/confluence_exports/TIKA/TikaEvalAndStructuralComponents_109454085.html
TIKA : PooledTimeSeriesParser Install Pooled Time Series,"mkdir -p $HOME/git && cd $HOME/git && git clonehttps://github.com/USCDataScience/hadoop-pot.git2.cd hadoop-pot/hadoop-pot-assembly && mvn install assembly:assembly3. Follow steps 3, 4 and 5 fromthe install guide from Pooled Time  Seriesand confirm thatpooled_time_seriesinstalled correctly. Note thepre-requisites from Pooled Time Seriesrequire you to install OpenCV and set some environment variables.  After above steps you must be able to executepooled_time_seriesthrough terminal and get below output  usage: pooled_time_series
 -d,--dir <directory>            A directory with image files in it
 -f,--file <file>                Path to a single file
 -h,--help                       Print this message.
 -j,--json                       Set similarity output format to JSON.
                                 Defaults to .txt
 -o,--outputfile <output file>   File containing similarity results.
                                 Defaults to ./similarity.txt
 -p,--pathfile <path file>       A file containing full absolute paths to
                                 videos. Previous default was
                                 memex-index_temp.txt",../data/confluence_exports/TIKA/PooledTimeSeriesParser_109454067.html
TIKA : PooledTimeSeriesParser RunPooledTimeSeriesParser from Tika-App,"Grab an MP4,QuickTime, or other video (supported by your OpenCV implementation). Then run the following command:  java -classpath tika-app/target/tika-app-X.Y.jar org.apache.tika.cli.TikaCLI --config=$HOME/git/pooled_time_series/src/main/resources/tika-config.xml -J yourfile.mov  which should output something like:  [
    {
        ""Content-Length"": ""1926487"",
        ""Content-Type"": ""video/quicktime"",
        ""X-Parsed-By"": [
            ""org.apache.tika.parser.CompositeParser"",
            ""org.apache.tika.parser.pot.PooledTimeSeriesParser""
        ],
        ""X-TIKA:content"": ""<html xmlns=\""http://www.w3.org/1999/xhtml\"">\n<head>\n<meta name=\""of_frames\"" content=\""358\"" />\n<meta name=\""Content-Length\"" content=\""1926487\"" />\n<meta name=\""of_vecSize\"" content=\""200\"" />\n<meta name=\""og_frames\"" content=\""358\"" />\n<meta name=\""X-Parsed-By\"" content=\""org.apache.tika.parser.CompositeParser\"" />\n<meta name=\""X-Parsed-By\"" content=\""org.apache.tika.parser.pot.PooledTimeSeriesParser\"" />\n<meta name=\""Content-Type\"" content=\""video/quicktime\"" />\n<meta name=\""resourceName\"" content=\""yourfile.mov\"" />\n<meta name=\""og_vecSize\"" content=\""200\"" />\n<title></title>\n</head>\n<body><h3>Histogram of Optical Flows (HOF)</h3>\n<table =\""358\"" =\""200\""><tr>\t<td>0</td>\t<td>106.000000</td>\t<td>77.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>95.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>13.000000</td>\t<td>92.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>3.000000</td>\t<td>3.000000</td>\t<td>102.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>65.000000</td>\t<td>20.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>5.000000</td>\t<td>92.000000</td>\t<td>6.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>6.000000</td>\t<td>14.000000</td>\t<td>1.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>5.000000</td>\t<td>12.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>2.000000</td>\t<td>46.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>2.000000</td>\t<td>26.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>4.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>78.000000</td>\t<td>30.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>13.000000</td>\t<td>1.000000</td>\t<td>2.000000</td></tr>\n<tr>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>3.000000</td>\t<td>40.000000</td>\t<td>9.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>1.000000</td>\t<td>1.000000</td>\t<td>2.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>19.000000</td>\t<td>17.000000</td>\t<td>15.000000</td>\t<td>18.000000</td>\t<td>4.000000</td>\t<td>14.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>1.000000</td>\t<td>4.000000</td>\t<td>2.000000</td>\t<td>1.000000</td>\t<td>12.000000</td>\t<td>35.000000</td>\t<td>18.000000</td>\t<td>1.000000</td>\t<td>7.000000</td>\t<td>36.000000</td>\t<td>3.000000</td>\t<td>0.000000</td>\t<td>1.000000</td>\t<td>11.000000</td>\t<td>4.000000</td>\t<td>3.000000</td>\t<td>14.000000</td>\t<td>98.872983</td>\t<td>58.159994</td>\t<td>55.624418</td>\t<td>109.918199</td>\t<td>154.628142</td>\t<td>274.379973</td>\t<td>260.610341</td>\t<td>216.563245</td>\t<td>161.240045</td>\t<td>110.679989</td>\t<td>101.618012</td>\t<td>179.517287</td>\t<td>242.615251</td>\t<td>135.289986</td>\t<td>131.035653</td>\t<td>120.141349</td>\t<td>108.120592</td>\t<td>110.649989</td>\t<td>109.483038</td>\t<td>121.264360</td>\t<td>129.601505</td></tr>\n</table>\n</body></html>"",
        ""X-TIKA:parse_time_millis"": ""32054"",
        ""of_frames"": ""358"",
        ""of_vecSize"": ""200"",
        ""og_frames"": ""358"",
        ""og_vecSize"": ""200"",
        ""resourceName"": ""yourfile.mov""
    }
]",../data/confluence_exports/TIKA/PooledTimeSeriesParser_109454067.html
TIKA : PooledTimeSeriesParser Will this work from Tika Server?,"Yes, it will! Start Tika server like so:  java -jar tika-server/target/tika-server-X.Y.jar --config=$HOME/git/pooled_time_series/src/main/resources/tika-config.xml  Then send your movie file to Tika server like so:  curl -T yourfile.mov http://localhost:9998/rmeta  which should produce in response:  [
    {
        ""Content-Type"": ""video/quicktime"",
        ""X-Parsed-By"": [
            ""org.apache.tika.parser.CompositeParser"",
            ""org.apache.tika.parser.pot.PooledTimeSeriesParser""
        ],
        ""X-TIKA:content"": ""<html xmlns=\""http://www.w3.org/1999/xhtml\"">\n<head>\n<meta name=\""of_frames\"" content=\""358\"" />\n<meta name=\""of_vecSize\"" content=\""200\"" />\n<meta name=\""og_frames\"" content=\""358\"" />\n<meta name=\""X-Parsed-By\"" content=\""org.apache.tika.parser.CompositeParser\"" />\n<meta name=\""X-Parsed-By\"" content=\""org.apache.tika.parser.pot.PooledTimeSeriesParser\"" />\n<meta name=\""Content-Type\"" content=\""video/quicktime\"" />\n<meta name=\""og_vecSize\"" content=\""200\"" />\n<title></title>\n</head>\n<body><h3>Histogram of Optical Flows (HOF)</h3>\n<table =\""358\"" =\""200\""><tr>\t<td>0</td>\t<td>106.000000</td>\t<td>77.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>95.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>13.000000</td>\t<td>92.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>3.000000</td>\t<td>3.000000</td>\t<td>102.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>65.000000</td>\t<td>20.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>5.000000</td>\t<td>92.000000</td>\t<td>6.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>6.000000</td>\t<td>14.000000</td>\t<td>1.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>5.000000</td>\t<td>12.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>2.000000</td>\t<td>46.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>2.000000</td>\t<td>26.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>4.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>78.000000</td>\t<td>30.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>13.000000</td>\t<td>1.000000</td>\t<td>2.000000</td></tr>\n<tr>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>3.000000</td>\t<td>40.000000</td>\t<td>9.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>0.000000</td>\t<td>245.873308</td>\t<td>153.544600</td>\t<td>103.059990</td>\t<td>162.541708</td>\t<td>252.468921</td>\t<td>319.609416</td>\t<td>292.059971</td>\t<td>274.583075</td>\t<td>228.970014</td>\t<td>164.473069</td>\t<td>108.219989</td>\t<td>146.701403</td>\t<td>219.940923</td>\t<td>271.829481</td>\t<td>268.119973</td>\t<td>239.719107</td>\t<td>189.607921</td>\t<td>144.830008</td>\t<td>121.639988</td>\t<td>152.505474</td>\t<td>213.557055</td>\t<td>255.007528</td>\t<td>70.309993</td>\t<td>65.385228</td>\t<td>70.524468</td>\t<td>86.334997</td>\t<td>99.789990</td>\t<td>99.137078</td>\t<td>91.163868</td>\t<td>79.987551</td>\t<td>77.109992</td>\t<td>72.579427</td>\t<td>70.198524</td>\t<td>69.390141</td>\t<td>73.559993</td>\t<td>72.062835</td>\t<td>71.696460</td>\t<td>73.634280</td>\t<td>94.779991</td>\t<td>85.952229</td>\t<td>70.536712</td>\t<td>53.734593</td>\t<td>51.209995</td>\t<td>59.868269</td>\t<td>76.378587</td>\t<td>89.771828</td>\t<td>114.999988</td>\t<td>106.977985</td>\t<td>89.542021</td>\t<td>67.454518</td>\t<td>51.209995</td>\t<td>55.368092</td>\t<td>81.877135</td>\t<td>104.533636</td>\t<td>177.449982</td>\t<td>170.156562</td>\t<td>141.561883</td>\t<td>98.872983</td>\t<td>58.159994</td>\t<td>55.624418</td>\t<td>109.918199</td>\t<td>154.628142</td>\t<td>274.379973</td>\t<td>260.610341</td>\t<td>216.563245</td>\t<td>161.240045</td>\t<td>110.679989</td>\t<td>101.618012</td>\t<td>179.517287</td>\t<td>242.615251</td>\t<td>135.289986</td>\t<td>131.035653</td>\t<td>120.141349</td>\t<td>108.120592</td>\t<td>110.649989</td>\t<td>109.483038</td>\t<td>121.264360</td>\t<td>129.601505</td></tr>\n</table>\n</body></html>"",
        ""X-TIKA:parse_time_millis"": ""34291"",
        ""of_frames"": ""358"",
        ""of_vecSize"": ""200"",
        ""og_frames"": ""358"",
        ""og_vecSize"": ""200""
    }
]",../data/confluence_exports/TIKA/PooledTimeSeriesParser_109454067.html
TIKA : GeoTopicParser An Automatic Approach for Discovering and Geocoding Locations in Domain-Specific Web Data,"In Proceedings of the IEEE International Conference on Information Reuse and Integration, Pittsburgh, Pennsylvania, USA, July 28-30, 2016 |Read this article Authors: Chris A. Mattmann, Madhav Sharan",../data/confluence_exports/TIKA/GeoTopicParser_109454053.html
TIKA : GeoTopicParser Installing the Lucene Gazetteer,"First you will need to download theLucene Geo Gazetteerproject and to install it. You can do so by: $ cd $HOME/src
$ git clone https://github.com/chrismattmann/lucene-geo-gazetteer.git
$ cd lucene-geo-gazetteer
$ mvn install assembly:assembly
$ add $HOME/src/lucene-geo-gazetteer/src/main/bin to your PATH environment variable Once done, you can verify that the installation worked by running the following command: $ lucene-geo-gazetteer --help
usage: lucene-geo-gazetteer
 -b,--build <gazetteer file>           The Path to the Geonames
                                       allCountries.txt
 -c,--count <number of results>        Number of best results to be
                                       returned for one location
 -h,--help                             Print this message.
 -i,--index <directoryPath>            The path to the Lucene index
                                       directory to either create or read
 -json,--json                          Formats output in well defined json
                                       structure
 -s,--search <set of location names>   Location names to search the
                                       Gazetteer for
 -server,--server                      Launches Geo Gazetteer Service You will now need to build a Gazetteer using the Geonames.org dataset. Instructions are provided below. Note that you will need least 1.2 GB disk space for building Lucene Index for the Gazetteer. $ cd $HOME/src/lucene-geo-gazetteer
$ curl -O http://download.geonames.org/export/dump/allCountries.zip
$ unzip allCountries.zip
$ lucene-geo-gazetteer -i geoIndex -b allCountries.txt You can verify that the Gazetteer build worked by searching e.g., for Pasadena, and/or Texas: $ lucene-geo-gazetteer -s Pasadena Texas -json
{
    ""Pasadena"": [
        {
            ""admin1Code"": ""CA"",
            ""admin2Code"": ""037"",
            ""countryCode"": ""US"",
            ""latitude"": 34.14778,
            ""longitude"": -118.14452,
            ""name"": ""Pasadena""
        }
    ],
    ""Texas"": [
        {
            ""admin1Code"": ""TX"",
            ""admin2Code"": """",
            ""countryCode"": ""US"",
            ""latitude"": 31.25044,
            ""longitude"": -99.25061,
            ""name"": ""Texas""
        }
    ]
} Now you need to start REST service of lucene-geo-gazetteer. Tika uses this service internally $ lucene-geo-gazetteer -server You can verify that the REST API is responding by searching e.g., for Pasadena, and/or Texas: $ curl ""http://localhost:8765/api/search?s=Pasadena&s=Texas""
{
    ""Pasadena"": [
        {
            ""admin1Code"": ""CA"",
            ""admin2Code"": ""037"",
            ""countryCode"": ""US"",
            ""latitude"": 34.14778,
            ""longitude"": -118.14452,
            ""name"": ""Pasadena""
        }
    ],
    ""Texas"": [
        {
            ""admin1Code"": ""TX"",
            ""admin2Code"": """",
            ""countryCode"": ""US"",
            ""latitude"": 31.25044,
            ""longitude"": -99.25061,
            ""name"": ""Texas""
        }
    ]
} Note that we used the convenience scriptlucene-geo-gazetteerwhich assumes that you created an indexed named geoIndex in the $HOME/src/lucene-geo-gazetter/geoIndex directory. We could have also used the pure Java command line to search. The return from the Gazetteer is a JSON List of Object structures in which the structure is a key->Object List map. The key is the location name given and the Object List is a list of most popular location objects in the Gazetteer for that name.",../data/confluence_exports/TIKA/GeoTopicParser_109454053.html
TIKA : GeoTopicParser Installing and downloading an NER model,"The next thing you'll need is a Named Entity Recognition model for places. TheGeoTopicParseruses Apache OpenNLP and with its 1.5 version, OpenNLP provides already trained models for location names in text data. You can download theen-ner-location.binfile already pre-trained by the OpenNLP folks. One thing to note is that OpenNLP's default name finder is not accurate, so building your own NER location model is highly recommended. In this case, please followthese instructions. The model needs to be placed on the classpath for your Tika installation in the following directory: org/apache/tika/parser/geo/ The following instructions show how to download the model and place it on the right path: $ mkdir $HOME/src/location-ner-model && cd $HOME/src/location-ner-model
$ curl -O https://opennlp.sourceforge.net/models-1.5/en-ner-location.bin
$ mkdir -p org/apache/tika/parser/geo/
$ mv en-ner-location.bin org/apache/tika/parser/geo/",../data/confluence_exports/TIKA/GeoTopicParser_109454053.html
TIKA : GeoTopicParser Test out theGeoTopicParser,"Now you can run Tika and try out theGeoTopicParser. At the moment since it's a Parser and not a Content-Handler (hopefully will develop it later), the parser is mapped to the MIME type application/geotopic which is a sub-class of text/plain. So, there are two steps to try the parser out now. Create a .geot file, you can use this samplefilefrom theNSF Polar data contributed to TREC. 2. Tell Tika about the application/geotopic MIME type. You can download thisfileand place it on the classpath in theorg/apache/tika/mimedirectory, e.g., by doing:$ mkdir $HOME/src/geotopic-mime && cd $HOME/src/geotopic-mime
$ mkdir -p org/apache/tika/mime
$ curl -O https://raw.githubusercontent.com/chrismattmann/geotopicparser-utils/master/mime/org/apache/tika/mime/custom-mimetypes.xml
$ mv custom-mimetypes.xml org/apache/tika/mime With those files in place, let's use theGeoTopicParserusing Tika-App: $ java -classpath tika-app/target/tika-app-<LATEST-VERSION>-SNAPSHOT.jar:tika-parsers/tika-parsers-ml/tika-parser-nlp-package/target/tika-parser-nlp-package-<LATEST-VERSION>-SNAPSHOT.jar:$HOME/src/location-ner-model:$HOME/src/geotopic-mime org.apache.tika.cli.TikaCLI -m polar.geot This should output: Content-Length: 881
Content-Type: application/geotopic
Geographic_LATITUDE: 27.33931
Geographic_LONGITUDE: -108.60288
Geographic_NAME: China
Optional_LATITUDE1: 39.76
Optional_LONGITUDE1: -98.5
Optional_NAME1: United States
X-Parsed-By: org.apache.tika.parser.DefaultParser
X-Parsed-By: org.apache.tika.parser.geo.topic.GeoParser
resourceName: polar.geot The output will output 3-tuples of {Name, Latitude, Longitude}. The *best* match for the location is the one that occurs most frequently in the text, and that is provided asGeographic_NAME, along with its correspondingGeographic_LATITUDEandGeographic_LONGITUDE. Places also identified as entities by the NER model in the provide text are also listed asOptional_NAME*N*, e.g.,Optional_NAME1for the 1st alternative location identified and its correspondingOptional_LATITUDE1andOptional_LONGITUDE1.",../data/confluence_exports/TIKA/GeoTopicParser_109454053.html
TIKA : GeoTopicParser Will this work from Tika Server?,"It sure will! When you start Tika Server, make sure that the NER model file and the custom MIME type are on your classpath, and that the lucene-geo-gazetteer is on the$PATHwhere Tika-Server is started, and you can post all the .geot files that you'd like and Tika-Server will happily call theGeoTopicParserto provide you location information. First, start up the Tika server with your NER model and .geot MIME type definition on the classpath: java -classpath $HOME/src/location-ner-model:$HOME/src/geotopic-mime:tika-server/tika-server-standard/target/tika-server-standard-<LATEST-VERSION>-SNAPSHOT.jar:tika-parsers/tika-parsers-ml/tika-parser-nlp-package/target/tika-parser-nlp-package-<LATEST-VERSION>-SNAPSHOT.jar org.apache.tika.server.core.TikaServerCli Then, try calling the/rmetaservice to get the returned metadata: curl -T $HOME/src/geotopicparser-utils/geotopics/polar.geot -H ""Content-Disposition: attachment; filename=polar.geot"" http://localhost:9998/rmeta And then look for it to return the following, that's it! [
   {
      ""Content-Type"":""application/geotopic"",
      ""Geographic_LATITUDE"":""39.76"",
      ""Geographic_LONGITUDE"":""-98.5"",
      ""Geographic_NAME"":""United States"",
      ""Optional_LATITUDE1"":""27.33931"",
      ""Optional_LONGITUDE1"":""-108.60288"",
      ""Optional_NAME1"":""China"",
      ""X-Parsed-By"":[
         ""org.apache.tika.parser.DefaultParser"",
         ""org.apache.tika.parser.geo.topic.GeoParser""
      ],
      ""X-TIKA:parse_time_millis"":""1634"",
      ""resourceName"":""polar.geot""
   }
]",../data/confluence_exports/TIKA/GeoTopicParser_109454053.html
TIKA : TikaOCR Issues with Installing via Brew,"If you have trouble installing via Brew, you can try installing Tesseractfrom source.",../data/confluence_exports/TIKA/TikaOCR_109454096.html
TIKA : TikaOCR Tesseract won't work with TIFF files,"If you are having trouble getting Tesseract to work with TIFF files, read thislink. Summary: uninstall tesseract:brew uninstall tesseractuninstall leptonica:brew uninstall leptonicainstall leptonica with tiff support:brew install leptonica --with-libtiffinstall tesseract:brew install tesseract tesseract-lang  Add ""epel"" to your yum repositories if it isn't already installed1a.wgethttps://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm(or appropriate version)1b.rpm -Uvh epel-release-latest-7.noarch.rpm2.yum install tesseract3. To add language packs, see what's availableyum search tesseractthen, e.g.yum install tesseract-langpack-ara  sudo apt-get updatesudo apt-get install tesseract-ocrTo add language packs, see what's available then, e.g.sudo apt-get install tesseract-ocr-fra  SeeUB-Mannheim.  There's some advice on the Tesseract github issues + wiki on ways to speed it up, eg#263and#1171andthis wiki page.  Once you have Tesseract installed, you should test it to make sure it's working. A nice command line test: tesseract -psm 3 /path/to/tiff/file.tiff out.txt You should see the output of the text extraction in out.txt. cat out.txt Look for the text extracted by Tesseract. Once you have confirmed Tesseract is working, then you can simply use the Tika-app, built with 1.7-SNAPSHOT or later to use Tika OCR. For example, try that same file above with Tika: tika -t /path/to/tiff/file.tiff That's it! You should see the text extracted by Tesseract and flowed through Tika.  Once you have Tesseract and a fresh build of Tika 1.7-SNAPSHOT (including Tika server), you can easily use Tika-Server with Tesseract. For example, to post a TIFF file to the server and get back its OCR extracted text, run the following commands:",../data/confluence_exports/TIKA/TikaOCR_109454096.html
"TIKA : TikaOCR in another window, start Tika server",java -jar /path/to/tika-server-1.7-SNAPSHOT.jar,../data/confluence_exports/TIKA/TikaOCR_109454096.html
"TIKA : TikaOCR in another window, issue a cURL request","curl -T /path/to/tiff/image.tiffhttp://localhost:9998/tika--header ""Content-type: image/tiff""",../data/confluence_exports/TIKA/TikaOCR_109454096.html
TIKA : TikaOCR Overriding the configured language as part of your request,"Different requests may need processing using different language models. These can be specified for specific requests using theX-Tika-OCRLanguagecustom header. An example of this is shown below: curl -T /path/to/tiff/image.jpghttp://localhost:9998/tika--header ""X-Tika-OCRLanguage: eng"" Or for multiple languages: curl -T /path/to/tiff/image.jpghttp://localhost:9998/tika--header ""X-Tika-OCRLanguage: eng+fra""  In Tika 2.x, users can modify configurations via atika-config.xml. With the exceptions of the paths, we document the defaults in the following: TesseractOCR Configuration<properties>
  <parsers>
    <parser class=""org.apache.tika.parser.DefaultParser"">
      <!-- this is not formally necessary, but prevents loading of unnecessary parser -->
      <parser-exclude class=""org.apache.tika.parser.ocr.TesseractOCRParser""/>
    </parser>
    <parser class=""org.apache.tika.parser.ocr.TesseractOCRParser"">
      <params>
        <!-- these are the defaults; you only need to specify the ones you want
             to modify -->
        <param name=""applyRotation"" type=""bool"">false</param>
        <param name=""colorSpace"" type=""string"">gray</param>
        <param name=""density"" type=""int"">300</param>
        <param name=""depth"" type=""int"">4</param>
        <param name=""enableImagePreprocessing"" type=""bool"">false</param>
        <param name=""filter"" type=""string"">triangle</param>
        <param name=""imageMagickPath"" type=""string"">/my/custom/imageMagicPath</param>
        <param name=""language"" type=""string"">eng</param>
        <param name=""maxFileSizeToOcr"" type=""long"">2147483647</param>
        <param name=""minFileSizeToOcr"" type=""long"">0</param>
        <param name=""pageSegMode"" type=""string"">1</param>
        <param name=""pageSeparator"" type=""string""></param>
        <param name=""preserveInterwordSpacing"" type=""bool"">false</param>
        <param name=""resize"" type=""int"">200</param>
        <param name=""skipOcr"" type=""bool"">false</param>
        <param name=""tessdataPath"" type=""string"">/my/custom/data</param>
        <param name=""tesseractPath"" type=""string"">/my/custom/path</param>
        <param name=""timeoutSeconds"" type=""int"">120</param>
      </params>
    </parser>
  </parsers>
</properties>  See alsoPDFParser notesfor more details on options for performing OCR on PDFs. Note: With Tika server 1.x, the PDFConfig is generated for each document, so any configurations that you may specify in the tika-config.xml file that you pass to the tika-server on startup are overwritten.  This behavior is changed in Tika 2.x, where the PDFConfig remembers settings from tika-config.xml and will only temporarily update custom configs sent via headers. To go with option 1 for OCR'ing PDFs (run OCR against inline images), you need to specify configurations for the PDFParser like so: curl -T testOCR.pdfhttp://localhost:9998/rmeta/text--header ""X-Tika-PDFextractInlineImages: true"" To go with option 2 (render each page and then run OCR on that rendered image), you need to specify the ocr strategy:curl -T testOCR.pdfhttp://localhost:9998/tika--header ""X-Tika-PDFOcrStrategy: ocr_only"" Note: These two options are independent.  If you setextractInlineImagesto true and select anOcrStrategythat includes OCR on the rendered page, Tika will run OCR on the extracted inline imagesandthe rendered page.  Tika's OCR will trigger on images embedded within, say, office documents in addition to images you upload directly. Because OCR slows down Tika, you might want to disable it if you don't need the results. You can disable OCR by simply uninstalling tesseract, but if that's not an option, here is a tika.xml config file that disables OCR: <?xml version=""1.0"" encoding=""UTF-8""?>
<properties>
  <parsers>
    <parser class=""org.apache.tika.parser.DefaultParser"">
      <parser-exclude class=""org.apache.tika.parser.ocr.TesseractOCRParser""/>
    </parser>
  </parsers>
</properties> In Tika 2.x, you can selectively turn off OCR per parse programmatically by settingskipOcron aTesseractOCRConfig. This will only affect that one call to parse. TesseractOCRConfig config = new TesseractOCRConfig();
        config.setSkipOcr(true);
        ParseContext context = new ParseContext();
        context.set(TesseractOCRConfig.class, config);
        
        Parser parser = new AutoDetectParser();
        parser.parse(inputStream, handler, metadata, context); In Tika 2.x,  withtika-server, add this header to skip OCR per request:X-Tika-OCRskipOcr: true  Tika will run preprocessing of images (rotation detection and image normalizing with ImageMagick) before sending the image to tesseract if the user has included dependencies (listed below) and if the user opts to include these preprocessing steps.",../data/confluence_exports/TIKA/TikaOCR_109454096.html
TIKA : TikaOCR To identify rotation,"python must be installed with scikit-image and numpy pip3 install numpy pip3 install scikit-image (As of January 5, 2021, there's a bug in the most recent numpy for Windows, specify 1.19.3:pip3 install numpy==1.19.3) In Tika 2.0,python3must be installed and callable aspython3.",../data/confluence_exports/TIKA/TikaOCR_109454096.html
TIKA : TikaOCR Install ImageMagick,"See:https://imagemagick.org/script/download.php iOS: brew install imagemagick Ubuntu: sudo apt install imagemagick Windows: download the binary installer from the above page, e.g.https://imagemagick.org/download/binaries/ImageMagick-7.0.10-55-Q16-HDRI-x64-dll.exe  TODO: document how to configure these options in Tika",../data/confluence_exports/TIKA/TikaOCR_109454096.html
TIKA : TikaEval Single Output from One Tool (Profile),"NOTE:assume the original input files are in a directory namedinput_docsand that the text extracts are written to theextractsdirectory, with each extract file having the same sub-directory path and same file name with '.json' or '.txt' appended to it. Create a directory of extract files that mirrors your input directory. These files may be UTF-8 text files with '.txt' appended to the original file's name or they may be the RecursiveParserWrapper's '.json' representation:java -jar tika-app-X.Y.jar -J -t -i input_docs -o extractsProfile the directory of extracts and create a local H2 database:java -jar tika-eval-X.Y.jar Profile -extracts extracts -db profiledbWrite reports from the database:java -jar tika-eval-X.Y.jar Report -db profiledb You'll have a directory of .xlsx reports under the ""reports"" directory.Note:if you don't need the full tika-eval-app, you can get many of these statistics at parse time via the TikaEvalMetadataFilter (see:ModifyingContentWithHandlersAndMetadataFilters).",../data/confluence_exports/TIKA/TikaEval_109454084.html
TIKA : TikaEval Comparing Output from Two Tools/Settings (Compare),"NOTE:assume the original input files are in a directory namedinput_docsand that the text extracts from tool A are written to theextractsAdirectory and the extracts from tool B are written toextractsB. Create two directories of extract files that mirror your input directory. These files may be UTF-8 text files with '.txt' appended to the original file's name or they may be the RecursiveParserWrapper's '.json' representation.Compare the extract directory A with extract directory B and write results to a local H2 database:java -jar tika-eval-X.Y.jar Compare -extractsA extractsA -extractsB extractsB -db comparisondbWrite reports from the database:java -jar tika-eval-X.Y.jar Report -db comparisondb You'll have a directory of .xlsx reports under the ""reports"" directory.",../data/confluence_exports/TIKA/TikaEval_109454084.html
TIKA : TikaEval Investigating the Database,"Launch the H2 localhost server:java -jar tika-eval-X.Y.jar StartDB– this callsjava -cp ... org.h2.tools.Console -webNavigate a browser tohttp://localhost:8082and enter the jdbc connector code followed by thefull pathto your db file:jdbc:h2:/C:/users/someone/mystuff/tika-eval/comparisondb If your reaction is: ""You call this a database?!"", please open tickets and contribute to improving the structure. SeeTikaEvalDbDesignfor more information on the underlying structure of the database.",../data/confluence_exports/TIKA/TikaEval_109454084.html
TIKA : TikaEval Evaluating Success via Common Words,"In the absence of ground truth, it is often helpful to count the number of common words that were extracted (seeTikaEvalMetricsfor a discussion of this). ""Common words"" are specified per language in the ""resources/commonwords"" directory.Each file is named for the language code, e.g. 'en', and each file is a UTF-8 text file with one word per line. The token processor runs language id against content and then selects the appropriate set of common words for its counts. If there is no common words file for a language, then it backs off to the default list, which is currently hardcoded to 'en'. Make sure that your common words have gone through the same analysis chain as specified by the Common Words analyzer in 'lucene-analyzers.json'!",../data/confluence_exports/TIKA/TikaEval_109454084.html
TIKA : TikaEval alterExtract,"Let's say you want to compare the output of Tika to another tool that extracts text. You happen to have a directory of .json files for Tika and a directory of UTF-8 .txt files from the other tool. If the other tool extracts embedded content, you'd want to concatenate all the content within Tika's .json file for a fair comparison:java -jar tika-eval-X.Y.jar Compare -extractsA tika_1_14 -extractsB tika_1_15 -db comparisondb -alterExtract concatenate_contentIf the other tool does not extract embedded content, you'd only want to look at the first metadata object (representing the container file) in the .json file:java -jar tika-eval-X.Y.jar Compare -extractsA tika_1_14 -extractsB tika_1_15 -db comparisondb -alterExtract first_only",../data/confluence_exports/TIKA/TikaEval_109454084.html
TIKA : TikaEval Min/Max Extract Size,"You may find that some extracts are too big to fit in memory, in which case use-maxExtractSize <maxBytes>, or you may want to focus only on extracts that are greater than a minimum length:-minExtractSize <minBytes>.",../data/confluence_exports/TIKA/TikaEval_109454084.html
TIKA : TikaEval Reports,"The module tika-eval comes with a list of reports. However, you might want to generate your own. Each report is specified by SQL and a few other configurations in an xml file. Seecomparison-reports.xmlandprofile-reports.xmlto get a sense of the syntax. To specify your own reports on the commandline, use-rf(report file):java -jar tika-eval-X.Y.jar Report -db comparisondb -rf myreports.xml If you'd like to write the reports to a root directory other than 'reports', specify that with-rd(report directory):java -jar tika-eval-X.Y.jar Report -db comparisondb -rd myreportdir Again, seeTikaEvalDbDesignfor more information on the underlying structure of the database.",../data/confluence_exports/TIKA/TikaEval_109454084.html
TIKA : ModifyingContentWithHandlersAndMetadataFilters 1. ContentHandlers,"These are applied during the parse by classes that implementorg.xml.saxContentHandler.  A small handful may cache contents in memory.  One small risk for these is that there's no guarantee that parsers will pass in meaningful amounts of text in the call tocharacters(); theoretically, a parser could write one character at a time, which would render a regex matching handler useless. Programmatically, users have control to use any of theContentHandlersintika-coreor they can write their ownContentHandlers.  If doing this, make sure to consider theContentHandlerDecoratorwhich allows overriding only the methods you need; also consider using theTeeContentHandler, which allows multiple handlers to be run during the parse. An example of using theTeeContentHandlerto add a language detection handler to the regularToXMLContentHandler: TeeContentHandler...
ContentHandler xmlHandler = new ToXMLContentHandler();
LanguageHandler langHandler = new LanguageHandler();
ContentHandler tee = new TeeContentHandler(xmlHandler, langHandler);
parser.parse(stream, tee, metadata, context);
LanguageResult result = langHandler.getLanguage();
metadata.set(TikaCoreProperties.LANGUAGE, result.getLanguage());
...  Some common content handlers are specified for tika-server's/tika json outputand the/rmetaendpoint by appending ""/xml"", ""/text"", ""/html"", ""/body"" or ""/ignore"" to the endpoint. To set customContentHandlerDecoratorsviatika-config.xml, set theContentHandlerDecoratorFactoryin the<autoDetectParserConfig/>element intika-config.xml. In this example, we're calling atest classthat simply upcases all characters in the content handler. ContentHandlerDecoratorFactory<?xml version=""1.0"" encoding=""UTF-8""?>
<properties>
  <!-- we're including the <parsers/> element to show that it is a separate element from the
       autoDetectParserConfig element.  If it is not included, the standard default parser will
       be used -->
  <parsers>
    <parser class=""org.apache.tika.parser.DefaultParser"">
      <parser-exclude class=""org.apache.tika.parser.microsoft.OfficeParser""/>
    </parser>
    <parser class=""org.apache.tika.parser.microsoft.OfficeParser"">
      <params>
        <param name=""byteArrayMaxOverride"" type=""int"">700000000</param>
      </params>
    </parser>
  </parsers>  
  <!-- note that the autoDetectParserConfig element is separate from the <parsers/> element.
       The composite parser built in the <parsers/> element is used as the base parser
       for the AutoDetectParser. -->
  <autoDetectParserConfig>
    <contentHandlerDecoratorFactory class=""org.apache.tika.sax.UpcasingContentHandlerDecoratorFactory""/>
  </autoDetectParserConfig>
</properties>",../data/confluence_exports/TIKA/ModifyingContentWithHandlersAndMetadataFilters_217385627.html
TIKA : ModifyingContentWithHandlersAndMetadataFilters 2. Metadata Filters,"These are applied at the end of the parse.  These are intended to modify the contents of a metadata object for different purposes: Enrich the data (similar to a ContentHandler) -- these metadata filters might run language detection on the cached contents at the end of the parse.Modify the metadata contents – one might want to run a regex over a specific field and extract only the information matching a regex, for example.Modify the metadata keys – If you need to rename metadata keys before emitting the object to, say, OpenSearch, you can use theFieldNameMappingFilterLimit the metadata fields -- let's say you only wantdc:titleandtext, you can use these:ExcludeFieldMetadataFilterorIncludeFieldMetadataFilterNOTE:these were created before we had MetadataWriteFilters; those are probably a better option for this behavior. Metadata filters are specified in the<metadataFilters/>element intika-config.xml.  They are run in order, and order matters. SeeTikaServerEndpointsComparedfor which endpoints apply metadataFilters in tika-server.  Metadata filters are applied in tika-pipes and tika-app when using the -J option.  MetadataFilters are not applied when Tika streams output.",../data/confluence_exports/TIKA/ModifyingContentWithHandlersAndMetadataFilters_217385627.html
TIKA : ModifyingContentWithHandlersAndMetadataFilters FieldNameMappingFilter,"This is used to select fields to include and to rename fields from the Tika names to preferred names.  This was initially designed for modifying field names before emitting document to OpenSearch or Solr. FieldNameMappingFilter<?xml version=""1.0"" encoding=""UTF-8"" ?>
<properties>
  <metadataFilters>
    <metadataFilter class=""org.apache.tika.metadata.filter.FieldNameMappingFilter"">
      <params>
        <excludeUnmapped>true</excludeUnmapped>
        <mappings>
          <mapping from=""X-TIKA:content"" to=""content""/>
          <mapping from=""Content-Length"" to=""length""/>
          <mapping from=""dc:creator"" to=""creators""/>
          <mapping from=""dc:title"" to=""title""/>
          <mapping from=""Content-Type"" to=""mime""/>
          <mapping from=""X-TIKA:EXCEPTION:container_exception"" to=""tika_exception""/>
        </mappings>
      </params>
    </metadataFilter>
  </metadataFilters>
</properties>",../data/confluence_exports/TIKA/ModifyingContentWithHandlersAndMetadataFilters_217385627.html
TIKA : ModifyingContentWithHandlersAndMetadataFilters DateNormalizingMetadataFilter,"Some file formats store timezone, others don't. By default, OpenSearch and Solr need timezones.  This filter respects dates with timezones, and blindly adds a UTC timezone to dates that do not have a time zone. DateNormalizingMetadataFilter<?xml version=""1.0"" encoding=""UTF-8"" ?>
<properties>
  <metadataFilters>
    <!-- depending on the file format, some dates do not have a timezone. This
     filter arbitrarily assumes dates have a UTC timezone and will format all
     dates as yyyy-MM-dd'T'HH:mm:ss'Z' whether or not they actually have a timezone.
     -->
    <metadataFilter class=""org.apache.tika.metadata.filter.DateNormalizingMetadataFilter""/>
  </metadataFilters>
</properties>",../data/confluence_exports/TIKA/ModifyingContentWithHandlersAndMetadataFilters_217385627.html
TIKA : ModifyingContentWithHandlersAndMetadataFilters GeoPointMetadataFilter,"If a metadata object has aTikaCoreProperties.LATITUDEand aTikaCoreProperties.LONGITUDE, this concatenates those fields with a comma delimiter asLATITUDE,LONGITUDEand adds that value to the field specified bygeoPointFieldName.Note:This was added in Tika 2.5.1. GeoPointMetadataFilter<?xml version=""1.0"" encoding=""UTF-8"" ?>
<properties>
  <metadataFilters>
    <metadataFilter class=""org.apache.tika.metadata.filter.GeoPointMetadataFilter"">
      <params>
        <-- default: ""location"" -->
        <geoPointFieldName>myGeoPoint</geoPointFieldName>
      </params>
    </metadataFilter>
  </metadataFilters>
</properties>",../data/confluence_exports/TIKA/ModifyingContentWithHandlersAndMetadataFilters_217385627.html
TIKA : ModifyingContentWithHandlersAndMetadataFilters TikaEvalMetadataFilter,"If the tika-eval-core jar is on the classpath, this filter should be added automatically. Users may specify it as below.  This runs Tika's custom version of OpenNLP's language detector and includes counts for tokens, unique tokens, alphabetic tokens and the ""oov"" (% out of vocabulary) statistic. SeeTikaEvalfor more details on thetika-eval-app. TikaEvalMetadataFilter<?xml version=""1.0"" encoding=""UTF-8"" ?>
<properties>
  <metadataFilters>
    <metadataFilter class=""org.apache.tika.eval.core.metadata.TikaEvalMetadataFilter""/>
  </metadataFilters>
</properties>",../data/confluence_exports/TIKA/ModifyingContentWithHandlersAndMetadataFilters_217385627.html
TIKA : ModifyingContentWithHandlersAndMetadataFilters LanguageDetection,"Two language detectors have a metadata filter option (OpenNLPMetadataFilter and the OptimaizeMetadataFilter).  These are applied to theX-TIKA:contentfield at the end of the parse.  This is an example of specifying the OptimaizeLanguageDetector. The language id will be added to the metadata  object with the TikaCoreProperties.TIKA_DETECTED_LANGUAGE key. LanguageDetection<?xml version=""1.0"" encoding=""UTF-8""?>
<properties>
  <metadataFilters>
    <metadataFilter class=""org.apache.tika.langdetect.optimaize.metadatafilter.OptimaizeMetadataFilter"">
      <params>
        <maxCharsForDetection>10000</maxCharsForDetection>
      </params>
    </metadataFilter>
  </metaFilters>
</properties>",../data/confluence_exports/TIKA/ModifyingContentWithHandlersAndMetadataFilters_217385627.html
TIKA : ModifyingContentWithHandlersAndMetadataFilters ClearByMimeMetadataFilter,"When using theRecursiveParserWrapper(the/rmetaendpoint intika-serveror the-Joption intika-app), you can delete metadata objects for specific file types. ClearByMimeMetadataFilter<?xml version=""1.0"" encoding=""UTF-8""?>
<properties>
  <metadataFilters>
    <metadataFilter class=""org.apache.tika.metadata.filter.ClearByMimeMetadataFilter"">
      <params>
		<!-- this will remove metadata objects for jpegs and pdfs; more seriously, 
             this may be useful for image files or emf or wmf depending on your use case -->
        <mimes>
          <mime>image/jpeg</mime>
          <mime>application/pdf</mime>
        </mimes>
      </params>
    </metadataFilter>
  </metaFilters>
</properties>",../data/confluence_exports/TIKA/ModifyingContentWithHandlersAndMetadataFilters_217385627.html
TIKA : ModifyingContentWithHandlersAndMetadataFilters IncludeFieldMetadataFilter,"This removes all other metadata fields after the parse except those specified here. MetadataFilters<?xml version=""1.0"" encoding=""UTF-8""?>
<properties>
  <metadataFilters>
    <metadataFilter class=""org.apache.tika.metadata.filter.IncludeFieldMetadataFilter"">
      <params>
        <include>
          <field>X-TIKA:content</field>
          <field>extended-properties:Application</field>
          <field>Content-Type</field>
        </include>
      </params>
    </metadataFilter>
  </metadataFilters>
</properties>",../data/confluence_exports/TIKA/ModifyingContentWithHandlersAndMetadataFilters_217385627.html
TIKA : ModifyingContentWithHandlersAndMetadataFilters 3. Metadata Write Filters,"These filters are applied during the parse. The primary goal of the metadata write filters is to limit the the amount of data written to a metadata object for two purposes: Limit the total number of bytes written to a metadata objects (prevent DoS from files with large amounts of metadata)Limit the fields written to a metadata object (decrease bytes held in memory during the parse and decrease the bytes sent over the wire/written to a file after the parse To configure theStandardWriteFilter, set the properties in its factory in the<autoDetectParserConfig/>element in thetika-config.xmlfile. StandardWriteFilter<?xml version=""1.0"" encoding=""UTF-8""?>
<properties>
  <autoDetectParserConfig>
    <metadataWriteFilterFactory class=""org.apache.tika.metadata.writefilter.StandardWriteFilterFactory"">
      <params>
		<!-- all measurements are in UTF-16 bytes. If any values are truncated, 
			TikaCoreProperties.TRUNCATED_METADATA is set to true in the metadata object -->

        <!-- the maximum size for a metadata key. -->
        <maxKeySize>1000</maxKeySize>

        <!-- max total size for a field in UTF-16 bytes.  If a field has multiple values, 
			their lengths are summed to calculate the field size. -->
        <maxFieldSize>10000</maxFieldSize>

        <!-- max total estimated byte is a sum of the key sizes and values -->
        <maxTotalEstimatedBytes>100000</maxTotalEstimatedBytes>
  
        <!-- limit the count of values for multi-valued fields -->
        <maxValuesPerField>100</maxValuesPerField>
        <!-- include only these fields. NOTE, however that there a several fields that are 
			 important to the parse process and these fields are always allowed in addition 
			 (see ALWAYS_SET_FIELDS and ALWAYS_ADD_FIELDS in the StandardWriteFilter -->
        <includeFields>
          <field>dc:creator</field>
          <field>dc:title</field>
        </includeFields>
      </params>
    </metadataWriteFilterFactory>
  </autoDetectParserConfig>
</properties> If you need different behavior, implement aWriteFilterFactory, add it to your classpath and specify it in thetika-config.xml.",../data/confluence_exports/TIKA/ModifyingContentWithHandlersAndMetadataFilters_217385627.html
TIKA : ModifyingContentWithHandlersAndMetadataFilters 4. AutoDetectParserConfig,"We've mentioned briefly above some of the factories that can be modified in theAutoDetectParserConfig.  There are other parameters that can be used to modify the behavior of theAutoDetectParservia thetika-config.xml. AutoDetectParserConfig<?xml version=""1.0"" encoding=""UTF-8""?>
<properties>
  <autoDetectParserConfig>
    <params>
      <!-- if the incoming metadata object has a ContentLength entry and it is larger than this
           value, spool the file to disk; this is useful for some file formats that are more efficiently
           processed via a file instead of an InputStream -->
      <spoolToDisk>100000</spoolToDisk>
      <!-- the next four are parameters for the SecureContentHandler -->
      <!-- threshold used in zip bomb detection. This many characters must be written
           before the maximum compression ratio is calculated -->
      <outputThreshold>10000</outputThreshold>
      <!-- maximum compression ratio between output characters and input bytes -->
      <maximumCompressionRatio>100</maximumCompressionRatio>
      <!-- maximum XML element nesting level -->
      <maximumDepth>100</maximumDepth>
      <!-- maximum embedded file depth -->
      <maximumPackageEntryDepth>100</maximumPackageEntryDepth>
      <!-- as of Tika &gt; 2.7.0, you can skip the check and exception for a zero-byte inputstream-->
      <throwOnZeroBytes>false</throwOnZeroBytes>
    </params>
    <!-- as of Tika 2.5.x, this is the preferred way to configure digests -->
    <digesterFactory class=""org.apache.tika.parser.digestutils.CommonsDigesterFactory"">
      <params>
        <markLimit>100000</markLimit>
        <!-- this specifies SHA256, base32 and MD5 -->
        <algorithmString>sha256:32,md5</algorithmString>
      </params>
    </digesterFactory>   
  </autoDetectParserConfig>
</properties> TODO: add an example of the EmbeddedDocumentExtractorFactory TODO: add a 5th? section for writelimiting",../data/confluence_exports/TIKA/ModifyingContentWithHandlersAndMetadataFilters_217385627.html
TIKA : tika-pipes Security Warning,"NOTE:The tika-pipes modules in combination with tika-server open potential security vulnerabilities if you do not carefully limit access to tika-server.  If the tika-pipes modules are turned on, anyone with access to your tika-server has the read and write permissions of the tika-server, and they will be able to read data and to forward the parsed results to whatever you've configured (see, for example:https://en.wikipedia.org/wiki/Server-side_request_forgery).  The tika-pipes modules for tika-server are intended to be run intightly controlled networks, e.g. Docker. DO NOTuse tika-pipes if your tika-server is exposed to the internet or if you do not carefully restrict access to tika-server. Consider adding two-way TLS encryption to your client and server, a beta version of which is available in 2.4.0:TikaServer#SSL(Beta).",../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes Overview,"The tika-pipes modules enablefetchingdata from various sources, running the parse and thenemittingthe output to various destinations.  These modules are built around theRecursiveParserWrapperoutput model (-Joption intika-appand/rmetaendpoint intika-server-standard).  Users can specify content format (text/html/body) and set limits (number of embedded files, max content length) viaFetchEmitTuples.  Further, users can addMetadata Filtersto select and modify the metadata that is extracted during the parse before emitting the output. We need to improve how to add dependencies.  Very few of the fetchers/emitters are embedded intika-apportika-server-standard.  For now, users can download required jars from maven central, e.g. the S3Emitter is available:https://repo1.maven.org/maven2/org/apache/tika/tika-emitter-s3/2.1.0/tika-emitter-s3-2.1.0.jar  See below (tika-app) for fully worked examples of using tika-app to fetch from a local file share, parse and send the output to Solr.  Fetchers allow users to specify sources of inputstream+metadata for the parsing process.  Fetchers are currently enabled in all oftika-server-standardand in the async option (-a) intika-app. With the exception of theFileSystemFetcher, users have to add the other fetcher dependencies to their class path.",../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes FileSystemFetcher,"A FileSystemFetcher allows the user to specify a base directory intika-config.xmland then at parse time, the user specifies the relative path for a file.  This class is included intika-coreand no external resources are required. For example, a minimaltika-config.xmlfile for a FileSystemFetcher would be: <properties><fetchers><fetcher class=""org.apache.tika.pipes.fetcher.fs.FileSystemFetcher""><params><name>fsf</name><basePath>/my/base/path1</basePath></params></fetcher></fetchers></properties>",../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes HttpFetcher,"The HttpFetcher requires that this dependency be on your class path:https://mvnrepository.com/artifact/org.apache.tika/tika-fetcher-http HttpFetcherРазвернуть исходный код<properties>
 <fetchers>
  <fetcher class=""org.apache.tika.pipes.fetcher.http.HttpFetcher"">
   <params>
    <name>http</name>
	<!-- these are optional; timeouts are all in milliseconds -->
    <authScheme></authScheme>
    <connectTimeout>30000</connectTimeout>
    <ntDomain></ntDomain>
    <password></password>
    <proxyHost></proxyHost>
    <proxyPort></proxyPort>
    <requestTimeout></requestTimeout>
    <socketTimeout></socketTimeout>
    <userName></userName>
   </params>
  </fetcher>
 </fetchers>
</properties>",../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes S3Fetcher,"S3FetcherРазвернуть исходный код<properties>
  <fetchers>
    <fetcher class=""org.apache.tika.pipes.fetcher.s3.S3Fetcher"">
      <params>
        <name>s3f</name>
        <region>us-east-1</region>
        <bucket>my-bucket</bucket>
        <!-- either use the instance as a credential -->
        <credentialsProvider>instance</credentialsProvider>
        <!-- or use a profile -->
        <credentialsProvider>profile</credentialsProvider>
        <profile>myProfile</profile>
        
        <!-- whether or not to spool the s3 object to a local temp file 
             before fetching. Default: true -->
        <spoolToTemp>true</spoolToTemp>

        <!-- these are all optional -->
        <!-- if your pipes iterator is working on a list of files under my-prefix -->
        <prefix>my-prefix</prefix>
        <!-- extract the s3 object user metadata and inject it into the Tika metadata -->
        <extractUserMetadata>false</extractUserMetadata>
        <!-- the s3 api sets a fairly low max. If you are running a heavily concurrent application, you
             may need to bump this. -->
        <maxConnections>100</maxConnections>
      </params>
    </fetcher>
  </fetchers>
</properties>",../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes SolrFetcher,"TheFileSystemEmitterrequires thetika-serializationmodule and is not included intika-core.  However, it is bundled withtika-appandtika-server-standard. For the other emitters, users have to add the other emitter dependencies to their class path.",../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes FileSystemEmitter,"AFileSystemEmitterallows the user to specify a base directory intika-config.xmland then at parse time, the user specifies the relative path for the emitted .json file. For example, a minimaltika-config.xmlfile for aFileSystemEmitterwould be: <properties><emitters><emitter class=""org.apache.tika.pipes.emitter.fs.FileSystemEmitter""><params><name>fse</name><basePath>/my/base/extracts</basePath></params></emitter></emitters></properties>",../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes SolrEmitter,tbd,../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes From FileShare to FileShare,"Process all files in a directory recursively and place the .json extracts in a parallel directory structure. N.B.For the logging to work correctly in the async pipes parser, you have to use >= 2.1.0 Place the tika-app jar and any other dependencies in abin/directoryUnzip this file (fs-to-fs-config.tgz) and place theconfig/directory at the same level as thebin/directory in the previous stepOpenconfig/tika-config-fs-to-fs.xmland update the <basePath/>elements in the fetcher and emitter sections to specify the absolute path to the root directory for the binary documents (fetcher) and to the target root directory for the extracts (emitter). Update the<basePath/>element in thepipesiteratorsection and make sure that it matches what you specified in thefetchersection.Commandline:java -Xmx512m -cp ""bin/*"" org.apache.tika.cli.TikaCLI -a --config=config/tika-config-fs-to-fs.xml",../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes From file list on FileShare to FileShare,"The input is a list of relative paths to files (e.g.file-list.txt) on a file share and the output is .json extract files on a file share. N.B.For the logging to work correctly in the async pipes parser, you have to use >= 2.1.0. Place the tika-app jar and any other dependencies in abin/directoryUnzip this file (file-list-config.tgz) and place theconfig/directory at the same level as thebin/directory in the previous step and the same level as thefile-list.txtOpenconfig/tika-config-filelist.xmland update the <basePath/>elements in the fetcher and emitter sections to specify the absolute path to the root directory for the binary documents (fetcher) and to the target root directory for the extracts (emitter).Commandline:java -Xmx512m -cp ""bin/*"" org.apache.tika.cli.TikaCLI -a --config=config/tika-config-filelist.xml",../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes From Fileshare to Solr,These examples were tested with Solr 8.9.0 on Ubuntu in single core mode (not cloud).  These examples require Tika >= 2.1.0.,../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes Index embedded files in a parent-child relationship,"Create collection:bin/solr create -c tika-example && bin/solr config -c tika-example -p 8983 -action set-user-property -property update.autoCreateFields -value falseSet schema with this filesolr-parent-child-schema.json:curl -F 'data=@solr-parent-child-schema.json'http://localhost:8983/solr/tika-example/schemaPut the latest tika app jar and tika-emitter-solr-2.1.0.jar in abin/directoryUnzip this config/ directorysolr-parent-child-config.tgzand put it at the same level as thebin/directoryOpenconfig/tika-config-fs-to-solr.xmland update the<basePath>elements in the fetcher AND the pipesiterator to point to the directory that you want to indexRun tika:java -cp ""bin/*"" org.apache.tika.cli.TikaCLI -a --config=config/tika-config-fs-to-solr.xml",../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes Treat each embedded file as a separate file,"Create collection:bin/solr create -c tika-example && bin/solr config -c tika-example -p 8983 -action set-user-property -property update.autoCreateFields -value falseSet schema with this filesolr-separate-docs-schema.json:curl -F 'data=@solr-separate-docs-schema.json'http://localhost:8983/solr/tika-example/schemaPut the latest tika app jar and tika-emitter-solr-2.1.0.jar in abin/directoryUnzip this config/ directorysolr-separate-docs-config.tgzand put it at the same level as thebin/directoryOpenconfig/tika-config-fs-to-solr.xmland update the<basePath>elements in the fetcher AND the pipesiterator to point to the directory that you want to indexRun tika:java -cp ""bin/*"" org.apache.tika.cli.TikaCLI -a --config=config/tika-config-fs-to-solr.xml",../data/confluence_exports/TIKA/tika-pipes_181306665.html
"TIKA : tika-pipes Legacy mode, concatenate content from embedded files","Create collection:bin/solr create -c tika-example && bin/solr config -c tika-example -p 8983 -action set-user-property -property update.autoCreateFields -value falseSet schema with this filesolr-concatenate-schema.json:curl -F 'data=@solr-concatenate-schema.json'http://localhost:8983/solr/tika-example/schemaPut the latest tika app jar and tika-emitter-solr-2.1.0.jar in abin/directoryUnzip this config/ directorysolr-concatenate-config.tgzand put it at the same level as thebin/directoryOpenconfig/tika-config-fs-to-solr.xmland update the<basePath>elements in the fetcher AND the pipesiterator to point to the directory that you want to indexRun tika:java -cp ""bin/*"" org.apache.tika.cli.TikaCLI -a --config=config/tika-config-fs-to-solr.xml",../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes From Fileshare to OpenSearch,The following require Tika >= 2.1.0. They will not work with the 2.0.0 release.  These examples were tested with OpenSearch 1.0.0 running in docker on an Ubuntu host.,../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes Index embedded files in a parent-child relationship,"This option requires specification of the parent child relationship in the mappings file.  The parent is currently hardcoded to becontainer, and the embedded files areembedded.  The OpenSearch emitter flattens relationships so that if there are deeply recursively embedded files, all embedded files are children of the single container/parent file; recursive relationships are not captured in the OpenSearch join relation.  However, the embedded path is stored in theX-TIKA:embedded_resource_pathmetadata value, and the recursive relations can be reconstructed from that path. Place the tika-app jar and thetika-emitter-opensearch-2.1.0.jarin thebin/directoryUnzip this fileopensearch-parent-child-config.tgzand place theconfig/directory at the same level as thebin/directoryOpenconfig/tika-config-fs-to-opensearch.xmland update the<basePath>elements in BOTH the fetcher and the pipesiterator to point to the directory that you want to indexCurl these mappingsopensearch-parent-child-mappings.jsonto OpenSearch:curl -k -T opensearch-mappings.json -u admin:admin -H ""Content-Type:application/json""https://localhost:9200/tika-testRun tika app:java -cp ""bin/*"" org.apache.tika.cli.TikaCLI -a --config=config/tika-config-fs-to-opensearch.xml",../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes Treat each embedded file as a separate file,"Place the tika-app jar and thetika-emitter-opensearch-2.1.0.jarin thebin/directoryUnzip this fileopensearch-parent-child-config.tgzand place theconfig/directory at the same level as thebin/directoryOpenconfig/tika-config-fs-to-opensearch.xmland update the<basePath>elements in the fetcher and the pipesiterator to point to the directory that you want to indexCurl these mappingsopensearch-mappings.jsonto OpenSearch:curl -k -I -T opensearch-mappings.jsonhttps://localhost:9200/tika-test-u admin:admin -H ""Content-Type: application/json""Run tika app:java -cp ""bin/*"" org.apache.tika.cli.TikaCLI -a --config=config/tika-config-fs-to-opensearch.xml",../data/confluence_exports/TIKA/tika-pipes_181306665.html
"TIKA : tika-pipes Legacy mode, concatenate content from embedded files","This emulates the legacy output fromtika-appand the/tikaendpoint intika-server-standard.  Note that this option hides exceptions from embedded files and metadata from embedded files.  The key difference between this config and the ""treat each embedded file as a separate file"" is theparseModeelement in thepipesIterator: <pipesIterator class=""org.apache.tika.pipes.pipesiterator.fs.FileSystemPipesIterator"">
    <params>
      <parseMode>CONCATENATE</parseMode>
  ...  Place the tika-app jar and thetika-emitter-opensearch-2.1.0.jarin thebin/directoryUnzip this fileopensearch-concatenate-config.tgzand place theconfig/directory at the same level as thebin/directoryOpenconfig/tika-config-fs-to-opensearch.xmland update the<basePath>elements in the fetcher and the pipesiterator to point to the directory that you want to indexCurl these mappingsopensearch-mappings.jsonto OpenSearch:curl -k -T opensearch-mappings.json -u admin:admin -H ""Content-Type:application/json""https://localhost:9200/tika-testRun tika app:java -cp ""bin/*"" org.apache.tika.cli.TikaCLI -a --config=config/tika-config-fs-to-opensearch.xml",../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes Fetchers in the classic tika-server endpoints,"For the classic tika-server endpoints (/rmeta, /tika, /unpack, /meta), users specifyfetcherNameandfetchKeyin the headers.  This replacesenableFileUrlfrom tika-1.x. Note thatenableUnsecureFeaturesmust still be set via the tika-config.xml:  <properties>
  <fetchers>
    <fetcher class=""org.apache.tika.pipes.fetcher.fs.FileSystemFetcher"">
      <params>
        <name>fsf</name>
        <basePath>/my/base/path1</basePath>
      </params>
    </fetcher>
  </fetchers>
<server>
  <params>
    <enableUnsecureFeatures>true</enableUnsecureFeatures>
  <params>
</server>
</properties> To parse/my/base/path1/path2/myfile.pdf: curl -X PUT http://localhost:9998/tika --header ""fetcherName: fsf"" --header ""fetchKey: path2/myfile.pdf"" If your file path has non-ASCII characters, you should specify the fetcherName and the fetchKey as query parameters in the request instead of in the headers: curl -X PUT 'http://tika:9998/rmeta/text?fetcherName=fsf&fetchKey=中文.txt' 
curl -X PUT 'http://tika:9998/rmeta/text?fetcherName=fsf&fetchKey=%E4%B8%AD%E6%96%87.txt'",../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes The/pipesendpoint,"This endpoint requires that at least one fetcher and one emitter be specified in the config file and thatenableUnsecureFeaturesbe set to true. In the following example, we have source documents in/my/base/path1, and we want to write extracts to/my/base/extracts. Unlike with the classic endpoints, users send a json FetchEmitTuple to tika-server. For full documentation of this object see:FetchEmitTuple <properties>
  <fetchers>
    <fetcher class=""org.apache.tika.pipes.fetcher.fs.FileSystemFetcher"">
      <params>
        <name>fsf</name>
        <basePath>/my/base/path1</basePath>
      </params>
    </fetcher>
  </fetchers>
  <emitters>
    <emitter class=""org.apache.tika.pipes.emitter.fs.FileSystemEmitter"">
      <params>
        <name>fse</name>
        <basePath>/my/base/extracts</basePath>
      </params>
    </emitter>
  </emitters>
  <server>
    <params>
      <enableUnsecureFeatures>true</enableUnsecureFeatures>
    </params>
  </server>
  <pipes>
    <params>
      <tikaConfig>/path/to/tika-config.xml</tikaConfig>
    <params>
  </pipes>
</properties>  To parse/my/base/path1/path2/myfile.pdf: curl -X POST -H ""Content-Type: application/json"" -d '{""fetcher"":""fsf"",""fetchKey"":""path2/myfile.pdf"",""emitter"":""fse"",""emitKey"":""path2/myfile.pdf""}' http://localhost:9998/pipes Note, by default, theFileSystemEmitterautomatically adds "".json"" to the end of theemitKey.",../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes The/asyncendpoint,"The only difference in the /async handler is that you send a list ofFetchEmitTuples: curl -X POST -H ""Content-Type: application/json"" -d '[{""fetcher"":""fsf"",""fetchKey"":""path2/myfile.pdf"",""emitter"":""fse"",""emitKey"":""path2/myfile.pdf""}]' http://localhost:9998/async",../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : tika-pipes Modifying Docker to use the pipes modules,For examples of how to load the pipes modules with Docker see:tika-pipes and Docker.,../data/confluence_exports/TIKA/tika-pipes_181306665.html
TIKA : PostingManyFilesToExtractingRequestHandler Posting Multiple Files To Extracting Request Handler,"To post multiple files(all files within current directory and it's sub-directories)to Solr's Extracting Request Handler, simply execute the following command after going to the appropriate directory.  find . -name ""*"" -type f -exec java -Dauto -jar post.jar ""{}"" \;  Also referReferencefor other ways of posting to Extracting Request Handler.",../data/confluence_exports/TIKA/PostingManyFilesToExtractingRequestHandler_109454068.html
TIKA : NMT-RTG Introduction,"The page provides details on how to translate documents (via theTika.translateAPI) using Reader Translator Generator, a neural machine translation toolkit. The benefits of using this approach for machine translation through Tika are as follows; It's free! As opposed to several other translation services currently available via Tika, NMT via RTG is free.You are not restricted under usage ceiling, and you don't have to allocate monthly payments. There is no paid service behind the scene, you can use this method completely unrestricted.You will have full control over the whole pipeline.You may either build NMT models or download pretrained models, set up server and manage backend.Your data and documents are not sent to any services outside of your pipeline. So you can guarantee privacy of your data. Though, you have to keep these in mind: Though you may run the model on CPU for testing, the translation will be very slow on CPUs. GPUs are highly recommended.NMT models are not interpretable and explainable. We cannot explain or guarantee that the translations are 100% correct. This is not specific to RTG/NMT; it is generally true for all neural machine translation services. This is relatively a new addition;the following translation models are currently available: Translation from 500 source languages to English To train models for your desired translation direction, please refer to the documentation athttps://isi-nlp.github.io/rtg/#_usage",../data/confluence_exports/TIKA/NMT-RTG_177049147.html
TIKA : NMT-RTG Integration: Overview,"The class org.apache.tika.language.translate.RTGTranslator glues Tika system with RTG REST API.  By default, it interacts with http://localhost:6060 .  This URL can be customized by adding translator.rtg.properties file to classpath with rtg.base.url property.",../data/confluence_exports/TIKA/NMT-RTG_177049147.html
TIKA : NMT-RTG Step 1: Start RTG Translator Service,"500-English model can be obtained from a docker image as follows Docker image can be run on CPU (i.e. without GPU, for testing):docker run --rm -i -p 6060:6060 tgowda/rtg-model:500toEng-v1Using GPU (e.g. Device 0) is recommended for translating a lot of documents:docker run --rm -i -p 6060:6060 --gpus '""device=0""' tgowda/rtg-model:500toEng-v1 Verify that the translator serive is actually running by accessinghttp://localhost:6060/",../data/confluence_exports/TIKA/NMT-RTG_177049147.html
TIKA : NMT-RTG Step 2: Start Tika Server Jar,Option 1: Obtain prebuilt jarNote: This option is for the future versions. The current prebuilt jars do not have this feature integrated. Go to Option 2. wgethttps://www.apache.org/dyn/closer.cgi/tika/tika-server-2.0.0.jar  Option 2: Build Tika Server from source $ git clone https://github.com/apache/tika.git$ cd tika # if the pull request is not merged yet; please pull from this repo$ git checkout -b TIKA-3329$git pullhttps://github.com/thammegowda/tika.gitTIKA-3329# Compile and package Tika $ mvn clean package -DskipTests  # Start Tika server$ java -jar tika-server/target/tika-server-2.0.0-SNAPSHOT.jar,../data/confluence_exports/TIKA/NMT-RTG_177049147.html
TIKA : NMT-RTG Step 3:Translate Documents via Tika + RTG,"printf ""Hola señor\nನಮಸ್ಕಾರ\nBonjour monsieur\nПривет\n"" > tmp.txt$ curlhttp://localhost:9998/translate/all/org.apache.tika.language.translate.RTGTranslator/x/eng-X PUT -T tmp.txt Hi, sir.NamaskarGood morning, sir.Hi. Optional: Change the base URL of RTG translator service You may deploy RTG service elsewhere (on a machine with GPU) and point its URL to tika. Step 1: Create a file namedtranslator.rtg.propertieswithrtg.base.urlproperty echo ""rtg.base.url=http://<myhost>:<port>/rtg/v1"" > translator.rtg.properties Step 2: Add the directory havingtranslator.rtg.propertiesto classpath; In this case . i.e, $PWDjava -cp '.:tika-server/target/tika-server-2.0.0-SNAPSHOT.jar' org.apache.tika.server.TikaServerCliStep 3: Interact with Tika Server as usual $ curlhttp://localhost:9998/translate/all/org.apache.tika.language.translate.RTGTranslator/x/eng-X PUT -T tmp.txt",../data/confluence_exports/TIKA/NMT-RTG_177049147.html
TIKA : NMT-RTG Acknowledgements,"If you wish to acknowledge or reference either RTG toolkit or  the 500-English model, please reference this article:https://arxiv.org/abs/2104.00290  @misc{gowda2021manytoenglish,title={Many-to-English Machine Translation Tools, Data, and Pretrained Models},author={Thamme Gowda and Zhao Zhang and Chris A Mattmann and Jonathan May},year={2021},eprint={2104.00290},archivePrefix={arXiv},primaryClass={cs.CL}}",../data/confluence_exports/TIKA/NMT-RTG_177049147.html
TIKA : TikaAndMITIE Installation,"Simple by downloadingmitie-resources: Use following commands to set up your mitie-resources.MAC OS Requirement: Download and installHomebrew.Linux/Windows: No pre-requisite.git clone https://github.com/manalishah/mitie-resources
 cd mitie-resources
 # absolute path to mitie-resources folder 
 export NER_RES=$PWD
 chmod a+x install.sh
 ./install.sh",../data/confluence_exports/TIKA/TikaAndMITIE_109454079.html
TIKA : TikaAndMITIE Running MITIE with Tika-App,"For running MITIE, the following steps are essential:  Supply the java.library.path as absolute path to jni shared library obtained from building MITIE (required for MAC OS only)Supply the javamitie.jar in classpathSupply the complete model path to ner_model obtained from building MITIESupply the ner implementation class as MITIENERecogniser  * For Mac OS*export TIKA_APP={your/path/to/tika-app}/target/tika-app-1.13-SNAPSHOT.jar

 java -Djava.library.path=$NER_RES/MITIE/mitielib -Dner.mitie.model=$NER_RES/MITIE/MITIE-models/english/ner_model.dat -Dner.impl.class=org.apache.tika.parser.ner.mitie.MITIENERecogniser -classpath $NER_RES/MITIE/mitielib/javamitie.jar:$TIKA_APP org.apache.tika.cli.TikaCLI --config=$NER_RES/tika-config.xml -m $NER_RES/sample.txt2. * For LINUX/Windows*export TIKA_APP={your/path/to/tika-app}/target/tika-app-1.13-SNAPSHOT.jar

 java -Dner.mitie.model=$NER_RES/MITIE/MITIE-models/english/ner_model.dat -Dner.impl.class=org.apache.tika.parser.ner.mitie.MITIENERecogniser -classpath $NER_RES/MITIE/mitielib/javamitie.jar:$TIKA_APP org.apache.tika.cli.TikaCLI --config=$NER_RES/tika-config.xml -m $NER_RES/sample.txtThis will output metadata keys along with named entities extracted using mitie:Content-Length: 63
 Content-Type: text/plain
 NER_LOCATION: Los Angeles
 NER_LOCATION: California
 X-Parsed-By: org.apache.tika.parser.CompositeParser
 X-Parsed-By: org.apache.tika.parser.ner.NamedEntityParser
 resourceName: sample.txt",../data/confluence_exports/TIKA/TikaAndMITIE_109454079.html
TIKA : TikaAndMITIE Running MITIE with Tika-Server,"* For Mac OS*export TIKA_SERVER={your/path/to/tika-server}/target/tika-server-1.13-SNAPSHOT.jar

 java -Djava.library.path=$NER_RES/MITIE/mitielib -Dner.mitie.model=$NER_RES/MITIE/MITIE-models/english/ner_model.dat -Dner.impl.class=org.apache.tika.parser.ner.mitie.MITIENERecogniser -classpath $NER_RES/MITIE/mitielib/javamitie.jar:$TIKA_SERVER org.apache.tika.server.TikaServerCli --config=$NER_RES/tika-config.xml -p 99982. * For LINUX/Windows*export TIKA_SERVER={your/path/to/tika-server}/target/tika-server-1.13-SNAPSHOT.jar

 java -Dner.mitie.model=$NER_RES/MITIE/MITIE-models/english/ner_model.dat -Dner.impl.class=org.apache.tika.parser.ner.mitie.MITIENERecogniser -classpath $NER_RES/MITIE/mitielib/javamitie.jar:$TIKA_SERVER org.apache.tika.server.TikaServerCli --config=$NER_RES/tika-config.xml -p 9998This will start the Tika-Server enabled with MITIE Named Entity Parser athttp://localhost:9998To test the server try the sample.txt file provided in the mitie-resources foldercurl -T $NER_RES/sample.txt http://localhost:9998/meta -H ""Accept: application/json""This should return metadata keys in a JSON format:{
  ""Content-Type"":""text/plain"",
  ""NER_LOCATION"":[""Los Angeles"",""California""],
  ""X-Parsed-By"":[""org.apache.tika.parser.CompositeParser"",""org.apache.tika.parser.ner.NamedEntityParser""],
  ""language"":""sl""
 }",../data/confluence_exports/TIKA/TikaAndMITIE_109454079.html
TIKA : TikaWithoutFiles Detection,"Tika will be able to do mime-magic detection, for the formats where that's possible, with just a re-windable / mark+resetInputStream. Generally Tika uses the first 64kb of the stream for this, but the first 4kb should be enough for the majority of the mime-magic matches  For some file formats, generally Container Based formats, Tika needs the whole of the File (plus the Tika Parsers package!) to be able to correctly detect the file type. This is because some formats only have mime-magic which identify the Container itself (eg Zip), and you need to look inside the Container to find the specific subtype (egXLSX=application/vnd.openxmlformats-officedocument.spreadsheetml.sheet). For this either the File object is needed, or Tika would need to spool the stream out to a Temporary File.  It is best to make use ofTikaInputStream.get(File)orTikaInputStream.get(InputStream)to have your stream/file wrapped for possible temp file creation / file access, if possible  Note - Tika should be given the filename if possible, to help guide mime-magic and container detection",../data/confluence_exports/TIKA/TikaWithoutFiles_109454100.html
TIKA : TikaWithoutFiles Parsing,"Some formats require a File to be able to be parsed. This is because of restrictions in the underlying Java libraries being used  Some formats work better with a File when parsing. This is because it permits lower memory use, as back-and-forth searching can be handled without buffering the whole stream",../data/confluence_exports/TIKA/TikaWithoutFiles_109454100.html
TIKA : TikaWithoutFiles As of Apache Tika 1.17,"As of Apache Tika 1.17 (note - other versions may differ because of changes in the underlying Java libraries), the following formats require Files or create temporary Files if not:  jpeg, zip (for detection) and derived (docx, xlsx, pptx), ole2 (for detection) and derived (doc, xls, ppt), mdb, pst, rar, 7zip, sqlite...",../data/confluence_exports/TIKA/TikaWithoutFiles_109454100.html
TIKA : ComparisonTikaAndPDFToText201811 Languages,"In the following, we show the top 20 languages identified in the extracted text.  The tika-eval module uses theoptimaize language detector(version 0.6) for language identification. The language codes are roughlyISO 639-1.  The first language is that identified in the extract from pdftotext, and the second is the language identified on the extract of PDFBox.  For example 'en->fa' means that language id returned 'en' on the pdftotext extract, but 'fa' on the Tika/PDFBox extract.  Language idNumber of Filesen->en143,784ru->ru44,460fr->fr38,872it->it36,433de->de30,151es->es18,335ja->ja16,106el->el9,761fa->fa8,486ko->ko8,213zh-cn->zh-cn5,815tr->tr5,477null3,132vi->vi2,981he->he2,280ar->ar2,087ca->ca1,275en->fa1,240pt->pt1,105de->en860  In the following, we show the top 10 language id pairs, where the language id differs between the extracts.  Language idsNumber of Filesen->fa1,240de->en860en->de519en->bn392ar->fa391it->en209en->it209fr->en201en->fr149br->bn146   In the following table, we present the number of ""common words"" extracted per language id [SQL3].  Note that this language ""id"" is based on the extracted text per tool,notground truth.  Some care has to be made in interpreting this data.                                                                           Language IdpdftotextTika/PDFBox% Changenull5,7292,757-51.9%af10,51311,1926.5%an113,32688,633-21.8%ar2,272,8911,832,365-19.4%ast15,50716,9279.2%be3474117.6%bg22,58225,88714.6%bn5,648201,7073471.3%br21,47021,282-0.9%ca301,284307,4742.1%cs26,93332,12619.3%cy72,36870,987-1.9%da24,70229,57619.7%de30,132,56431,337,4064.0%el9,432,3109,572,5941.5%en245,298,469256,492,0564.6%es39,476,60140,478,6832.5%et38,17423,226-39.2%eu17,77114,476-18.5%fa19,376,86821,751,81412.3%fi10,19510,6284.2%fr61,980,43165,709,5866.0%ga28,80925,165-12.6%gl206,752220,0556.4%gu3,4563,84011.1%he3,389,1053,358,894-0.9%hi266,969264,400-1.0%hr30,78030,632-0.5%ht8,4983,570-58.0%hu13,28315,66117.9%id215,824193,661-10.3%is14,48013,250-8.5%it45,985,47347,562,7813.4%ja45,187,66546,921,0423.8%km5980.0%kn4,0904,2854.8%ko4,908,5284,990,8361.7%lt5,7665,8221.0%lv12,46710,453-16.2%mk5471,400155.9%ml1,2811,280-0.1%mr22,77523,4523.0%ms234,536286,10022.0%mt39,41629,780-24.4%ne76234207.9%nl563,761584,3033.6%no55,05356,5482.7%oc8385,017498.7%pa7910735.4%pl51,97655,8907.5%pt2,196,2762,395,8629.1%ro40,35832,007-20.7%ru79,309,91181,574,5182.9%sk9,1317,760-15.0%sl9,24313,66547.8%so281,724350,40224.4%sq3,0895,99794.1%sr6907072.5%sv56,95669,11221.3%sw1,1801,003-15.0%ta1,3081,303-0.4%te3,3605,82473.3%th5,6738,67552.9%tl1,2772,987133.9%tr878,127917,6164.5%uk3,9775,91048.6%ur29,7119,575-67.8%vi2,310,5562,455,0516.3%yi283214.3%zh-cn16,938,97218,231,0357.6%zh-tw703,272646,268-8.1%                                                                           When we require that both extracts for a given file have the same language id, we see some different patterns [SQL4].                                                                           Language IdpdftotextTika/PDFBox% Changeaf10,27410,3300.5%an68,41663,324-7.4%ar2,021,2121,652,127-18.3%ast9,93110,0090.8%be33343.0%bg20,52220,5490.1%bn4,5564,7544.3%br7,2077,3401.8%ca242,815246,3461.5%cs24,87225,2971.7%cy51,64451,330-0.6%da22,73723,4123.0%de29,478,16429,814,2861.1%el9,427,7939,371,790-0.6%en243,539,028244,571,2040.4%es38,876,29938,835,666-0.1%et16,08016,5162.7%eu11,01811,1271.0%fa19,278,44817,596,803-8.7%fi9,7679,9131.5%fr61,593,22762,039,8060.7%ga22,25622,131-0.6%gl174,194166,007-4.7%gu3,4563,6335.1%he3,378,6383,206,568-5.1%hi265,152242,522-8.5%hr30,12430,3540.8%ht2,7982,8692.5%hu13,26013,5952.5%id190,295190,072-0.1%is10,71810,7770.6%it45,675,11245,575,218-0.2%ja45,046,24045,903,6411.9%km5980.0%kn3,9383,9500.3%ko4,864,3904,914,2511.0%lt5,5875,6841.7%lv10,78610,435-3.3%mk5451,398156.5%ml1,2811,280-0.1%mr22,69522,523-0.8%ms221,213226,9322.6%mt18,25318,7772.9%ne738313.7%nl548,538552,6250.7%no41,58842,4822.1%oc6056090.7%pa7910735.4%pl50,84851,7761.8%pt2,090,1272,144,4912.6%ro30,28230,8892.0%ru79,195,31978,271,782-1.2%sk8,7456,776-22.5%sl8,2908,5192.8%so224,340212,438-5.3%sq2,8824,26948.1%sr6897032.0%sv40,34741,3132.4%sw959951-0.8%ta1,3081,303-0.4%te3,3603,4071.4%th5,0785,0880.2%tl1,1751,2072.7%tr865,494878,2971.5%uk3,8985,15332.2%ur21,4595,553-74.1%vi2,254,7382,264,3840.4%yi283214.3%zh-cn16,608,32117,263,4363.9%zh-tw347,067349,0290.6%  Further evaluation and analysis are required, but we should look into:  Why there are so many ""common words"" forbnin the first common tokens by language table? 2. Are there systematic areas for improvements in PDFBox forhi(-8.5%),he(-5.1%) and Arabic script languages:ar(-18%),fa(-9%),ur(-74%)?  Most importantly, we need to determine if any of the above areas for inquiry are based on faults in tika-eval that should be fixed.",../data/confluence_exports/TIKA/ComparisonTikaAndPDFToText201811_109454043.html
"TIKA : ComparisonTikaAndPDFToText201811 1. Why there are so many ""common words"" forbnin the first common tokens by language table?","I ran [SQL5], and I manually reviewed results.  I observed the following points:                                                                           There was only one out of the 100 documents that had what looked like Bangla words in the top 10 most common words for those documents.   2. My intuition from previous experience with Optimaize, and it was confirmed in looking at the top 10 words for these documents is that Optimaize prefersbnwhen there are many numerals and very little other language content.   3. As in previous work with Optimaize, I was struck that the confidence levels are typically very high (~0.999) even when there is very little content.  For example,commoncrawl3/7A/7AZUB5NHLJN3TBCMEP2YRSRK6DDNBP5Fis mostly comprised of the UTF-8 replacement character, ""EF BF BD"" (equivalent to U+FFFD) (~13,000 of these); there are a few new lines, a few tabs, a few numerals, and the word 'untitled', and yet Optimaize's confidence is0.9999907612800598that this is Bangla.   4. The current OOV% metric does not take calculate a confidence.  If there's just one alphanumeric term and it happens to be in the dictionary, then the OOV% is 0%, which is less than entirely useful.  It would be better to improve our ""language-y"" score or its inverse, the ""junk"" score (seeTIKA-1443), to include a confidence interval based on the amount of input.  5. When tika-eval doesn't have a ""common words"" list for a language, e.g.,bn, it backs-off and uses the English list.  Given that the internet is overwhelmingly English and given that thecommoncrawl3regression corpus contains quite a bit of English, and given that content from the title metadata field slipped into the extracted text for the PDFBox/Tika extracts, this backing-off to English can lead to misleading results.  My conclusion is that most of the documents that received a language id ofbnactually contain a high percentage of junk.  Recommendations:  We should experiment with other language detectors and evaluate them for the traditional language-id performance measures: accuracy and speed on known language content.  However, we should also evaluate them on how well they handle various types of degraded text to confirm that the confidence scores are related to the noise – content that contains 98% junk text should not receive a language id confidence of 99.999%. 2. We should augment our ""common words"" lists to cover all languages identified by whichever language detector we choose.  We should not back-off to the English list for ""common words"". 3. We should continue to work on/develop a junk metric that is more nuanced than the simple sum of ""Common Tokens"" and the OOV%.  The metrics should take the following into account:Amount of evidence. 2. Alignment of distribution of token lengths relative to the ""id'd"" language (this will be useless with CJK, which are simply bigrammed by tika-eval; but it might be very useful for most other languages). 3. Amount of symbols and U+FFFD characters vs. the alphabetic tokens. 4. Instead of binary OOV%, it might be useful to calculate alignment to a Zipf distribution or simply similarity to a language model – we'd need to include % of words in the common words file. 5. Incorrect duplication of text. For file,commoncrawl3/2E/2EXCWC7T6P5ZY6DINFI3X2UQNIMAISKT, tika-eval shows an increase in Common Tokens of 50,372 tokens if switching from pdftotext to PDFBox/Tika.  However, this file has an absurd amount of duplicate text in the headers – 17,000 occurrences of ""training"" in the PDFBox/Tika extract, and only 230 in the pdftotext extract. PDFBox/Tika correctly suppresses these duplicate text portions ifsetSuppressDuplicateOverlappingTextis set totrue, but Tika's default is not to suppress duplicate text.  One consideration is that for this file, the % of OOV is 39% in pdftotext but only 8% in the text extracted by PDFBox/Tika.  This suggests that it might be better, instead of simply summing the common tokens, to sum them only in files which have an OOV% which is within the norm (say, one stddev).  As a side note, 40% is fairly common for OOV for English documents – the median is 45%, and the stddev is 14%.",../data/confluence_exports/TIKA/ComparisonTikaAndPDFToText201811_109454043.html
"TIKA : ComparisonTikaAndPDFToText201811 2. Are there systematic areas for improvements in PDFBox forhi(-8.5%),he(-5.1%) and Arabic script languages:ar(-18%),fa(-8%),ur(-74%)?","I don't know these languages, but I ran [SQL7] and then put the contents ofTOP_10_UNIQUE_TOKEN_DIFFS_AandTOP_10_UNIQUE_TOKEN_DIFFS_Bthrough Google translate.  For example, for the top 10 unique words incommoncrawl3_refetched/XH/XHYIWIBT5QPY64UYUPLXZXAYC2I5JPZS:                                                                           ميں: 532 | ہے: 520 | كے: 450 | ہيں: 370 | كہ: 365 | كو: 343 | سے: 342 | كا: 297 | ہم: 280 | جناب: 254  are translated as:  I: 532 | Is: 520 | Of: 450 | Are: 370 | Yes: 365 | Who: 343 | From: 342 | : 297 | We: 280 | Mr.: 254  Whereas PDFBox/Tika's unique tokens  ںيم: 564 | ےہ: 537 | ےك: 468 | ںيہ: 386 | ہك: 365 | وك: 360 | ےس: 348 | اك: 306 | مہ: 281 | انجب: 250  are translated as:  Th: 564 | Yes: 537 | S: 468 | Yes: 386 | Hak: 365 | Ki: 360 | S: 348 | A: 306 | Mah: 281 | Ingredients: 250  Overall, this method wasn't able to yield satisfactory insight about general patterns.  In some cases, the individual terms looked better in one tool or the other andvice versa.  I did note that there were more cases in PDFBox's extracted text of numerals concatenated with words as incommoncrawl3/JG/JGE6WTYI5SEI3Z4JUULIPSSRTNL3VMIG:  TOP_10_UNIQUE_TOKEN_DIFFS_A  1: 167 | رياضي: 167 | 9: 44 | 8: 38 | 7: 28 | 6: 16 | 5: 9 | 4: 6 | 3: 2 | 9622243  TOP_10_UNIQUE_TOKEN_DIFFS_B  رياضي: 44 | 8رياضي: 38 | 7رياضي: 28 | 10رياضي: 24 | 6رياضي: 16 | 5رياضي: 9 | 4رياضي: 6 | 3رياضي: 2 | 96222431: 1",../data/confluence_exports/TIKA/ComparisonTikaAndPDFToText201811_109454043.html
TIKA : ComparisonTikaAndPDFToText201811 Overall improvements to this process,"The wrapper around pdftotext should have ""caught"" the exception written to stderr and stored that as we do with exceptions from Tika.Tika currently includes the file's 'title' metadata in the content of the file.  This gives the misleading impression that some content was extracted from the file when, in fact, only the title was extracted from the XMP or metadata.  Next time, we should use a content handler that only includes the extracted text.Next time we run this evaluation, we should specify-cfgfrom the commandline and/or figure out why ourpdfrcfile wasn't being read where we placed it.",../data/confluence_exports/TIKA/ComparisonTikaAndPDFToText201811_109454043.html
TIKA : ComparisonTikaAndPDFToText201811 Improvements to tika-eval,"We observed a handful of cases where the number of ""common words"" increased, but the content extracted was probably worse between two tools.  This happened when one tool added spaces incorrectly, but the sub-words were actual words within the language.  See, for example:commoncrawl3/TF/TFNFGXL27M77Q6X42ECYWJNSJ32WES74(Russian) andcommoncrawl3/FS/FSEHYPPOEV6EUYND5BRP3BBNAI5FVPYP(German) (""fachgruppe"" vs ""fach gruppe"" and ""ermoglicht"" and ""ermog"" ""licht"")If there's an ""extract exception"", meaning an empty file or an incomplete json file, we include that information in the containers table, but we don't include a row for that file in the profiles table.  This causes some of the SQL that ships with tika-eval to result in not-quite-fair comparisons; some of the SQL that takes into account ""runtime exceptions"" fails to take into account ""extract exceptions.""See the point above about improving the ""junk"" metric.   [SQL1]                                                                           select sum(cb.num_common_tokens) from contents_b cb
join profiles_b pb on pb.id=cb.id
left join profiles_a pa on pb.id=pa.id
left join contents_a ca on pa.id=ca.id
where pa.is_embedded = false and pb.is_embedded=false
and (ca.lang_id_1 = cb.lang_id_1
or ca.lang_id_1 is null)                                                                           [SQL2]                                                                           select sum(ca.num_common_tokens) from contents_a ca
join profiles_a pa on pa.id=ca.id
left join profiles_b pb on pa.id=pb.id
left join contents_b cb on pb.id=cb.id
where pa.is_embedded = false and pb.is_embedded=false
and (cb.lang_id_1 = ca.lang_id_1
or cb.lang_id_1 is null)                                                                           [SQL3]                                                                           select lang_id_1, sum(num_common_tokens) as total_common_tokens
from contents_b
group by lang_id_1
order by lang_id_1                                                                           [SQL4]                                                                           select ca.lang_id_1, sum(ca.num_common_tokens)
from contents_a ca
join contents_b  cb on ca.id=cb.id
where ca.lang_id_1=cb.lang_id_1
group by ca.lang_id_1
order by ca.lang_id_1                                                                           [SQL5]                                                                           select ca.lang_id_1, ca.top_n_tokens, cb.top_n_tokens from contents_b cb
join contents_a ca on cb.id=ca.id
where cb.lang_id_1 = 'bn'
order by rand()
limit 100;                                                                           [SQL6]                                                                           select  ca.id, file_path, 
1-(cast(ca.num_common_tokens as float) / cast(ca.num_alphabetic_tokens as float)) as OOV_A,
ca.num_alphabetic_tokens,
1-(cast(cb.num_common_tokens as float) / cast(cb.num_alphabetic_tokens as float)) as OOV_B,
cb.num_alphabetic_tokens,
ca.lang_id_1, ca.lang_id_prob_1,
cb.lang_id_1, cb.lang_id_prob_1,
ca.top_n_tokens, cb.top_n_tokens 
from contents_b cb
join contents_a ca on cb.id=ca.id
join profiles_a pa on ca.id=pa.id
join containers c on pa.container_id=c.container_id
where cb.lang_id_1 = 'bn' and
ca.num_alphabetic_tokens > 0
and cb.num_alphabetic_tokens > 0
order by OOV_B asc
limit 100;                                                                           [SQL7]                                                                           select file_path, ca.top_n_tokens, cb.top_n_tokens,
(cb.num_common_tokens-ca.num_common_tokens) as delta_common_tokens,
top_10_unique_token_diffs_a, top_10_unique_token_diffs_b
from contents_a ca 
join contents_b cb on ca.id=cb.id
join content_comparisons cc on cc.id=ca.id
join profiles_a pa on ca.id=pa.id
join containers cc on pa.container_id=cc.container_id
where ca.lang_id_1='ur'
and cb.lang_id_1='ur'
order by delta_common_tokens asc   Exceptions aside, the critical file iscontent/content_diffs_with_exceptions.xlsx.  This shows differences in the content that was extracted.  ColumnTOP_10_UNIQUE_TOKEN_DIFFS_Arecords the top 10 most frequent tokens that appear only in ""A"" extracts (pdftotext);TOP_10_UNIQUE_TOKEN_DIFFS_Brecords the top 10 most frequent tokens that appear only in ""B"" extracts (Tika/PDFBox);NUM_COMMON_TOKENS_DIFF_IN_Brecords whether there has been an increase (positive number) or a decrease in ""common tokens"" if one were to move from ""A"" to ""B"" as the extraction tool.  For example, for filecommoncrawl3_refetched/7L/7L6BDSEYCY3QVVPM7YYK3FVCK2ZLOSA7,NUM_COMMON_TOKENS_DIFF_IN_Bhas a value of '38', which suggests that there are 38 more ""common words"" in the text extracted by Tika/PDFBox than by pdftotext.  TOP_10_UNIQUE_TOKEN_DIFFS_Ahas  bklasse: 2 | gehoben: 2 | jahressiegers: 2 | untersagt: 2 | verlasslichkeit: 2 | 3321: 1 | 50jahrigen: 1 | 5282708: 1 | 60jahriges: 1 | 970843: 1  TOP_10_UNIQUE_TOKEN_DIFFS_Bhas  e: 23 | gen: 9 | be: 6 | te: 6 | schaft: 5 | gung: 4 | nant: 4 | nen: 4 | o: 4 | ten: 4  This probably means that some words were incorrectly split by PDFBox (""tool B""); and it may mean that a hyphen was incorrectly dropped in a few words by pdftotext (""tool A""): ""bklasse"", ""50jahrigen"" and ""60jahriges"", which should probably be ""b-klasse"", ""50-jahrigen"" and ""60-jahriges""",../data/confluence_exports/TIKA/ComparisonTikaAndPDFToText201811_109454043.html
TIKA : PDFBOX 2 X NOTES NonseqParser,"With 2.x, the older parser is gone, and theNonSequentialparser is the main/only parser available.  In 1.8.x, users of Tika can configure the use of theNonSequentialparser via the config file.  This choice will disappear in 2.x.",../data/confluence_exports/TIKA/PDFBOX-2-X-NOTES_109454064.html
TIKA : PDFBOX 2 X NOTES Speed/Memory,"This is still in a state of flux.  With some changes over the last few days, the speed appears to be equivalent between 1.8.x and the non-sequential parser and 2.x – that said, the speed is slightly slower with the nonsequential parser (TODO: benchmarks);  Memory use configuration is currently going through some upgrades. It looks like the clients will be able to set a threshold and PDFBox will choose when to buffer to disk to stay under the desired memory threshold.",../data/confluence_exports/TIKA/PDFBOX-2-X-NOTES_109454064.html
TIKA : PDFBOX 2 X NOTES Character Encodings,"I've noticed a handful of cases where ligatures in 1.8 are ""spelled out"" in 2.0 – e.g. ""identi[fi]cation"" in 1.8 has become ""identification"" in 2.0 (at least in 003403.pdf from govdocs1).",../data/confluence_exports/TIKA/PDFBOX-2-X-NOTES_109454064.html
TIKA : PDFBOX 2 X NOTES TIFF Extraction,"Tiffs are no longer extracted by PDFBox without supplementary, non-Apache friendly libraries added to the classpath by consumers.  For now, with straight Tika+PDFBox, if ""extractInlineImages"" is set to true, and a TIFF is encountered, a zero-byte inputstream will be sent to the embedded (TIFF) parser.  This in turn throws an exception.  With the standardAutoDetectParser(), this embedded doc exception is caught, swallowed and ignored.  TheRecursiveParserWrapperwill catch these exceptions and allow users to see how many TIFFs they aren't getting, and allow users to see which files contain TIFFs.  To get a sense of the external libraries you'll need to add, take a look at thispom",../data/confluence_exports/TIKA/PDFBOX-2-X-NOTES_109454064.html
TIKA : TikaEvalOnVM An Example with Apache PDFBox,"Clean up from any previous runsRemove tika-app-X-Y.jar from /data1/tools/tika/batch/bin – make sure to leave in the other ""optional"" jars:jai-imageio-jpeg2000-1.3.0.jar, sqlite-jdbc-3.32.3.2.jar and zstd-jni-1.4.5-6.jarRemove or rename/data1/tools/tika/batch/logsRemove or rename/data1/tools/tika/batch/nohup.outRun the current ""A"" versionPlace the ""A"" version of tika-app-X.Y.jar in/data1/tools/tika/batch/binModifyappBatchExecutor.shtoput the output in a new output directory-o /data1/extracts/pdfboxAif using a file list, confirm that the correct file list is specified-fileList fileLists/ccAndBugTracker_pdfs.txtExecute:nohup ./appBatchExecutor.sh &Wait for the ""A"" version to complete before starting the ""B"" versionBuild and run the ""B"" versionUpdate PDFBox from SVN,mvn clean installUpdate the PDFBox, Fontbox and jbig2-imageio versions in the Tika project tika-parsers/pom.xmlRunmvn cleanon the whole Tika project and make sure that your IDE has picked up the changesRun the PDFParser tests in tika-parsers/src/test/java/o.a.t.parsers.pdf.* to make sure that at least the Tika unit tests work.Build the entire Tika project (even though you'll only use tika-app.jar):mvn clean installOn the VM, remove the tika.app-A.jar from/data1/tools/tika/batch/bin, rename the existingnohup.outtonohup-A.out, renamelogs/tologs-A/Drop the new tika-app-B.jar into (you guessed it!):/data1/tools/tika/batch/binModifyappBatchExecutor.shtoput the output in a new output directory-o /data1/extracts/pdfboxBif using a file list, confirm that the correct file list is specified-fileList fileLists/ccAndBugTracker_pdfs.txtExecute:nohup ./appBatchExecutor.sh &Wait for the ""B"" version to complete before starting the comparisons and reportsMake the comparisons and reportIn/data1/tools/tika/eval, remove the existing db filepdfboxAvsB.mv.dbif you don't want to rename it.nohup java -jar tika-eval-X.Y.jar Compare -extractsA /data1/extracts/pdfboxA -extractsB /data1/extracts/pdfboxB -db pdfboxAvsB&When that completes,Remove any files left over from the last run inreports/:rm -r reportsWrite the reportsjava -Djava.io.tmpdir=tmp -jar tika-eval-X.Y.jar Report -db pdfboxAvsB–Note the -Djava.io.tmpdir=tmp – need to set the tmp directory to something writeable by 'collab' When this process completes, you'll have all of the reports written to/data1/tools/tika/eval/reports/.",../data/confluence_exports/TIKA/TikaEvalOnVM_109454090.html
TIKA : TikaEvalOnVM H2 to Postgresql and Reports,"With the expansion of the regression corpus, I'm finding that H2 isn't able to write the reports – no matter the -Xmx, even after a few hours, it doesn't even get to the point of creating thereportsdirectory. I should set up postgres on our VM, but I haven't gotten around to that yet. For now, I'm copying the H2 db to Postgresql and then writing the reports from there. The code to copy H2->postgres is available here:tika-addons. I had to modify the report SQL slightly to work with Postgresql, and I stripped out some of the reports/calculations that aren't critical to the full regression tests. The modified report SQL is availablecomparison-reports-pg.xml",../data/confluence_exports/TIKA/TikaEvalOnVM_109454090.html
TIKA : Migrating to Tika 2.0.0 Breaking Metadata Key Changes Between 1.x and 2.x,"These are changes in locations of keys, not in the key names that consumers/clients will see: Metadata.RESOURCE_NAME_KEYhas been renamedTikaCoreProperties.RESOURCE_NAME_KEY.TikaCoreProperties.KEYWORDShas been removed in favor ofOffice.KEYWORDS",../data/confluence_exports/TIKA/Migrating-to-Tika-2.0.0_173083194.html
TIKA : Migrating to Tika 2.0.0 Changed Metadata Keys,There are a few other subtle changes in key names listed below: Tika 1.xTika 2.xX-Parsed-ByX-TIKA:Parsed-ByX-TIKA:EXCEPTION:runtimeX-TIKA:EXCEPTION:container_exception,../data/confluence_exports/TIKA/Migrating-to-Tika-2.0.0_173083194.html
TIKA : Migrating to Tika 2.0.0 Removed duplicate/triplicate keys,"Background:In early 1.x, we had basic metadata keys that were created somewhatad hoc.  We then added metadata keys based on standards such as Dublin Core, or we at least tried to add namespaces to the metadata keys for specific file formats.  To maintain backwards compatibility, we kept the old keys and added new keys.  This led to quite a bit of metadata bloat, where we'd have the same information two or three times.  In Tika 2.x, we slimmed down the metadata keys and relied only on the standards-based or name-spaced keys.  In the table below, we document the mappings.  If you notice any missing, please let us know or update the wiki.  Tika 1.xTika 2.xAuthor, meta:author, dc:creatordc:creatorLast-Author, meta:last-authormeta:last-authortitle, dc:titledc:titleCreation-Date, date, dcterms:createddcterms:createdLast-Modified, modified, dcterms:modifieddcterms:modifiedLast-Save-Date, meta:save-datemeta:save-datew:commentsw:CommentsApplication-Name, extended-properties:Applicationextended-properties:ApplicationCharacter Count, meta:character-countmeta:character-countCompany, extended-properties:Companyextended-properties:CompanyEdit-Time, extended-properties:TotalTimeextended-properties:TotalTimeKeywords, meta:keyword, dc:subjectmeta:keyword, dc:subjectPage-Count, meta:page-countmeta:page-countRevision-Number, cp:revisioncp:revisionsubject, cp:subject, dc:subjectdc:subjectTemplate, extended-properties:Templateextended-properties:TemplateWord-Count, meta:word-countmeta:word-countidentifierdc:identifierpublisherdc:publisherdc:description, subject (as in MSG files)dc:description (dc:subject was added back in 2.4.0).  In 2.x, we've moved all configuration into atika-config.xmlfile.  Two popular parsers used to rely on *.properties files; see their individual pages for details:PDFParserandTesseractOCRParser. See other individual parser pages for available configurations:TikaParserNotes.  If you notice any missing parsers, please help us document configurations for all parsers.  In Tika 2.x, we separated the 1.xtika-parsersmodule into three modules and packages: tika-parsers-standard – the most common parsers – should not require rest calls nor native libs (NOTE: despite the goal of this package, we do include the TesseractOCR parser which will run Tesseract if you have that installed)tika-parsers-extended – may include native libs and/or dependencies that not everyone wants (e.g.netcdf)tika-parsers-ml – may include heavy dependencies (e.g.dl4j) or parsers that rely on rest calls and external services The goal is to allow users to select only the parsers (and dependencies) that they want. When usingtika-parsersin your project, you need to change the dependencies from: pom.xml from 1.27<dependency>
  <groupId>org.apache.tika</groupId>
  <artifactId>tika-parsers</artifactId>
  <version>1.27</version>
</dependency> to at leasttika-parsers-standard-package.  If you wantnetcdfparsing and/orsqlite3parsing – both of which were included intika-parsersin 1.x – you'll need to includetika-parser-scientific-packageand/or thetika-parser-sqlite3-package. pom.xml for 2.0.0+<dependency>
  <groupId>org.apache.tika</groupId>
  <artifactId>tika-parsers-standard-package</artifactId>
  <version>2.7.0</version>
</dependency>
<dependency>
  <groupId>org.apache.tika</groupId>
  <artifactId>tika-parser-scientific-package</artifactId>
  <version>2.7.0</version>
</dependency>
<dependency>
  <groupId>org.apache.tika</groupId>
  <artifactId>tika-parser-sqlite3-package</artifactId>
  <version>2.7.0</version>
</dependency> NOTE:As of Tika 2.7.0, we have addedtika-parser-nlp-packageto our release artifacts. NOTE:As in Tika 1.x, if you need detection on container formats (e.g. OLE2: .doc, .ppt, .xls or zip-based: .xlsx, .pptx, .docx or .ogg based), you need to include the underlying Tika parsers that will parse the container files and make the detection based on the information in the container.  In Tika 2.x, this means that you need to includetika-parsers-standard-package!",../data/confluence_exports/TIKA/Migrating-to-Tika-2.0.0_173083194.html
TIKA : Migrating to Tika 2.0.0 Lesser parser notes that may only affect early versions of 2.x,"Also, there's a small transitive dependency issue with jcl-over-slf4j between tika-parsers-standard-package 2.0.0 and tika-parser-scientific-module:2.0.0. So if you are using maven enforcer plugin, you will need to fix it by adding this: pom.xml<!-- Fix tika-parsers-standard-package 2.0.0 vs tika-parser-scientific-module:2.0.0 transitive dependency -->
<dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>jcl-over-slf4j</artifactId>
    <version>1.7.31</version>
</dependency> If you are checking for CVEs (recommended), the tika-parser-scientific-module:2.0.0 comes with a transitive dependency on quartz 2.2.0 which should be fixed like this: quartz<dependency>
    <groupId>edu.ucar</groupId>
    <artifactId>netcdf4</artifactId>
    <version>${netcdf-java.version}</version>
    <exclusions>
      ...
      <exclusion>
        <groupId>org.quartz-scheduler</groupId>
        <artifactId>quartz</artifactId>
      </exclusion>
    </exclusions>
  </dependency>
  <dependency>
    <groupId>org.quartz-scheduler</groupId>
    <artifactId>quartz</artifactId>
    <version>2.3.2</version>
  </dependency>",../data/confluence_exports/TIKA/Migrating-to-Tika-2.0.0_173083194.html
TIKA : Migrating to Tika 2.0.0 Language Detection,"When using lang detection, you need to use: pom.xml 2.0.0<dependency>
  <groupId>org.apache.tika</groupId>
  <artifactId>tika-langdetect-optimaize</artifactId>
  <version>2.1.0</version>
</dependency> Also note thatorg.apache.tika.langdetect.OptimaizeLangDetector.getDefaultLanguageDetectorhas moved toorg.apache.tika.langdetect.optimaize.OptimaizeLangDetector.getDefaultLanguageDetector. For OCR, you can not use anymoreTesseractOCRConfig.setTesseractPath(String)andTesseractOCRConfig.setTessdataPath(String)methods. They moved to theTesseractOCRParserclass.",../data/confluence_exports/TIKA/Migrating-to-Tika-2.0.0_173083194.html
TIKA : Migrating to Tika 2.0.0 zstd,"The zstd dependency includes native libs and is not packaged with the tika-parsers-module.  If you'd like to parse zstd files, include: zstd-jni<dependency>
      <groupId>com.github.luben</groupId>
      <artifactId>zstd-jni</artifactId>
      <version>1.5.0-4</version>
    </dependency>",../data/confluence_exports/TIKA/Migrating-to-Tika-2.0.0_173083194.html
TIKA : Migrating to Tika 2.0.0 TIFF and JPEG2000,"If you plan to write TIFFs with Tika (rendering of PDF pages for OCR), and if the BSD-3 with nuclear disclaimer license is acceptable to you, include: jai-imageio-core<dependency>
  <groupId>com.github.jai-imageio</groupId>
  <artifactId>jai-imageio-core</artifactId>
  <version>1.4.0</version>
</dependency> If you plan on processing JPEG2000 images (most common use case would be rendering PDF pages for OCR), and if the BSD-3 with nuclear disclaimer license is acceptable to you,  include: jpeg2000<dependency>
  <groupId>com.github.jai-imageio</groupId>
  <artifactId>jai-imageio-jpeg2000</artifactId>
  <version>1.4.0</version>
</dependency> Note! In 2.x, Tika will not warn you if a PDF page that you're trying to render has a JPEG2000 in it.  PDFBox will log a warning.   tbd",../data/confluence_exports/TIKA/Migrating-to-Tika-2.0.0_173083194.html
TIKA : Migrating to Tika 2.0.0 General,"enableFileUrl has been removed in favor of two separate fetchers, one for files and one for URLs (seetika-pipes#FetchersInClassicServerEndpoints).FileSystemFetcher(which is packaged with tika-core) for filesHttpFetcher(requires an external jar fromhttps://mvnrepository.com/artifact/org.apache.tika/tika-fetcher-http) for URLs.",../data/confluence_exports/TIKA/Migrating-to-Tika-2.0.0_173083194.html
TIKA : Migrating to Tika 2.0.0 tika-pipes,See thetika-pipespage.,../data/confluence_exports/TIKA/Migrating-to-Tika-2.0.0_173083194.html
TIKA : Migrating to Tika 2.0.0 tika-langid,"In the 1.x branch, the default (hardwired) language identification component was the wrapper around Optimaize.  If you used the following in 1.x: pom.xml 1.27<dependency>
  <groupId>org.apache.tika</groupId>
  <artifactId>tika-langdetect</artifactId>
  <version>1.27</version>
</dependency> In 2.x, change this to: optimaize-lang-detect<dependency>
  <groupId>org.apache.tika</groupId>
  <artifactId>tika-langdetect-optimaize</artifactId>
  <version>2.0.x</version>
</dependency> The original  language id component that was built by Tika devs and that used to be in tika-core is now in the tika-langdetect-tika module.",../data/confluence_exports/TIKA/Migrating-to-Tika-2.0.0_173083194.html
TIKA : TikaEvalMetrics Common Words/Out of Vocabulary (OOV),"We gathered the top 30k most common words for ~120 languages from wikipedia or the Leipzig corpus.  These word lists are available in thecommon_tokensdirectory. When text is extracted for a given document, we run language id on the string and then count the number of common words in the extracted text divided by the number of alphabetic words total extracted.  This gives us a percentage of common words or the inverse, the Out of Vocabulary (OOV) statistic.  This is some indication of how ""languagey"" the extracted text is. If the assumption is that your documents generally contain natural language (e.g., not just parts lists or numbers), the OOV% statistic can identify outliers for manual review.  For example, when we were testing the Tika 1.15 release candidate against the 1.14 version, we found that the text extracted by 1.14 had a 100% out of vocabulary (language id was Chinese, and the top 10 tokens were: 捳敨: 18 | 獴档: 14 | 略獴: 14 | m: 11 | 杮湥: 11 | 瑵捳: 11 | 畬杮: 11 | 档湥: 10 | 搠敩: 9 | 敮浨: 9), however, FOR THE SAME FILE, the text extracted by the 1.15 release candidate had an OOV of 54% (language id was German and the top 10 tokens were:  die: 11 | und: 8 | von: 8 | deutschen: 7 | deutsche: 6 | 1: 5 | das: 5 | der: 5 | finanzministerium: 5 | oder: 5).  In short, text that has a high OOVmightindicate a failed parse.  Or, when comparing two different extraction methods, it is likely that the text with the lower OOV is better (assuming the extractors aren't hallucinating common words). Tilman Hausherr originally recommended this metric as a comparison metric when comparing the output from different versions of PDFBox. For our initial collaboration with PDFBox, we found a list of common English words and removed those that had fewer than four characters. The intuition is that if tool A extracts 500, but tool B extracts 1,000, there issomeindication that tool B may have done a better job.",../data/confluence_exports/TIKA/TikaEvalMetrics_109454089.html
TIKA : TikaEvalMetrics Implementation Details,"For now, we've set up an Analyzer chain in Lucene that: Filters out tokens that don't contain an alphabetic or ideographic character.Maps urls to ""url"" and emails to ""email"" (We don't want to penalize documents with urls and emails).Requires that a token be at least 4 characters longunlessit is comprised entirely of CJK characters. But wait, what's a word for non-whitespace (e.g. Chinese/Japanese) languages? We've followed common practice for non-whitespace languages of tokenizing bigrams...this is linguistically abhorrent, but it is mildly useful if inaccurate for our purposes. Benefits: Easy to implement. Risks: If an OCR engine relies solely on dictionary lookup and does not allow for out-of-vocabulary terms, the generated text will contain only known words, and the ""common words"" score will be incorrectly high. Yes, the text contains known words, but they mightnotreflect the correct text.If a document contains part numbers or other non-natural language tokens, then this metric will not accurately reflect success.Multi-lingual documents can cause challenges for interpretation. If the language id component ""detects"" English, even though the majority of the document is in Chinese, this metric will be misleading.",../data/confluence_exports/TIKA/TikaEvalMetrics_109454089.html
TIKA : TikaGDAL Errors encountered with brew and Mavericks,"Note if you encounter errors while upgrading to Mavericks here, the answer is to first:  $  brew rm $(join <(brew leaves) <(brew deps gdal --complete ))  Note the above instructions are definitely Mac centric. We recommend checking outGDAL's Websitefor specific instructions on installing GDAL on your operating system.  Once GDAL is installed, the following command should be available on your path.  gdalinfo  Runninggdalinfoshould produce something like:  Usage: gdalinfo [--help-general] [-mm] [-stats] [-hist] [-nogcp] [-nomd]
                [-norat] [-noct] [-nofl] [-checksum] [-proj4]
                [-listmdd] [-mdd domain|`all`]*
                [-sd subdataset] datasetname

FAILURE: No datasource specified.  If that works you are in business!   To use Tika and GDAL grab the 1.7-SNAPSHOT latest of Tika and then grab a geospatial file, e.g., this example will use a Flexible Image Transport System (FITS) file as an example. Then run:  java -jar tika-app-1.7-SNAPSHOT.jar -m WFPC2u5780205r_c0fx.fits  This should produce, e.g.,  ALLG-MAX: 3.777701E3
ALLG-MIN: -7.319537E1
ATODCORR: COMPLETE
..
X-Parsed-By: org.apache.tika.parser.DefaultParser
X-Parsed-By: org.apache.tika.parser.gdal.GDALParser  If you see X-Parsed-By: ..GDALParser and a bunch of geospatial metadata, you are in business!   Once you have GDAL and a fresh build of Tika 1.7-SNAPSHOT (including Tika server), you can easily use Tika-Server with GDAL.  For example, to post a FITS file to the server and get back its metadata, run the following commands:",../data/confluence_exports/TIKA/TikaGDAL_109454092.html
"TIKA : TikaGDAL in another window, start Tika server",java -jar /path/to/tika-server-1.7-SNAPSHOT.jar,../data/confluence_exports/TIKA/TikaGDAL_109454092.html
"TIKA : TikaGDAL in another window, issue a cURL request","curl -T /path/to/fits/image.fitshttp://localhost:9998/tika--header ""Content-type: application/fits""   On TIKA-2684, Susan Borda, reports on some important steps to get a full FITS parse with GDAL. See Susan'scomment, and her pointer to properly loadingfitsio.",../data/confluence_exports/TIKA/TikaGDAL_109454092.html
TIKA : Metadata Filters FieldNameMappingFilter,"This filter allows users to map field names from the Tika field names to custom field names.  In the following example, the filter change the names of the metadata fields from the from element to the to element, and (becauseexcludeUnmappedis set totrue, this filter will remove all metadata that does not have a key of ""X-TIKA:content"", ""dc:title"" or ""dc:created"".  IfexcludeUnmappedis set tofalse, this filter will apply the mappings but maintain all other metadata. <properties>
  <metadataFilters>
    <metadataFilter class=""org.apache.tika.metadata.filter.FieldNameMappingFilter"">
      <params>
        <excludeUnmapped>true</excludeUnmapped>
        <mappings>
          <mapping from=""X-TIKA:content"" to=""content""/>
          <mapping from=""dc:title"" to=""title""/>
          <mapping from=""dc:created"" to=""date""/>
        </mappings>
      </params>
    </metadataFilter>
  </metadataFilters>",../data/confluence_exports/TIKA/Metadata-Filters_181309141.html
TIKA : Metadata Filters IncludeFieldMetadataFilter,"This filter will include only the fields specified and will leave the field names as they are. <properties>
    <metadataFilters>
        <metadataFilter class=""org.apache.tika.metadata.filter.IncludeFieldMetadataFilter"">
            <params>
                <include>
                    <field>X-TIKA:content</field>
                    <field>extended-properties:Application</field>
                    <field>Content-Type</field>
                </include>
            </params>
        </metadataFilter>
    </metadataFilters>
</properties>",../data/confluence_exports/TIKA/Metadata-Filters_181309141.html
TIKA : Metadata Filters ExcludeFieldMetadataFilter,"This filter will allow all metadata items to pass through except for the excluded keys. <properties>
  <metadataFilters>
    <metadataFilter class=""org.apache.tika.metadata.filter.ExcludeFieldMetadataFilter"">
      <params>
        <exclude>
          <field>dc:title</field>
          <field>dc:creator</field>
        </exclude>
      </params>
    </metadataFilter>
  </metadataFilters>
</properties>",../data/confluence_exports/TIKA/Metadata-Filters_181309141.html
TIKA : Metadata Filters ClearByMImeMetadataFilter,"This filter removes all metadata from files of specific mime types.  For example, you may want to parse EMF files because they can contain embedded files, but you might not want to include the metadata or content from those files in what you show to users. <properties>
  <metadataFilters>
    <metadataFilter class=""org.apache.tika.metadata.filter.ClearByMimeMetadataFilter"">
      <params>
        <mimes>
          <mime>image/emf</mime>
          <mime>image/jpeg</mime>
        </mimes>
      </params>
    </metadataFilter>
  </metadataFilters>
</properties>",../data/confluence_exports/TIKA/Metadata-Filters_181309141.html
TIKA : Metadata Filters TikaEvalMetadataFilter,TODO: fill in,../data/confluence_exports/TIKA/Metadata-Filters_181309141.html
TIKA : PDFParser (Apache PDFBox) Configuration options,"There are three ways of configuring the PDFParser. Programmatically via setter methods on the PDFParser.Programmatically via the PDFParserConfig object submitted through theParseContext.Via the tika-config.xml file (many thanks to Thamme Gowda and Chris Mattmann's work on TIKA-1508). The first two are fairly self-explanatory through the javadocs. In the following, we document all of the parameters for the PDFParser in Tika 2.x. You only need to specify the parameters you want to change. PDFParser Configuration<properties>
  <parsers>
    <parser class=""org.apache.tika.parser.DefaultParser"">
      <!-- this is not formally necessary, but prevents loading of unnecessary parser -->
      <parser-exclude class=""org.apache.tika.parser.pdf.PDFParser""/>
    </parser>
    <parser class=""org.apache.tika.parser.pdf.PDFParser"">
      <params>
       <!-- these are the defaults; you only need to specify the ones you want 
            to modify -->
        <param name=""allowExtractionForAccessibility"" type=""bool"">true</param>
        <param name=""averageCharTolerance"" type=""float"">0.3</param>
        <param name=""detectAngles"" type=""bool"">false</param>
        <param name=""extractAcroFormContent"" type=""bool"">true</param>
        <param name=""extractActions"" type=""bool"">false</param>
        <param name=""catchIntermediateIOExceptions"" type=""bool"">true</param>
        <param name=""dropThreshold"" type=""float"">2.5</param>
        <param name=""enableAutoSpace"" type=""bool"">true</param>
        <param name=""extractAnnotationText"" type=""bool"">false</param>
        <param name=""extractBookMarksText"" type=""bool"">true</param>
        <param name=""extractFontNames"" type=""bool"">false</param>
        <param name=""extractInlineImages"" type=""bool"">false</param>
        <param name=""extractUniqueInlineImagesOnly"" type=""bool"">true</param>
        <param name=""ifXFAExtractOnlyXFA"" type=""bool"">false</param>
        <param name=""maxMainMemoryBytes"" type=""long"">-1</param>
        <param name=""ocrDPI"" type=""int"">300</param>
        <param name=""ocrImageFormatName"" type=""string"">png</param>
        <param name=""ocrImageQuality"" type=""float"">1.0</param>
        <param name=""ocrRenderingStrategy"" type=""string"">ALL</param>
        <param name=""ocrStrategy"" type=""string"">auto</param>
        <param name=""ocrStrategyAuto"" type=""string"">better</param>
        <param name=""ocrImageType"" type=""string"">gray</param>
        <param name=""setKCMS"" type=""bool"">false</param>
        <param name=""sortByPosition"" type=""bool"">false</param>
        <param name=""spacingTolerance"" type=""float"">0.5</param>
        <param name=""suppressDuplicateOverlappingText"" type=""bool"">false</param> 
      </params>
    </parser>
  </parsers>
</properties>",../data/confluence_exports/TIKA/109454066.html
TIKA : PDFParser (Apache PDFBox) Optional Dependencies,"If you need to process TIFF or JPEG2000 images within PDFs (either for inline image extraction or OCR), please consider adding the optional dependencies specified byPDFBox. These dependencies are not compatible with ASL 2.0; please make sure that any third party licenses are suitable for your project. Finally,M. Caruana Galiziaalerted us to the need to use maven-shade'sServicesResourceTransformerbecause the third-party dependencies' services file will be overwritten unless you transform the services. See an example:here.  Note: the configuration of some of these features via the config file requires a nightly build of Tika after 11/8/2016 or Tika version >= 1.15. Start with the instructions onTikaOCR. In short, you need to have Tesseract installed. There are two ways of running OCR on PDFs: Extracting the inline images and letting Tesseract run on each inline image.Rendering each PDF page as a single image and running Tesseract on that single image. We have not carried out evaluations to determine which strategy is better. We suspect that the tried and trueIt Depends(TM)is operative here. We added OCR'ing of the single image option because some PDFs can contain hundreds of images per page where each image is a tiny part of the overall page, and OCR would be useless. However, we recognize, that if the page is logically broken into sections, running OCR on the individual inline images might yield better results. Note: These two options are independent.  If you setextractInlineImagesto true and select anOcrStrategythat includes OCR on the rendered page, Tika will run OCR on the extracted inline imagesandthe rendered page.",../data/confluence_exports/TIKA/109454066.html
TIKA : PDFParser (Apache PDFBox) Option 1: Configuring OCR on Inline Images,"This will extract inline images as if they were attachments, and then, if Tesseract is correctly configured, it should run against the images. Note: by default, extracting inline images is turned off because some rare PDFs contain thousands of inline images per page, and it has a big hit on performance, both memory usage and time. ...
        <parser class=""org.apache.tika.parser.pdf.PDFParser"">
            <params>
                <param name=""extractInlineImages"" type=""bool"">true</param>
            </params>
        </parser
...",../data/confluence_exports/TIKA/109454066.html
TIKA : PDFParser (Apache PDFBox) Option 2: Configuring OCR on Rendered Pages,"This will render each PDF page and then run OCR on that image. This method of OCR is triggered by theocrStrategyparameter, but users can manipulate other parameters, including theimage type(seeorg.apache.pdfbox.rendering.ImageTypefor options) and the dots per inchdpi. The defaults are:grayand300respectively. ForocrStrategy, we currently have:no_ocr(rely on regular text extraction only),ocr_only(don't bother extracting text, just run OCR on each page),ocr_and_text(both extract text and run OCR) and (as of Tika 1.21)auto(try to extract text, but run OCR if fewer than 10 characters were extracted of if there are more than 10 characters with unmapped Unicode values). ...
        <parser class=""org.apache.tika.parser.pdf.PDFParser"">
            <params>
                <param name=""ocrStrategy"" type=""string"">ocr_only</param>
                <param name=""ocrImageType"" type=""string"">rgb</param>
                <param name=""ocrDPI"" type=""int"">100</param>
            </params>
        </parser>
...",../data/confluence_exports/TIKA/109454066.html
TIKA : PDFParser (Apache PDFBox) Setting Parse Time/Per File configurations via tika-server,See:Configuring Parsers At Parse Time in tika-server.,../data/confluence_exports/TIKA/109454066.html
TIKA : PDFParser (Apache PDFBox) Optional Dependencies,"Note, you should include the following dependency to process JBIG2 images: <dependency>
        <groupId>org.apache.pdfbox</groupId>
        <artifactId>jbig2-imageio</artifactId>
        <version>3.0.2</version>
    </dependency> Note,if their licenses are compatible with your application, you may want to include the following jai libraries in your classpath to handle jp2, jpeg2000 and tiff files.The licenses are not Apache 2.0 compatible! <dependency>
        <groupId>com.github.jai-imageio</groupId>
        <artifactId>jai-imageio-core</artifactId>
        <version>1.4.0</version>
    </dependency>
    <dependency>
        <groupId>com.github.jai-imageio</groupId>
        <artifactId>jai-imageio-jpeg2000</artifactId>
        <version>1.3.0</version>
        <scope>test</scope>
    </dependency> If you are using Java 8, make sure to seepdf-renderingfor JVM settings that may improve the speed of processing.",../data/confluence_exports/TIKA/109454066.html
TIKA : PDFParser (Apache PDFBox) Common Text Extraction Challenges with PDFs,"This is mostly a stub. The focus of this section is on extracting electronic text from the PDF with no OCR. One could write several volumes on how text extraction from PDFs could go wrong. It would only be poetic justice for said author to print out those volumes, pour coffee on the paper, scan them in as PDFs on different scanners, some with OCR, some without, at different angles of rotation with user-defined fonts randomly deleted. High level preliminaries: 0. Your matrix algebra (or, your tool's matrix algebra) has to be moderately advanced to do text extraction well. The PDF format is display-based not text-basedOne major goal is to display the same content on different devices b. A PDF may be image-only and contain no actual electronic text c. When there is electronic text, there may be no space characters stored in the text, rather spaces may appear in the rendering of the image via specific coordinates for the characters.2. The PDF format is page-based",../data/confluence_exports/TIKA/109454066.html
TIKA : PDFParser (Apache PDFBox) Tables Aren't Extracted as Tables,"Right. In PDF/UA (Universal Accessibility) tables can be stored with structural markup.  As of Tika 1.24, you can set ""extractMarkedContent"" = ""true"" via the PDFParserConfig, and Tika will extracted marked content, including tables, if the PDF was generated with marked content. In many PDFs, tables are often not stored as tables. A human is easily able to see tables, but all that is stored in the PDF is text chunks and coordinates on a page (if there's any text at all). One needs to apply some advanced computation to extract table structure from a PDF. Tika does not currently do this. Please seeTabulaPDFas one open source project that extracts tables from PDFs and maintains their structure. Note that as of 2023, we still see papers at research conferences on extracting structural elements (including tables) from PDFs – THIS IS NOT A SOLVED PROBLEM.  And, humorously, this kind of task is sometimes called ""PDF remediation"".",../data/confluence_exports/TIKA/109454066.html
TIKA : PDFParser (Apache PDFBox) No spaces/Extra spaces,"See 1c. above. Depending on how the PDF was generated, it is possible that it doesn't store actual space characters. Rather software has to use coordinates on the page plus matrix algebra plus font information about the width of characters to ""impute"" where spaces would be. The math is the simple part; sometimes there can be missing or wrong font information that can lead to no spaces or extra spaces.",../data/confluence_exports/TIKA/109454066.html
TIKA : PDFParser (Apache PDFBox) Character Encoding/Unicode Mappings,See alsodiagnosing PDF text problems.,../data/confluence_exports/TIKA/109454066.html
TIKA : PDFParser (Apache PDFBox) See also,Upgrading toPDFBox 2.x,../data/confluence_exports/TIKA/109454066.html
TIKA : AgeDetectionParser Download and BuildAgePredictor,cd $HOME/src && git clonehttps://github.com/USCDataScience/AgePredictor.git2. cd AgePredictor && mvn install,../data/confluence_exports/TIKA/AgeDetectionParser_109454038.html
TIKA : AgeDetectionParser TestAgePredictor,"{{`}}java -cp age-predictor-assembly/target/age-predictor-assembly-1.1-SNAPSHOT-jar-with-dependencies.jar edu.usc.irds.agepredictor.authorage.AgePredicterLocal I am actually very young now{{`}}  The above should print something like:  $ java -cp age-predictor-assembly/target/age-predictor-assembly-1.1-SNAPSHOT-jar-with-dependencies.jar edu.usc.irds.agepredictor.authorage.AgePredicterLocal I am actually very young now
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/07/06 23:23:24 INFO SparkContext: Running Spark version 2.0.0
17/07/06 23:23:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/06 23:23:24 INFO SecurityManager: Changing view acls to: mattmann
17/07/06 23:23:24 INFO SecurityManager: Changing modify acls to: mattmann
17/07/06 23:23:24 INFO SecurityManager: Changing view acls groups to: 
17/07/06 23:23:24 INFO SecurityManager: Changing modify acls groups to: 
17/07/06 23:23:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(mattmann); groups with view permissions: Set(); users  with modify permissions: Set(mattmann); groups with modify permissions: Set()
17/07/06 23:23:25 INFO Utils: Successfully started service 'sparkDriver' on port 54970.
17/07/06 23:23:25 INFO SparkEnv: Registering MapOutputTracker
17/07/06 23:23:25 INFO SparkEnv: Registering BlockManagerMaster
17/07/06 23:23:25 INFO DiskBlockManager: Created local directory at /private/var/folders/n5/1d_k3z4s2293q8ntx_n8sw54mm5n_8/T/blockmgr-aa033554-acff-4ea1-a5d1-250257f467dc
17/07/06 23:23:25 INFO MemoryStore: MemoryStore started with capacity 2004.6 MB
17/07/06 23:23:25 INFO SparkEnv: Registering OutputCommitCoordinator
17/07/06 23:23:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/07/06 23:23:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.20.10.2:4040
17/07/06 23:23:25 INFO Executor: Starting executor ID driver on host localhost
17/07/06 23:23:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54971.
17/07/06 23:23:25 INFO NettyBlockTransferService: Server created on 172.20.10.2:54971
17/07/06 23:23:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.20.10.2, 54971)
17/07/06 23:23:25 INFO BlockManagerMasterEndpoint: Registering block manager 172.20.10.2:54971 with 2004.6 MB RAM, BlockManagerId(driver, 172.20.10.2, 54971)
17/07/06 23:23:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.20.10.2, 54971)
17/07/06 23:23:26 WARN SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/07/06 23:23:26 INFO SharedState: Warehouse path is 'file:/Users/mattmann/git/AgePredictor/spark-warehouse'.
17/07/06 23:23:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 6.1 MB, free 1998.5 MB)
17/07/06 23:23:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 488.5 KB, free 1998.0 MB)
17/07/06 23:23:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.20.10.2:54971 (size: 488.5 KB, free: 2004.1 MB)
17/07/06 23:23:39 INFO SparkContext: Created broadcast 0 from broadcast at CountVectorizer.scala:243
17/07/06 23:23:42 INFO CodeGenerator: Code generated in 173.162228 ms
17/07/06 23:23:42 INFO SparkContext: Starting job: first at AgePredicterLocal.java:114
17/07/06 23:23:42 INFO DAGScheduler: Got job 0 (first at AgePredicterLocal.java:114) with 1 output partitions
17/07/06 23:23:42 INFO DAGScheduler: Final stage: ResultStage 0 (first at AgePredicterLocal.java:114)
17/07/06 23:23:42 INFO DAGScheduler: Parents of final stage: List()
17/07/06 23:23:42 INFO DAGScheduler: Missing parents: List()
17/07/06 23:23:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at javaRDD at AgePredicterLocal.java:112), which has no missing parents
17/07/06 23:23:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.9 KB, free 1998.0 MB)
17/07/06 23:23:42 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KB, free 1998.0 MB)
17/07/06 23:23:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.20.10.2:54971 (size: 5.4 KB, free: 2004.1 MB)
17/07/06 23:23:42 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1012
17/07/06 23:23:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at javaRDD at AgePredicterLocal.java:112)
17/07/06 23:23:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/07/06 23:23:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 6671 bytes)
17/07/06 23:23:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/07/06 23:23:42 INFO CodeGenerator: Code generated in 21.816953 ms
17/07/06 23:23:42 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3381 bytes result sent to driver
17/07/06 23:23:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 123 ms on localhost (1/1)
17/07/06 23:23:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/07/06 23:23:42 INFO DAGScheduler: ResultStage 0 (first at AgePredicterLocal.java:114) finished in 0.139 s
17/07/06 23:23:42 INFO DAGScheduler: Job 0 finished: first at AgePredicterLocal.java:114, took 0.221228 s

===================

Text received- 'I am actually very young now ' 
 Predicted Age - 34.983567


===================

17/07/06 23:23:43 INFO SparkContext: Invoking stop() from shutdown hook
17/07/06 23:23:43 INFO SparkUI: Stopped Spark web UI at http://172.20.10.2:4040
17/07/06 23:23:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/07/06 23:23:43 INFO MemoryStore: MemoryStore cleared
17/07/06 23:23:43 INFO BlockManager: BlockManager stopped
17/07/06 23:23:43 INFO BlockManagerMaster: BlockManagerMaster stopped
17/07/06 23:23:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/07/06 23:23:43 INFO SparkContext: Successfully stopped SparkContext
17/07/06 23:23:43 INFO ShutdownHookManager: Shutdown hook called
17/07/06 23:23:43 INFO ShutdownHookManager: Deleting directory /private/var/folders/n5/1d_k3z4s2293q8ntx_n8sw54mm5n_8/T/spark-31e98436-00e3-4020-a4c5-784ec75b16de
$  In this case you are good shape. If not, please report a bughere   To runAgeRecogniser, download and install Tika 1.16 or later, and then run the following (make sure you have a $TIKASRC/tika-nlp/model directory populated with models before running this per above)  {{`}}cd $HOME/src/ && git clonehttps://github.com/apache/tika.git{{`}} 2. {{`}}cd tika-nlp && echo ""I am a test file"" > test.txt{{`}} 2. {{`}}java -cp ../tika-app/target/tika-app-1.16-SNAPSHOT.jar:target/tika-nlp-1.16-SNAPSHOT-jar-with-dependencies.jar:./model org.apache.tika.cli.TikaCLI --config=src/test/resources/org/apache/tika/parser/recognition/tika-config-age.xml -m test.txt{{`}}  You should then see:  $java -cp ../tika-app/target/tika-app-1.16-SNAPSHOT.jar:target/tika-nlp-1.16-SNAPSHOT-jar-with-dependencies.jar:./model org.apache.tika.cli.TikaCLI --config=src/test/resources/org/apache/tika/parser/recognition/tika-config-age.xml -m test.txt
Jul 07, 2017 3:38:53 PM org.apache.tika.config.InitializableProblemHandler$3 handleInitializableProblem
WARNING: JBIG2ImageReader not loaded. jbig2 files will be ignored
See https://pdfbox.apache.org/2.0/dependencies.html#jai-image-io
for optional dependencies.
TIFFImageWriter not loaded. tiff files will not be processed
See https://pdfbox.apache.org/2.0/dependencies.html#jai-image-io
for optional dependencies.
J2KImageReader not loaded. JPEG2000 files will not be processed.
See https://pdfbox.apache.org/2.0/dependencies.html#jai-image-io
for optional dependencies.

Jul 07, 2017 3:38:53 PM org.apache.tika.config.InitializableProblemHandler$3 handleInitializableProblem
WARNING: Tesseract OCR is installed and will be automatically applied to image files unless
you've excluded the TesseractOCRParser from the default parser.
Tesseract may dramatically slow down content extraction (TIKA-2359).
As of Tika 1.15 (and prior versions), Tesseract is automatically called.
In future versions of Tika, users may need to turn the TesseractOCRParser on via TikaConfig.
Jul 07, 2017 3:38:53 PM org.apache.tika.config.InitializableProblemHandler$3 handleInitializableProblem
WARNING: org.xerial's sqlite-jdbc is not loaded.
Please provide the jar on your classpath to parse sqlite files.
See tika-parsers/pom.xml for the correct version.
INFO  Running Spark version 2.0.0
WARN  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
INFO  Changing view acls to: mattmann
INFO  Changing modify acls to: mattmann
INFO  Changing view acls groups to: 
INFO  Changing modify acls groups to: 
INFO  SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(mattmann); groups with view permissions: Set(); users  with modify permissions: Set(mattmann); groups with modify permissions: Set()
INFO  Successfully started service 'sparkDriver' on port 60109.
INFO  Registering MapOutputTracker
INFO  Registering BlockManagerMaster
INFO  Created local directory at /private/var/folders/n5/1d_k3z4s2293q8ntx_n8sw54mm5n_8/T/blockmgr-253e0d76-fef5-42bb-b2a9-1500e807797c
INFO  MemoryStore started with capacity 2004.6 MB
INFO  Registering OutputCommitCoordinator
INFO  Logging initialized @1726ms
INFO  jetty-9.2.z-SNAPSHOT
INFO  Started o.s.j.s.ServletContextHandler@12bcd0c0{/jobs,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@4879f0f2{/jobs/json,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@47db5fa5{/jobs/job,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@354fc8f0{/jobs/job/json,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@41813449{/stages,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@4678a2eb{/stages/json,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@5b43fbf6{/stages/stage,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@1080b026{/stages/stage/json,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@58ebfd03{/stages/pool,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@5b07730f{/stages/pool/json,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@1fdfafd2{/storage,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@a4b2d8f{/storage/json,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@dcfda20{/storage/rdd,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@6d304f9d{/storage/rdd/json,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@f73dcd6{/environment,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@5c87bfe2{/environment/json,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@2fea7088{/executors,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@40499e4f{/executors/json,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@51cd7ffc{/executors/threadDump,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@30d4b288{/executors/threadDump/json,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@4cc6fa2a{/static,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@40f1be1b{/,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@7a791b66{/api,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@6f2cb653{/stages/stage/kill,null,AVAILABLE}
INFO  Started ServerConnector@71b3bc45{HTTP/1.1}{0.0.0.0:4040}
INFO  Started @1831ms
INFO  Successfully started service 'SparkUI' on port 4040.
INFO  Bound SparkUI to 0.0.0.0, and started at http://192.168.1.65:4040
INFO  Starting executor ID driver on host localhost
INFO  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60110.
INFO  Server created on 192.168.1.65:60110
INFO  Registering BlockManager BlockManagerId(driver, 192.168.1.65, 60110)
INFO  Registering block manager 192.168.1.65:60110 with 2004.6 MB RAM, BlockManagerId(driver, 192.168.1.65, 60110)
INFO  Registered BlockManager BlockManagerId(driver, 192.168.1.65, 60110)
INFO  Started o.s.j.s.ServletContextHandler@7db0565c{/metrics/json,null,AVAILABLE}
WARN  Use an existing SparkContext, some configuration may not take effect.
INFO  Started o.s.j.s.ServletContextHandler@7692cd34{/SQL,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@32c0915e{/SQL/json,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@7dd712e8{/SQL/execution,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@22ee2d0{/SQL/execution/json,null,AVAILABLE}
INFO  Started o.s.j.s.ServletContextHandler@4b770e40{/static/sql,null,AVAILABLE}
INFO  Warehouse path is 'file:/Users/mattmann/tmp/tika1.15/tika-nlp/spark-warehouse'.
INFO  Block broadcast_0 stored as values in memory (estimated size 6.1 MB, free 1998.5 MB)
INFO  Block broadcast_0_piece0 stored as bytes in memory (estimated size 488.5 KB, free 1998.0 MB)
INFO  Added broadcast_0_piece0 in memory on 192.168.1.65:60110 (size: 488.5 KB, free: 2004.1 MB)
INFO  Created broadcast 0 from broadcast at CountVectorizer.scala:243
INFO  Code generated in 1537.312409 ms
INFO  Starting job: first at AgePredicterLocal.java:114
INFO  Got job 0 (first at AgePredicterLocal.java:114) with 1 output partitions
INFO  Final stage: ResultStage 0 (first at AgePredicterLocal.java:114)
INFO  Parents of final stage: List()
INFO  Missing parents: List()
INFO  Submitting ResultStage 0 (MapPartitionsRDD[3] at javaRDD at AgePredicterLocal.java:112), which has no missing parents
INFO  Block broadcast_1 stored as values in memory (estimated size 10.5 KB, free 1998.0 MB)
INFO  Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KB, free 1998.0 MB)
INFO  Added broadcast_1_piece0 in memory on 192.168.1.65:60110 (size: 5.3 KB, free: 2004.1 MB)
INFO  Created broadcast 1 from broadcast at DAGScheduler.scala:1012
INFO  Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at javaRDD at AgePredicterLocal.java:112)
INFO  Adding task set 0.0 with 1 tasks
INFO  Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 6477 bytes)
INFO  Running task 0.0 in stage 0.0 (TID 0)
INFO  Code generated in 14.170306 ms
INFO  Finished task 0.0 in stage 0.0 (TID 0). 3228 bytes result sent to driver
INFO  Finished task 0.0 in stage 0.0 (TID 0) in 80 ms on localhost (1/1)
INFO  Removed TaskSet 0.0, whose tasks have all completed, from pool 
INFO  ResultStage 0 (first at AgePredicterLocal.java:114) finished in 0.094 s
INFO  Job 0 finished: first at AgePredicterLocal.java:114, took 0.154587 s
Content-Length: 17
Content-Type: text/plain
Estimated-Author-Age: 32.29913797083779
X-Parsed-By: org.apache.tika.parser.CompositeParser
X-Parsed-By: org.apache.tika.parser.recognition.AgeRecogniser
resourceName: test.txt
INFO  Invoking stop() from shutdown hook
INFO  Stopped ServerConnector@71b3bc45{HTTP/1.1}{0.0.0.0:4040}
INFO  Stopped o.s.j.s.ServletContextHandler@6f2cb653{/stages/stage/kill,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@7a791b66{/api,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@40f1be1b{/,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@4cc6fa2a{/static,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@30d4b288{/executors/threadDump/json,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@51cd7ffc{/executors/threadDump,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@40499e4f{/executors/json,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@2fea7088{/executors,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@5c87bfe2{/environment/json,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@f73dcd6{/environment,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@6d304f9d{/storage/rdd/json,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@dcfda20{/storage/rdd,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@a4b2d8f{/storage/json,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@1fdfafd2{/storage,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@5b07730f{/stages/pool/json,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@58ebfd03{/stages/pool,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@1080b026{/stages/stage/json,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@5b43fbf6{/stages/stage,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@4678a2eb{/stages/json,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@41813449{/stages,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@354fc8f0{/jobs/job/json,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@47db5fa5{/jobs/job,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@4879f0f2{/jobs/json,null,UNAVAILABLE}
INFO  Stopped o.s.j.s.ServletContextHandler@12bcd0c0{/jobs,null,UNAVAILABLE}
INFO  Stopped Spark web UI at http://192.168.1.65:4040
INFO  MapOutputTrackerMasterEndpoint stopped!
INFO  MemoryStore cleared
INFO  BlockManager stopped
INFO  BlockManagerMaster stopped
INFO  OutputCommitCoordinator stopped!
INFO  Successfully stopped SparkContext
INFO  Shutdown hook called
INFO  Deleting directory /private/var/folders/n5/1d_k3z4s2293q8ntx_n8sw54mm5n_8/T/spark-fd116873-eec8-437d-9ad0-7b7a09889d92
$",../data/confluence_exports/TIKA/AgeDetectionParser_109454038.html
TIKA : TikaServer Installation of Tika Server,The current installation process for Tika server post 1.23 and prior to 1.24 is a bit in flux.  Read on below for some options:,../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Building from source,"If you need to customize Tika server in some way, and/or need the very latest version to try out a fix, then to build from source: Checkout the source from SVN as detailed on theApache Tika contributions pageor retrieve the latest code fromGithub,Build source using MavenRun the Apache Tika JAXRS server runnable jar. git clone https://github.com/apache/tika.git tika-trunk
cd ./tika-trunk/
mvn install
cd ./tika-server/target/
java -jar tika-server-x.x.jar Remember to replace x.x with the version you have built.",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Running the Tika Server as a Jar file,"The Tika Server binary is a standalone runnable jar.  Download the latest stable release binary from theApache Tika downloads page, via your favorite local mirror. You want thetika-server-1.x.jarfile, e.g.tika-server-1.24.jar You can start it by calling java with the-jaroption, eg something likejava -jar tika-server-1.24.jar You will then see a message such as the following: $ java -jar tika-server-1.24-SNAPSHOT.jar
19-Jan-2015 14:23:36 org.apache.tika.server.TikaServerCli main
INFO: Starting Apache Tika 1.8-SNAPSHOT server
19-Jan-2015 14:23:36 org.apache.cxf.endpoint.ServerImpl initDestination
INFO: Setting the server's publish address to be http://localhost:9998/
19-Jan-2015 14:23:36 org.slf4j.impl.JCLLoggerAdapter info
INFO: jetty-8.y.z-SNAPSHOT
19-Jan-2015 14:23:36 org.slf4j.impl.JCLLoggerAdapter info
INFO: Started SelectChannelConnector@localhost:9998
19-Jan-2015 14:23:36 org.apache.tika.server.TikaServerCli main
INFO: Started Which lets you know that it started correctly. You can specify additional information to change the host name and port number: java -jar tika-server-x.x.jar --host=intranet.local --port=12345 Once the server is running, you can visit the server's URL in your browser (eghttp://localhost:9998/), and the basic welcome page will confirm that the Server is running, and give links to the various endpoints available. Below is some basic documentation on how to interact with the services using cURL and HTTP.",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Using prebuilt Docker image,Official image for Tika can be found atDockerHub.  You can download and start it with: docker run -d -p 9998:9998 apache/tika:<version> Full set of documentation can be found atGithub.,../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Running Tika Server as Unix Service,"Shipping in Tika 1.24 is a new Service Installation Script that lets you install Tika server as a service on Linux.  This script was heavily influenced by theApache Solrproject's script, so read up on that documentation if you want to customize the script. Currently the script only supports CentOS, Debian, Red Hat, Suse and Ubuntu Linxu distributions. Before running the script, you need to determine a few parameters about your setup. Specifically, you need to decide where to install Tika server and which system user should be the owner of the Tika files and process To run the scripts, you'll need the 1.24 (or later)Tika distribution.  It will have a-binsuffix e.g.tika-server-1.24-bin.tgz.  Extract the installation script from the distribution via:  tar xzf tika-server-1.24-bin.tgz tika-server-1.24-bin/bin/install_tika_service.sh --strip-components=2 This will extract the install_tika_service.sh script from the archive into the current directory.  If installing on Red Hat, please make surelsofis installed before running the Tika installation script (sudo yum install lsof). The installation script must be run as root: sudo bash ./install_tika_service.sh tika-server-1.24-bin.tgz By default, the script extracts the distribution archive into/opt/tika, configures Tika to write files into/var/tika, and runs Tika as thetikauser on the default port. Consequently, the following command produces the same result as the previous command: sudo bash ./install_tika_service.sh tika-server-1.24-bin -i /opt -d /var/tika -u tika -s tika -p 9998 You can customize the service name, installation directories, port, and owner using options passed to the installation script. To see available options, simply do:  sudo bash ./install_tika_service.sh -help  Once the script completes, Tika server will be installed as a service and running in the background on your server (on port 9998). To verify, you can do:  sudo service tika status  Your specific customization to Tika server setup are stored in the/etc/init.d/tikafile.  SeeTikaServer in Tika 2.xfor the details of configuring tika-server in 2.x.  All services that take files use HTTP ""PUT"" requests. When ""PUT"" is used, the original file must be sent in request body without any additional encoding (do not use multipart/form-data or other containers). Additionally,TikaResource, Metadata andRecursiveMetadataServices accept POST multipart/form-data requests, where the original file is sent as a single attachment. Information services (e.g. defined mimetypes, defined parsers etc) work with HTML ""GET"" requests. You may optionally specify content type in ""Content-Type"" header. If you do not specify mime type, Tika will use its detectors to guess it. You may specify additional identifier in URL after resource name, like ""/tika/my-file-i-sent-to-tika-resource"" for ""/tika"" resource. Tikaserver uses this name only for logging, so you may put there file name, UUID or any other identifier (do not forget to url-encode any special characters). Resources may return following HTTP codes: 200 Ok - request completed sucessfully204 No content - request completed sucessfully, result is empty422 Unprocessable Entity - Unsupported mime-type, encrypted document & etc500 Error - Error while processing document NOTE:Please seeTikaServerEndpointsComparedfor a quick comparison of the features of some of these endpoints.",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Metadata Resource,"/meta HTTP PUTs a document to the /meta service and you get back ""text/csv"" of the metadata. Some Example calls with cURL: $ curl -X PUT --data-ascii @zipcode.csv http://localhost:9998/meta --header ""Content-Type: text/csv""
$ curl -T price.xls http://localhost:9998/meta Returns: ""Content-Encoding"",""ISO-8859-2""
""Content-Type"",""text/plain"" Get metadata as JSON: $ curl -T test_recursive_embedded.docx http://localhost:9998/meta --header ""Accept: application/json"" Or XMP: $ curl -T test_recursive_embedded.docx http://localhost:9998/meta --header ""Accept: application/rdf+xml"" Get specific metadata key's value as simple text string: $ curl -T test_recursive_embedded.docx http://localhost:9998/meta/Content-Type --header ""Accept: text/plain"" Returns: application/vnd.openxmlformats-officedocument.wordprocessingml.document Get specific metadata key's value(s) as CSV: $ curl -T test_recursive_embedded.docx http://localhost:9998/meta/Content-Type --header ""Accept: text/csv"" Or JSON: $ curl -T test_recursive_embedded.docx http://localhost:9998/meta/Content-Type --header ""Accept: application/json"" Or XMP: $ curl -T test_recursive_embedded.docx http://localhost:9998/meta/Content-Type --header ""Accept: application/rdf+xml"" Note: when requesting specific metadata keys value(s) in XMP, make sure to request the XMP name, e.g. ""dc:creator"" vs. ""Author""",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Multipart Support,"Metadata Resource also accepts the files as multipart/form-data attachments with POST. Posting files as multipart attachments may be beneficial in cases when the files are too big for them to be PUT directly in the request body. Note that Tika JAX-RS server makes the best effort at storing some of the multipart content to the disk while still supporting the streaming: curl -F upload=@price.xls URL http://localhost:9998/meta/form Note that the address has an extra ""/form"" path segment.",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Tika Resource,"/tika HTTP PUTs a document to the /tika service and you get back the extracted text in text, html or ""body"" format (see below). See also the/rmetaendpoint for text and metadata of embedded objects. HTTP GET prints a greeting stating the server is up. Some Example calls with cURL:",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Get HELLO message back,"$ curl -X GET http://localhost:9998/tika
This is Tika Server. Please PUT",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Get the Text of a Document,"$ curl -X PUT --data-binary @GeoSPARQL.pdf http://localhost:9998/tika --header ""Content-type: application/pdf""
$ curl -T price.xls http://localhost:9998/tika --header ""Accept: text/html""
$ curl -T price.xls http://localhost:9998/tika --header ""Accept: text/plain"" Use the Boilerpipe handler (equivalent to tika-app's--text-main) with text output: $ curl -T price.xls http://localhost:9998/tika/main --header ""Accept: text/plain""  With Tika 1.27 and greater, you can get the text and metadata of a file in json format with: $ curl -T price.xls http://localhost:9998/tika --header ""Accept: application/json"" To specify whether you want the content to be text (vs. html) specify the handler type after/tika: $ curl -T price.xls http://localhost:9998/tika/text --header ""Accept: application/json""",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Multipart Support,"Tika Resource also accepts the files as multipart/form-data attachments with POST. Posting files as multipart attachments may be beneficial in cases when the files are too big for them to be PUT directly in the request body. Note that Tika JAX-RS server makes the best effort at storing some of the multipart content to the disk while still supporting the streaming: curl -F upload=@price.xls URL http://localhost:9998/tika/form Note that the address has an extra ""/form"" path segment.",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Detector Resource,/detect/stream HTTP PUTs a document and uses the Default Detector from Tika to identify its MIME/media type. The caveat here is that providing a hint for the filename can increase the quality of detection. Default return is a string of the Media type name. Some Example calls with cURL:,../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer PUT an RTF file and get back RTF,$ curl -X PUT --data-binary @TODO.rtf http://localhost:9998/detect/stream,../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer PUT a CSV file without filename hint and get back text/plain,$ curl -X PUT --upload-file foo.csv http://localhost:9998/detect/stream,../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer PUT a CSV file with filename hint and get back text/csv,"$ curl -X PUT -H ""Content-Disposition: attachment; filename=foo.csv"" --upload-file foo.csv http://localhost:9998/detect/stream",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Language Resource,/language/stream HTTP PUTs or POSTs a UTF-8 text file to theLanguageIdentifierto identify its language. NOTE: This endpoint does not parse files.  It runs detection on a UTF-8 string. Default return is a string of the 2 character identified language. Some Example calls with cURL:,../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer PUT a TXT file with English This is English! and get back en,"$ curl -X PUT --data-binary @foo.txt http://localhost:9998/language/stream
en",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer PUT a TXT file with French comme çi comme ça and get back fr,"curl -X PUT --data-binary @foo.txt http://localhost:9998/language/stream
fr /language/string HTTP PUTs or POSTs a text string to theLanguageIdentifierto identify its language. Default return is a string of the 2 character identified language. Some Example calls with cURL:",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer PUT a string with English This is English! and get back en,"$ curl -X PUT --data ""This is English!"" http://localhost:9998/language/string
en",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer PUT a string with French comme çi comme ça and get back fr,"curl -X PUT --data ""comme çi comme ça"" http://localhost:9998/language/string
fr",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Translate Resource,"/translate/all/translator/src/dest HTTP PUTs or POSTs a document to the identified *translator* and translates from *src* language to *dest* Default return is the translated string if successful, else the original string back. Note that: * *translator* should be a fully qualified Tika class name (with package) e.g., org.apache.tika.language.translate.Lingo24Translator * *src* should be the 2 character short code for the source language, e.g., 'en' for English * *dest* should be the 2 character short code for the dest language, e.g., 'es' for Spanish. Some Example calls with cURL:",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer PUT a TXT file named sentences with Spanish me falta práctica en Español and get back the English translation using Lingo24,"$ curl -X PUT --data-binary @sentences http://localhost:9998/translate/all/org.apache.tika.language.translate.Lingo24Translator/es/en
lack of practice in Spanish",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer PUT a TXT file named sentences with Spanish me falta práctica en Español and get back the English translation using Microsoft,"$ curl -X PUT --data-binary @sentences http://localhost:9998/translate/all/org.apache.tika.language.translate.MicrosoftTranslator/es/en
I need practice in Spanish",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer PUT a TXT file named sentences with Spanish me falta práctica en Español and get back the English translation using Google,"$ curl -X PUT --data-binary @sentences http://localhost:9998/translate/all/org.apache.tika.language.translate.GoogleTranslator/es/en
I need practice in Spanish /translate/all/src/dest HTTP PUTs or POSTs a document to the identified *translator* and auto-detects the *src* language usingLanguageIdentifiers, and then translates *src* to *dest* Default return is the translated string if successful, else the original string back. Note that: * *translator* should be a fully qualified Tika class name (with package) e.g., org.apache.tika.language.translate.Lingo24Translator * *dest* should be the 2 character short code for the dest language, e.g., 'es' for Spanish.",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer PUT a TXT file named sentences2 with French comme çi comme ça and get back the English translation using Google auto-detecting the language,"$ curl -X PUT --data-binary @sentences2 http://localhost:9998/translate/all/org.apache.tika.language.translate.GoogleTranslator/en
so so",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Recursive Metadata and Content,"/rmeta Returns a JSONified list of Metadata objects for the container document and all embedded documents. The text that is extracted from each document is stored in the metadata object under ""X-TIKA:content"". $ curl -T test_recursive_embedded.docx http://localhost:9998/rmeta Returns: [
 {""Application-Name"":""Microsoft Office Word"",
  ""Application-Version"":""15.0000"",
  ""X-Parsed-By"":[""org.apache.tika.parser.DefaultParser"",""org.apache.tika.parser.microsoft.ooxml.OOXMLParser""],
  ""X-TIKA:content"":""embed_0 ""
  ...
 },
 {""Content-Encoding"":""ISO-8859-1"",
  ""Content-Length"":""8"",
  ""Content-Type"":""text/plain; charset=ISO-8859-1""
  ""X-TIKA:content"":""embed_1b"",
  ...
 }
 ...
] The default format for ""X-TIKA:content"" is XML. However, you can select ""text only"" with /rmeta/text HTML with /rmeta/html and no content (metadata only) with /rmeta/ignore",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Multipart Support,"Metadata Resource also accepts the files as multipart/form-data attachments with POST. Posting files as multipart attachments may be beneficial in cases when the files are too big for them to be PUT directly in the request body. Note that Tika JAX-RS server makes the best effort at storing some of the multipart content to the disk while still supporting the streaming: curl -F upload=@test_recursive_embedded.docx URL http://localhost:9998/rmeta/form Note that the address has an extra ""/form"" path segment.",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Specifying Limits,"As of Tika 1.25, you can limit the maximum number of embedded resources and the write limit per handler. To specify the maximum number of embedded documents, set themaxEmbeddedResourcesin the header.  Note that the container document does not count towards this number.  The following will return the results for the container document only. curl -T test_recursive_embedded.docx --header ""maxEmbeddedResources: 0"" http://localhost:9998/rmeta  To specify a write limit per handler, set thewriteLimitparameter in a header.   This limit applies to each handler (each embedded document).  If this is triggered, the parse will continue on to the next embedded object and storeX-TIKA:Exception:write_limit_reached=""true""in the metadata object for the embedded file that triggered the write limit. curl -T test_recursive_embedded.docx --header ""writeLimit: 1000"" http://localhost:9998/rmeta NOTE:In Tika 2.0, the writeLimit applies to the full document including the embedded files, not to each handler.",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Filtering Metadata Keys,"The/rmetaendpoint can return far more metadata fields than a user might want to process.   As of Tika 1.25, users can configure a MetadataFilter that eitherincludesorexcludesfields by name. Note:theMetadataFiltersonly work with the/rmetaendpoint.  Further, they do not shortcut metadata extraction within Parsers.  They only delete the unwanted fields after the parse.  This still can save resources in storage and network bandwidth. A user can map Tika field names to names they prefer. IfexcludeUnmappedis set to true, only those fields that are included in the mapping are passed back to the client. FieldNameMappingFilter<properties>
  <metadataFilters>
    <metadataFilter class=""org.apache.tika.metadata.filter.FieldNameMappingFilter"">
      <params>
        <excludeUnmapped>true</excludeUnmapped>
        <mappings>
          <mapping from=""X-TIKA:content"" to=""content""/>
          <mapping from=""a"" to=""b""/>
        </mappings>
      </params>
    </metadataFilter>
  </metadataFilters>
</properties>   A user can set the following in atika-config.xmlfile to have the/rmetaend point only return three fields: IncludeFieldMetadataFilter<properties>
  <metadataFilters>
    <metadataFilter class=""org.apache.tika.metadata.filter.IncludeFieldMetadataFilter"">
      <params>
        <include>
          <field>X-TIKA:content</include>
          <field>extended-properties:Application</include>
          <field>Content-Type</include>
        </param>
      </params>
    </metadataFilter>
  </metadataFilters>
</properties>  To exclude those three fields but include all other fields:  ExcludeFieldMetadataFilter<properties>
  <metadataFilters>
    <metadataFilter class=""org.apache.tika.metadata.filter.ExcludeFieldMetadataFilter"">
      <params>
        <exclude>
          <field>X-TIKA:content</field>
          <field>extended-properties:Application</field>
          <field>Content-Type</field>
        </param>
      </params>
    </metadataFilter>
  </metadataFilters>
</properties>",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Filtering Metadata Objects,"A user may want to parse a file type to get at the embedded contents within it, but s/he may not want a metadata object or contents for the file type itself.  For example,image/emffiles often contain duplicative text, but they may contain an embedded PDF file.  If the client had turned off theEMFParser, the embedded PDF file would not be parsed.  When the/rmetaendpoint is configured with the following, it will delete the entire metadata object for files of typeimage/emf. ClearByMimeMetadataFilter<properties>
  <metadataFilters>
    <metadataFilter class=""org.apache.tika.metadata.filter.ClearByMimeMetadataFilter"">
      <params>
        <mimes>
          <mime>image/emf</mime>
        </mimes>
      </params>
    </metadataFilter>
  </metadataFilters>
</properties>",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Integration with tika-eval,"As of Tika 1.25, if a user adds thetika-evaljar to the server jar's classpath, the/rmetaendpoint will add key ""profiling"" statistics from thetika-evalmodule, including: language identified, number of tokens, number of alphabetic tokens and the ""out of vocabulary"" percentage.  These statistics can be used to decide to reprocess a file with OCR or to reprocess an HTML file with a different encoding detector. To accomplish this, one may put both the tika-eval jar and the server jar in abin/directory and then run: java -cp bin/* org.apache.tika.server.TikaServerCli See theTikaEvalpage for more details.  Please open issues on our JIRA if you would like other statistics included or if you'd like to make the calculated statistics configurable.",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Unpack Resource,"/unpack HTTP PUTs an embedded document type to the /unpack service and you get back a zip or tar of the raw bytes of the embedded files.  Note that this does not operate recursively; it extracts only the child documents of the original file. You can also use /unpack/all to get back the text and metadata from the container file.  If you want the text and metadata from all embedded files, consider using the /rmeta end point. Default return type is ZIP (without internal compression). Use ""Accept"" header for TAR return type. Some example calls with cURL:",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer PUT zip file and get back met file zip,"$ curl -X PUT --data-binary @foo.zip http://localhost:9998/unpack --header ""Content-type: application/zip""",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer PUT doc file and get back met file tar,"$ curl -T Doc1_ole.doc -H ""Accept: application/x-tar"" http://localhost:9998/unpack > /var/tmp/x.tar",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer PUT doc file and get back the content and metadata,"$ curl -T Doc1_ole.doc http://localhost:9998/unpack/all > /var/tmp/x.zip Text is stored inTEXTfile, metadata cvs inMETADATA. Use ""accept"" header if you want TAR output.",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Available Endpoints,"/ Hitting the route of the server in your web browser will give a basic report of all the endpoints defined in the server, what URL they have etc",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Defined Mime Types,"/mime-types Mime types, their aliases, their supertype, and the parser. Available as plain text, json or human readable HTML",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Available Detectors,"/detectors The top level Detector to be used, and any child detectors within it. Available as plain text, json or human readable HTML",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Available Parsers,"/parsers Lists all of the parsers currently available /parsers/details List all the available parsers, along with what mimetypes they support",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Specifying a URL Instead of Putting Bytes in Tika 2.x,"In Tika 2.x, use a FileSystemFetcher, a UrlFetcher or or an HttpFetcher. See:tika-pipes (FetchersInClassicServerEndpoints) We have entirely removed the-enableFileUrlcapability that we had in 1.x.",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer Specifying a URL Instead of Putting Bytes in Tika 1.x,"In Tika 1.10, we removed this capability because it posed a security vulnerability (CVE-2015-3271). Anyone with access to the service had the server's access rights; someone could request local files viafile:///or pages from an intranet that they might not otherwise have access to. In Tika 1.14, we added the capability back, but the user has to acknowledge the security risk by including two commandline arguments: $ java -jar tika-server-x.x.jar -enableUnsecureFeatures -enableFileUrl This allows the user to specify afileUrlin the header: curl -i -H ""fileUrl:http://tika.apache.org"" -H ""Accept:text/plain"" -X PUT http://localhost:9998/tika or curl -i -H ""fileUrl:file:///C:/data/my_test_doc.pdf"" -H ""Accept:text/plain"" -X PUT http://localhost:9998/tika By adding back this capability, we did not remove the security vulnerability. Rather, if a user is confident that only authorized clients are able to submit a request, the user can choose to operate tika-server with this insecure setting.BE CAREFUL! Also, please be polite. This feature was added as a convenience. Please consider using a robust crawler (instead of our simpleTikaInputStream.get(new URL(fileUrl))) that will allow for better configuration of redirects, timeouts, cookies, etc.; and a robust crawler will respect robots.txt!  As of Tika 1.24.1, users can turn ongzipcompression for either files on their way totika-serveror the output fromtika-server. If you want togzipyour files before sending totika-server, add curl -T test_my_doc.pdf -H ""Content-Encoding: gzip"" http://localhost:9998/rmeta  If you wanttika-serverto compress the output of the parse: curl -T test_my_doc.pdf -H ""Accept-Encoding: gzip"" http://localhost:9998/rmeta   See below.  In Tika 2.x, the-spawnChildfunctionality is turned on by default.  This has the effect that tika-server may occasionally be unavailable when it is restarting.  Clients should have logic to wait for a restart if tika-server has had to restart. To turn off this behavior and to go back to the more dangerous legacy behavior, start tika-server with the--noForkoption.  As of Tika 1.19, users can make tika-server more robust by running it with the-spawnChildoption. This starts tika-server in a child process, and if there's an OOM, a timeout or other catastrophic problem with the child process, the parent process will kill and/or restart the child process. The following options are available only with the-spawnChildoption. -maxFiles: restart the child process after it has processedmaxFiles. If there is a slow building memory leak, this restart of the JVM should help. The default is 100,000 files. To turn off this feature:-maxFiles -1. The child and/or parent will log the cause of the restart asHIT_MAXwhen there is a restart because of this threshold.-taskTimeoutMillisand-taskPulseMillis:taskPulseMillisspecifies how often to check to determine if a parse/detect task has timed outtaskTimeoutMillis-pingTimeoutMillisand-pingPulseMillis:pingPulseMillisspecifies how often for the parent process to ping the child process to check status.pingTimeoutMillishow long the parent process should wait to hear back from the child process before restarting it and/or how long the child process should wait to receive a ping from the parent process before shutting itself down. If the child process is in the process of shutting down, and it gets a new request it will return503 -- Service Unavailable. If the server times out on a file, the client will receive an IOException from the closed socket. Note that all other files that are being processed will end with an IOException from a closed socket when the child process shuts down; e.g. if you send three files to tika-server concurrently, and one of them causes a catastrophic problem requiring the child to shut down, you won't be able to tell which file caused the problems. In the future, we may implement a gentler shutdown than we currently have. NOTE 1:-spawnChildhas become the default in Tika 2.x.  If you need to return to the legacy 1.x behavior, configure tika-server element in the tika-config.xml with<noFork>true</noFork>or add--noForkas the commandline argument. NOTE 2: In Tika 1.x, to specify the JVM args for the child process, prepend the arguments with-Jas in-JXmx4gafter the-jar tika-server.x.x.jarcall as in: $ java -Dlog4j.configuration=file:log4j_server.xml -jar tika-server-x.x.jar -spawnChild -JXmx4g -JDlog4j.configuration=file:log4j_child.xml}} NOTE 3:Before Tika 1.27, we strongly encourage-JXX:+ExitOnOutOfMemoryError, which admittedly has limitations:https://bugs.openjdk.java.net/browse/JDK-8155004.When a JVM is struggling with memory, it is possible that the final trigger for the OOM happens in reading bytes from the client or writing bytes to the client NOT during the parse.  In short, OOMs can happen outside of Tika's code, and our internal watcher can't see/respond to some OOMs.  In 1.27 and later (and in 2.x), we added a shutdown hook in TesseractOCRParser to decrease the chances of orphaning tesseract.  The use of-JXX:+ExitOnOutOfMemoryErrorprevents the shutdown hooks from working, and tesseract processes may more easily be orphaned on an out of memory error. NOTE 4:When using the-spawnChildoption, clients will need to be aware that the server could be unavailable temporarily while it is restarting.  Clients will need to have a retry logic.  In Tika 1.x, you can customize logging via the usuallog4jcommandline argument, e.g.-Dlog4j.configuration=file:log4j_server.xml. If using-spawnChild, specify the configuration for the child process with the-Jprepended as injava -jar tika-server-X.Y-jar -spawnChild -JDlog4j.configuration=file:log4j_server.xml. Some important notes for logging in the child process in versions <= 1.19.1: 1) make sure that thedebugoption is off, and 2) do not log to stdout (this is used for interprocess communication between the parent and child!). The default level of logging isdebug, but you can also set logging toinfovia the commandline:-log info. In Tika 2.x, you can set log4j2.xml configuration for the forked process in the <jvmArgs/> element.  See below.",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : TikaServer ServerStatus,"Tika Server uses ServerStatus object to maintain and track current status of server. The data can be exported to REST resource and JMX MBean (from 1.26). To enable REST endpoint and JMX MBean: java -jar tika-server-x.x.jar -status REST resource to access server status: /status MBean: Object name: org.apache.tika.server.mbean:type=basic,name=ServerStatusExporter NOTE:In Tika 2.x, this endpoint is enabled by eitherenablingUnsecureFeaturesor by specifying it as an endpoint. <properties>
  <server>
    <params>
      <enableUnsecureFeatures>true</enableUnsecureFeatures>
    </params>
  </server>
</properties>  <properties>
  <server>
    <params>
      <port>9999</port>
      <forkedJvmArgs>
        <arg>-Xmx2g</arg>
        <arg>-Dlog4j.configurationFile=my-forked-log4j2.xml</arg>
      </forkedJvmArgs>
      <endpoints>
        <endpoint>status</endpoint>
        <endpoint>rmeta</endpoint>
      </endpoints>
    </params>
  </server>
</properties>   Tika Server now has the ability to be spawned with SSL enabled by providing a keystore/Truststore as part of the configuration, this is likely to change but is available as part of Tika 2.4.0. Example: <properties>
  <server>
    <params>
      <port>9999</port>
      <forkedJvmArgs>
        <arg>-Xmx2g</arg>
      </forkedJvmArgs>
      <endpoints>
        <endpoint>rmeta</endpoint>
      </endpoints>
    </params>
    <tlsConfig>
      <params>
        <active>true</active>
        <keyStoreType>myType</keyStoreType>
        <keyStorePassword>pass</keyStorePassword>
        <keyStoreFile>/something/or/other</keyStoreFile>
        <trustStoreType>myType2</trustStoreType>
        <trustStorePassword>pass2</trustStorePassword>
        <trustStoreFile>/something/or/other2</trustStoreFile>
      </params>
    </tlsConfig>
  </server>
</properties>  If you are new to TLS, seeour README.txtfor how we generated client and server keystores and truststores for our unit tests.  SeeConfiguring Parsers At Parse Time in tika-server.  Tika Server is based on JSR 311 for a network serve. The server package uses theApache CXFframework that provides an implementation of JAX-RS for Java.",../data/confluence_exports/TIKA/TikaServer_148639291.html
TIKA : VirtualMachine Install software (this has been updated for Ubuntu),"sudo apt updategpgsudo apt install gnupgjavawget -qO - https://adoptopenjdk.jfrog.io/adoptopenjdk/api/gpg/key/public | sudo apt-key add -sudo apt-get install -y software-properties-commonsudo add-apt-repository --yeshttps://adoptopenjdk.jfrog.io/adoptopenjdk/deb/sudo apt-get install adoptopenjdk-8-hotspotsudo apt-get install adoptopenjdk-11-hotspotsudo apt-get install adoptopenjdk-14-hotspotsudo apt-get install fontconfig (https://github.com/AdoptOpenJDK/openjdk-build/issues/693via Dominik Stadler)sudo apt install ttf-dejavu (same as above)sudo apt-get install groovysudo apt-get install mavensudo apt-get install subversionsudo apt-get install gitsudo apt-get install fileinstalled docker following:https://docs.docker.com/engine/install/ubuntu/  On 12 November 2020, I ran tika-eval's new FileProfile on the corpus.  This includes file type detection by Tika and by 'file', digests and file sizes. We configured the reverse proxy for /datasette: ProxyPreserveHost OnProxyPass /datasettehttp://0.0.0.0:8001ProxyPassReverse /datasettehttp://0.0.0.0:8001 The .db is in /data1/publish.  cd to that directory and then: docker run --name datasette -d -p 8001:8001 -v `pwd`:/mnt datasetteproject/datasette datasette -p 8001 -h 0.0.0.0 /mnt/file_profiles.db --config sql_time_limit_ms:120000 --config max_returned_rows:100000 --configbase_url:/datasette/",../data/confluence_exports/TIKA/VirtualMachine_109454104.html
TIKA : VirtualMachine HTTPD,/etc/apache2 public directories are symlinks in /usr/share/corpora,../data/confluence_exports/TIKA/VirtualMachine_109454104.html
TIKA : VirtualMachine config/admin stuff,"nano /etc/ssh/sshd_config disallow root to log inadd allowed users toAllowUsers Install, configure and run fail2ban. Thanks totecmint. yum install fail2ban vi /etc/fail2ban/jail.conf systemctl start fail2ban",../data/confluence_exports/TIKA/VirtualMachine_109454104.html
TIKA : VirtualMachine opening port 9998 for TIKA-1301,To open port 9998:firewall-cmd --zone=public --add-port=9998/tcp --permanent firewall-cmd --reload,../data/confluence_exports/TIKA/VirtualMachine_109454104.html
TIKA : VirtualMachine permission management,adduser <user> sudo 2.passwd <user> 3.groupadd <admingroup> 4.usermod -g <admingroup> <user> 5.modify /etc/ssh/sshd_config to add user toAllowUsers 6.systemctl restart sshd.service--restart sshd on RHEL 7,../data/confluence_exports/TIKA/VirtualMachine_109454104.html
TIKA : VirtualMachine mkdirs,/public/corpora/govdocs1 ...,../data/confluence_exports/TIKA/VirtualMachine_109454104.html
TIKA : VirtualMachine prep govdocs1,cp zipfilelist /public/corpora/govdocs1/archive 2.wget -i zipfilelist.txt 3. (go get some coffee) 4.cd govdocs1/scripts 5.groovy unzip.groovy 0 6. (go get some more coffee) 7.groovy rmBugged.groovy,../data/confluence_exports/TIKA/VirtualMachine_109454104.html
TIKA : VirtualMachine prep nsfpolardata,"scp -r <user>@nsfpolardata.dyndns.org:/usr/local/ndeploy/data/AcadisCrawl . 2. (go get some coffee) 3.scp -r <user>@nsfpolardata.dyndns.org:/usr/local/ndeploy/data/AcadisCrawl2 . 4. (go get some coffee) 5.scp -r <user>@nsfpolardata.dyndns.org:/home/mattmann/polar-data/nutch_trunk/runtime/local/bin/crawlId . 6. (go get some coffee) 7.cd /data1/public/archives/nsf-polar-data/ 8.export NUTCH_OPTS=""-Xmx8192m -XX:MaxPermSize=8192m"" 9../bin/nutch dump -outputDir out -segment /data1/public/archives/nsf-polar-data/acadis/AcadisCrawl/segments/ 10../bin/nutch dump -outputDir out2 -segment /data1/public/archives/nsf-polar-data/acadis/AcadisCrawl2/segments/ 11../bin/nutch dump -outputDir out3 -segment /data1/public/archives/nsf-polar-data/nasa-amd/crawlId/segments/",../data/confluence_exports/TIKA/VirtualMachine_109454104.html
TIKA : VirtualMachine add more disc,"From Rackspace website, add block storage volume and attach it to server. mkfs.ext3 /dev/xvdb 2.mkdir /data1 3.mount /dev/xvdb /data1 4.nano /etc/fstabadd these lines (e.g.):/dev/xvdb               /data1                  ext3    defaults        1 2 /dev/xvdc               /data2                  ext3    defaults        1 2 4b. When you wreck the fstab file and can't log into your system after a hard reboot and you are in recovery mode: blkidto figure out the drive types mount -t ext3 /dev/xvdb1 /mntto mount the system 4c. Before you hit 4b, trymount -favto see if there are any errors in your fstab file.",../data/confluence_exports/TIKA/VirtualMachine_109454104.html
TIKA : VirtualMachine httpd,config file in the usual place : /etc/httpd/conf/httpd.conf Set robots.txt to disallow all: /var/www/html 2. Link data dir(s) under: /var/www/html 3. Configure config file to allow links and to show directories 4. Show long file names...add to config file:IndexOptionsFancyIndexingSuppressDescriptionNameWidth=* 5. start: apachectl start,../data/confluence_exports/TIKA/VirtualMachine_109454104.html
TIKA : VirtualMachine pdftotext,"Downloads from:https://www.xpdfreader.com/download.html Current version: 4.00; Released: 2017 Aug 10 Downloaded 64-bit LinuxXpdfReader; executed:XpdfReader-linux64-4.00.01.run; unpacked and cp xpdf to /usr/local/bin 2. Downloaded 64-bit LinuxXpdf tools; unpacked and cp bin64/* to /usr/local/bin 3. Downloaded language support packages: Arabic, Chinese/simplified, Chinese/traditional, Cyrillic, Greek, Hebrew, Japanese, Korean, Latin2, Thai and Turkish; unzipped them all, cat all add-to-xpdfrc >> tmp_xpdfrc and cp all to /usr/local/share/xpdf 4. cat xpdf-tools-linux-4.00/doc/sample-xpdfrc tmp_xpdfrc >> /usr/local/etc/xpdfrc NOTE:We found thatpdftotextwas not correctly reading thexpdfrcfile in this location. We found no differences in extracted text when we removed thexpdfrcfile and when we had it there. We did find a difference, especially in CJK PDFs, when we specified thexpdfrcfile from the commandline with the-cfgoption.",../data/confluence_exports/TIKA/VirtualMachine_109454104.html
TIKA : VirtualMachine ffmpeg,sudo yum install epel-release 2.sudo yum localinstall --nogpgcheckhttps://download1.rpmfusion.org/free/el/rpmfusion-free-release-7.noarch.rpmhttps://download1.rpmfusion.org/nonfree/el/rpmfusion-nonfree-release-7.noarch.rpm 3.sudo yum install ffmpeg ffmpeg-devel,../data/confluence_exports/TIKA/VirtualMachine_109454104.html
TIKA : VirtualMachine Other data,SeeApacheTikaHtmlEncodingStudyfor a description of gathering data for TIKA-2038. SeeCommonCrawl3for a description of refreshing data for TIKA-2750. SeeComparisonTikaAndPDFToText201811for a comparison of text extracted from PDFs by Apache Tika/Apache PDFBox and pdftotext.,../data/confluence_exports/TIKA/VirtualMachine_109454104.html
TIKA : VirtualMachine Security,Review bad logins or bad httpd requests: sudo tail /var/log/securesudo vi /var/log/httpd/access_log*Block ipssudo vi /etc/hosts.denyALL: xxx.xx.xx.xxxsudo systemctl restart sshd,../data/confluence_exports/TIKA/VirtualMachine_109454104.html
TIKA : TikaEvalDatasetteExamples Full data:,"select FILE_PATH, FILE_NAME, fp.FILE_EXTENSION,LENGTH, SHA256, tm.mime_string, tf.mime_stringfrom file_profiles fpjoin file_mimes tm on tm.mime_id=fp.tika_mime_idjoin file_mimes tf on tf.mime_id=fp.file_mime_idorder by length desclimit 100",../data/confluence_exports/TIKA/TikaEvalDatasetteExamples_165225246.html
"TIKA : TikaEvalDatasetteExamples Total Files (4,263,744)",select count(1) from file_profiles,../data/confluence_exports/TIKA/TikaEvalDatasetteExamples_165225246.html
"TIKA : TikaEvalDatasetteExamples Total Distinct Files (3,726,422)",select count(1) from (select distinct(sha256) from file_profiles),../data/confluence_exports/TIKA/TikaEvalDatasetteExamples_165225246.html
TIKA : TikaEvalDatasetteExamples Top 100 mime types as identified by Tika:,"select tm.mime_string, count(1) as cntfrom file_profiles fpjoin file_mimes tm on tm.mime_id=fp.tika_mime_idgroup by tm.mime_stringorder by cnt desclimit 100",../data/confluence_exports/TIKA/TikaEvalDatasetteExamples_165225246.html
TIKA : TikaEvalDatasetteExamples Counts of 100 most common SHA256s,"select SHA256, count(1) as cntfrom file_profiles fpgroup by SHA256order by cnt desclimit 100",../data/confluence_exports/TIKA/TikaEvalDatasetteExamples_165225246.html
TIKA : TikaEvalDatasetteExamples List files associated with the 100 most common SHA256s,"select shas.sha256, cnt, fp.file_path from(selectfp.SHA256 as sha256, count(1) as cntfrom file_profiles fpgroup by fp.sha256order by count(1) desclimit 100) as shasjoin file_profiles fp on shas.sha256=fp.sha256order by shas.cnt desc, shas.sha256, file_path",../data/confluence_exports/TIKA/TikaEvalDatasetteExamples_165225246.html
TIKA : TikaEvalDatasetteExamples Get the file paths for 100 PDFs,select FILE_PATHfrom file_profiles fpjoin file_mimes m on m.mime_id=fp.tika_mime_idwhere m.mime_string like '%pdf'limit 100,../data/confluence_exports/TIKA/TikaEvalDatasetteExamples_165225246.html
TIKA : TikaEvalDatasetteExamples Get the URLs for 100 PDFs,select'https://corpora.tika.apache.org/base/docs/'||FILE_PATHas URLfrom file_profiles fpjoin file_mimes m on m.mime_id=fp.tika_mime_idwhere m.mime_string like '%pdf'limit 100,../data/confluence_exports/TIKA/TikaEvalDatasetteExamples_165225246.html
"TIKA : TikaEvalDatasetteExamples I want all files, but I don't want a file if its SHA256 is already in the list",select first_value(file_path) over ( partition by sha256 order by file_path) as pathfrom file_profiles fpgroup by sha256order by file_path,../data/confluence_exports/TIKA/TikaEvalDatasetteExamples_165225246.html
"TIKA : TikaEvalDatasetteExamples I want all PDFs, but I don't want a file if its SHA256 is already in the list",select first_value(file_path) over ( partition by sha256 order by file_path) as pathfrom file_profiles pjoin file_mimes m on p.tika_mime_id=mime_idwhere mime_string like '%pdf'group by sha256order by file_path,../data/confluence_exports/TIKA/TikaEvalDatasetteExamples_165225246.html
TIKA : TikaEvalDatasetteExamples Most common differences between Tika and File mime identification,"select tm.mime_string as 'tika_mime', fm.mime_string as 'file_mime', count(1) as cntfrom file_profiles fpjoin file_mimes tm on fp.tika_mime_id=tm.mime_idjoin file_mimes fm on fp.file_mime_id=fm.mime_idwhere tm.mime_string <> fm.mime_stringgroup by tm.mime_string, fm.mime_stringorder by cnt desc",../data/confluence_exports/TIKA/TikaEvalDatasetteExamples_165225246.html
TIKA : TikaEvalDatasetteExamples Most common file types that 'file' identifies but Tika doesn't,"selectfm.mime_string as 'file_mime', count(1) as cntfromfile_profiles fpjoin file_mimes tm on fp.tika_mime_id = tm.mime_idjoin file_mimes fm on fp.file_mime_id = fm.mime_idwheretm.mime_string <> fm.mime_stringand (tm.mime_string is nullor length(tm.mime_string) == 0or tm.mime_string = 'application/octet-stream')group by m.mime_stringorder by cnt desc",../data/confluence_exports/TIKA/TikaEvalDatasetteExamples_165225246.html
TIKA : TikaEvalDatasetteExamples Most common file types that Tika identifies but 'file' doesn't,"select tm.mime_string as 'file_mime', count(1) as cntfromfile_profiles fpjoin file_mimes tm on fp.tika_mime_id = tm.mime_idjoin file_mimes fm on fp.file_mime_id = fm.mime_idwheretm.mime_string <> fm.mime_stringand (fm.mime_string is nullor length(fm.mime_string) == 0or fm.mime_string = 'application/octet-stream')group bytm.mime_stringorder by cnt desc",../data/confluence_exports/TIKA/TikaEvalDatasetteExamples_165225246.html
TIKA : ImageCaption Tika and Tensorflow Image Captioning Using REST Server,We are going to start a python flask based REST API server and tell tika to connect to it. All the dependencies and setup complexities are isolated in the docker image. Requirements : Docker -- VisitDocker.comand install latest version of Docker. (Note: tested on docker v17.03.1),../data/confluence_exports/TIKA/ImageCaption_148640956.html
TIKA : ImageCaption Step 1. Setup REST Server,You can either start the REST server in an isolated docker container or natively on the host that runs tensorflow v1.0,../data/confluence_exports/TIKA/ImageCaption_148640956.html
TIKA : ImageCaption a. Using docker (Recommended),"Toggle line numbers 1git clone https://github.com/USCDataScience/tika-dockers.git &&cdtika-dockers2docker build -f Im2txtRestDockerfile -t uscdatascience/im2txt-rest-tika .3docker run -p 8764:8764 -it uscdatascience/im2txt-rest-tika Once it is done, test the setup by visitinghttp://localhost:8764/inception/v3/caption/image?url=https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/Marcus_Thames_Tigers_2007.jpg/1200px-Marcus_Thames_Tigers_2007.jpgin your web browser. Sample output from API: {
        ""captions"": [{
                        ""confidence"": 0.010706611316269087,
                        ""sentence"": ""a baseball player swinging a bat at a ball""
                },
                {
                        ""confidence"": 0.004686326913725872,
                        ""sentence"": ""a baseball player swinging a bat at a ball .""
                },
                {
                        ""confidence"": 0.0041084865981657155,
                        ""sentence"": ""a baseball player swinging a bat on a field""
                }
        ],
        ""beam_size"": 3,
        ""max_caption_length"": 20,
        ""time"": {
                ""read"": 407,
                ""captioning"": 1632,
                ""units"": ""ms""
        }
} Note: MAC USERS: If you are using an older version, say, 'Docker toolbox' instead of the newer 'Docker for Mac', you need to add port forwarding rules in your Virtual Box default machine. Open the Virtual Box Manager.Select your Docker Machine Virtual Box image.Open Settings -> Network -> Advanced -> Port Forwarding.Add an appname,Host IP 127.0.0.1 and set both ports to 8764.",../data/confluence_exports/TIKA/ImageCaption_148640956.html
TIKA : ImageCaption b. Without Using docker,"If you chose to setup REST server without a docker container, you are free to manually install all the required tools specified in thedocker file. Note: docker file has setup instructions for Ubuntu, you will have to transform those commands for your environment.  Toggle line numbers 1python tika-parsers/src/main/resources/org/apache/tika/parser/captioning/tf/im2txtapi.py",../data/confluence_exports/TIKA/ImageCaption_148640956.html
TIKA : ImageCaption Step 2. Create a Tika-Config XML to enable Tensorflow parser.,"A sample config can be found in Tika source code attika-parsers/src/test/resources/org/apache/tika/parser/recognition/tika-config-tflow-rest.xml Here is an example: <properties>
    <parsers>
        <parser class=""org.apache.tika.parser.recognition.ObjectRecognitionParser"">
            <mime>image/jpeg</mime>
            <mime>image/png</mime>
            <mime>image/gif</mime>
            <params>
                <param name=""apiBaseUri"" type=""uri"">http://localhost:8764/inception/v3</param>
                <param name=""captions"" type=""int"">5</param>
                <param name=""maxCaptionLength"" type=""int"">15</param>
                <param name=""class"" type=""string"">org.apache.tika.parser.captioning.tf.TensorflowRESTCaptioner</param>
            </params>
        </parser>
    </parsers>
</properties> Description of parameters :  Param NameTypeMeaningRangeExampleapiBaseUriuriHTTP URL that will be used to create apiUri & healthUriany HTTP URLhttp://localhost:8764/inception/v3captionsintNumber of captions to outputa non-zero positive integer3 to recieve 3 captionsmaxCaptionLengthintMaximum length of a captiona non-zero positive integer(recommended >=15)for 15 the sentence length of a caption won't be greater than 15classstringName of class that Implements Object recognition Contractconstant stringorg.apache.tika.parser.recognition.tf.TensorflowRESTCaptioner",../data/confluence_exports/TIKA/ImageCaption_148640956.html
TIKA : ImageCaption Step 3. Demo,"$ java -jar tika-app/target/tika-app-1.17-SNAPSHOT.jar \
             --config=tika-parsers/src/test/resources/org/apache/tika/parser/recognition/tika-config-tflow-im2txt-rest.xml \
             https://upload.wikimedia.org/wikipedia/commons/f/f6/Working_Dogs%2C_Handlers_Share_Special_Bond_DVIDS124942.jpg The input image is:  And, the output is Toggle line numbers 1...23INFO  Available = true, API Status = HTTP/1.0 200 OK4INFO  Captions = 5, MaxCaptionLength = 155INFO  Recogniser = org.apache.tika.parser.captioning.tf.TensorflowRESTCaptioner6INFO  Recogniser Available = true7INFO  minConfidence = 0.05, topN=28INFO  Time taken 1779ms9<?xml version=""1.0"" encoding=""UTF-8""?><htmlxmlns=""http://www.w3.org/1999/xhtml"">10<head>11<metaname=""org.apache.tika.parser.recognition.object.rec.impl""content=""org.apache.tika.parser.captioning.tf.TensorflowRESTCaptioner""/>12<metaname=""X-Parsed-By""content=""org.apache.tika.parser.CompositeParser""/>13<metaname=""X-Parsed-By""content=""org.apache.tika.parser.recognition.ObjectRecognitionParser""/>14<metaname=""resourceName""content=""Working_Dogs%2C_Handlers_Share_Special_Bond_DVIDS124942.jpg""/>15<metaname=""Content-Length""content=""295937""/>16<metaname=""CAPTION""content=""a man standing next to a dog on a leash . (0.00017)""/>17<metaname=""CAPTION""content=""a man standing next to a dog on a bench . (0.00017)""/>18<metaname=""CAPTION""content=""a man and a dog are sitting on a bench . (0.00014)""/>19<metaname=""CAPTION""content=""a man and a dog sitting on a bench . (0.00013)""/>20<metaname=""CAPTION""content=""a man and a dog are sitting on a bench (0.00009)""/>21<metaname=""Content-Type""content=""image/jpeg""/>22<title/>23</head>24<body><olid=""captions""><liid=""0"">a man standing next to a dog on a leash . [en](confidence = 0.000167)</li>25<liid=""1"">a man standing next to a dog on a bench . [en](confidence = 0.000167)</li>26<liid=""2"">a man and a dog are sitting on a bench . [en](confidence = 0.000138)</li>27<liid=""3"">a man and a dog sitting on a bench . [en](confidence = 0.000131)</li>28<liid=""4"">a man and a dog are sitting on a bench [en](confidence = 0.000092)</li>29</ol>30</body></html>31$",../data/confluence_exports/TIKA/ImageCaption_148640956.html
TIKA : ImageCaption Questions / Suggestions / Improvements / Feedback ?,"If it was useful, let us know on twitter by mentioning@ApacheTikaIf you have questions, let us know byusing Mailing ListsIf you find any bugs,use Jira to report them",../data/confluence_exports/TIKA/ImageCaption_148640956.html
TIKA : Tika2_0MigrationGuide Tika Modules,"In Tika 2.x the tika-parsers project has been split into 15 separate modules. With Tika's ever growing list of parsers the modules give developers the ability to pick and choose sub-groupings of parsers without bringing every parser dependency into a project. For example projects using Tika 1.12 parsers would include the following entry in an Apache Maven pom.xml dependency element:  <dependency>
    <groupId>org.apache.tika</groupId>
    <artifactId>tika-parsers</artifactId>
    <version>1.12</version>
</dependency> If this project were only being used to parse PDF files this could be refactored to the entry below on Tika 2.x:  <dependency>
    <groupId>org.apache.tika</groupId>
    <artifactId>tika-parser-pdf-module</artifactId>
    <version>2.0</version>
</dependency> The 2.x branch also introduces theParserProxy,DetectorProxy, andEncodingDetectorProxyclasses that allow developers to compose Parsers, Detectors andEncodingDetectorsusing classes that may or may not exist on the classpath. For example theOutlookExtractorexists in the tika-parser-office-module. An Outlook message may contain HTML content but the user may not want to include the tika-parser-web-module that contains theHtmlParser. By wrapping theHtmlParserin aParserProxy:  this.htmlParserProxy = new ParserProxy(""org.apache.tika.parser.html.HtmlParser"", getClass().getClassLoader()); TheOutlookExtractorwill only parse the HTML content if that module is included. If not the parser fails silently by default. In cases where the developer wants to be warned that a proxy has failed the developer may set the following System Property:  org.apache.tika.service.proxy.error.warn=true Which will print a warning message when the class being proxied is not found.",../data/confluence_exports/TIKA/Tika2_0MigrationGuide_148640966.html
TIKA : Tika2_0MigrationGuide Tika Bundles,In addition to replacing tika-parsers the tika-bundle will also be replaced in Tika 2.x. Instead of a single tika-bundle there will be a bundle for each parser module created above. For example tika-parser-pdf-module will have a corresponding tika-parser-pdf-bundle. As in tika-bundle the dependencies in the bundle projects will be embedded in the jar file to allow OSGi unfriendly projects to be easily included. The bundles will also start up an OSGi service for each listed service in the META-INF/services/ file entries. This will allow OSGi developers an easier way to get access to individual parsers through the OSGi service registry. Finally aTikaServiceclass will also be added to the OSGi Service registry that will serve as a means to put a document through all the parsers available in the OSGi service registry.,../data/confluence_exports/TIKA/Tika2_0MigrationGuide_148640966.html
TIKA : ErrorsAndExceptions Corrupt File,"If the file is corrupted in some way, and cannot be processed, a CorruptedFileException (subclass of TikaException) should be thrown (seeParser contract)",../data/confluence_exports/TIKA/ErrorsAndExceptions_109454049.html
TIKA : ErrorsAndExceptions File cannot be read,"If an IO problem occurs when reading the document, an IOException should be thrown (seeParser contract)  The only exception is if an underlying library throws an IOException to indicate a broken / corrupted file, in which case catching the IOException and re-throwing a CorruptedFileExceptionmaybe done by the parser to give a more helpful error.",../data/confluence_exports/TIKA/ErrorsAndExceptions_109454049.html
"TIKA : ErrorsAndExceptions ""Empty"" File (No Text)","If there is no text in the file, either because it's empty (eg 0 byte text file), or because it's a format that doesn't have text (eg an image), then ???  TBC - should the body be opened then immediately closed, or something else?",../data/confluence_exports/TIKA/ErrorsAndExceptions_109454049.html
TIKA : ErrorsAndExceptions File is password protected,"EncryptedDocumentException(a subtype of TikaException) should be thrown if the file is password protected and no/incorrect password is given.  (A PasswordProvider should be placed on the ParseContext if known, to allow the parsing to proceed without error)",../data/confluence_exports/TIKA/ErrorsAndExceptions_109454049.html
TIKA : ErrorsAndExceptions File would require too much memory to process,"Where the file format permits predicting the amount of memory required to process it (eg a length field near the start of a block thatmustbe fully processed to proceed), and the parser knows this is more than available / permitted, a TikaMemoryLimitException should be thrown to abort the processing.  If it isn't possible to detect this in advance, parsersshould tryto abort with a TikaMemoryLimitException when they spot things going wrong.  Otherwise, there's a reason we suggest using a separate JVM via ForkParser or TikaServer or similar....",../data/confluence_exports/TIKA/ErrorsAndExceptions_109454049.html
TIKA : ErrorsAndExceptions Parser can't handle File,"If the file is in a sub-format that the parser can't handle (eg parser supports v2 and v3, document is v1, all share the same mimetype), or uses some options that means that parser can't sensibly handle it, then a UnsupportedFormatException should be thrown  Where possible though, the mimetype and/or detection should be updated to prevent an unsupported version of the file format ending up with the parser!",../data/confluence_exports/TIKA/ErrorsAndExceptions_109454049.html
TIKA : ErrorsAndExceptions Document Structure is Broken,"If something is very broken with the file / file structure, and it will be impossible to output valid XML for it for some reason, then probably a SAXException is the right thing",../data/confluence_exports/TIKA/ErrorsAndExceptions_109454049.html
TIKA : ErrorsAndExceptions Document requests no extraction,"Some file formats allow extraction, but request politely that it not be done. Depending on your Tika use case (eg forensics vs user facing search), you may wish to abort processing, or you may wish to process + pass the permission details up the chain.  To pass but inform the caller of the access restrictions, populate thehttps://tika.apache.org/1.19/api/org/apache/tika/metadata/AccessPermissions.html AccessPermissions metadata properties  To abort, eg if the caller requested strict permissions enforcements, throw a AccessPermissionException   These Exceptions should only be thrown by the core of Tika itself, not the underlying parsers",../data/confluence_exports/TIKA/ErrorsAndExceptions_109454049.html
TIKA : ErrorsAndExceptions Broken Tika Config,"If the user specifies some configuration for Tika, either explicitly or implicitly (eg extension resources with the right magic specific name on the classpath), and the config supplied is invalid and can't be worked around, a TikaConfigException will be thrown during the initialisation of Tika",../data/confluence_exports/TIKA/ErrorsAndExceptions_109454049.html
TIKA : ErrorsAndExceptions Zero byte length file,"Tika can provide a suggested mime type for these, but not extract any content. Caused typically byRight Click -> Newortouch, or an error in getting the file to where it is now.  If passed one of these for detection, no error should be given, and a filename-only answer supplied  If passed one of these for parsing, a ZeroByteFileException should be thrown",../data/confluence_exports/TIKA/ErrorsAndExceptions_109454049.html
TIKA : TikaAndNER Named Entity Recognition (NER) with Tika,"Named Entity Recognition is supported intika-parsers, introduced inTIKA-1787. This page describes the steps required to configure and activate theNamedEntityParser.",../data/confluence_exports/TIKA/TikaAndNER_148640962.html
TIKA : TikaAndNER Activate Named Entity Parser,"Before moving ahead to configure NER implementations,org.apache.tika.parser.ner.NamedEntityParser, the parser responsible for handling the name recognition task needs to be enabled. This can be done with Tika Config XML file, as follows:  <?xml version=""1.0"" encoding=""UTF-8""?>
<properties>
    <parsers>
        <parser class=""org.apache.tika.parser.ner.NamedEntityParser"">
            <mime>text/plain</mime>
            <mime>text/html</mime>
            <mime>application/xhtml+xml</mime>
        </parser>
    </parsers>
</properties> This configuration has to be supplied in the later phases, so store it as 'tika-config.xml'. Note: TheNamedEntityParserparser does not restrict mimetypes, it uses Tika's auto detect parser to read text content from non-text streams.",../data/confluence_exports/TIKA/TikaAndNER_148640962.html
TIKA : TikaAndNER Using Apache OpenNLP NER,"The NE Parser is configured to use an implementation based onApache OpenNLP. However, the NER models need to be added to the Tika's classpath to make this work. The following table shows types of entities and the paths to place the model file. Entity TypePath for modelURL to getPERSONorg/apache/tika/parser/ner/opennlp/ner-person.binhttp://opennlp.sourceforge.net/models-1.5/en-ner-person.binLOCATIONorg/apache/tika/parser/ner/opennlp/ner-location.binhttp://opennlp.sourceforge.net/models-1.5/en-ner-location.binORAGANIZATIONorg/apache/tika/parser/ner/opennlp/ner-organization.binhttp://opennlp.sourceforge.net/models-1.5/en-ner-organization.binDATEorg/apache/tika/parser/ner/opennlp/ner-date.binhttp://opennlp.sourceforge.net/models-1.5/en-ner-date.binTIMEorg/apache/tika/parser/ner/opennlp/ner-time.binhttp://opennlp.sourceforge.net/models-1.5/en-ner-time.binPERCENTorg/apache/tika/parser/ner/opennlp/ner-percentage.binhttp://opennlp.sourceforge.net/models-1.5/en-ner-percentage.binMONEYorg/apache/tika/parser/ner/opennlp/ner-money.binhttp://opennlp.sourceforge.net/models-1.5/en-ner-money.bin Notes: You can use any combination of the models. If you are interested in only the LOCATION names, then skip other NER models save LOCATION.NER Models for other languages are also availablehttp://opennlp.sourceforge.net/models-1.5/. If you choose to use different language, use those URLs in the below script.",../data/confluence_exports/TIKA/TikaAndNER_148640962.html
TIKA : TikaAndNER Tika App + OpenNLP NER in action,"#Create a directory for keeping all the models.
#Choose any convenient path but make sure to use absolute path
export NER_RES=$HOME/tika/tika-ner-resources
mkdir -p $NER_RES
cd $NER_RES

PATH_PREFIX=""$NER_RES/org/apache/tika/parser/ner/opennlp""
URL_PREFIX=""http://opennlp.sourceforge.net/models-1.5""

mkdir -p $PATH_PREFIX

# using three entity types from the above table for demonstration
wget ""$URL_PREFIX/en-ner-person.bin"" -O $PATH_PREFIX/ner-person.bin
wget ""$URL_PREFIX/en-ner-location.bin"" -O $PATH_PREFIX/ner-location.bin
wget ""$URL_PREFIX/en-ner-organization.bin"" -O $PATH_PREFIX/ner-organization.bin

export TIKA_APP={your/path/to/tika-app}/target/tika-app-1.12-SNAPSHOT.jar

java -classpath $NER_RES:$TIKA_APP org.apache.tika.cli.TikaCLI --config=tika-config.xml -m http://people.apache.org/committer-index.html

# Are there any metadata keys starting with ""NER_"" ?",../data/confluence_exports/TIKA/TikaAndNER_148640962.html
TIKA : TikaAndNER Using Stanford CoreNLP NER,The 'org.apache.tika.parser.ner.corenlp.CoreNLPNERecogniser' class provides runtime bindings toStanford CoreNLP CRF classifiersfor named entity recognition. The following steps are necessary to use this NER implementation: Add Core NLP library and its dependencies to classpathAdd models to class pathSet NER Implementation to CoreNLP NOTE: The latest release of Stanford CoreNLP requires JDK8.,../data/confluence_exports/TIKA/TikaAndNER_148640962.html
TIKA : TikaAndNER Tika + CoreNLP in action,"cd /$HOME/src 
git clone https://github.com/thammegowda/tika-ner-corenlp.git
cd tika-ner-corenlp
mvn clean compile package assembly:single -PtikaAddon

#this should produce target/tika-ner-corenlp-addon-*-jar-with-dependencies.jar
export CORE_NLP_JAR=`find $PWD/target/tika-ner-corenlp-addon-*jar-with-dependencies.jar`

export TIKA_APP={your/path/to/tika-app}/target/tika-app-1.12-SNAPSHOT.jar

java -Dner.impl.class=org.apache.tika.parser.ner.corenlp.CoreNLPNERecogniser \
      -classpath $TIKA_APP:$CORE_NLP_JAR org.apache.tika.cli.TikaCLI \
      --config=tika-config.xml -m http://www.hawking.org.uk

# Observe metadata keys starting with NER_ 

# To use 3class NER model  (Default is 7 class model)

 java  -Dner.corenlp.model=edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz \
       -Dner.impl.class=org.apache.tika.parser.ner.corenlp.CoreNLPNERecogniser \
       -classpath $TIKA_APP:$CORE_NLP_JAR org.apache.tika.cli.TikaCLI \
       --config=tika-config.xml -m http://www.hawking.org.uk The CoreNLP CRF classifier recognised the following from the text content ofhttp://www.hawking.org.ukpage: NER_DATE: 2009
NER_DATE: 1963
NER_DATE: 1663
NER_DATE: 1982
NER_DATE: 1979
NER_LOCATION: Gonville
NER_LOCATION: Einstein
NER_LOCATION: London
NER_LOCATION: Cambridge
NER_LOCATION: Santa Cruz
NER_ORGANIZATION: Leiden University
NER_ORGANIZATION: NASA
NER_ORGANIZATION: CBE
NER_ORGANIZATION: Brief History of Time
NER_ORGANIZATION: University of California
NER_ORGANIZATION: Cambridge Lectures Publications Books Images Films
NER_ORGANIZATION: Caius College
NER_ORGANIZATION: Royal Society
NER_ORGANIZATION: About Stephen The Computer Stephen
NER_ORGANIZATION: US National Academy of Science
NER_ORGANIZATION: Department of Applied Mathematics
NER_ORGANIZATION: ESA
NER_ORGANIZATION: The Universe
NER_ORGANIZATION: Sally Tsui Wong-Avery Director of Research
NER_ORGANIZATION: the University of Cambridge
NER_ORGANIZATION: Theoretical Physics
NER_ORGANIZATION: Baby Universe
NER_PERSON: Einstein
NER_PERSON: P. Oesch
NER_PERSON: R. Bouwens
NER_PERSON: George
NER_PERSON: Stephen Hawking
NER_PERSON: Isaac Newton
NER_PERSON: D. Magee
NER_PERSON: Annie
NER_PERSON: G. Illingworth
NER_PERSON: Stephen
NER_PERSON: Dennis Stanton Avery",../data/confluence_exports/TIKA/TikaAndNER_148640962.html
TIKA : TikaAndNER Using Regular Expressions,Theorg.apache.tika.parser.ner.regex.RegexNERecogniserprovides an implementation based on Regular expressions. The following steps are required to use this implementation: Configure regular expressions in 'org/apache/tika/parser/ner/regex/ner-regex.txt'Set System propertyner.impl.classtoorg.apache.tika.parser.ner.regex.RegexNERecogniser,../data/confluence_exports/TIKA/TikaAndNER_148640962.html
TIKA : TikaAndNER Tika + RegexNER in action,"# Create a regex file and add it to classpath
export NER_RES=$HOME/tika/tika-ner-resources
mkdir -p $NER_RES
cd $NER_RES
mkdir -p org/apache/tika/parser/ner/regex/

echo ""PHONE_NUMBER=((\+\d{1,2}\s?)?\(?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4})"" > org/apache/tika/parser/ner/regex/ner-regex.txt
echo ""EMAIL=([a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?))"" >> org/apache/tika/parser/ner/regex/ner-regex.txt

export TIKA_APP={your/path/to/tika-app}/target/tika-app-1.12-SNAPSHOT.jar

java -Dner.impl.class=org.apache.tika.parser.ner.regex.RegexNERecogniser \
    -classpath $NER_RES:$TIKA_APP org.apache.tika.cli.TikaCLI \
    --config=tika-config.xml -m  http://www.cs.usc.edu/faculty_staff/faculty

# Observe values of keys NER_PHONE_NUMBER and NER_EMAIL",../data/confluence_exports/TIKA/TikaAndNER_148640962.html
TIKA : TikaAndNER Creating a custom NER,Create a class and implementorg.apache.tika.parser.ner.NERecogniserSet class name as value to system propertyner.impl.classsimilar to Regex or CoreNLP,../data/confluence_exports/TIKA/TikaAndNER_148640962.html
TIKA : TikaAndNER Chaining all the above at once,"Multiple class names can be provided by setting the system propertyner.impl.classto a comma separtes class names Example :-Dner.impl.class = org.apache.tika.parser.ner.opennlp.OpenNLPNERecogniser,org.apache.tika.parser.ner.regex.RegexNERecogniser",../data/confluence_exports/TIKA/TikaAndNER_148640962.html
TIKA : TikaServer in Tika 2.x Major changes,"Modularization – We've modularized tika-server:tika-server-coreincludes all of the functionality oftika-server, but with no bundled parsers.  Users might want this if they are only parsing a few file formats or want to use only their custom parsers.tika-server-standardis what most people will want to use.  As with thetika-parsers-standardmodule, this includes most of the common file format parsers. If needed, users may also add thetika-parser-scientific-packageandtika-parser-sqlite3-packageto the class path.  In 1.x, the first was included in tika-server 1.x by default, and the second was included only if users added xerial's sqlite3 jar on the classpath.--spawnChildmode is now default.  In Tika 1.x, users had to specify this on the commandline to forcetika-serverto fork a process that did the actual parsing.  This option is far more robust against timeouts, OOMs, crashes and other mishaps; the forking process monitors the forked process and will restart on timeouts, etc.NOTE:Client code needs to be able to handle the times whentika-serveris restarting and is not available; this typically only takes a few seconds.  To disable this mode, use-noForkon the commandline.Configuringtika-serverin Tika 2.x.  See below.  We've moved most configuration options intotika-config.xmland dramatically limited the commandline options.The namespace has changed slightly forTikaServerClitoorg.apache.tika.server.core.TikaServerCli. If adding optional jars to the class path in, say, abin/directory, start tika-server with:java -cp ""bin/*"" org.apache.tika.server.core.TikaServerCli -c tika-config.xmlenableFileUrl-- We have removed this capability from tika-server in 2.x.  We have replaced it with the FileSystemFetcher, which is available in tika-core.  SeeFetchersInClassicServerEndpoints.",../data/confluence_exports/TIKA/TikaServer-in-Tika-2.x_191334866.html
TIKA : TikaServer in Tika 2.x Configuring tika-server in Tika 2.x,"As with other components, in Tika 2.x, we moved configuration intotika-config.xml.  We have left only a few commandline options available (to see the options:java -jar tika-server-standard-2.x.x.jar --help).Please note that all command-line option values will override their counterparts in the xml configuration file. -h, --host – hostname-p, --port – which port to bind to.  Can specify ranges, e.g.9990-9999, and Tika will launch 10 servers in forked processes on each of those ports. Can also specify a comma-delimited list, e.g. (9996,9998,9999).-?, --help-c, --config – specify the tika-config.xml file to use for this tika-server and its forked processes.-i, --id – specify the id for this server.  This is used in logging and in the/statusendpoint.-noFork – run tika-server in legacy mode without forking a process.  tika-config.xml<?xml version=""1.0"" encoding=""UTF-8""?>
<properties>
  <!-- <parsers etc.../> -->
  <server>
    <params>
      <!-- which port to start the server on. If you specify a range,
          e.g. 9995-9998, TikaServerCli will start four forked servers,
          one at each port.  You can also specify multiple forked servers
          via a comma-delimited value: 9995,9997.

      -->
      <port>9998</port>
      <host>localhost</host>
      <!-- if specified, this will be the id that is used in the
          /status endpoint and elsewhere.  If an id is specified
          and more than one forked processes are invoked, each process
          will have an id followed by the port, e.g my_id-9998. If a
          forked server has to restart, it will maintain its original id.
          If not specified, a UUID will be generated.
          -->
      <id></id>       
      <!-- Origin URL for cors requests. Set to '*' if you
          want to allow all CORS requests. Leave blank or remove element
          if you do not want to enable CORS. -->
      <cors>*</cors>       
      <!-- which digests to calculate, comma delimited (e.g. md5,sha256);
          optionally specify encoding followed by a colon (e.g. ""sha1:32"").
          Can be empty if you don't want to calculate a digest -->
      <digest>sha256</digest>
      <!-- how much to read to memory during the digest phase before
          spooling to disc...only if digest is selected -->
      <digestMarkLimit>1000000</digestMarkLimit>
      <!-- request URI log level 'debug' or 'info' -->
      <logLevel>info</logLevel>
      <!-- whether or not to return the stacktrace in the data returned 
           to the user when a parse exception happens-->
      <returnStackTrace>false</returnStackTrace>
      <!-- If set to 'true', this runs tika server ""in process""
          in the legacy 1.x mode.
          This means that the server will be susceptible to infinite loops
          and crashes.
          If set to 'false', the server will spawn a forked
          process and restart the forked process on catastrophic failures
          (this was called -spawnChild mode in 1.x).
          noFork=false is the default in 2.x
      -->
      <noFork>false</noFork>
      <!-- maximum time to allow per parse before shutting down and restarting
          the forked parser. Not allowed if noFork=true. -->
      <taskTimeoutMillis>300000</taskTimeoutMillis>
      <!-- maximum amount of time to wait for a forked process to
          start up.
          Not allowed if noFork=true. -->
      <maxForkedStartupMillis>120000</maxForkedStartupMillis>
      <!-- maximum number of times to allow a specific forked process
          to be restarted.
          Not allowed if noFork=true. -->
      <maxRestarts>-1</maxRestarts>
      <!-- maximum files to parse per forked process before
          restarting the forked process to clear potential
          memory leaks.
          Not allowed if noFork=true. -->
      <maxFiles>100000</maxFiles>
      <!-- if you want to specify a specific javaPath for
          the forked process.  This path should end
          the application 'java', e.g. /my/special-java/java
          Not allowed if noFork=true. -->
      <javaPath>java</javaPath>
      <!-- jvm args to use in the forked process -->
      <forkedJvmArgs>
        <arg>-Xms1g</arg>
        <arg>-Xmx1g</arg>
        <arg>-Dlog4j.configurationFile=my-forked-log4j2.xml</arg>
       </forkedJvmArgs>
      <!-- this must be set to true for any handler that uses a fetcher or emitter.  These pipes features are inherently unsecure because
           the client has the same read/write access as the tika-server process.  Implementers must secure Tika server so that only their clients can reach it.
           A byproduct of setting this to true is that the /status endpoint is turned on -->
      <enableUnsecureFeatures>false</enableUnsecureFeatures>
      <!-- you can optionally select specific endpoints to turn on/load.  This can improve resource usage and decrease your attack surface.
           If you want to access the status endpoint, specify it here or set unsecureFeatures to true -->
      <endpoints>
        <endpoint>status</endpoint>
        <endpoint>rmeta</endpoint>
      </endpoints>     
    </params>
  </server>
</properties>",../data/confluence_exports/TIKA/TikaServer-in-Tika-2.x_191334866.html
TIKA : MetadataRoadmap Introduction,"With Tika’s focus on mimetype detection and content extraction, the support for content metadata is somewhat suboptimal as of today. There is not much usage of standard metadata semantics and related namespaces and when trying to map the information to a data model like XMP, the current implementation is fairly limited.  This page shall provide a roadmap about how to improve the support for metadata in Tika going forward and shall serve as a basis to discuss this. The idea of providing a unified access to a common set of format independent metadata is one main aspect of this discussion. The output of metadata as XMP data model is another.",../data/confluence_exports/TIKA/MetadataRoadmap_109454062.html
TIKA : MetadataRoadmap Current situation,"The Metadata implementation in Tika as of April 2012:  Each parser fills a Metadata map which is a simple key-value list where values can also be multi-valuesMostly the keys for the Metadata map are taken from fixed lists which are defined as interfaces in the Metadata classThose keys are usually Property objects, where the Property class also serves as a static list which registers every property that is created in the Metadata interfaces. This property class resembles the XMP data model to some extend but does not store e.g. any hierarchical information. And it leaves every client the choice to store property names with prefixes or not.Any metadata outputter just iterates over the Metadata map and could query the Property list for additional information.In case of the XMP outputter (XMPContentHandler) only those properties are outputted which are stored with a prefix in the Property list.",../data/confluence_exports/TIKA/MetadataRoadmap_109454062.html
TIKA : MetadataRoadmap The general idea of what to accomplish,"Tika should support a unifying access to common metadata properties like title, description, keywords, creator, rating, etc. So there should be a clear semantic for those common properties regardless of the underlying implementation in various metadata containers. And the access to these properties should be easy and fast. On the other hand, Tika should allow to access and manage file format specific metadata and its underlying semantic in a consolidated and flexible way, i.e. using one data model to provide information to clients.  While the current Metadata map can be used to offer easy access to the common set of properties, an XMP output could be used to offer a more extensive, flexible and semantically clearer access to a file's metadata.  The recommendation is to use Dublin Core and the semantic of the ISO part of XMP - which builds on top of DC - for common and file format neutral Tika properties as both are already being used by several standards like IPTC and MWG for this purpose.",../data/confluence_exports/TIKA/MetadataRoadmap_109454062.html
TIKA : MetadataRoadmap Roadmap,"The following steps shall provide a roadmap to reach the above goal  Reorganize metadata keys internally  The Metadata keys and their interfaces should be reorganized and renamed in two groups: First in namespaces and second in standards which just contain lists of aliases with the properties they use from the namespace interfaces. The reason is that only those two concepts have unambiguous and clearly defined semantics where each client knows what to do with it.  Properties which are currently not connected to a namespace (like the properties from MSOffice interface) would also be moved to an appropriate namespace interface.To not have to prefix each namespace property, the namespace interfaces should be removed from Metadata class and aliases be added to the class to keep backwards compatibility.  No parser or client has to be changed.  Move XMP output to an extra XMP module of Tika  Add an extra Tika module that takes the metadata map from Tika-core and transforms it into XMP. Add XMPCore library from Maven.org to this module and use create XMP output.  Add a static Tika-to-XMP mapping table for the common set of properties and file formats to have a first working version of XMP output.  Correct parsers where necessary  Adjust the parsers to map metadata not only to the current mappings but also to the correct set of common properties and namespaces (i.e.DublinCoreand XMP ones) and maybe add file format specific properties.  Declare current mappings deprecated if needed.Still no client changes needed.  Add support for structured data to metadata class  There is need to have a data model which is able to faithfully store all metadata information, also structured one. There are several potential ways to solve this. Either the currentHashMapis extended to also support structured properties or other structured data models like XMP could be used as internal representation for that. The Metadata API will be kept as is, just the internal representation of the data will be changed. Additional APIs should be introduced to manipulate structured properties.  Any client provided data that cannot be mapped to existing namespaces, will be stored in a special Tika namespace.Still no client has to change.  Introduce versioning scheme for metadata mappings  This is very useful if mappings of metadata properties need to be changed in the future. Such changes are versioned and Clients can then pass the mapping version they are interested in through the parsing context. This will ensure backwards compatibility while allowing for changes and improvements.  The default implementation provides the latest version.  Introduce the ability for clients to define own mappings  This is an optional step that would allow clients to pass in own metadata mappings they are interested in, in case they want to have access to data that the default mapping is not providing.",../data/confluence_exports/TIKA/MetadataRoadmap_109454062.html
TIKA : API Bindings for Tika Kubernetes Chart,Kubernetes Chart by Bitnami,../data/confluence_exports/TIKA/API-Bindings-for-Tika_109454035.html
TIKA : API Bindings for Tika Perl,Apache-Tika-Async by CORION (CPAN)Apache-Tika by RIBUGENT (CPAN),../data/confluence_exports/TIKA/API-Bindings-for-Tika_109454035.html
TIKA : API Bindings for Tika R,R-Tika,../data/confluence_exports/TIKA/API-Bindings-for-Tika_109454035.html
TIKA : API Bindings for Tika Julia,Taro,../data/confluence_exports/TIKA/API-Bindings-for-Tika_109454035.html
TIKA : API Bindings for Tika Clojure,PantomimeCLJ-Tika,../data/confluence_exports/TIKA/API-Bindings-for-Tika_109454035.html
TIKA : API Bindings for Tika PHP,PHP Tika WrapperPHP Apache TikaApache Tika BundleFunstaff Tika PHP Wrapper,../data/confluence_exports/TIKA/API-Bindings-for-Tika_109454035.html
TIKA : API Bindings for Tika .NET,Tika.NET,../data/confluence_exports/TIKA/API-Bindings-for-Tika_109454035.html
TIKA : API Bindings for Tika JRuby,Rika: A JRuby wrapper for Apache Tika to extract content and metadata from various file formats.,../data/confluence_exports/TIKA/API-Bindings-for-Tika_109454035.html
TIKA : API Bindings for Tika Ruby,Tika-Client: Ruby Bindings for Tika Server,../data/confluence_exports/TIKA/API-Bindings-for-Tika_109454035.html
TIKA : API Bindings for Tika Python,Tika-PythonTika App PythonAptivate Tika,../data/confluence_exports/TIKA/API-Bindings-for-Tika_109454035.html
TIKA : API Bindings for Tika Node.js,node-tika,../data/confluence_exports/TIKA/API-Bindings-for-Tika_109454035.html
TIKA : API Bindings for Tika Go,go-tika,../data/confluence_exports/TIKA/API-Bindings-for-Tika_109454035.html
TIKA : API Bindings for Tika Debian Pkg,Open Semantic Search Tika Debian PackageDebian Tika Package,../data/confluence_exports/TIKA/API-Bindings-for-Tika_109454035.html
TIKA : API Bindings for Tika OpenShiftCartridge,Openshift Tika Cartridge,../data/confluence_exports/TIKA/API-Bindings-for-Tika_109454035.html
TIKA : API Bindings for Tika Docker Container,Tika Docker Container,../data/confluence_exports/TIKA/API-Bindings-for-Tika_109454035.html
TIKA : Configuring Parsers At Parse Time in tika-server Setting Parameters via Headers,"As of February 2023 (version 2.7.0), Tika only directly supports configuration of the PDFParser and the TesseractOCRParser via headers.  To set parameters for the PDFParser: 1) Create the header key by prependingX-Tika-PDFto a parameter, e.g.X-Tika-PDFOcrStrategy 2) Set the value (e.g.ocr_only) To see the available parameters for the PDFParser, see:PDFParser (Apache PDFBox). To set parameters for the TesseractOCRParser, prependX-Tika-OCRto the parameter. See the unit tests in thetika-server-standard'sTikaResourceTest, e.g.testOCRLanguageConfig().",../data/confluence_exports/TIKA/Configuring-Parsers-At-Parse-Time-in-tika-server_240883999.html
TIKA : Configuring Parsers At Parse Time in tika-server Customizing Configuration,"This functionality can be extended to other parsers by adding a class that implementsParseContextConfigand loading it via SPI. See the example ofPDFServerConfig, and make sure to add your class via services, e.g.org.apache.tika.server.core.ParseContextConfig.",../data/confluence_exports/TIKA/Configuring-Parsers-At-Parse-Time-in-tika-server_240883999.html
TIKA : MSOfficeParsers Beta SAX Parsers for .docx and .pptx,"As of Tika 1.15, there are experimental/beta SAX parsers for .docx files.  On very large files (e.g. ""War and Peace""), this parser appears to be 4x faster and require far less memory than our traditional DOM based parsers.  For smaller files, the gain is not nearly as great.  For the 386MB pptx submitted on TIKA-2201, it would have taken ~60GB to load the file in memory.  These parsers are still in their early stages and don't have all of the features of the DOM parsers.  However, the .docx parser does offer parameterization to include or exclude deleted text.  To select it programmatically, setsetUseSAXDocxExtractororsetUseSAXPptxExtractortotrueon anOfficeParserConfigand put that in theParseContext:context.set(OfficeParserConfig.class, officeParserConfig);.  To set it via the config file, try:  <properties>
    <parsers>
        <parser class=""org.apache.tika.parser.DefaultParser""/>
        <parser class=""org.apache.tika.parser.microsoft.ooxml.OOXMLParser"">
            <params>
                <param name=""useSAXDocxExtractor"" type=""bool"">true</param>
                <param name=""useSAXPptxExtractor"" type=""bool"">true</param>
            </params>
        </parser>
    </parsers>
</properties>  SeeTIKA-1321for the parser andTIKA-2180andTIKA-2201for some symptoms that the current DOM parser might be slowing you down.",../data/confluence_exports/TIKA/MSOfficeParsers_109454059.html
TIKA : MSOfficeParsers How to build Tika with POI's trunk,"You'll need to have the following build tools installed: Ant, Forrest and Maven. You'll also need the source code for both projects via svn, git or download of src.  BuildPOI– ""gradlew clean build jar"".  Optionally, to run the integration tests: ""ant test-integration"".  Note, you can also grab a nightly build fromJenkins.  As of this writing, you still have to generate the poms.  So, the nightly build saves you from building POI, but it doesn't yet save you from having to download the source code (install Ant, etc.) to build the poms. 2.  Generate the poms – ""ant maven-poms"". 3.  Frombuild/dist/maven, install this new build of POI into your local Maven repo – e.g.:mvn install:install-file -Dfile=poi-4.0.0-SNAPSHOT.jar -DpomFile=poi-4.0.0-SNAPSHOT.pom

mvn install:install-file -Dfile=poi-ooxml-schemas-4.0.0-SNAPSHOT.jar -DpomFile=poi-ooxml-schemas-4.0.0-SNAPSHOT.pom

mvn install:install-file -Dfile=poi-ooxml-4.0.0-SNAPSHOT.jar -DpomFile=poi-ooxml-4.0.0-SNAPSHOT.pom

mvn install:install-file -Dfile=poi-scratchpad-4.0.0-SNAPSHOT.jar -DpomFile=poi-scratchpad-4.0.0-SNAPSHOT.pom4.  Update the version of POI in Tika'stika-parsers/pom.xml. 5.  Make any necessary modifications to Tika (if there are mods to POI's API) 6.  Build Tika – ""mvn clean install""  And there you have a shiny, sparkling, new Tika with the dev version of POI!",../data/confluence_exports/TIKA/MSOfficeParsers_109454059.html
TIKA : Release Process for tika-helm Apache JFrog Artifactory Repository and Access,You need permissions to release the Apache Tika Helm chart to the Apache Infra Artifactory instance. This is controlled by the ASF Infra team and can be requested through aINFRA JIRA ticket. Make sure to tag the ticket with theArtifactorylabel.,../data/confluence_exports/TIKA/Release-Process-for-tika-helm_177051610.html
TIKA : Release Process for tika-helm tika-helm repo,"ThisGithub repositorycontains the Chart which needs to be configured for minimal and full Tika Docker images. If you are an Apache Tika Committer or PMC you should already have access to this repository. If not, then you will want to check over on the dev@tika mailing list.",../data/confluence_exports/TIKA/Release-Process-for-tika-helm_177051610.html
TIKA : Release Process for tika-helm tika-docker Image Types,"There are two image types: Minimal - containing just Apache Tika and it's base dependencies (i.e. Java)Full - containing Apache Tika, it's dependencies, as well as Tesseract and GDAL. This is to say that for every stable release of the Apache Tika source binaries, two (minimal and full) Docker images are released.",../data/confluence_exports/TIKA/Release-Process-for-tika-helm_177051610.html
TIKA : Release Process for tika-helm Creating Git Release,"Based on the above general information, we therefore make two releases of the corresponding Helm Chart. In order to do this, we need to update 3 lines of code, namely https://github.com/apache/tika-helm/blob/main/Chart.yaml#L22https://github.com/apache/tika-helm/blob/main/Chart.yaml#L23https://github.com/apache/tika-helm/blob/main/values.yaml#L26 appVersionneeds to match the corresponding upstream tika-docker release tag. Once you have examined the changes, you can commit them tomainbranch. Then tag them from there and push the tag to the tika-helm repository. Tag and pushb to tika-helm origin$ git add -A
$ git commit -m ""Release branch for tika-helm ${target_version}""
$ git push origin main
$ git tag -a ${target_version} -m ""Release tika-helm ${target_version}""
$ git push --tags Once this is done, you cannavigate to the${target_version}tag you just pushedand 'release' it (by clicking on the three dots button on the right hand side).",../data/confluence_exports/TIKA/Release-Process-for-tika-helm_177051610.html
TIKA : Release Process for tika-helm Promoting Release to the Apache JFrog Artifactory Repository,N.B.You should still be in the${target_version}branch,../data/confluence_exports/TIKA/Release-Process-for-tika-helm_177051610.html
TIKA : Release Process for tika-helm Prerequisites,"Helmhelm-push-artifactory-plugin(if you are using Helm v3 (recommended) then add--version 1.0.2), for example Install prerequisites# Assuming Mac
$ brew install helm # Which will install Helm >=3.X
$ helm plugin install https://github.com/belitre/helm-push-artifactory-plugin --version 1.0.2 # Which is required for >= Helm 3.X
...
Installing plugin for Helm v3...
Downloading and installing helm-push-artifactory v1.0.2 ...
https://github.com/belitre/helm-push-artifactory-plugin/releases/download/v1.0.2/helm-push-artifactory-v1.0.2-darwin-amd64.tar.gz
Installed plugin: push-artifactory",../data/confluence_exports/TIKA/Release-Process-for-tika-helm_177051610.html
TIKA : Release Process for tika-helm Promoting the Release,"# Add the tika Helm repository
$ helm repo add tika https://apache.jfrog.io/artifactory/tika

# Push the tika helm artifact to Artifactory
$ helm push-artifactory . https://apache.jfrog.io/artifactory/tika --username '${username}' --password ""${password}""
...
Pushing tika-1.26-full.tgz to https://apache.jfrog.io/artifactory/tika/tika/tika-1.26-full.tgz...
Done.
Reindex helm repository tika...  Congratulations, by this stage you've just released the tika-helm Helm Chart. Go ahead and make some release announcements... probably on user@ and dev@ tika.apache.org mailing list.",../data/confluence_exports/TIKA/Release-Process-for-tika-helm_177051610.html
TIKA : Metadata Overview Tika Process,"These capture behavior of parsers or other components during the parse. KeyNotesX-TIKA:Parsed-ByWhich parser parsed a given fileX-TIKA:Parsed-By-Full-SetAll the parsers that touched a given file and its embedded files.  This key is reported in the metadata object of the primary fileX-TIKA:parse_time_millisMilliseconds it took to parse a given file and its embedded files.X-TIKA:EXCEPTION:container_exceptionX-TIKA:EXCEPTION:embedded_exceptionIf there's parse exception while parsing an embedded file, the stack trace is stored with this key.",../data/confluence_exports/TIKA/Metadata-Overview_235835139.html
TIKA : Metadata Overview Tika General,"KeyNotesContent-TypeThis is the file's mime type as identified by Tika. Example:application/pdfX-TIKA:digest:MD5If you've configured digests, they are returned with a key of the form X-TIKA:digest:ALGORITHM.resourceNameFile nameContent-LengthWhen available, the number of bytes in a streamX-TIKA:contentThis is the text that is extracted from the filesX-TIKA:content_handlerThis is the content handler that was used for handling the text (e.g. Text, XHTML, etc.)X-TIKA:embedded_resource_pathX-TIKA:embedded_depthX-TIKA:encryptedIf a parser throws an EncryptedDocumentException, the parser also sets this value to true in the metadata.tika:file_extFile extension",../data/confluence_exports/TIKA/Metadata-Overview_235835139.html
TIKA : Metadata Overview Dublin Core,KeyNotesdc:creatordcterms:createddcterms:modifieddc:rightsdc:contributordc:titledc:relationdc:typedc:identifierdc:publisherdc:descriptiondc:subjectdc:languagedc:format,../data/confluence_exports/TIKA/Metadata-Overview_235835139.html
TIKA : Metadata Overview XMP (eXtensible Metadata Platform),KeyNotesxmp:Aboutxmp:CreateDatexmp:CreatorToolxmp:Identifierxmp:Labelxmp:MetadataDatexmp:ModifyDatexmp:RatingxmpDM:albumxmpDM:albumArtistxmpDM:artistxmpDM:audioChannelTypexmpDM:audioCompressorxmpDM:audioSampleRatexmpDM:audioSampleTypexmpDM:compilationxmpDM:composerxmpDM:copyrightxmpDM:discNumberxmpDM:durationxmpDM:genrexmpDM:logCommentxmpDM:releaseDatexmpDM:trackNumberxmpDM:videoCompressorxmpMM:DerivedFrom:DocumentIDxmpMM:DerivedFrom:InstanceIDxmpMM:DocumentIDxmpMM:History:ActionxmpMM:History:InstanceIDxmpMM:History:SoftwareAgentxmpMM:History:WhenxmpTPg:NPages,../data/confluence_exports/TIKA/Metadata-Overview_235835139.html
TIKA : Metadata Overview PDF Metadata,"PDF metadata is typically stored via two mechanisms, one is the ""native"" PDFdocinfometadata object and the other is via XMP.  For cases where there may be the same key, e.g. ""created,"" in both the docinfo and the XMP, Tika reports the information in the XMP.  In this case, the created date in the XMP would be reported asdcterms:created. Some users want to extract the literaldocinfoinformation (irrespective of the XMP), and for that Tika prefixes keys withpdf:docinfo. Note that XMP metadata may have custom keys, and some PDFs store custom metadata in the docinfo. PDF is a ""page-based"" file format, and the number of pages is stored inxmpTPg:NPages.  KeyNotesaccess_permission:assemble_documentaccess_permission:can_modifyaccess_permission:can_printaccess_permission:can_print_degradedaccess_permission:extract_contentaccess_permission:extract_for_accessibilityaccess_permission:fill_in_formaccess_permission:modify_annotationspdf:actionTriggerpdf:annotationSubtypespdf:annotationTypespdf:charsPerPagepdf:docinfo:custom:*Custom metadata stored in the docinfo dictionary, e.g.pdf:docinfo:custom:_dlc_policyIdpdf:docinfo:createdpdf:docinfo:creatorpdf:docinfo:creator_toolpdf:docinfo:keywordspdf:docinfo:modifiedpdf:docinfo:producerpdf:docinfo:titlepdf:docinfo:trappedpdf:has3Dpdf:hasAcroFormFieldspdf:hasCollectionpdf:hasMarkedContentpdf:hasXFApdf:hasXMPpdf:PDFExtensionVersionpdf:PDFVersionpdf:producerpdf:unmappedUnicodeCharsPerPagepdfa:PDFVersionpdfaid:conformancepdfaid:partpdfuaid:partpdfvt:modifiedpdfvt:versionpdfx:conformancepdfx:versionpdfxid:version",../data/confluence_exports/TIKA/Metadata-Overview_235835139.html
TIKA : Metadata Overview Microsoft Office Files,KeyNotesembeddedRelationshipId,../data/confluence_exports/TIKA/Metadata-Overview_235835139.html
TIKA : Metadata Overview RTF Files,"KeyNotesrtf_meta:emb_app_versionrtf_meta:emb_classrtf_meta:thumbnailrtf_pict:*metadata around embedded images in RTF. A few examples include: rtf_pict:borderLeftColor, rtf_pict:borderRightColor, rtf_pict:borderTopColor, rtf_pict:dhgt, rtf_pict:dxHeightHR, rtf_pict:dxTextLeft, rtf_pict:dxTextRight, rtf_pict:dxWidthHR",../data/confluence_exports/TIKA/Metadata-Overview_235835139.html
TIKA : Metadata Overview Tiff Files,KeyNotestiff:ImageWidthtiff:ImageLengthtiff:BitsPerSample,../data/confluence_exports/TIKA/Metadata-Overview_235835139.html
TIKA : Metadata Overview Exif Keys,KeyNotesExif SubIFD:Metering ModeExif SubIFD:White Balance ModeExif SubIFD:Scene Capture TypeExif SubIFD:Exposure Mode,../data/confluence_exports/TIKA/Metadata-Overview_235835139.html
TIKA : Metadata Overview Text/Html-based Files,KeyNotesContent-Encoding,../data/confluence_exports/TIKA/Metadata-Overview_235835139.html
TIKA : Metadata Overview Tika Eval,"To get this metadata, you need to have thetika-eval-core jaron your class path. KeyNotestika-eval:numTokensNumber of tokens (words) in the extracted text.tika-eval:numUniqueTokensNumber of unique tokens (words), when used with the numTokens, useful for measuring vocabulary richness/repetitiontika-eval:numAlphaTokensNumber of alphabetic tokenstika-eval:numUniqueAlphaTokensNumber of unique alphabetic tokenstika-eval:langLanguage automatically detected by Tika's modified OpenNLP language detectortika-eval:langConfidenceConfidence of that languagetika-eval:oovOut of vocabulary statistic.  The tika-eval module has lists of the top 20k most common words for each of 120+ languages.  Based on the detected languages, the number of ""common tokens"" is divided by the number of alphabetic tokens, we then subtract this value from 1 to calculate the percentage of words that are not in the top 20k ""common words"" for the identified language.  This is very helpful for junk detection (identifying when text extraction failed) and for comparing the output of two parsers.  SeePopat's paper.",../data/confluence_exports/TIKA/Metadata-Overview_235835139.html
TIKA : Metadata Overview Siegfried Detector,"To extract Siegfried detection information, you have to haveSiegfried commandline applicationinstalled (and callable as ""sf"" on the commandline) and you need to add thetika-detector-siegfried jarto your class path. KeyNotessf:pronom:mimesf:pronom:formatsf:pronom:versionsf:pronom:idsf:pronom:basissf:errors",../data/confluence_exports/TIKA/Metadata-Overview_235835139.html
TIKA : tika-pipes and Docker Example using the tika-fetcher-http module with the /tika endpoint,"Docker file:Dockerfile(based onhttps://github.com/LogicalSpark/docker-tikaserver/tree/2.0.0) tika-config file:tika-config.xml Start tika-server with Docker:docker run --name my-tika-container -v $(pwd):/config -p 9998:9998 my-tika -c ./config/tika-config.xml CURL a request:curl -X PUThttp://localhost:9998/tika-H ""fetcherName: http"" -H ""fetchKey:http://tika.apache.org""",../data/confluence_exports/TIKA/tika-pipes-and-Docker_186877907.html
TIKA : UsingGit JIRA contribution,"Grab the user's patch file, and then apply it locally. Before doing so, create the feature branch by following step 1 from the Developer workflow section. Then, assuming the patch file is named according to the conventions from step 6 in the Developer workflow section, perform the following steps (make sure you are on the TIKA-xxx feature branch): git apply < TIKA-xxx.<your last name>.<yyMMdd>.patch.txt2. Steps 3-4 from Developer workflow guide. 3. Final 2 steps from Developer workflow guide (aka the merge of feature branch step and then the push to origin trunk)",../data/confluence_exports/TIKA/UsingGit_109454103.html
TIKA : UsingGit Github contribution,"If they are contributing using Github they are submitting a Pull Request for review you can easily merge their pull request into your local feature branch using Git. Assuming the Github user is ""user01"" and the branch they have created is ""fix-tika-stuff"" (you can find this information in the Tika Pull request, for example inthis pull request #65, the username issmahdaand branch name isTIKA-1803), you can merge it into your local feature branch like so (again, make sure you are on the local feature branch for your issue, TIKA-xxx first by typinggit checkout TIKA-xxxif you aren't already): git pullhttps://github.com/user01/tika.gitfix-tika-stuff(will find the remote github branch and merge locally) 2. test and make sure PR works, etc. Fix conflicts, etc. 3. Final 2 steps from Developer workflow guide (aka the merge of feature branch step and then the push to origin trunk)",../data/confluence_exports/TIKA/UsingGit_109454103.html
TIKA : FetchEmitTuple HandlerConfig,"Users may specify limitations and format of extracted text with theHandlerConfig: {
    ""fetcher"": ""fsf"",
    ""fetchKey"": ""hello_world.pdf"",
    ""emitter"": ""fse"",
    ""emitKey"": ""hello_world.pdf.json"",
    ""handlerConfig"": {
        ""maxEmbeddedResources"": 10,
        ""type"": ""xml"",
        ""writeLimit"": 10000
    }
}  Users may inject external metadata into the output of the extract.  Users specify this in themetadataelement with key/value or key/values: {
    ""emitKey"": ""emitKey1"",
    ""emitter"": ""my_emitter"",
    ""fetchKey"": ""fetchKey1"",
    ""fetcher"": ""my_fetcher"",
    ""handlerConfig"": {
        ""maxEmbeddedResources"": 10,
        ""type"": ""xml"",
        ""writeLimit"": 10000
    },
    ""id"": ""myTaskId"",
    ""metadata"": {
        ""m1"": [
            ""v1"",
            ""v2""
        ],
        ""m2"": ""v3:
    }
}",../data/confluence_exports/TIKA/FetchEmitTuple_181309049.html
TIKA : Release Process Before the release,Runmvn ossindex:audit -Dossindex.fail=trueto see if there are any vulnerable dependencies. Runmvn versions:display-plugin-updatesandmvn versions:display-dependency-updatesto identify any updates that need to be made. Make sure to run full regression tests after making updates withmvn -Prelease-profile clean verify.,../data/confluence_exports/TIKA/Release-Process_109454070.html
TIKA : Release Process Making a release,"If you've done this before, have all programs installed, all necessary karma granted, and everything configured, steps 1 though 9 will take around an hour (tests take a while to run). If you've never done this before, you will probably run into an issue not covered in this set of instructions. So, it will take longer (maybe a couple hours, maybe a couple days). Either way, please update these instructions with any clarifications you think might help the next release manager. git clonehttps://github.com/apache/tika.gitrelease OR git clone https://github.com/apache/tika.git -b branch_1xAdd to the <scm> section in tika-parent's pom<connection>scm:git:https://github.com/apache/</connection>
    <developerConnection>scm:git:https://github.com/apache/</developerConnection>
    <url>https://github.com/apache/tika</url>Background: for some reason in the 2.x branch these lines are deleted during each release process.  They are not deleted in the 1.x branch.Update CHANGES.txt with release date (Release X.Y.Z - MM/dd/yyyy (date format matters!) and (if needed) add additional changelog entries.On the Tika JIRA, create versions X.Y.Z, X.(Y+1), and X.(Y+2), if not already done.Move any lingering unresolved issues from X.Y.Z to X.(Y+1).Query open issues: e.g. ""project = Tika AND resolution = unresolved AND fixVersion = 1.16""Upper right, select bulk changeSelect all, modify to fixVersion = X.(Y+1)At the bottom of the screen, unselect the email updatesgit add CHANGES.txt && git commit -m ""Update CHANGES.txt for X.Y.Z release."" && git push originRunmvn apache-rat:checkand fix any missing license headers / add excluded for properly non-licensed failssetenv MAVEN_OPTS ""-Xms128m -Xmx256m""(orexport MAVEN_OPTS=""-Xms128m -Xmx256m""if you are using bash)mvn release:prepareMake sure all the release numbers are set to X.Y.Z (from X.Y.Z-SNAPSHOT)When prompted, the scm tag name should be X.Y.Z (version number). If this is a release candidate, the tag should be X.Y.Z-rcN.Watch for aBUILD SUCCESSFULmessageMake sure you havegnupg(or variant) installed (seehttps://www.apache.org/dev/openpgp.html)mvn release:performMake sure you can log on tohttps://repository.apache.org/Make sure you have the following entry in your~/.m2/settings.xmlfile:https://maven.apache.org/settings.html#Servers. The server ID should beapache.releases.https.Head over tohttps://repository.apache.org/(Apache's Nexus Server)Login with your ASF username and passwordMake sure there is only one repo from your build and that it contains everything. A transmission failure can lead to multiple repositories, none of which are valid... even whenmvn release:performallegesSUCCESS""Close"" the staging repository with messageApache Tika X.Y.Z release candidate #N.Copy the URL from the closed staged repository for Tika (you'll need this later)Update CHANGES.txt with a ""Release X.Y.Z + 1 - Current Development"" section.Check the contents oftarget/checkout/target/X.Y.Z/*intohttps://dist.apache.org/repos/dist/dev/tika/(tika-X.Y.Z-src.zip{.asc|.sha512},tika-app-X.Y.Z.jar{.asc|.sha512}andtika-server-X.Y.Z.jar{.asc|.sha512}, andCHANGES.txt.svn co https://dist.apache.org/repos/dist/dev/tika/ dist.dev.tikacd dist.dev.tikamkdir X.Y.Zrm -r X.Y.Z-1(remove the former dist dev version folder)cp ../release/target/checkout/target/X.Y.Z/* X.Y.Z(copy the artifacts into the new dist.dev.tika/X.Y.Z directory)cd X.Y.Zmv CHANGES.txt CHANGES-X.Y.Z.txtmake sure the KEYS files is atdist.dev.tika/KEYS-- if it isn't there already copy theKEYSfile to go along with the jars and other release artifacts; add your key toKEYSif you haven't alreadysvn add X.Y.Zsvn commit -m ""Add Tika X.Y.Z RC#N artifacts.""See the file intarget/checkout/target/vote.txt- it contains the contents of the release email you should send (with subject[VOTE] Apache Tika X.Y.Z Release Candidate Ntodev@tika.apache.organd touser@tika.apache.org). Ensure all URLs are correct and update the email to include links to the artifacts from #8 (including .sha512, .asc and CHANGES.txt)If the VOTE passes:Send[RESULT] [VOTE]tallying the VOTE todev@tika.apache.organduser@tika.apache.orgHead over to Nexus from #6 and ""Release"" the repository with message Apache Tika X.Y.Z release (and make sure the box is checked to auto drop).svn rmhttps://dist.apache.org/repos/dist/release/tika/X.Y.Z-1*(delete the previous release)svn mvhttps://dist.apache.org/repos/dist/dev/tika/X.Y.Z/*https://dist.apache.org/repos/dist/release/tika/X.Y.Z/*git clonehttps://github.com/apache/tika.gitX.Y.Z-rcNAdd final tag for the release and delete the X.Y.Z-rcN tag:Look up commit for X.Y.Z-rcN commit:git rev-list \-n 1 1.25-rc2; let's say '0090ebac8e4ff4083a9c0c5d3dc55f545ad6f951'git tag -a X.Y.Z 0090eba -m ""Tagging X.Y.Z release""git tag -d X.Y.Z-rcNgit push --tagsgit push origin :refs/tags/X.Y.Z-rcNIf VOTE fails:Drop the new Tika staging repository onhttps://repository.apache.orgMake an entry in CHANGES.txt which documents the subsequent changes between release candidates. This is extremely useful if the candidates are cut from master and master has moved on since the release candidate was originally cut.Go back to #1.Update the version of any unreleased modules (like tika-dotnet).Update Tika site.Update parent version number in the site pom.xml file.Update the Documentation section of src/site/site.xml. Make sure to add thecollapsetag for the now old version.Update the ""Full List of Supported Formats"" section of the formats page of version X.Y.Z using TIKA-411; e.g.java -jar release-X.Y.Z/tika-app/target/tika-app-1.15.jar --list-parser-details-apt >> src/site/apt/1.15/formats.aptUpdate the version towards the top of X.Y.Z/formats.apt:src/site/apt/1.15/formats.aptUpdate the version number in the dependency examples and the commandline options in the new X.Y.Z src/site/apt/X.Y.Z/gettingstarted.apt page.Update src/site/resources/doap.rdf with the new release.Create a new example and formats page (truncated to where step 15.3 should go next time) for the next version (but don't list it in the sidebar).Update src/site/apt/index.apt.vm to include the announcement and change link to previous release CHANGES.txt file (fromhttps://dist.apache.org/repos/dist/release/tika/X.Y.Z/CHANGES-X.(Y-1).txttohttps://archive.apache.org/dist/tika/X.(Y-1)/CHANGES-X.(Y-1).txt).Update src/site/apt/X.Y.Z/index.apt.Generate the contributor list withhttps://github.com/chrismattmann/apachestuff/blob/master/extract-tika-contribsUpdate the link to the JIRA query (e.g.https://s.apache.org/XowY) via Apache's shortening servicehttps://s.apache.orgGenerate the issues list by runninghttps://github.com/chrismattmann/apachestuff/blob/master/extract-tika-issues.pyCHANGES-X.Y.Z.txt output.txt X.Y.ZGenerate the Javadoc.unzip tika-X.Y.Z-src.zipmvn javadoc:aggregatemkdir .../tika-site/publish/X.Y.Zmv target/site/apidocs .../tika-site/publish/X.Y.Z/apiUpdate tika-server documentation:In the unzipped src release,cd tika-servermvn -Pserver install(in the tika-server module to build the miredot documentation)mkdir .../tika-site/src/site/resources/X.Y.Zrm -r tika-site/src/site/resources/X.Y.Z/miredot(if it exists?!)cp -r tika-server/target/miredot .../tika/site/src/site/resources/X.Y.ZUpdate the downloads section.Check everything looks good withmvn site:run.mvn clean installsvn add <any files changed for site>(probably:src/site/apt/X.Y.Zandsrc/site/resources/X.Y.Z/miredot) &&svn commit -m ""Update website for X.Y.Z release.""On the Tika JIRA, ""release"" version X.Y.Z and update any (new) straggler X.Y.Z issues to X.(Y+1).Send announcements touser@tika.apache.org,dev@tika.apache.org, andannounce@apache.orglists (from your Apache email) - seehttps://s.apache.org/ytaodfor a sample. If editing and resending an earlier email, make sure that you use text format and/or make sure that editing the links updates the underlying links and not just the anchor text.Log on tohttps://reporter.apache.org/addrelease.html?tikaand add the release data (version and date) to the database NOTE:if anything goes wrong duringmvn:release-prepareormvn release:perform: mvn release:cleanUndo any commits that changed the version numberDelete the X.Y.Z-rcN tag:gitpush --delete origin X.Y.Z-rcN && git tag --delete X.Y.Z-rcN After the release, consider making a PR to make the upgrade in Apache Solr. SeeUpgradingTikaInSolr. After the release, announce any CVEs that were fixed and updatesecurity.aptand republish the site.",../data/confluence_exports/TIKA/Release-Process_109454070.html
TIKA : Release Process Release the Docker Image,Release Process for tika-docker,../data/confluence_exports/TIKA/Release-Process_109454070.html
TIKA : Release Process Release the Helm Chart,Release Process for tika-helm,../data/confluence_exports/TIKA/Release-Process_109454070.html
TIKA : GrobidQuantitiesParser Installation,"Steps to install: Install Grobid Quantities by following the steps fromgithuband make sure the quantity model is trained as per the instructions provided  After installing and training the model, start the REST server using the following command",../data/confluence_exports/TIKA/GrobidQuantitiesParser_109454056.html
TIKA : GrobidQuantitiesParser Start Grobid Quantities Server,$ mvn -Dmaven.test.skip=true jetty:run-war  The server starts by default on port number 8080 and the server can be seen running onhttp://127.0.0.1:8080.,../data/confluence_exports/TIKA/GrobidQuantitiesParser_109454056.html
TIKA : GrobidQuantitiesParser Preparing resources for Grobid Quantities in Tika-App,"Activate Named Entity ParserIn order to use any of theNamedEntityParserimplementations in Tika , the parser responsible for handling the name recognition task needs to be enabled. This can be done with Tika Config XML file, as follows<?xml version=""1.0"" encoding=""UTF-8""?>
 <properties>
     <parsers>
         <parser class=""org.apache.tika.parser.ner.NamedEntityParser"">
             <mime>text/plain</mime>
             <mime>text/html</mime>
             <mime>application/xhtml+xml</mime>
         </parser>
     </parsers>
 </properties>This configuration has to be supplied in the later phases, so store it as 'tika-config.xml'.2.SupplyGrobidServer.properties fileIt is imperative that Tika should know on what host you are running thegrobid-quantities-server. By default Tika will assume your server runs on port 8080. In order to specify any other port, you must supply aGrobidServer.properties file. SampleGrobidServer.properties file. My file looks like the following:grobid.server.url=http://localhost:8080
grobid.endpoint.text=/processQuantityTextIn a nutshell#Create a directory for keeping the config and properties file.
 export GROBID_QUANTITIES_RES=$HOME/GrobidQuantitiesRest-resources
 mkdir -p $GROBID_QUANTITIES_RES
 cd $GROBID_QUANTITIES_RES
 #config file must be stored in this directory
 pwd

 export PATH_PREFIX=""$GROBID_QUANTITIES_RES/org/apache/tika/parser/ner/grobid""
 mkdir -p $PATH_PREFIX
 #create and edit the properties file
 vim $PATH_PREFIX/GrobidServer.properties",../data/confluence_exports/TIKA/GrobidQuantitiesParser_109454056.html
TIKA : GrobidQuantitiesParser Running Grobid Quantities with Tika,"export TIKA_APP={your/path/to/tika-app}/target/tika-app-1.13-SNAPSHOT.jar

#set the system property to use GrobidNERecogniser class
java -Dner.impl.class=org.apache.tika.parser.ner.grobid.GrobidNERecogniser -classpath $GROBID_QUANTITIES_RES:$TIKA_APP org.apache.tika.cli.TikaCLI --config=$GROBID_QUANTITIES_RES/tika-config.xml -m  https://en.wikipedia.org/wiki/Time",../data/confluence_exports/TIKA/GrobidQuantitiesParser_109454056.html
TIKA : TikaBatchUsage TikaBatchvia tika-app-X.Y.jar,"You can see the commandline arguments via the regular ""-help"" commands. There is a separate section at the end for tika-batch options. In the current dev version. Tika-app decides if it is in batch mode based on one of two signals: There are only two arguments and the first one is an existing directory-inputDir or -i is specified in the commandline Once the app knows that it is in batch mode, it converts some of the traditional tika-app commandline arguments for use by org.apache.tika.batch.fs.FSBatchProcessCLI. Some examples: *Most basic (with output to a directory called ""output""): java -jar tika-app.X.Y.jar <inputDirectory> <outputDirectory> *Specify input and output directories: java -jar tika-app.X.Y.jar -i /mydata/src/dir -o /mydata/output/dir *Set the number of file consumer threads: java -jar tika-app.X.Y.jar -numConsumers 10 -i <inputDirectory> -o <outputDirectory> *Output text instead of xml java -jar tika-app.X.Y.jar -t -i <inputDirectory> -o <outputDirectory> *Use the RecursiveParserWrapper and store text for each document:java -jar tika-app.X.Y.jar -J -t -i <inputDirectory> -o <outputDirectory> *Customize the behavior of Tika through the tika-config.xml configuration file:java -jar tika-app.X.Y.jar -c my-custom-tika-config.xml -J -t -i <inputDirectory> -o <outputDirectory> *Specify jvm args to be used by the child process (prepend a ""J"" to the regular args): java -jar tika-app.X.Y.jar -JXmx2g -JDlog4j.configuration=log4j.xml -i <inputDirectory> -o <outputDirectory> *Commandline to generate output files for tika-eval...only process those files listed in pdfs_random_50000.csv: java -Dlog4j.configuration=file:log4j_driver.xml -cp ""bin/*"" org.apache.tika.cli.TikaCLI -JXX:-OmitStackTraceInFastThrow -JXmx5g -JDlog4j.configuration=file:log4j.xml -bc tika-batch-config-basic-test.xml -i <input_directory> -o <output_directory> -fileList pdfs_random_50000.csv",../data/confluence_exports/TIKA/TikaBatchUsage_109454083.html
TIKA : TikaBatchUsage Example logging config files,tika-batch-sh.zip,../data/confluence_exports/TIKA/TikaBatchUsage_109454083.html
TIKA : TikaBatchUsage Some notes,"*The watchdog process will restart the child process unless the child process exits with a ""do not restart value""=254. If you want to kill all processing, make sure to kill the parent process and then the child process. *Make sure to add -JXX:-OmitStackTraceInFastThrow to the child process's commandline arguments so that Java doesn't swallow your stack traces.",../data/confluence_exports/TIKA/TikaBatchUsage_109454083.html
TIKA : TikaBatchUsage TikaBatchServer,Module not yet implemented...want to contribute? This would require hardening the server and creating an example client to be used withinTikaBatchFS framework.,../data/confluence_exports/TIKA/TikaBatchUsage_109454083.html
TIKA : TikaBatchUsage TikaBatchHadoop,Module not yet implemented within Tika project...want to contribute? SeeTikaInHadoop.,../data/confluence_exports/TIKA/TikaBatchUsage_109454083.html
TIKA : TikaAndVision Tika and Tensorflow Image Recognition,"Tika has two different ways of bindings to Tensorflow: Using Commandline Invocation -- Recommended for quick testing, not for production useUsing REST API -- Recommended for production use",../data/confluence_exports/TIKA/TikaAndVision_148640953.html
TIKA : TikaAndVision 1. Tensorflow Using Commandline Invocation,Pros of this approach: This parser is easy to setup and test Cons: Very inefficient/slow as it loads and unloads model for every parse call,../data/confluence_exports/TIKA/TikaAndVision_148640953.html
TIKA : TikaAndVision Step 1. Install the dependencies,"To install tensorflow, follow the instructions onthe official site herefor your environment.Unless you know what you are doing, you are recommended to follow pip installation. Then clone the repositorytensorflow/modelsor download thezip file. git clonehttps://github.com/tensorflow/models.git Add 'models/slim' folder to the environment variable, PYTHONPATH. $ export PYTHONPATH=""$PYTHONPATH:/path/to/models/slim"" To test the readiness of your environment : $ python -c 'import tensorflow, numpy, dataset; print(""OK"")' If the above command prints the message ""OK"", then the requirements are satisfied.",../data/confluence_exports/TIKA/TikaAndVision_148640953.html
TIKA : TikaAndVision Step 2. Create a Tika-Config XML to enable Tensorflow parser.,"A sample config can be found in Tika source code attika-parsers/src/test/resources/org/apache/tika/parser/recognition/tika-config-tflow.xml Here is an example: Toggle line numbers 1<properties>2<parsers>3<parserclass=""org.apache.tika.parser.recognition.ObjectRecognitionParser"">4<mime>image/jpeg</mime>5<params>6<paramname=""topN""type=""int"">2</param>7<paramname=""minConfidence""type=""double"">0.015</param>8<paramname=""class""type=""string"">org.apache.tika.parser.recognition.tf.TensorflowImageRecParser</param>9</params>10</parser>11</parsers>12</properties> Description of parameters :  Param NameTypeMeaningRangeExampletopNintNumber of object names to outputa non-zero positive integer1 to receive top 1 object nameminConfidencedoubleMinimum confidence required to output the name of detected objects[0.0 to 1.0] inclusive0.9 for outputting object names iff at least 90% confidentclassstringClass that implements object recognition functionalityconstant stringorg.apache.tika.parser.recognition.tf.TensorflowImageRecParser",../data/confluence_exports/TIKA/TikaAndVision_148640953.html
TIKA : TikaAndVision Step 3: Demo,"To use the vision capability via Tensorflow, just supply the above configuration to Tika. For example, to use in Tika App (Assuming you havetika-appJAR and it is ready to run):  $ java -jar tika-app/target/tika-app-1.15-SNAPSHOT.jar \
         --config=tika-parsers/src/test/resources/org/apache/tika/parser/recognition/tika-config-tflow.xml \
         https://upload.wikimedia.org/wikipedia/commons/f/f6/Working_Dogs%2C_Handlers_Share_Special_Bond_DVIDS124942.jpg The input image is:  And, the top 2 detections are: Toggle line numbers 1...2<metaname=""OBJECT""content=""German shepherd, German shepherd dog, German police dog, alsatian (0.78435)""/>3<metaname=""OBJECT""content=""military uniform (0.06694)""/>4...",../data/confluence_exports/TIKA/TikaAndVision_148640953.html
TIKA : TikaAndVision 2. Tensorflow Using REST Server,"This is the recommended way for utilizing visual recognition capability of Tika.This approach uses Tensorflow over REST API.To get this working, we are going to start a python flask based REST API server and tell tika to connect to it.All these dependencies and setup complexities are isolated in docker image. Requirements : Docker -- VisitDocker.comand install latest version of Docker. (Note: tested on docker v17.03.1)",../data/confluence_exports/TIKA/TikaAndVision_148640953.html
TIKA : TikaAndVision Step 1. Setup REST Server,You can either start the REST server in an isolated docker container or natively on the host that runs tensorflow.,../data/confluence_exports/TIKA/TikaAndVision_148640953.html
TIKA : TikaAndVision a. Using docker (Recommended),"Toggle line numbers 1git clone https://github.com/USCDataScience/tika-dockers.git &&cdtika-dockers2docker build -f InceptionRestDockerfile -t uscdatascience/inception-rest-tika .3docker run -p 8764:8764 -it uscdatascience/inception-rest-tika Once it is done, test the setup by visitinghttp://localhost:8764/inception/v4/classify/image?topn=2&min_confidence=0.03&url=https://upload.wikimedia.org/wikipedia/commons/f/f6/Working_Dogs%2C_Handlers_Share_Special_Bond_DVIDS124942.jpgin your web browser. Sample output from API: {
   ""confidence"":[
      0.7843596339225769,
      0.06694009155035019
   ],
   ""classnames"":[
      ""German shepherd, German shepherd dog, German police dog, alsatian"",
      ""military uniform""
   ],
   ""classids"":[
      236,
      653
   ],
   ""time"":{
      ""read"":7403,
      ""units"":""ms"",
      ""classification"":470
   }
} Note: MAC USERS: If you are using an older version, say, 'Docker toolbox' instead of the newer 'Docker for Mac', you need to add port forwarding rules in your Virtual Box default machine. Open the Virtual Box Manager.Select your Docker Machine Virtual Box image.Open Settings -> Network -> Advanced -> Port Forwarding.Add an appname,Host IP 127.0.0.1 and set both ports to 8764.",../data/confluence_exports/TIKA/TikaAndVision_148640953.html
TIKA : TikaAndVision b. Without Using docker,"If you chose to setup REST server without a docker container, you are free to manually install all the required tools specified in thedocker file. Note: docker file has setup instructions for Ubuntu, you will have to transform those commands for your environment.  Toggle line numbers 1python tika-parsers/src/main/resources/org/apache/tika/parser/recognition/tf/inceptionapi.py  --port 8764",../data/confluence_exports/TIKA/TikaAndVision_148640953.html
TIKA : TikaAndVision Step 2. Create a Tika-Config XML to enable Tensorflow parser.,"A sample config can be found in Tika source code attika-parsers/src/test/resources/org/apache/tika/parser/recognition/tika-config-tflow-rest.xml Here is an example: <properties>
    <parsers>
        <parser class=""org.apache.tika.parser.recognition.ObjectRecognitionParser"">
            <mime>image/jpeg</mime>
            <params>
                <param name=""topN"" type=""int"">2</param>
                <param name=""minConfidence"" type=""double"">0.015</param>
                <param name=""class"" type=""string"">org.apache.tika.parser.recognition.tf.TensorflowRESTRecogniser</param>
            </params>
        </parser>
    </parsers>
</properties> Description of parameters :  Param NameTypeMeaningRangeExampletopNintNumber of object names to outputa non-zero positive integer1 to receive top 1 object nameminConfidencedoubleMinimum confidence required to output the name of detected objects[0.0 to 1.0] inclusive0.9 for outputting object names iff at least 90% confidentclassstringName of class that Implements Object recognition Contractconstant stringorg.apache.tika.parser.recognition.tf.TensorflowRESTRecogniserhealthUriuriHTTP URL to check availability of API serviceany HTTP URL that gets 200 status code when availablehttp://localhost:8764/inception/v4/pingapiUriuriHTTP URL to POST image dataany HTTP URL that returns data in the JSON format as shown in the sample API outputhttp://localhost:8764/inception/v4/classify/image?topk=10",../data/confluence_exports/TIKA/TikaAndVision_148640953.html
TIKA : TikaAndVision Step 3. Demo,"This demo is same as the Commandline Invocation approach, but this is faster and efficient  $ java -jar tika-app/target/tika-app-1.15-SNAPSHOT.jar \
    --config=tika-parsers/src/test/resources/org/apache/tika/parser/recognition/tika-config-tflow-rest.xml \
    https://upload.wikimedia.org/wikipedia/commons/f/f6/Working_Dogs%2C_Handlers_Share_Special_Bond_DVIDS124942.jpg The input image is:  And, the top 2 detections are: Toggle line numbers 1...2<metaname=""OBJECT""content=""German shepherd, German shepherd dog, German police dog, alsatian (0.78435)""/>3<metaname=""OBJECT""content=""military uniform (0.06694)""/>4...",../data/confluence_exports/TIKA/TikaAndVision_148640953.html
"TIKA : TikaAndVision Changing the default topN, API port or URL","To change the defaults, update the parameters in config XML file accordingly Here is an example scenario: Run REST API on port 3030, and get top 4 object names if the confidence is above 10%. You may also change host to something else than 'localhost' if required. Example Config File <properties>
    <parsers>
        <parser class=""org.apache.tika.parser.recognition.ObjectRecognitionParser"">
            <mime>image/jpeg</mime>
            <params>
                <param name=""topN"" type=""int"">4</param>
                <param name=""minConfidence"" type=""double"">0.1</param>
                <param name=""class"" type=""string"">org.apache.tika.parser.recognition.tf.TensorflowRESTRecogniser</param>
                <param name=""healthUri"" type=""uri"">http://localhost:3030/inception/v4/ping</param>
                <param name=""apiUri"" type=""uri"">http://localhost:3030/inception/v4/classify/image?topk=4</param>
            </params>
        </parser>
    </parsers>
</properties> To Start the service on port 3030: Using Docker: docker run -it -p 3030:8764 uscdatascience/inception-rest-tika Without Using Docker: python tika-parsers/src/main/resources/org/apache/tika/parser/recognition/tf/inceptionapi.py  --port 3030",../data/confluence_exports/TIKA/TikaAndVision_148640953.html
TIKA : TikaAndVision Questions / Suggestions / Improvements / Feedback ?,"If it was useful, let us know on twitter by mentioning@ApacheTikaIf you have questions, let us know byusing Mailing ListsIf you find any bugs,use Jira to report them",../data/confluence_exports/TIKA/TikaAndVision_148640953.html
TIKA : TikaEvalJdbc Basic Profile,"java -cp ""bin/*"" org.apache.tika.eval.TikaEvalCLI Profile -jdbc ""jdbc:postgresql:tika_eval?user=user&password=superSecret"" -extracts extracts",../data/confluence_exports/TIKA/TikaEvalJdbc_109454088.html
TIKA : TikaEvalJdbc Basic Profile with Custom Table Prefixes,"Now, let's say you want to profile 3 separate runs of OCR with different settings, sayquality=1.0,quality=0.5andquality=0.1. You'll need to prefix your tables with a different prefix for each run.  java -cp ""bin/*"" org.apache.tika.eval.TikaEvalCLI Profile -jdbc ""jdbc:postgresql:tika_eval?user=user&password=superSecret"" -extracts extracts10 -tablePrefix o10  java -cp ""bin/*"" org.apache.tika.eval.TikaEvalCLI Profile -jdbc ""jdbc:postgresql:tika_eval?user=user&password=superSecret"" -extracts extracts05 -tablePrefix o05  java -cp ""bin/*"" org.apache.tika.eval.TikaEvalCLI Profile -jdbc ""jdbc:postgresql:tika_eval?user=user&password=superSecret"" -extracts extracts01 -tablePrefix o01  This will leave you with 24 tables, 7 for each run and then 3 reference tables.  Then, let's say you want to compare the number of ""common tokens"" in each table for each container file. Unfortunately, the ""ids"" are uniqueper run, so you can really only focus on the container files, and you must join them by the file path.  So, step 1 is to build indices on the file path:  create unique index path01_idx on o01_containers(file_path);
    create unique index path05_idx on o05_containers(file_path);
    create unique index path10_idx on o10_containers(file_path);  Then, join on the file_path to get the paths, and then add in the contents tables for each run:  select o10c.file_path,
        o10ct.num_common_tokens,
        o05ct.num_common_tokens,
        o01ct.num_common_tokens

    from o10_containers o10c
        left join o05_containers o05c on o05c.file_path=o10c.file_path
	left join o01_containers o01c on o01c.file_path=o10c.file_path

        left join o10_contents o10ct on o10c.container_id=o10ct.id
        left join o05_contents o05ct on o05c.container_id=o05ct.id
        left join o01_contents o01ct on o01c.container_id=o01ct.id",../data/confluence_exports/TIKA/TikaEvalJdbc_109454088.html
TIKA : TikaServer Windows Service Here is a quick example of how to install:,"Installation bat file:  Run CMD as Admin, navigate to the daemons installation folder and run the install batch file created above.  TikaServer should now be installed as a Windows service and be runnable:",../data/confluence_exports/TIKA/TikaServer-Windows-Service_211884898.html
TIKA : GrobidJournalParser Installing GROBID,"Currently, to install GROBID, it's necessary to start from the source code. We are currently working with the GROBID community to get pre-build binaries into Maven central, which is being tracked withissue #59. For now, a git checkout of head is recommended, as detailed here.  You should be able to install GROBID from a Git checkout such as the below.  cd $HOME/src2.git clonehttps://github.com/kermitt2/grobid.gitnow wait a while, the download is ~600MB3. now build GROBID by typingcd grobid && mvn install  You can verify GROBID works by running its batch runner:  cd $HOME/src/grobid2.mkdir papers && mkdir outand put some PDF paper files in papers. 3.java -Xmx1024m -jar grobid-core/target/grobid-core-0.3.4-SNAPSHOT.one-jar.jar -gH ./grobid-home/ -gP ./grobid-home/config/grobid.properties -dIn ./papers/ -dOut out -exe processFullText  Check theoutdirectory, you should see*.tei.xmlfiles in there.",../data/confluence_exports/TIKA/GrobidJournalParser_109454054.html
TIKA : GrobidJournalParser Start the GROBID Service,"To use GROBID with Tika, you need to start theGROBID Service. To do so, perform the following (note the service will start by default on port 8080, but that can be changed in the Jetty properties by going toGrobid Service's pom.xmland editingline 180).  cd $HOME/src/grobid/grobid-service2.mvn -Dmaven.test.skip=true jetty:run-war  Once the server is started, you're good to proceed!",../data/confluence_exports/TIKA/GrobidJournalParser_109454054.html
TIKA : GrobidJournalParser Running GROBID using Tika-App,"Grab the latest 1.11-SNAPSHOT or later version of Tika-app and run Grobid by following the commands below.  First we need to create theGrobidExtractor.properties file that points to the Grobid REST Service. My file looks like the following:  grobid.server.url=http://localhost:8080  You can downloadGrobidExtractor.propertiesas a sample. Or better yet, you can install the following Github project and then modify theGrobidExtractor.properties file accordingly.  cd $HOME/src && git clonehttps://github.com/chrismattmann/grobidparser-resources.git2. edit$HOME/src/grobidparser-resources/org/apache/tika/parser/journal/GrobidExtractor.properties  Now you can run GROBID via Tika-app with the following command on a sample PDF file.  java -classpath $HOME/src/grobidparser-resources/:tika-app-1.11-SNAPSHOT.jar org.apache.tika.cli.TikaCLI --config=$HOME/src/grobidparser-resources/tika-config.xml -J $HOME/src/grobid/papers/ICSE06.pdf  Which should produce as output (e.g., if piped topython -mjson.toolfor pretty printing):  [
    {
        ""Author"": ""End User Computing Services"",
        ""Company"": ""ACM"",
        ""Content-Length"": ""200435"",
        ""Content-Type"": ""application/pdf"",
        ""Creation-Date"": ""2006-02-15T21:13:58Z"",
        ""Last-Modified"": ""2006-02-15T21:16:01Z"",
        ""Last-Save-Date"": ""2006-02-15T21:16:01Z"",
        ""SourceModified"": ""D:20060215211344"",
        ""X-Parsed-By"": [
            ""org.apache.tika.parser.CompositeParser"",
            ""org.apache.tika.parser.journal.JournalParser""
        ],
        ""X-TIKA:content"": ""<html xmlns=\""http://www.w3.org/1999/xhtml\"">\n<head>\n<meta name=\""access_permission:extract_for_accessibility\"" content=\""true\"" />\n<meta name=\""meta:save-date\"" content=\""2006-02-15T21:16:01Z\"" />\n<meta name=\""grobid:header_Affiliation\"" content=\""1 Jet Propulsion Laboratory California Institute of Technology; 2 Computer Science Department University of Southern California\"" />\n<meta name=\""Content-Length\"" content=\""200435\"" />\n<meta name=\""dcterms:created\"" content=\""2006-02-15T21:13:58Z\"" />\n<meta name=\""Author\"" content=\""End User Computing Services\"" />\n<meta name=\""date\"" content=\""2006-02-15T21:16:01Z\"" />\n<meta name=\""access_permission:can_modify\"" content=\""true\"" />\n<meta name=\""creator\"" content=\""End User Computing Services\"" />\n<meta name=\""access_permission:modify_annotations\"" content=\""true\"" />\n<meta name=\""Creation-Date\"" content=\""2006-02-15T21:13:58Z\"" />\n<meta name=\""grobid:header_Address\"" content=\""Pasadena, CA 91109 USA Los Angeles, CA 90089 USA \"" />\n<meta name=\""meta:author\"" content=\""End User Computing Services\"" />\n<meta name=\""created\"" content=\""Wed Feb 15 13:13:58 PST 2006\"" />\n<meta name=\""access_permission:fill_in_form\"" content=\""true\"" />\n<meta name=\""grobid:header_FullAffiliations\"" content=\""[Affiliation {orgName=Jet Propulsion Laboratory California Institute of Technology , address=Pasadena, CA 91109 USA},Affiliation {orgName=Computer Science Department University of Southern California , address=Los Angeles, CA 90089 USA}[Affiliation {orgName=Jet Propulsion Laboratory California Institute of Technology , address=Pasadena, CA 91109 USA},Affiliation {orgName=Computer Science Department University of Southern California , address=Los Angeles, CA 90089 USA}]\"" />\n<meta name=\""grobid:header_Class\"" content=\""org.apache.tika.metadata.Metadata\"" />\n<meta name=\""dc:format\"" content=\""application/pdf; version=1.4\"" />\n<meta name=\""access_permission:can_print\"" content=\""true\"" />\n<meta name=\""Company\"" content=\""ACM\"" />\n<meta name=\""xmp:CreatorTool\"" content=\""Acrobat PDFMaker 6.0 for Word\"" />\n<meta name=\""resourceName\"" content=\""ICSE06.pdf\"" />\n<meta name=\""Last-Save-Date\"" content=\..snip"",
        ""X-TIKA:parse_time_millis"": ""4302"",
        ""access_permission:assemble_document"": ""true"",
        ""access_permission:can_modify"": ""true"",
        ""access_permission:can_print"": ""true"",
        ""access_permission:can_print_degraded"": ""true"",
        ""access_permission:extract_content"": ""true"",
        ""access_permission:extract_for_accessibility"": ""true"",
        ""access_permission:fill_in_form"": ""true"",
        ""access_permission:modify_annotations"": ""true"",
        ""created"": ""Wed Feb 15 13:13:58 PST 2006"",
        ""creator"": ""End User Computing Services"",
        ""date"": ""2006-02-15T21:16:01Z"",
        ""dc:creator"": ""End User Computing Services"",
        ""dc:format"": ""application/pdf; version=1.4"",
        ""dc:title"": ""Proceedings Template - WORD"",
        ""dcterms:created"": ""2006-02-15T21:13:58Z"",
        ""dcterms:modified"": ""2006-02-15T21:16:01Z"",
        ""grobid:header_Address"": ""Pasadena, CA 91109 USA Los Angeles, CA 90089 USA "",
        ""grobid:header_Affiliation"": ""1 Jet Propulsion Laboratory California Institute of Technology; 2 Computer Science Department University of Southern California"",
        ""grobid:header_Authors"": ""Chris A Mattmann 1,2 Daniel J Crichton 1 Nenad  Medvidovic 2 Steve  Hughes 1 "",
        ""grobid:header_Class"": ""org.apache.tika.metadata.Metadata"",
        ""grobid:header_FullAffiliations"": ""[Affiliation {orgName=Jet Propulsion Laboratory California Institute of Technology , address=Pasadena, CA 91109 USA},Affiliation {orgName=Computer Science Department University of Southern California , address=Los Angeles, CA 90089 USA}[Affiliation {orgName=Jet Propulsion Laboratory California Institute of Technology , address=Pasadena, CA 91109 USA},Affiliation {orgName=Computer Science Department University of Southern California , address=Los Angeles, CA 90089 USA}]"",
        ""grobid:header_Keyword"": ""\""D2 Software Engineering, D211 Domain Specific Architectures\"""",
        ""grobid:header_TEIJSONSource"": ""{\""TEI\"":{\""text\"":{..snip"",
        ""grobid:header_TEIXMLSource"": ""<?xml version=\""1.0\"" encoding=\""UTF-8\""?>\n<?xml-model href=\""file:///Users/mattmann/git/grobid/grobid-home/schemas/rng/Grobid.rng\"" schematypens=\""http://relaxng.org/ns/structure/1.0\""?>\n<TEI xmlns=\""http://www.tei-c.org/ns/1.0\"">\n\t<teiHeader xml:lang=\""en\"">\n\t\t<fileDesc>\n\t\t\t<titleStmt>\n\t\t\t\t<title level=\""a\"" type=\""main\"">A Software Architecture-Based Framework for Highly Distributed and Data Intensive Scientific Applications</title>\n\t\t\t</titleStmt>\n\t\t\t<publicationStmt>\..snip</TEI>\n"",
        ""grobid:header_Title"": ""A Software Architecture-Based Framework for Highly Distributed and Data Intensive Scientific Applications"",
        ""meta:author"": ""End User Computing Services"",
        ""meta:creation-date"": ""2006-02-15T21:13:58Z"",
        ""meta:save-date"": ""2006-02-15T21:16:01Z"",
        ""modified"": ""2006-02-15T21:16:01Z"",
        ""pdf:PDFVersion"": ""1.4"",
        ""pdf:encrypted"": ""false"",
        ""producer"": ""Acrobat Distiller 6.0 (Windows)"",
        ""resourceName"": ""ICSE06.pdf"",
        ""title"": ""Proceedings Template - WORD"",
        ""xmp:CreatorTool"": ""Acrobat PDFMaker 6.0 for Word"",
        ""xmpTPg:NPages"": ""10""
    }
]",../data/confluence_exports/TIKA/GrobidJournalParser_109454054.html
TIKA : GrobidJournalParser Will this work from Tika Server?,"It sure will! When you start Tika Server, use the following command.  java -classpath $HOME/src/grobidparser-resources/:tika-server-1.11-SNAPSHOT.jar org.apache.tika.server.TikaServerCli --config $HOME/src/grobidparser-resources/tika-config.xml  Then, PUT a file to Tika-server like so:  curl -T $HOME/src/grobid/papers/ICSE06.pdf -H ""Content-Disposition: attachment;filename=ICSE06.pdf"" http://localhost:9998/rmeta  Which will output (if e.g., usingpython -mjson.tool):  [
    {
        ""Author"": ""End User Computing Services"",
        ""Company"": ""ACM"",
        ""Content-Type"": ""application/pdf"",
        ""Creation-Date"": ""2006-02-15T21:13:58Z"",
        ""Last-Modified"": ""2006-02-15T21:16:01Z"",
        ""Last-Save-Date"": ""2006-02-15T21:16:01Z"",
        ""SourceModified"": ""D:20060215211344"",
        ""X-Parsed-By"": [
            ""org.apache.tika.parser.CompositeParser"",
            ""org.apache.tika.parser.journal.JournalParser""
        ],
        ""X-TIKA:content"": ""\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProceedings Template - WORD\n\n\nA Software Architecture-Based Framework for Highly \nDistributed and Data Intensive Scientific Applications \n\n \nChris A. Mattmann1, 2        Daniel J. Crichton1        Nenad Medvidovic2        Steve Hughes1 \n\n \n1Jet Propulsion Laboratory \n\nCalifornia Institute of Technology \nPasadena, CA 91109, USA \n\n{dan.crichton,mattmann,steve.hughes}@jpl.nasa.gov \n\n2Computer Science Department \nUniversity of Southern California  \n\nLos Angeles, CA 90089, USA \n{mattmann,neno}@usc.edu \n\n \nABSTRACT \nModern scientific research is increasingly conducted by virtual \ncommunities of scientists distributed around the world. The data \nvolumes created by these communities are extremely large, and \ngrowing rapidly. The management of the resulting highly \ndistributed, virtual data systems is a complex task, characterized \nby a number of formidable technical challenges, many of which \nare of a software engineering nature.  In this paper we describe \nour experience over the past seven years in constructing and \ndeploying OODT, a software framework that supports large, \ndistributed, virtual scientific communities. We outline the key \nsoftware engineering challenges that we faced, and addressed, \nalong the way. We argue that a major contributor to the success of \nOODT was its explicit focus on software architecture. We \ndescribe several large-scale, real-world deployments of OODT, \nand the manner in which OODT helped us to address the domain-\nspecific challenges induced by each deployment.  \n\nCategories and Subject Descriptors \nD.2 Software Engineering, D.2.11 Domain Specific Architectures \n\nKeywords \nOODT, Data Management, Software Architecture. \n\n1. INTRODUCTION ..snip.."",
        ""X-TIKA:parse_time_millis"": ""957"",
        ""access_permission:assemble_document"": ""true"",
        ""access_permission:can_modify"": ""true"",
        ""access_permission:can_print"": ""true"",
        ""access_permission:can_print_degraded"": ""true"",
        ""access_permission:extract_content"": ""true"",
        ""access_permission:extract_for_accessibility"": ""true"",
        ""access_permission:fill_in_form"": ""true"",
        ""access_permission:modify_annotations"": ""true"",
        ""created"": ""Wed Feb 15 13:13:58 PST 2006"",
        ""creator"": ""End User Computing Services"",
        ""date"": ""2006-02-15T21:16:01Z"",
        ""dc:creator"": ""End User Computing Services"",
        ""dc:format"": ""application/pdf; version=1.4"",
        ""dc:title"": ""Proceedings Template - WORD"",
        ""dcterms:created"": ""2006-02-15T21:13:58Z"",
        ""dcterms:modified"": ""2006-02-15T21:16:01Z"",
        ""grobid:header_Address"": ""Pasadena, CA 91109 USA Los Angeles, CA 90089 USA "",
        ""grobid:header_Affiliation"": ""1 Jet Propulsion Laboratory California Institute of Technology; 2 Computer Science Department University of Southern California"",
        ""grobid:header_Authors"": ""Chris A Mattmann 1,2 Daniel J Crichton 1 Nenad  Medvidovic 2 Steve  Hughes 1 "",
        ""grobid:header_Class"": ""org.apache.tika.metadata.Metadata"",
        ""grobid:header_FullAffiliations"": ""[Affiliation {orgName=Jet Propulsion Laboratory California Institute of Technology , address=Pasadena, CA 91109 USA},Affiliation {orgName=Computer Science Department University of Southern California , address=Los Angeles, CA 90089 USA}[Affiliation {orgName=Jet Propulsion Laboratory California Institute of Technology , address=Pasadena, CA 91109 USA},Affiliation {orgName=Computer Science Department University of Southern California , address=Los Angeles, CA 90089 USA}]"",
        ""grobid:header_Keyword"": ""\""D2 Software Engineering, D211 Domain Specific Architectures\"""",
        ""grobid:header_TEIJSONSource"": ""{\""TEI\"":{\""text\"":{\""xml:lang\"":\""en\""},\""teiHeader\"": ..snip"",
        ""grobid:header_TEIXMLSource"": ""<?xml version=\""1.0\"" encoding=\""UTF-8\""?>\n<?xml-model href=\""file:///Users/mattmann/git/grobid/grobid-home/schemas/rng/Grobid.rng\"" schematypens=\""http://relaxng.org/ns/structure/1.0\""?>\n<TEI xmlns=\""http://www.tei-c.org/ns/1.0\"">\n\t<teiHeader xml:lang=\""en\"">\n\t\t<fileDesc>\n\t\t\t<titleStmt>\n\t\t\t\t<title level=\""a\"" type=\""main\"">A Software Architecture-Based Framework for Highly Distributed and Data Intensive Scientific Applications</title>..snip..</TEI>\n"",
        ""grobid:header_Title"": ""A Software Architecture-Based Framework for Highly Distributed and Data Intensive Scientific Applications"",
        ""meta:author"": ""End User Computing Services"",
        ""meta:creation-date"": ""2006-02-15T21:13:58Z"",
        ""meta:save-date"": ""2006-02-15T21:16:01Z"",
        ""modified"": ""2006-02-15T21:16:01Z"",
        ""pdf:PDFVersion"": ""1.4"",
        ""pdf:encrypted"": ""false"",
        ""producer"": ""Acrobat Distiller 6.0 (Windows)"",
        ""resourceName"": ""ICSE06.pdf"",
        ""title"": ""Proceedings Template - WORD"",
        ""xmp:CreatorTool"": ""Acrobat PDFMaker 6.0 for Word"",
        ""xmpTPg:NPages"": ""10""
    }
]",../data/confluence_exports/TIKA/GrobidJournalParser_109454054.html
TIKA : MetadataDiscussion Original Problem,"The original inspiration for this page was a Tika user who wanted to get access to the metadata for every document in an archive (e.g. zip, tar.gz, etc.). A way to get recursive metadata is described in theRecursiveMetadataarticle.",../data/confluence_exports/TIKA/MetadataDiscussion_109454061.html
TIKA : MetadataDiscussion Goals for this Page,"The goals for this page are bigger than the original problem. This page should hold a discussion about how to better meet different metadata needs for the different kinds of documents supported by Tika, and for the different kinds of users supported by Tika.  Since the title of this page isMetadataDiscussion, the page isn't limited to a particular set of goals, only to goals that relate to Tika's metadata handling. The goals for this page should at least include the following:  Identify different kinds of documents and their metadata needsIdentify different kinds of users and their metadata needsIdentify potential metadata solutions, attempting to satisfy the needs of as many document types and user types as is practical   This section attempts to identify document types from the point of view of metadata. This section is not trying to identify specific formats such as zip or tar, instead it is trying to identify categories of document formats that have different metadata needs such as simple, compound, and container.  The document types that have been identified so far are the following:  SimpleStructuredCompoundSimple ContainerContainer with Text",../data/confluence_exports/TIKA/MetadataDiscussion_109454061.html
TIKA : MetadataDiscussion Simple Document,"A simple document is a single document contained in a single file. Some examples of simple documents include text files, xml files as parsed by DcXMLParser, etc. Any metadata associated with a simple document is for the entire document.",../data/confluence_exports/TIKA/MetadataDiscussion_109454061.html
TIKA : MetadataDiscussion Structured Document,"Like simple documents, structured documents are single documents in single files. What makes a structured document different is that the document has internal structure, and there is metadata associated specific parts of a document. For example, a PDF document has an internal structure for representing each page of the PDF, and there may be metadata associated with individual pages instead of with the entire document.",../data/confluence_exports/TIKA/MetadataDiscussion_109454061.html
TIKA : MetadataDiscussion Compound Document,A compound document is logically a single document that is made up of many separate files. Usually the separate files are stored inside of a single container. Examples of compound documents include the following:  .doc (several named streams inside of an OLE2 file).xlsx (several named xml files inside of a zip file),../data/confluence_exports/TIKA/MetadataDiscussion_109454061.html
TIKA : MetadataDiscussion Simple Container,"A simple container is a container file that contains other files. The container itself does not contain text, and instead it contains files that could be any document types identified so far. Examples include zip, tar, tar.gz, tar.bz2, etc.",../data/confluence_exports/TIKA/MetadataDiscussion_109454061.html
TIKA : MetadataDiscussion Container with Text,"Some documents have text of their own and contain other files. Examples include the following:  an email with attachmentsA .doc with embedded spreadsheets   The following users have beed identified so far:  ContentHandlerimplementersTika class usersXHTML usersBulk analysis users  For the most part the users are identified by the API they use Tika through, ranging from low levelContentHandlerusers to users who look at XHTML files generated by Tika.  The bulk analysis users aren't a specific API user, and instead are a reminder for an alternate use case. Some Tika users are using Tika to extract text and metadata from millions of documents, potentially all contained in a single container file. The metadata solution used by Tika should work even with container files that contain unreasonable numbers of files.   There are some good reasons why some seemingly good solutions don't actually work. This section captures solutions that have been rejected and why to help the discussion move on to solutions that will work.",../data/confluence_exports/TIKA/MetadataDiscussion_109454061.html
TIKA : MetadataDiscussion A Naive Non-Solution,"When I first started using Tika, I had the naive dream that I could point theAutoDetectParserat anything and it would automatically find the document boundaries that matter to me and make everything I consider a single document look like the following:  <html xmlns=""http://www.w3.org/1999/xhtml"">
  <head>
    <title>...</title>
    <thismeta>...</thismeta>
    <thatmeta>...</thatmeta>
    <theothermeta>...</theothermeta>
  </head>
  <body>
    ...
  </body>
</html>  It turns out that, for lots of good reasons, the Tika developers decided not to make things work this way. Here is my understanding of why this is the case, and there is a good chance that my understanding is wrong:  The literal XHTML produced by Tika, and similarly theContentHandlerevents generated by Tika, produce XHTML that a user could view with a browser and see something that the average person would consider the extracted text for a document.this means that all tags must be legal XHTMLthis also means that only metadata like ""title"" that a user expects to see when reading a document should appear in the XHTML in a way that a browser would display itunfortunately since documents can be Structured, Compound, Simple Containers, or Containers with Text, short of reading a user's mind there is no way for Tika to normalize all of those documents into the above structure.",../data/confluence_exports/TIKA/MetadataDiscussion_109454061.html
TIKA : MetadataDiscussion A Slightly Less Naive Non-Solution,"This solution is like the first naive solution, except it uses legal XHTML  <html xmlns=""http://www.w3.org/1999/xhtml"">
  <head>
    <title>...</title>
    <meta name=""description"" content=""Example XHTML"" />
    <meta name=""author"" content=""Paul Jakubik the Naive Programmer"" />
  </head>
  <body>
    ...
  </body>
</html>  This solution is better since it is legal XHTML, and since meta tags aren't displayed by a browser. Unfortunately this solution doesn't handle any of the following:  Structured documents with metadata associated with parts of the documentCompound documents with metadata associated with parts of the compound documentSimple containment (no representation of the contained documents and their relationship to the container)Containers with text (same issue)",../data/confluence_exports/TIKA/MetadataDiscussion_109454061.html
TIKA : MetadataDiscussion Div Sections: No Place for Metadata,"The first two non-solutions ignored that decisions have already been made about how Tika will represent structured documents and simple containers in XHTML. Tika represents a simple container document something like the following:  <html xmlns=""http://www.w3.org/1999/xhtml"">
  <head>
    <title>...</title>
  </head>
  <body>
    <div class=""package-entry"">
      <h1>testfiles/5.txt</h1>
      <p>...</p>
    </div>
    <div class=""package-entry"">
      <h1>testfiles/getty11.zip</h1>
      <div class=""package-entry"">
        <h1>testfiles/getty11.txt</h1>
        <p>...</p>
      </div>
    </div>
  </body>
</html>  Tika uses the<div>tag to show subparts of a document. The subparts can have their own subparts by nesting<div>sections inside of other<div>sections. The result is a structured XHTML document that can show arbitrary levels of nesting and can represent the text extracted from nearly any kind of document.  The problem is that there is no place to put the metadata that is legal XHTML. The<meta>tags can only appear in the<head>section. Even if we wanted to put all metadata in the<head>section, doing so would mean that Tika could not stream the XHTML events, and instead of have to parse entire containers in two passes: once to gather the metadata, and a second time to output all of the text.                                                                           If XHTML had a way to specify arbitrary name-value pairs somewhere in the<div>section, that could be used as a place to associate metadata with a<div>section. As far as I can tell from the specification [http://www.w3schools.com/tags/tag_div.asp] there isn't a place for arbitrary name-value pairs.                                                                            Hopefully we can find some solutions that actually work, and work for many kinds of users. It doesn't look like there is a way to represent metadata for nested sections or nested documents in XHTML, but there may be other ways to make metadata nested metadata available to some users.",../data/confluence_exports/TIKA/MetadataDiscussion_109454061.html
TIKA : MetadataDiscussion Metadata forContentHandlerImplementors: Metadata Stack inParseContext,"If you are going to the effort of implementing aContentHandler, theRecursiveMetadatapage describes how you can ret access to recursive metadata.",../data/confluence_exports/TIKA/MetadataDiscussion_109454061.html
TIKA : MetadataDiscussion A Solution for Users Who Don't ImplementContentHandler,"If XHTML doesn't offer a legal way to associate arbitrary name-value pairs with a<div>section, then there don't seem to be options for providing full metadata in a single XHTML document. There are at least a couple of possibilities for providing a better-than-nothing solution for users who want all of the metadata without having to write their ownContentHandler.  The possibilities identified so far are the following:  Add aConvertToMultiDocContentHandlerthat produces multiple XHTML files, one file per line of the output file.Add aConvertToXmlDocContentHandlerthat produces an XML file that is similar to XHTML, but allows<meta>tags in each<div>sectionAdd aConvertToXmlDocContentHandlerthat produces an XML file that is completely unlike XHTML to avoid confusion, and is capable of representing nested sections, each having their own metadata.  Once there is a way to implement aContentHandlerand get all of the nested metadata associated with each<div>section, special content handlers could be written that translate the events into something other than a single XHTML document.  If aContentHandlerwas going to create an output file with multiple XHTML documents in it, it would have to decide where the document boundaries are (while not perfect for everyone, probably good enough for 80% of users), and it would have to have a way of encoding newlines so the each XHTML document could be put on a single line (for easy parsing later). This could be as simple as surrounding each line with<p></p>and dropping the\rand\ncharacters.  Yet another alternate content handler could convert the output to XML that has a place for associating metadata with each subsection. This XML could resemble XHTML, or to avoid confusion, could be completely different from XHTML.   Identify other use casesIdentify other solutionsDiscuss pros and cons of solutionsReach consensus on an adequate path forward for nested metadata",../data/confluence_exports/TIKA/MetadataDiscussion_109454061.html
TIKA : TikaEvalDbDesign Background,"The tika-eval module was initially designed to work with the output from Tika's RecursiveParserWrapper (-J -tin tika-app-ese or/rmeta/textfrom tika-server).  The basic idea is that for each input file, there is a list of Metadata objects.  The first Metadata object in the list contains the information from the outer/container input file, and if there are attachments or embedded files, those are added to the list.  The content that was extracted is stored in the ""X-TIKA:content"" key in each Metadata object.  In the following discussion, we use the term ""container"" to refer to the initial input file, and we use ""file"" to refer either to the container file or to an embedded object.  An ""extract"" is the file that contains the extracted metadata/content.  Because of this one->many mapping, the table structure became fairly complex fairly quickly.  The initial flat file didn't work so well.",../data/confluence_exports/TIKA/TikaEvalDbDesign_109454086.html
TIKA : TikaEvalDbDesign Profile,"PROFILES – this is the main table. This contains all files, embedded and not, whether or not there was an exception.  The primary key is ID, and this table has foreign keys to the CONTAINERS table (CONTAINER_ID), and to the MIMES table (MIME_TYPE_ID).  This includes a boolean value for whether or not this file was an embedded file or a container file (IS_EMBEDDED).  Note that the ID==CONTAINER_ID for the container file; this makes some useful joins much easier.CONTAINERS – this contains information about the container file. The primary key is CONTAINER_ID and that maps to the PROFILES' table's CONTAINER_ID foreign key.CONTENTS – this contains statistics about the content that was extracted.  The primary key is ID and this maps directly to the PROFILES' table's ID.MIMES – this contains mime strings and the most common file extension for each mime type.  The primary key is MIME_TYPE_ID, and this maps to the MIME_TYPE_ID column in the PROFILES table.PARSE_EXCEPTIONS – this contains information about parse exceptions. The primary key is ID and that maps to the ID column in the PROFILES table.  This includes the original stack trace, a slightly truncated stack trace used for sorting and grouping by, and a foreign key to the class/type of exception.REF_PARSE_EXCEPTION_TYPES – this is a reference table that contains the different classes/types of parse exceptions: runtime, encryption (password protected files), access_permission (e.g. a PDF that won't allow extraction of text) and unsupported version.  We have custom code that maps stacktraces to these exception types.  Let us know if you find any problems!ERRORS – this contains records for errors reading the extract file or errors that were caused during the parse that caused tika-app in batch mode to restart.  By mapping to the REF_EXTRACT_ERROR_TYPES and the REF_PARSE_ERROR_TYPES, you can identify the cause of the problem.  There's a todo to integrate the code that reads in the parser error log and update this table...that functionality is on its way.EMB_FILE_NAMES – this records the full path of an embedded filewithinthe file.  If there's a zip with a zip in it, you might see: /zip1.zip/zip2.zip/my_docx.docx.  Again, the ID in this table maps to the ID in the PROFILES table.",../data/confluence_exports/TIKA/TikaEvalDbDesign_109454086.html
TIKA : TikaEvalDbDesign Comparison,"Nearly all of the above exist with '_A' or '_B' added to the end of the table names.  The ID in PROFILES_A matches the ID in PROFILES_B.  If a given container file contained a different number of attachments or if the code couldn't figure out which attachments map to which attachments, there can be incorrect double entries between the two.  In addition to the profiling tables, there is also:  CONTENT_COMPARISONS – this records the output of comparing the content.  Again, the ID maps to the ID in PROFILES_A and PROFILES_B.",../data/confluence_exports/TIKA/TikaEvalDbDesign_109454086.html
TIKA : TikaEvalDbDesign CONTENTS,"CONTENT_LENGTH – literal string lengthNUM_TOKENS – number of tokens (as analyzed by the Lucene analysis chain)NUM_UNIQUE_TOKENS – number of unique tokens (also known as ""types"")COMMON_TOKENS_LANG – which language was selected for counting ""common tokens""NUM_COMMON_TOKENS – number of ""common tokens"" (not number of unique tokens) that were foundTOP_N_TOKENS – top 10 most frequent tokensNUM_ALPHABETIC_TOKENS – number of tokens that contained at least one alphabetic/ideographic characterLANG_ID_1 – language id as determined by OptimaizeLANG_ID_1_PROB_1 – language id probability.LANG_ID_2, etc.  if Optimaize thought there could be more than one language classification.UNICODE_CHAR_BLOCKS – number of characters per Unicode code blockTOKEN_ENTROPY_RATE – token entropy, a measure of diversity of terms. A word list would have a very high entropy, a novel by someone with a 10 word vocabulary would have a very low entropy rate.TOKEN_LENGTH_SUM, *_MEAN, *_STD_DEV – sum, mean, stdev of the lengths of the tokens",../data/confluence_exports/TIKA/TikaEvalDbDesign_109454086.html
TIKA : TikaEvalDbDesign CONTENT_COMPARISONS,"TOP_10_UNIQUE_TOKEN_DIFFS_A – what are the top 10 most frequent terms that appear in A and never appear in B.TOP_10_UNIQUE_TOKEN_DIFFS_B –vice versaTOP_10_MORE_IN_A – the top 10 terms that appear more frequently in A than B.  The number is the difference between A and B per term (e.g. you'd see""cat"": 10if ""cat"" appeared 30 times in A and 20 times in B )TOP_10_MORE_IN_B –vice versaDICE_COEFFICIENT – focuses on the overlap based on unique tokens (not tokens): 2*overlap/(totalUniqueTokensA+totalUniquTokensB)OVERLAP – focuses on the overlap of tokens: (2*overlap)/(totalTokensA+totalTokensB)",../data/confluence_exports/TIKA/TikaEvalDbDesign_109454086.html
TIKA : Logging Important note,"Since Tika 2.5.0 (released 2022-10-03) depends onslf4j-api2.0.x which requires downstream library users to update logging backend to compatible version. Tika 2.0.0 – 2.4.1 depends onslf4j-api1.7.x. Otherwise you will receive something like following message: SLF4J: No SLF4J providers were found.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: Seehttps://www.slf4j.org/codes.html#noProvidersfor further details.SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.SLF4J: Ignoring binding found atfile:/home/gross/.gradle/caches/modules-2/files-2.1/org.jboss.slf4j/slf4j-jboss-logmanager/1.2.0.Final/baff8ae78011e6859e127a5cb6f16332a056fd93/slf4j-jboss-logmanager-1.2.0.Final.jar!/org/slf4j/impl/StaticLoggerBinder.classSLF4J: Seehttps://www.slf4j.org/codes.html#ignoredBindingsfor an explanation.ERROR StatusLogger Log4j2 could not find a logging implementation. Please add log4j-core to the classpath. Using SimpleLogger to log to the console.. Updates for popular logging backends: Apache Log4j 2.x:org.apache.logging.log4j:log4j-slf4j-impl→org.apache.logging.log4j:log4j-slf4j2-implLogback:ch.qos.logback:logback-classic1.2.x → 1.3.x (uses olderjavax.*APIs) or 1.4.x (usesjakarta.*APIs)Apache Log4j 1.2.x:org.slf4j:slf4j-log4j12→org.slf4j:slf4j-reload4j(thoughslf4j-log4j12has relocation relocation directive toslf4j-reload4jsince 1.7.34) or migrate to the Log4j 2.x since log4j 1.2.x is in the End of Life status since 2015 and hasknown vulnerabilities JBoss Logging (slf4j-jboss-logging/slf4j-jboss-logmanager) as of 2022-11-09 are still onslf4j-api1.7.x, seehttps://issues.redhat.com/browse/JBLOGGING-165. Currently you can try downgradingorg.slf4j:slf4j-apiversion to 1.7.36 if you have to use Tika with JBoss Logging (e.g. if you use Quarkus or WildFly native logging).",../data/confluence_exports/TIKA/Logging_109454058.html
TIKA : Logging Tika parser modules,"tika-parser-*-moduleartifacts depend on many Apache and thirdparty libraries. Tika itself useslf4j-apibut underlying libraries use different logging API (commons-logging,java.util.logging,log4j 1.2.x,log4j 2.x,slf4j). By default Tika will bringslf4j-apiviatika-coreand some bridges likeorg.slf4j:jcl-over-slf4jandorg.slf4j:jul-to-slf4jas opinionated default. Depending on your logging backend and preferred configuration you'll need different dependency exclusions and bridges/implementations. In you have no preference about logging backend it's enough to addorg.apache.logging.log4j:log4j-core,org.apache.logging.log4j:log4j-slf4j2-implandorg.apache.logging.log4j:log4j-1.2-api(ororg.slf4j:log4j-over-slf4j) and excludelog4j:log4j,commons-logging:commons-logging,ch.qos.logback:logback-classic,ch.qos.logback:logback-core,ch.qos.reload4j:reload4jandorg.slf4j:slf4j-reload4j. As ofmainbranch (and Tika 2.6.0) all Tika source useslf4j-apias a logging API withorg.apache.logging.log4j:log4j-core:2.xas the backend for applications liketika-app/tika-eval-app/tika-server. Following sections shows how to configure different logging solutions/backends dependencies to avoid conflicts. Loggers configuration are out of scope of this document, you should look at relevant library documentation.  If you useApache Mavendependency section inpom.xmlwill contain something like this:",../data/confluence_exports/TIKA/Logging_109454058.html
TIKA : Logging Common sections,"<!-- Merge with your properties section -->
<properties>
  <!-- components versions, feel free keep only required for your case -->
  <tika.version>2.6.0</tika.version>
  <slf4j.version>2.0.3</slf4j.version>
  <log4j2.version>2.19.0</log4j2.version>
  <logback.version>1.4.4</logback.version> <!-- 1.4.4 for Jakarta EE 9+ or 1.3.4 if you use Java EE or Jakarta EE 8 --><reload4j.version>1.2.22</reload4j.version>
</properties>

<dependencyManagement>
  <dependencies>
    <dependency>
      <groupId>org.apache.tika</groupId>
      <artifactId>tika-bom</artifactId>
      <version>${tika.version}</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
    <dependency>
      <groupId>org.apache.logging.log4j</groupId>
      <artifactId>log4j-bom</artifactId>
      <version>${log4j2.version}</version>
      <type>pom</type>
      <scope>import</scope>
    </dependency>
  </dependencies>
</dependencyManagement>

<!-- Merge with your dependencies section -->
<dependencies><dependency><groupId>org.apache.tika</groupId><artifactId>tika-core</artifactId></dependency>
  <dependency>
    <groupId>org.apache.tika</groupId>
    <artifactId>tika-parsers-standard-package</artifactId>
    <exclusions>
      <!--
        This exclusions will become obsolete at some point but better to keep it now.
        tika-parser-*-module should exclude commons-logging explicitly but upstream libraries
        may add it to their transitive dependencies
      -->
      <exclusion>
        <groupId>commons-logging</groupId>
        <artifactId>commons-logging</artifactId>
      </exclusion><!--These exclusions aren't necessary for tika-parsers-standard-packagebut may be required for other artifacts to have explicit logging configurationand avoid logging backend loops.-->
      <exclusion>
        <groupId>log4j</groupId>
        <artifactId>log4j</artifactId>
      </exclusion><exclusion><groupId>org.slf4j</groupId><artifactId>slf4j-log4j12</artifactId></exclusion>
      <exclusion>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-reload4j</artifactId>
      </exclusion><exclusion>
        <groupId>ch.qos.logback</groupId>
        <artifactId>logback-core</artifactId>
      </exclusion>
      <exclusion>
        <groupId>ch.qos.logback</groupId>
        <artifactId>logback-classic</artifactId>
      </exclusion>
      <exclusion>
        <groupId>ch.qos.reload4j</groupId>
        <artifactId>reload4j</artifactId>
      </exclusion>
    </exclusions>
  </dependency>

  <!-- You may want to add these dependencies to the dependencyManagement to force consistent versions and omit their versions here -->
  <dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>slf4j-api</artifactId>
    <version>${slf4j.version}</version>
  </dependency><!-- java.util.logging to slf4j adapter, requires additional configuration, seehttps://www.slf4j.org/api/org/slf4j/bridge/SLF4JBridgeHandler.html-->
  <dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>jul-to-slf4j</artifactId>
    <version>${slf4j.version}</version>
  </dependency><!-- commons-logging (JCL) to slf4j bridge -->
  <dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>jcl-over-slf4j</artifactId>
    <version>${slf4j.version}</version><scope>runtime</scope>
  </dependency><!-- log4j 1.2.x to slf4j bridge -->
  <dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>log4j-over-slf4j</artifactId>
    <version>${slf4j.version}</version><scope>runtime</scope>
  </dependency>
</dependencies>",../data/confluence_exports/TIKA/Logging_109454058.html
TIKA : Logging Apache Log4j 2.x with slf4j bridges,"<!-- Merge with your dependencies section --><dependencies><!-- logging backend: log4j 2.x --><dependency><groupId>org.apache.logging.log4j</groupId><artifactId>log4j-api</artifactId><!-- version is omitted since there's org.apache.logging.log4j:log4j-bom in the dependencyManagement section --></dependency><dependency><groupId>org.apache.logging.log4j</groupId><artifactId>log4j-core</artifactId><!-- version is omitted since there's org.apache.logging.log4j:log4j-bom in the dependencyManagement section --><scope>runtime</scope></dependency><!-- slf4j implementation that forwards to log4j 2.x -->
  <dependency>
    <groupId>org.apache.logging.log4j</groupId>
    <artifactId>log4j-slf4j2-impl</artifactId> <!-- for slf4j 1.7.x use log4j-slf4j-impl instead --><!-- version is omitted since there's org.apache.logging.log4j:log4j-bom in the dependencyManagement section --><scope>runtime</scope></dependency></dependencies>",../data/confluence_exports/TIKA/Logging_109454058.html
TIKA : Logging Logback,<!-- Merge with your dependencies section --><dependencies><!-- slf4j implementation --><dependency><groupId>ch.qos.logback</groupId><artifactId>logback-classic</artifactId><version>${logback.version}</version><scope>runtime</scope></dependency><!-- log4j2 to slf4j adapter --><dependency><groupId>org.apache.logging.log4j</groupId><artifactId>log4j-to-slf4j</artifactId><!-- version is omitted since there's org.apache.logging.log4j:log4j-bom in the dependencyManagement section --><scope>runtime</scope></dependency></dependencies>,../data/confluence_exports/TIKA/Logging_109454058.html
TIKA : Logging Apache Log4j 2.x with native bridges,"<dependencies>
  <dependency>
    <groupId>org.apache.tika</groupId>
    <artifactId>tika-parsers-standard-package</artifactId>
    <exclusions>
      <exclusion>
        <groupId>commons-logging</groupId>
        <artifactId>commons-logging</artifactId>
      </exclusion>
      <exclusion>
        <groupId>log4j</groupId>
        <artifactId>log4j</artifactId>
      </exclusion>
      <exclusion>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-log4j12</artifactId>
      </exclusion>
      <exclusion>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-reload4j</artifactId>
      </exclusion>
      <exclusion>
        <groupId>ch.qos.logback</groupId>
        <artifactId>logback-core</artifactId>
      </exclusion>
      <exclusion>
        <groupId>ch.qos.logback</groupId>
        <artifactId>logback-classic</artifactId>
      </exclusion>
      <exclusion>
        <groupId>ch.qos.reload4j</groupId>
        <artifactId>reload4j</artifactId>
      </exclusion>

      <!-- Additionally exclude slf4j bridges -->
      <exclusion>
        <groupId>org.slf4j</groupId>
        <artifaftId>jul-to-slf4j</artifactId>
      </exclusion>
      <exclusion>
        <groupId>org.slf4j</groupId>
        <artifaftId>jul-to-slf4j</artifactId>
      </exclusion>
      <exclusion>
        <groupId>org.slf4j</groupId>
        <artifaftId>jcl-over-slf4j</artifactId>
      </exclusion>
      <exclusion>
        <groupId>org.slf4j</groupId>
        <artifaftId>log4j-over-slf4j</artifactId>
      </exclusion>
    </exclusions>
  </dependency>

  <!-- slf4j implementation to forward logs to log4j 2.x -->
  <dependency>
    <groupId>org.apache.logging.log4j</groupId>
    <artifactId>log4j-slf4j2-impl</artifactId> <!-- for slf4j 1.7.x use log4j-slf4j-impl instead -->
    <!-- version is omitted since there's org.apache.logging.log4j:log4j-bom in the dependencyManagement section -->
    <scope>runtime</scope>
  </dependency>

  <!-- log4j 2.x bridges to forward java.util.logging, jcl/commons-logging and log4j 1.2.x to log4j 2.x -->
  <dependency>
    <groupId>org.apache.logging.log4j</groupId>
    <artifactId>log4j-jul</artifactId>
    <!-- version is omitted since there's org.apache.logging.log4j:log4j-bom in the dependencyManagement section -->
  </dependency>
  <dependency>
    <groupId>org.apache.logging.log4j</groupId>
    <artifactId>log4j-jcl</artifactId>
    <!-- version is omitted since there's org.apache.logging.log4j:log4j-bom in the dependencyManagement section -->
    <scope>runtime</scope>
  </dependency>
  <dependency>
    <groupId>org.apache.logging.log4j</groupId>
    <artifactId>log4j-1.2-api</artifactId>
    <!-- version is omitted since there's org.apache.logging.log4j:log4j-bom in the dependencyManagement section -->
    <scope>runtime</scope>
  </dependency>

  <!-- logging backend: log4j 2.x -->
  <!-- this dependency declarations are optional since org.apache.logging.log4j:log4j-slf4j-impl depends on them transitively -->
  <dependency>
    <groupId>org.apache.logging.log4j</groupId>
    <artifactId>log4j-api</artifactId>
    <!-- version is omitted since there's org.apache.logging.log4j:log4j-bom in the dependencyManagement section -->
  </dependency>
  <dependency>
    <groupId>org.apache.logging.log4j</groupId>
    <artifactId>log4j-core</artifactId>
    <!-- version is omitted since there's org.apache.logging.log4j:log4j-bom in the dependencyManagement section -->
    <scope>runtime</scope>
  </dependency>
</dependencies>",../data/confluence_exports/TIKA/Logging_109454058.html
TIKA : Logging Common sections,"dependencies {// Import Maven BOM for Tika and Log4j 2.x.// Depending on your setup `api` could be used instead of `implementation` when `java-library` plugin is activated.implementation(platform(""org.apache.tika:tika-bom:2.6.0""))implementation(platform(""org.apache.logging.log4j:log4j-bom:2.19.0""))constraints {// versions from constraints work like a dependencyManagement section in Mavenimplementation(""org.slf4j:slf4j-api:2.0.3"")implementation(""org.slf4j:jul-to-slf4j:2.0.3"")implementation(""org.slf4j:jcl-over-slf4j:2.0.3"")implementation(""org.slf4j:log4j-over-slf4j:2.0.3"")}implementation(""org.apache.tika:tika-core"")implementation(""org.apache.tika:tika-parsers-standard-package"")}configurations.all {// remove if using Apache Log4j 2.x log4j-jcl native bridge instead of jcl-over-slf4jexclude(""commons-logging"", ""commons-logging"")}",../data/confluence_exports/TIKA/Logging_109454058.html
TIKA : Logging Apache Log4j 2.x with slf4j bridges,"// merge with common section abovedependencies {// versions from platform/BOMimplementation(""org.apache.logging.log4j:log4j-api"")runtimeOnly(""org.apache.logging.log4j:log4j-core"")runtimeOnly(""org.apache.logging.log4j:log4j-slf4j2-impl"") // for slf4j 1.7.x use log4j-slf4j-impl insteadimplementation(""org.slf4j:jul-to-slf4j"") // java.util.logging to slf4j, requires additional configuration, seehttps://www.slf4j.org/api/org/slf4j/bridge/SLF4JBridgeHandler.htmlruntimeOnly(""org.slf4j:jcl-over-slf4j"") // commons-logging (JCL) to slf4jruntimeOnly(""org.slf4j:log4j-over-slf4j"") // log4j 1.2.x to slf4j}",../data/confluence_exports/TIKA/Logging_109454058.html
TIKA : Logging Logback,"// merge with common section abovedependencies {constraints {// 1.4.x for slf4j 2.x & Jakarta EE 9+, 1.3.x for slf4j 2.x & Jakarta EE 8/Java EE 8, and 1.2.x for slf4j 1.7.ximplementation(""ch.qos.logback:logback-core:1.4.4"")implementation(""ch.qos.logback:logback-classic:1.4.4"")}runtimeOnly(""ch.qos.logback:logback-classic"") // slf4j logging backendimplementation(""org.slf4j:jul-to-slf4j"") // java.util.logging to slf4j, requires additional configuration, seehttps://www.slf4j.org/api/org/slf4j/bridge/SLF4JBridgeHandler.htmlruntimeOnly(""org.slf4j:jcl-over-slf4j"") // commons-logging (JCL) to slf4jruntimeOnly(""org.slf4j:log4j-over-slf4j"") // log4j 1.2.x to slf4jruntimeOnly(""org.apache.logging.log4j:log4j-to-slf4j"") // log4j 2.x to slf4j adapter}",../data/confluence_exports/TIKA/Logging_109454058.html
TIKA : Logging Apache Log4j 2.x with native bridges,"dependencies {// versions from platform/BOMimplementation(""org.apache.logging.log4j:log4j-api"")runtimeOnly(""org.apache.logging.log4j:log4j-core"")runtimeOnly(""org.apache.logging.log4j:log4j-slf4j2-impl"") // for slf4j 1.7.x use log4j-slf4j-impl insteadruntimeOnly(""org.apache.logging.log4j:log4j-jul"") // java.util.logging to log4j 2.x adapter, requires additional configuration seehttps://logging.apache.org/log4j/2.x/log4j-jul/index.htmlruntimeOnly(""org.apache.logging.log4j:log4j-jcl"") // commons-logging (JCL) to log4j 2.x bridgeruntimeOnly(""org.apache.logging.log4j:log4j-1.2-api"") // log4j 1.2.x to log4j 2.x bridge}",../data/confluence_exports/TIKA/Logging_109454058.html
TIKA : MockParser Background,"So, you've integrated Apache Tika into your framework, tried it on a couple of thousand files and all works well. Problem solved! No. In very, very rare cases, Tika can do some really bad things. We try to fix these problems when we can, but if history is any indication (e.g.TIKA-1132andTIKA-2040to name a few), if you are processing millions/billions of files from the wild, you'll need to defend against: Regular catchable exceptionsOutOfMemory errors which can put the jvm in an unreliable statePermanent hangs (Tika can chew up massive amounts of resources and goforever)Security vulnerabilities (e.g.CVE-2016-6809andCVE-2016-4434) Please note that for 3., permanent hangs – you cannot terminate the Thread. Thread'sstop,suspend,destroysound like they'll do the trick, but they won't.You need to kill the entire process.SeeTIKA-456. As of Tika 1.15, we added aMockParserin the tika-core-tests.jar that will allow you to test your framework against items 1-3. Simply add that jar to your class path and then include a <mock> xml file in your set of test documents, and crash, crash away. If you'd like to test 4., you can do that too! While you should be protected from an XXE (let us know if you're not!), you could create a deserialization attack...just create your own malicious Throwable class, add it to the classpath and send in a mock file that includes: <throw class=""my.evil.DeserializationAttack"">bwahahaha</throw>",../data/confluence_exports/TIKA/MockParser_109454063.html
TIKA : MockParser Usage,Below are several options for adding the dependency.,../data/confluence_exports/TIKA/MockParser_109454063.html
TIKA : MockParser Including the tika-core-tests dependency in your project,<dependency><groupId>${project.groupId}</groupId><artifactId>tika-core</artifactId><version>${project.version}</version><type>test-jar</type><scope>test</scope></dependency>,../data/confluence_exports/TIKA/MockParser_109454063.html
TIKA : MockParser Tika-app,"Place the tika-app.jar and the tika-core-tests.jar in a ""bin"" directory. java -cp ""bin/*"" org.apache.tika.TikaCLI mock_example.xml",../data/confluence_exports/TIKA/MockParser_109454063.html
TIKA : MockParser Tika-server,"Place the tika-server.jar and the tika-core-tests.jar in a ""bin"" directory. java -cp ""bin/*"" org.apache.tika.server.TikaServerCli Then curl away: curl -T mock_example.xmlhttp://localhost:9998/rmeta/text",../data/confluence_exports/TIKA/MockParser_109454063.html
TIKA : MockParser Your Framework,Place the tika-core-tests.jar on your class path (NOT IN PRODUCTION!!!) and then add some mock.xml files to your batch of documents.,../data/confluence_exports/TIKA/MockParser_109454063.html
TIKA : MockParser Mock options,"See the mock example.xml file in tika-parsers/src/test/resources/test-documents/mock. This shows all of the examples of what you can do. <?xml version=""1.0"" encoding=""UTF-8"" ?>

<mock>
    <!-- this file offers all of the options as documentation.
    Parsing will stop at an IOException, of course
    -->

    <!-- action can be ""add"" or ""set"" -->
    <metadata action=""add"" name=""author"">Nikolai Lobachevsky</metadata>

    <!-- element is the name of the sax event to write, p=paragraph
        if the element is not specified, the default is <p> -->

    <write element=""p"">some content</write>

    <!-- write something to System.out -->
    <print_out>writing to System.out</print_out>

    <!-- write something to System.err -->
    <print_err>writing to System.err</print_err>

    <!-- hang
        millis: how many milliseconds to pause.  The actual hang time will probably
            be a bit longer than the value specified.        
        heavy: whether or not the hang should do something computationally expensive.
            If the value is false, this just does a Thread.sleep(millis).
            This attribute is optional, with default of heavy=false.
        pulse_millis: (required if ""heavy"" is true), how often to check to see
            whether the thread was interrupted or that the total hang time exceeded the millis
        interruptible: whether or not the parser will check to see if its thread
            has been interrupted; this attribute is optional with default of true
    -->
    <hang millis=""100"" heavy=""true"" pulse_millis=""10"" interruptible=""true"" />

    <!-- As of Tika 1.27/2.0, we've integrated FakeLoad (https://github.com/msigwart/fakeload)
         which enables much more precision for resource consumption than does 
         ""hang""
    
         millis = milliseconds to run, cpu is an integer % of the cpu to peg 
         and mb is the integer amount of memory to consume in megabytes -->
    <fakeload millis=""100"" cpu=""10"" mb=""10""/>
    
    <!-- throw an exception or error; optionally include a message or not -->
    <throw class=""java.io.IOException"">not another IOException</throw>
    
    <!-- perform a genuine OutOfMemoryError -->
    <oom/>

    <!-- perform a system exit...what parser would do that?!  We had one once... -->
    <system_exit/>

    <!-- interrupt the thread -->
    <thread_interrupt/>

    <!-- print to stdout -->
    <print_out>some junk</print_out>

    <!-- print to stderr -->
    <print_err>some junk</print_err>

</mock>",../data/confluence_exports/TIKA/MockParser_109454063.html
TIKA : MockParser References,Tika to Ride2.Evaluating Text Extraction,../data/confluence_exports/TIKA/MockParser_109454063.html
TIKA : GSoC Google Summer of Code,"This is workspace for Google Summer of Code students and mentors of Tika project  YearStudentTopicMentor(s)Proposal*2017Thejan WijesingheSupporting Image-to-Text (Image Captioning) in Tika for Image MIME TypesThamme Gowda,Chris Mattmannsee proposal",../data/confluence_exports/TIKA/GSoC_109454052.html
TIKA : TikaAndNLTK Installation,"Simple with pip$ pip install --process-dependency-links nltkrest2.Setuptools and/or Distribute: The module can be downloaded fromgithuband then installed with the following commands:$ cd NLTKRest/nltkrest
  $ python setup.py install nltkrest  After installation you will have a command callednltk-server. By default it starts a REST server on port 8881. You can change the port by typing--portor-p. You can also turn on verbose mode by typing-v.",../data/confluence_exports/TIKA/TikaAndNLTK_109454080.html
TIKA : TikaAndNLTK Start NLTK Server,$ nltk-server -v --port 8888  You should see a message:Running onhttp://127.0.0.1:8888The server is up and ready!,../data/confluence_exports/TIKA/TikaAndNLTK_109454080.html
TIKA : TikaAndNLTK Preparing resources for NLTK in Tika-App,"You can either perform steps 1 & 2 together or just 3.  Activate Named Entity ParserIn order to use any of theNamedEntityParserimplementations in Tika , the parser responsible for handling the name recognition task needs to be enabled. This can be done with Tika Config XML file, as follows<?xml version=""1.0"" encoding=""UTF-8""?>
 <properties>
     <parsers>
         <parser class=""org.apache.tika.parser.ner.NamedEntityParser"">
             <mime>text/plain</mime>
             <mime>text/html</mime>
             <mime>application/xhtml+xml</mime>
         </parser>
     </parsers>
 </properties>This configuration has to be supplied in the later phases, so store it as 'tika-config.xml'.2.Supply NLTKServer.properties fileIt is imperative that Tika should know on what host you are running thenltk-server. By default Tika will assume your server runs on port 8881. In order to specify any other port, you must supply a NLTKServer.properties file. Sample NLTKServer.properties file. My file looks like the following:nltk.server.url=http://localhost:8881In an nutshell#Create a directory for keeping the config and properties file.
 export NLTK_RES=$HOME/NLTKRest-resources
 mkdir -p $NLTK_RES
 cd $NLTK_RES
 #config file must be stored in this directory
 pwd

 export PATH_PREFIX=""$NLTK_RES/org/apache/tika/parser/ner/nltk""
 mkdir -p $PATH_PREFIX
 #create and edit the properties file
 vim $PATH_PREFIX/NLTKServer.properties3.Download NLTKRest-resourcesBetter yet, you could skip the previous two steps completely and save the hassle. Simply download the projectNLTKRest-resourcesand edit the properties filecd $HOME && git clone https://github.com/manalishah/NLTKRest-resources
 export NLTK_RES=$HOME/NLTKRest-resources
 vim $NLTK_RES/org/apache/tika/parser/ner/nltk/NLTKServer.properties",../data/confluence_exports/TIKA/TikaAndNLTK_109454080.html
TIKA : TikaAndNLTK Running NLTK with Tika,"Finally, we've reached this point where we can smile and let Tika do the working!  export TIKA_APP={your/path/to/tika-app}/target/tika-app-1.13-SNAPSHOT.jar

#set the system property to use NLTKNERecogniser class
java -Dner.impl.class=org.apache.tika.parser.ner.nltk.NLTKNERecogniser -classpath $NLTK_RES:$TIKA_APP org.apache.tika.cli.TikaCLI --config=$NLTK_RES/tika-config.xml -m  http://www.hawking.org.uk/  This will output metadata along with named entities extracted using nltk:  NER_NAMES: Gonville
NER_NAMES: Einstein
NER_NAMES: Briefer History
NER_NAMES: ALS
NER_NAMES: Unbreakable Code
NER_NAMES: Click
NER_NAMES: Cambridge
NER_NAMES: George
NER_NAMES: Lucasian Professor
NER_NAMES: Stephen Hawking
NER_NAMES: Latest
NER_NAMES: Hubble
NER_NAMES: 1979
NER_NAMES: Brief History
NER_NAMES: California
NER_NAMES: University
NER_NAMES: 1663
NER_NAMES: 1982
NER_NAMES: London
NER_NAMES: US National Academy
NER_NAMES: Baby Universe
NER_NAMES: Home About Stephen The Computer Stephen
NER_NAMES: Applied Mathematics
NER_NAMES: Santa Cruz
NER_NAMES: Leiden University
NER_NAMES: CBE
NER_NAMES: Science
NER_NAMES: Caius College
NER_NAMES: HUDF09 Team
NER_NAMES: Dennis Stanton Avery
NER_NAMES: 2009
NER_NAMES: ESA
NER_NAMES: Annie
NER_NAMES: NASA
NER_NAMES: Black Holes
NER_NAMES: Universe
NER_NAMES: Sally Tsui
NER_NAMES: eXtreme Deep Field
NER_NAMES: Centre
NER_NAMES: Royal Society
NER_NAMES: Weebly
NER_NAMES: Cambridge Lectures Publications Books Images Films Videos Stephen
NER_NAMES: 1963
NER_NAMES: Unbreakable Code Hot
NER_NAMES: Isaac Newton
NER_NAMES: Lucy
NER_NAMES: Stephen",../data/confluence_exports/TIKA/TikaAndNLTK_109454080.html
TIKA : EXIFToolParser On Mac,brew install exiftool,../data/confluence_exports/TIKA/EXIFToolParser_109454048.html
TIKA : EXIFToolParser On Linux (CentOS),"sudo yum install perl-Image-ExifTool  To verify that EXIFTool works correctly, run:  exiftool -ver  which should output something like:9.72   To use EXIFTool you'll need a custom Tika config that will override Tika's default MP4 parser (if you are dealing with MP4 files). You can do so by creating a file such as the one below:  <properties>
  <parsers>
    <parser class=""org.apache.tika.parser.DefaultParser"">
    </parser>
    <parser class=""org.apache.tika.parser.mp4.MP4Parser"">
      <mime-exclude>video/mp4</mime-exclude>
    </parser>
    <parser class=""org.apache.tika.parser.external.CompositeExternalParser"">
      <mime>video/mp4</mime>
    </parser>
  </parsers>
</properties>  Note that this config file initializes theDefaultParseraCompositeParser, and theCompositeExternalParser, and the MP4Parser. For the MP4Parser, it uses a new directive, mime-exclude, to exclude that parser from thevideo/mp4type, and then to declare thatCompositeExternalParserwill supportvideo/mp4. Since EXIFTool is anExternalParserthis configuration will make sure it gets called.  Once you have the config file made above, save it as a file, e.g.,exif-tika-config.xmlin the current directory. Then to call Tika, you can use Tika-App and/or Tika Server.",../data/confluence_exports/TIKA/EXIFToolParser_109454048.html
TIKA : EXIFToolParser Using Tika-App,"Use the following command on a file, e.g.,spaghetti-to-sushi.mp4:  java -Dtika.config=exif-tika-config.xml -classpath tika-app/target/tika-app-1.9-SNAPSHOT.jar org.apache.tika.cli.TikaCLI -m spaghetti-to-sushi.mp4  This should output:  Audio Bits Per Sample: 16
Audio Channels: 2
Audio Format: mp4a
Audio Sample Rate: 22050
Average Bitrate: 0
Avg Bitrate: 1.26 Mbps
Balance: 0
Bit Depth: 24
Buffer Size: 0
Compatible Brands: mp41
Compressor ID: avc1
Compressor Name: h264
Content Create Date: created.with.SUPER(C).v2006.19
Content Create Date (ja): created.with.SUPER(C).v2006.19
Content-Length: 353985630
Content-Type: video/mp4
Create Date: 2006:12:17 18:50:47
Current Time: 0 s
Duration: 0:37:19
Elementary Stream Track: 201 101
ExifTool Version Number: 9.72
File Access Date/Time: 2015:05:25 21:18:08-07:00
File Inode Change Date/Time: 2014:09:26 20:32:27-07:00
File Modification Date/Time: 2011:07:28 13:01:54-07:00
File Name: spaghetti-to-sushi.mp4
File Permissions: rwxr-xr-x
File Size: 338 MB
File Type: MP4
Graphics Mode: srcCopy
Handler Description: GPAC MPEG-4 BIFS Handler
Handler Type: Metadata
Handler Vendor ID: Apple
Image Height: 480
Image Size: 640x480
Image Width: 640
MIME Type: video
Major Brand: MP4 v2 [ISO 14496-14]
Matrix Structure: 1 0 0 0 1 0 0 0 1
Max Bitrate: 0
Media Create Date: 2006:12:16 20:07:48
Media Duration: 1.00 s
Media Header Version: 0
Media Language Code: und
Media Modify Date: 2006:12:16 20:07:48
Media Time Scale: 90000
Minor Version: 0.0.1
Modify Date: 2006:12:17 18:50:47
Movie Data Offset: 473003
Movie Data Size: 353512586
Movie Header Version: 0
Next Track ID: 201
Op Color: 0 0 0
Other Format: mp4s
Poster Time: 0 s
Preferred Rate: 1
Preferred Volume: 100.00
Preview Duration: 0 s
Preview Time: 0 s
Rotation: 0
Selection Duration: 0 s
Selection Time: 0 s
Source Image Height: 480
Source Image Width: 720
Time Scale: 90000
Title: From Spaghetti to Sushi.mpeg
Title (ja): From Spaghetti to Sushi.mpeg
Track Create Date: 2006:12:17 18:50:47
Track Duration: 0:37:19
Track Header Version: 0
Track ID: 201
Track Layer: 0
Track Modify Date: 2006:12:16 20:07:48
Track Volume: 0.00
Vendor ID: FFmpeg
Video Frame Rate: 25
X Resolution: 72
X-Parsed-By: org.apache.tika.parser.CompositeParser
X-Parsed-By: org.apache.tika.parser.external.CompositeExternalParser
X-Parsed-By: org.apache.tika.parser.external.ExternalParser
Y Resolution: 72
resourceName: spaghetti-to-sushi.mp4",../data/confluence_exports/TIKA/EXIFToolParser_109454048.html
TIKA : EXIFToolParser Using Tika Server,"You can also use Tika-Server. First, start it up:  java -Dtika.config=exif-tika-config.xml -classpath tika-server/target/tika-server-1.9-SNAPSHOT.jar org.apache.tika.server.TikaServerCli  Now, PUT a file to it, e.g.,spaghetti-to-sushi.mp4:  curl -T $HOME/Movies/spaghetti-to-sushi.mp4 -H ""Content-Disposition: attachment;filename=spaghetti-to-sushi.mp4"" http://localhost:9998/rmeta  Which should return:  [
   {
      ""Audio Bits Per Sample"":""16"",
      ""Audio Channels"":""2"",
      ""Audio Format"":""mp4a"",
      ""Audio Sample Rate"":""22050"",
      ""Average Bitrate"":""0"",
      ""Avg Bitrate"":""1.26 Mbps"",
      ""Balance"":""0"",
      ""Bit Depth"":""24"",
      ""Buffer Size"":""0"",
      ""Compatible Brands"":""mp41"",
      ""Compressor ID"":""avc1"",
      ""Compressor Name"":""h264"",
      ""Content Create Date"":""created.with.SUPER(C).v2006.19"",
      ""Content Create Date (ja)"":""created.with.SUPER(C).v2006.19"",
      ""Content-Type"":""video/mp4"",
      ""Create Date"":""2006:12:17 18:50:47"",
      ""Current Time"":""0 s"",
      ""Duration"":""0:37:19"",
      ""Elementary Stream Track"":""201 101"",
      ""ExifTool Version Number"":""9.72"",
      ""File Access Date/Time"":""2015:05:25 21:20:47-07:00"",
      ""File Inode Change Date/Time"":""2015:05:25 21:20:46-07:00"",
      ""File Modification Date/Time"":""2015:05:25 21:20:46-07:00"",
      ""File Name"":""apache-tika-3052147227532168299.tmp"",
      ""File Permissions"":""rw-r--r--"",
      ""File Size"":""338 MB"",
      ""File Type"":""MP4"",
      ""Graphics Mode"":""srcCopy"",
      ""Handler Description"":""GPAC MPEG-4 BIFS Handler"",
      ""Handler Type"":""Metadata"",
      ""Handler Vendor ID"":""Apple"",
      ""Image Height"":""480"",
      ""Image Size"":""640x480"",
      ""Image Width"":""640"",
      ""MIME Type"":""video"",
      ""Major Brand"":""MP4 v2 [ISO 14496-14]"",
      ""Matrix Structure"":""1 0 0 0 1 0 0 0 1"",
      ""Max Bitrate"":""0"",
      ""Media Create Date"":""2006:12:16 20:07:48"",
      ""Media Duration"":""1.00 s"",
      ""Media Header Version"":""0"",
      ""Media Language Code"":""und"",
      ""Media Modify Date"":""2006:12:16 20:07:48"",
      ""Media Time Scale"":""90000"",
      ""Minor Version"":""0.0.1"",
      ""Modify Date"":""2006:12:17 18:50:47"",
      ""Movie Data Offset"":""473003"",
      ""Movie Data Size"":""353512586"",
      ""Movie Header Version"":""0"",
      ""Next Track ID"":""201"",
      ""Op Color"":""0 0 0"",
      ""Other Format"":""mp4s"",
      ""Poster Time"":""0 s"",
      ""Preferred Rate"":""1"",
      ""Preferred Volume"":""100.00"",
      ""Preview Duration"":""0 s"",
      ""Preview Time"":""0 s"",
      ""Rotation"":""0"",
      ""Selection Duration"":""0 s"",
      ""Selection Time"":""0 s"",
      ""Source Image Height"":""480"",
      ""Source Image Width"":""720"",
      ""Time Scale"":""90000"",
      ""Title"":""From Spaghetti to Sushi.mpeg"",
      ""Title (ja)"":""From Spaghetti to Sushi.mpeg"",
      ""Track Create Date"":""2006:12:17 18:50:47"",
      ""Track Duration"":""0:37:19"",
      ""Track Header Version"":""0"",
      ""Track ID"":""201"",
      ""Track Layer"":""0"",
      ""Track Modify Date"":""2006:12:16 20:07:48"",
      ""Track Volume"":""0.00"",
      ""Vendor ID"":""FFmpeg"",
      ""Video Frame Rate"":""25"",
      ""X Resolution"":""72"",
      ""X-Parsed-By"":[
         ""org.apache.tika.parser.CompositeParser"",
         ""org.apache.tika.parser.external.CompositeExternalParser"",
         ""org.apache.tika.parser.external.ExternalParser""
      ],
      ""X-TIKA:parse_time_millis"":""3638"",
      ""Y Resolution"":""72"",
      ""resourceName"":""spaghetti-to-sushi.mp4""
   }
]",../data/confluence_exports/TIKA/EXIFToolParser_109454048.html
"TIKA : Apache Tika Meetups November 9, 2021, Tuesday 11am EST/4pm UTC","The dial-in information is available to those who register via Meetup. This workshop is designed for hands-on tech folks who can run Tika from the commandline or cancurlto a local tika-server. Stay tuned for prerequisites, resources and an agenda! The following is all a work in progress.  Please check back right before the workshop!",../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
TIKA : Apache Tika Meetups Prerequisites:,java >= 8tika-eval app and tika-app jars:https://dlcdn.apache.org/tika/2.1.0/tika-eval-app-2.1.0.jarandhttps://dlcdn.apache.org/tika/2.1.0/tika-app-2.1.0.jarJSON editor/viewer (jqshould be sufficient. I like Sublime with the PrettyJSON pluginhttps://github.com/dzhibas/SublimePrettyJson)XLSX viewer (Excel or Open/LibreOffice),../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
TIKA : Apache Tika Meetups Optional materials:,"tika-server-standard jar:https://dlcdn.apache.org/tika/2.1.0/tika-server-standard-2.1.0.jartika-eval-core.jar:https://repo1.maven.org/maven2/org/apache/tika/tika-eval-core/2.1.0/tika-eval-core-2.1.0.jarIf you'd like to experiment with tesseract, make sure that tesseract is installed and callable as 'tesseract' from your commandline.Some knowledge of SQL",../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
"TIKA : Apache Tika Meetups Example docs, extracts and config files:tika-eval-workshop-20211109.tgz","Before the class, you should unzip the tika-eval-workshop-20211109.tgz (tar -xzvf tika-eval-workshop-20211109.tgz), move thetika-app-2.1.0.jarinto thetika-eval-workshop-20211109/folder and  run tika-app on thedocsdirectory:java -jar tika-app-2.1.0.jar -J -t -i docs -o extracts/my_extracts  Note: There's a bug in the default logging configuration for tika-app in batch mode (e.g.""No configuration found for '4b85612c' at 'null' in 'null'..."").  This is fixed in the latest tika-app and will be available in the next release 2.1.1.",../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
"TIKA : Apache Tika Meetups December 2, 2021, Thursday 12pm (NOON) EST/5pm UTC","The dial-in information is available to those who register via Meetup. I'm currently working on this, and it should be ready by 11am EST/4pm UTC – an hour before the start Useful documentation:tika-pipes",../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
TIKA : Apache Tika Meetups Prerequisites:,java >= 8curl (or postman or something similar)Unziptika-pipes-tutorial-20211202.zipIntika-pipes-tutorial-20211202/app-bin/:https://dlcdn.apache.org/tika/2.1.0/tika-app-2.1.0.jarhttps://repo1.maven.org/maven2/org/apache/tika/tika-emitter-fs/2.1.0/tika-emitter-fs-2.1.0.jarhttps://repo1.maven.org/maven2/org/apache/tika/tika-emitter-solr/2.1.0/tika-emitter-solr-2.1.0.jarORhttps://repo1.maven.org/maven2/org/apache/tika/tika-emitter-opensearch/2.1.0/tika-emitter-opensearch-2.1.0.jartika-core-2.1.1-SNAPSHOT-test-jar-with-dependencies.jarIntika-pipes-tutorial-20211202/server-bin/:tika-server-standard jar:https://dlcdn.apache.org/tika/2.1.0/tika-server-standard-2.1.0.jarhttps://repo1.maven.org/maven2/org/apache/tika/tika-emitter-fs/2.1.0/tika-emitter-fs-2.1.0.jarhttps://repo1.maven.org/maven2/org/apache/tika/tika-emitter-solr/2.1.0/tika-emitter-solr-2.1.0.jarORhttps://repo1.maven.org/maven2/org/apache/tika/tika-emitter-opensearch/2.1.0/tika-emitter-opensearch-2.1.0.jartika-core-2.1.1-SNAPSHOT-test-jar-with-dependencies.jarInstallation of Apache Solr (~8.9.x) and/or OpenSearch (~1.x) and/or Elasticsearch (7.x),../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
TIKA : Apache Tika Meetups Advanced/Optional:,jqor similar,../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
TIKA : Apache Tika Meetups Exercises,"Use fetcher in traditional/tika /rmetaendpointsupdateconfigs/tika-config-basic.xml<basePath> element to get the full path totika-pipes-tutorial-20211202/docs:FileSystemFetcherРазвернуть исходный код<fetcher class=""org.apache.tika.pipes.fetcher.fs.FileSystemFetcher"">
    <params>
      <name>fsf</name>
      <basePath>/Users/allison/Desktop/tika-pipes-tutorial-20211202/docs</basePath>
    </params>
  </fetcher>start the server:java -cp ""server-bin/*"" org.apache.tika.server.core.TikaServerCli -cconfigs/tika-config-basic.xmlcurl -X PUThttp://localhost:9998/rmeta-H ""fetcherName:fsf"" -H ""fetchKey:testPDF.pdf"" | jq --sort-keysUse /pipes handler to read from and write to a local file shareupdateconfigs/tika-config-basic.xml<basePath> element to get the full path totika-pipes-tutorial-20211202/docs:FileSystemEmitterРазвернуть исходный код<emitters>
    <emitter class=""org.apache.tika.pipes.emitter.fs.FileSystemEmitter"">
      <params>
        <name>fse</name>
        <basePath>/Users/allison/Desktop/tika-pipes-tutorial-20211202/extracts</basePath>
      </params>
    </emitter>
  </emitters>start the server:java -cp ""server-bin/*"" org.apache.tika.server.core.TikaServerCli -cconfigs/tika-config-basic.xmlcurl -X POST -H ""Content-Type: application/json"" -d @configs/pipes-request-minimal.jsonhttp://localhost:9998/pipesConfigure metadata filters and rerun 2.Copy this and paste it intoconfigs/tika-config-basic.xmlMetadata FiltersРазвернуть исходный код<metadataFilters>
  <!-- depending on the file format, some dates do not have a timezone. This
         filter arbitrarily assumes dates have a UTC timezone and will format all
         dates as yyyy-MM-dd'T'HH:mm:ss'Z' whether or not they actually have a timezone.
    -->
  <metadataFilter class=""org.apache.tika.metadata.filter.DateNormalizingMetadataFilter""/>
  <metadataFilter class=""org.apache.tika.metadata.filter.FieldNameMappingFilter"">
    <params>
      <excludeUnmapped>true</excludeUnmapped>
      <mappings>
        <mapping from=""X-TIKA:content"" to=""content_s""/>
        <mapping from=""Content-Length"" to=""length_i""/>
        <mapping from=""dc:creator"" to=""creators_ss""/>
        <mapping from=""dc:title"" to=""title_s""/>
        <mapping from=""Content-Type"" to=""mime_s""/>
        <mapping from=""X-TIKA:EXCEPTION:container_exception"" to=""tika_exception_s""/>
      </mappings>
    </params>
  </metadataFilter>
</metadataFilters>Restart the serverRerun the curl command and look at the output (cat extracts/testPDF.pdf.json | jq --sort-keys)Use/asynchandler file share to file sharecurl -X POST -H ""Content-Type: application/json"" -d @configs/async-request-minimal.jsonhttp://localhost:9998/asynccurl -X POST -H ""Content-Type: application/json"" -d @configs/async-request-full.jsonhttp://localhost:9998/asyncRun the async processor via tika-appConfigure thebasePathelement inFileSystemPipesIteratorandFileSystemPipesIteratorinconfigs/tika-config-app.xmlFileSystemPipesIteratorРазвернуть исходный код<fetcher class=""org.apache.tika.pipes.fetcher.fs.FileSystemFetcher"">
      <params>
        <name>fsf</name>
        <basePath>/Users/allison/Desktop/tika-pipes-tutorial-20211202/docs</basePath>
      </params>
    </fetcher> 
  <pipesIterator class=""org.apache.tika.pipes.pipesiterator.fs.FileSystemPipesIterator"">
    <params>
      <fetcherName>fsf</fetcherName>
      <emitterName>fse</emitterName>
      <basePath>/Users/allison/Desktop/tika-pipes-tutorial-20211202/docs</basePath>
    </params>
  </pipesIterator>java -cp ""app-bin/*"" org.apache.tika.cli.TikaCLI -a --config=configs/tika-config-app.xmlConfigure Solr/OpenSearch/ElasticSearch emitter and run/pipeshandler",../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
TIKA : Apache Tika Meetups Solr Example (fileshare to Solr),"From the solr directorybin/solr startbin/solr create -c tika-example && bin/solr config -c tika-example -p 8983 -action set-user-property -property update.autoCreateFields -value falseFrom the tika-pipes-tutorial directorySet the schema in Solr:curl -F 'data=@configs/solr/solr-parent-child-schema.json'http://localhost:8983/solr/tika-example/schemaConfigure thebasePathelement inFileSystemPipesIteratorandFileSystemFetcherinconfigs/solr/tika-config-solr.xmlFileSystemPipesIteratorРазвернуть исходный код<fetcher class=""org.apache.tika.pipes.fetcher.fs.FileSystemFetcher"">
      <params>
        <name>fsf</name>
        <basePath>/Users/allison/Desktop/tika-pipes-tutorial-20211202/docs</basePath>
      </params>
    </fetcher>
...
  <pipesIterator class=""org.apache.tika.pipes.pipesiterator.fs.FileSystemPipesIterator"">
    <params>
      <fetcherName>fsf</fetcherName>
      <emitterName>solr1</emitterName>
      <basePath>/Users/allison/Desktop/tika-pipes-tutorial-20211202/docs</basePath>
    </params>
  </pipesIterator>java -cp ""app-bin/*"" org.apache.tika.cli.TikaCLI -a --config=configs/solr/tika-config-solr.xml",../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
TIKA : Apache Tika Meetups OpenSearch/Elasticsearch Example (fileshare to OpenSearch/ElasticSearch),"Start opensearch via Docker:docker pull opensearchproject/opensearch:1.2.0docker run-p9200:9200-p9600:9600-e""discovery.type=single-node""opensearchproject/opensearch:1.2.0Curl schema to opensearch:curl -k -T configs/opensearch/opensearch-parent-child-mappings.json -u admin:admin -H ""Content-Type:application/json""https://localhost:9200/tika-testConfigure thebasePathelement inFileSystemPipesIteratora ndFileSystemFetcher inconfigs/opensearch/tika-config-opensearch.xmlFileSystemPipesIteratorРазвернуть исходный код<fetcher class=""org.apache.tika.pipes.fetcher.fs.FileSystemFetcher"">
      <params>
        <name>fsf</name>
        <basePath>/Users/allison/Desktop/tika-pipes-tutorial-20211202/docs</basePath>
      </params>
    </fetcher>
....
  <pipesIterator class=""org.apache.tika.pipes.pipesiterator.fs.FileSystemPipesIterator"">
    <params>
      <fetcherName>fsf</fetcherName>
      <emitterName>ose</emitterName>
      <basePath>/Users/allison/Desktop/tika-pipes-tutorial-20211202/docs</basePath>
    </params>
  </pipesIterator>",../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
TIKA : Apache Tika Meetups Helpful commands,delete a collection in solr:bin/solr delete -c collection_nameShow all docs in OpenSearch/Elasticsearch:https://localhost:9200/tika-test/_search?pretty=true&q=*:*,../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
"TIKA : Apache Tika Meetups January 24, 2022, Monday 11am EST/4pm UTC","The dial-in information is available to those who register via Meetup. I'm currently working on this, and it should be ready by 10:30am EST/4:30pm UTC – a half hour before the start Useful documentation:tika-pipes",../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
TIKA : Apache Tika Meetups Prerequisites:,java >= 8curl (or postman or something similar)Unziptika-pipes-tutorial-20220124.tgzIntika-pipes-tutorial-20220124/app-bin/:https://dlcdn.apache.org/tika/2.2.1/tika-app-2.2.1.jarhttps://repo1.maven.org/maven2/org/apache/tika/tika-emitter-fs/2.2.1/tika-emitter-fs-2.2.1.jarhttps://repo1.maven.org/maven2/org/apache/tika/tika-emitter-solr/2.2.1/tika-emitter-solr-2.2.1.jarORhttps://repo1.maven.org/maven2/org/apache/tika/tika-emitter-opensearch/2.2.1/tika-emitter-opensearch-2.2.1.jarhttps://ci-builds.apache.org/job/Tika/job/tika-main-jdk8/437/org.apache.tika$tika-core/artifact/org.apache.tika/tika-core/2.2.2-20220124.115541-55/tika-core-2.2.2-20220124.115541-55-test-jar-with-dependencies.jarOptional: Intika-pipes-tutorial-20220124/server-bin/:tika-server-standard jar:https://dlcdn.apache.org/tika/2.2.1/tika-server-standard-2.2.1.jarhttps://repo1.maven.org/maven2/org/apache/tika/tika-emitter-fs/2.2.1/tika-emitter-fs-2.2.1.jarhttps://repo1.maven.org/maven2/org/apache/tika/tika-emitter-solr/2.2.1/tika-emitter-solr-2.2.1.jarORhttps://repo1.maven.org/maven2/org/apache/tika/tika-emitter-opensearch/2.2.1/tika-emitter-opensearch-2.2.1.jarhttps://ci-builds.apache.org/job/Tika/job/tika-main-jdk8/437/org.apache.tika$tika-core/artifact/org.apache.tika/tika-core/2.2.2-20220124.115541-55/tika-core-2.2.2-20220124.115541-55-test-jar-with-dependencies.jarInstallation of Apache Solr (~8.9.x) and/or OpenSearch (~1.x) and/or Elasticsearch (7.x),../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
TIKA : Apache Tika Meetups A) Fileshare to Fileshare warm up,"Run the async processor via tika-appConfigure thebasePathelement inFileSystemPipesIteratorandFileSystemPipesIteratorinconfigs/tika-config-app-fs-to-fs.xmlFileSystemPipesIteratorРазвернуть исходный код<fetcher class=""org.apache.tika.pipes.fetcher.fs.FileSystemFetcher"">
      <params>
        <name>fsf</name>
        <basePath>/Users/allison/Desktop/tika-pipes-tutorial-20220124/docs</basePath>
      </params>
    </fetcher> 
  <pipesIterator class=""org.apache.tika.pipes.pipesiterator.fs.FileSystemPipesIterator"">
    <params>
      <fetcherName>fsf</fetcherName>
      <emitterName>fse</emitterName>
      <basePath>/Users/allison/Desktop/tika-pipes-tutorial-20220124/docs</basePath>
    </params>
  </pipesIterator>Configure thebasePathelement inFileSystemPipesEmitterinconfigs/tika-config-app-fs-to-fs.xmlFileSystemPipesIteratorРазвернуть исходный код<emitters>
    <emitter class=""org.apache.tika.pipes.emitter.fs.FileSystemEmitter"">
      <params>
        <name>fse</name>
        <basePath>/Users/allison/Desktop/tika-pipes-tutorial-20220124/extracts</basePath>
      </params>
    </emitter>
  </emitters>java -cp ""app-bin/*"" org.apache.tika.cli.TikaCLI -a --config=configs/tika-config-app-fs-to-fs.xml",../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
TIKA : Apache Tika Meetups B) OpenSearch/Elasticsearch Parent-Child Example (fileshare to OpenSearch/ElasticSearch),"Start opensearch via Docker:docker pull opensearchproject/opensearch:1.2.4docker run-p9200:9200-p9600:9600-e""discovery.type=single-node""opensearchproject/opensearch:1.2.4Curl schema to opensearch:curl -k -T configs/opensearch/opensearch-parent-child-mappings.json -u admin:admin -H ""Content-Type:application/json""https://localhost:9200/tika-test-parent-childConfigure thebasePathelement inFileSystemPipesIteratorandFileSystemFetcher inconfigs/opensearch/tika-config-fs-to-opensearch-parent-child.xmljava -cp ""app-bin/*"" org.apache.tika.cli.TikaCLI -a --config=configs/opensearch/tika-config-fs-to-opensearch-parent-child.xml",../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
TIKA : Apache Tika Meetups B) Solr Parent-Child Example (fileshare to Solr),"From the solr directorybin/solr startbin/solr create -c tika-example-parent-child && bin/solr config -c tika-example-parent-child -p 8983 -action set-user-property -property update.autoCreateFields -value falseFrom the tika-pipes-tutorial directorySet the schema in Solr:curl -F 'data=@configs/solr/solr-parent-child-schema.json'http://localhost:8983/solr/tika-example-parent-child/schemaConfigure thebasePathelement inFileSystemPipesIteratorandFileSystemFetcherinconfigs/solr/tika-config-solr-parent-child.xmlFileSystemPipesIteratorРазвернуть исходный код<fetcher class=""org.apache.tika.pipes.fetcher.fs.FileSystemFetcher"">
      <params>
        <name>fsf</name>
        <basePath>/Users/allison/Desktop/tika-pipes-tutorial-20220124/docs</basePath>
      </params>
    </fetcher>
...
  <pipesIterator class=""org.apache.tika.pipes.pipesiterator.fs.FileSystemPipesIterator"">
    <params>
      <fetcherName>fsf</fetcherName>
      <emitterName>solr1</emitterName>
      <basePath>/Users/allison/Desktop/tika-pipes-tutorial-20220124/docs</basePath>
    </params>
  </pipesIterator>java -cp ""app-bin/*"" org.apache.tika.cli.TikaCLI -a --config=configs/solr/tika-config-solr-parent-child.xml",../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
TIKA : Apache Tika Meetups C) OpenSearch/Elasticsearch Individual Files Example (fileshare to OpenSearch/ElasticSearch),"Start opensearch via Docker:docker pull opensearchproject/opensearch:1.2.4docker run-p9200:9200-p9600:9600-e""discovery.type=single-node""opensearchproject/opensearch:1.2.4Curl schema to opensearch:curl -k -T configs/opensearch/opensearch-indiv-files-mappings.json -u admin:admin -H ""Content-Type:application/json""https://localhost:9200/tika-test-indiv-filesConfigure thebasePathelement inFileSystemPipesIteratorandFileSystemFetcher inconfigs/opensearch/tika-config-fs-to-opensearch-indiv-files.xmljava -cp ""app-bin/*"" org.apache.tika.cli.TikaCLI -a --config=configs/opensearch/tika-config-fs-to-opensearch-indiv-files.xml",../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
TIKA : Apache Tika Meetups C) Solr Indiv Files Example (fileshare to Solr),"From the solr directorybin/solr startbin/solr create -c tika-example-indiv-files && bin/solr config -c tika-example-indiv-files -p 8983 -action set-user-property -property update.autoCreateFields -value falseFrom the tika-pipes-tutorial directorySet the schema in Solr:curl -F 'data=@configs/solr/solr-indiv-files-schema.json'http://localhost:8983/solr/tika-example-indiv-files/schemaConfigure thebasePathelement inFileSystemPipesIteratorandFileSystemFetcherinconfigs/solr/tika-config-solr-indiv-files.xmlFileSystemPipesIteratorРазвернуть исходный код<fetcher class=""org.apache.tika.pipes.fetcher.fs.FileSystemFetcher"">
      <params>
        <name>fsf</name>
        <basePath>/Users/allison/Desktop/tika-pipes-tutorial-20220124/docs</basePath>
      </params>
    </fetcher>
...
  <pipesIterator class=""org.apache.tika.pipes.pipesiterator.fs.FileSystemPipesIterator"">
    <params>
      <fetcherName>fsf</fetcherName>
      <emitterName>solr1</emitterName>
      <basePath>/Users/allison/Desktop/tika-pipes-tutorial-20220124/docs</basePath>
    </params>
  </pipesIterator>java -cp ""app-bin/*"" org.apache.tika.cli.TikaCLI -a --config=configs/solr/tika-config-solr-indiv-files.xml",../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
TIKA : Apache Tika Meetups D) OpenSearch/Elasticsearch Legacy Example (fileshare to OpenSearch/ElasticSearch),"Start opensearch via Docker:docker pull opensearchproject/opensearch:1.2.4docker run-p9200:9200-p9600:9600-e""discovery.type=single-node""opensearchproject/opensearch:1.2.4Curl schema to opensearch:curl -k -T configs/opensearch/opensearch-legacy-mappings.json -u admin:admin -H ""Content-Type:application/json""https://localhost:9200/tika-test-legacyConfigure thebasePathelement inFileSystemPipesIteratorandFileSystemFetcher inconfigs/opensearch/tika-config-fs-to-opensearch-legacy.xmljava -cp ""app-bin/*"" org.apache.tika.cli.TikaCLI -a --config=configs/opensearch/tika-config-fs-to-opensearch-legacy.xml",../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
TIKA : Apache Tika Meetups D) Solr Legacy Example (fileshare to Solr),"From the solr directorybin/solr startbin/solr create -c tika-example-legacy && bin/solr config -c tika-example-legacy -p 8983 -action set-user-property -property update.autoCreateFields -value falseFrom the tika-pipes-tutorial directorySet the schema in Solr:curl -F 'data=@configs/solr/solr-legacy-schema.json'http://localhost:8983/solr/tika-example-legacy/schemaConfigure thebasePathelement inFileSystemPipesIteratorandFileSystemFetcherinconfigs/solr/tika-config-solr-legacy.xmlFileSystemPipesIteratorРазвернуть исходный код<fetcher class=""org.apache.tika.pipes.fetcher.fs.FileSystemFetcher"">
      <params>
        <name>fsf</name>
        <basePath>/Users/allison/Desktop/tika-pipes-tutorial-20220124/docs</basePath>
      </params>
    </fetcher>
...
  <pipesIterator class=""org.apache.tika.pipes.pipesiterator.fs.FileSystemPipesIterator"">
    <params>
      <fetcherName>fsf</fetcherName>
      <emitterName>solr1</emitterName>
      <basePath>/Users/allison/Desktop/tika-pipes-tutorial-20220124/docs</basePath>
    </params>
  </pipesIterator>java -cp ""app-bin/*"" org.apache.tika.cli.TikaCLI -a --config=configs/solr/tika-config-solr-legacy.xml",../data/confluence_exports/TIKA/Apache-Tika-Meetups_191336977.html
TIKA : Home Named Entity Recognition (NER) support,Getting Tika up and running with Stanford Core NLP and with OpenNLP- How to use Tika with Stanford NER/NLP and with Apache Open NLP.Getting Tika up and running with NLTK- How to use Tika with the Python Natural Language Toolkit (NLTK).Getting Tika up and running with Grobid Quantities Measurement Parsing- How to use Tika with the Grobid Quantities measurement parser.Getting Tika up and running with MIT Lincoln Lab's MIT-nlp Information Extraction (MITIE) toolkit- How to use Tika with MITIE from MIT-NLP.Getting Tika up and running with automatic Age Detection from Text- How to use Tika with USC IRDS age detection tools.,../data/confluence_exports/TIKA/Home_109454051.html
TIKA : Home Images,Getting Tika up and running for Image Visual Recognition- How to use Tika with Tensorflow's Inception-V4 ImageNet for visual recognition of images.Getting Tika up and running for Image Visual Recognition using Deep Learning 4J (DL4J)- How to use Tika with Tensorflow's Inception V-3 ImageNet and VGG-16 for visual recognition in pure Java.Getting Tika up and running for Computer Vision - Image Captioning- How to use Tika with Tensorflow for combining Computer Vision and NLP to automatically generate captions of images.,../data/confluence_exports/TIKA/Home_109454051.html
TIKA : Home Video,Getting Tika up and running for Video Visual Recognition- How to use Tika with Tensorflow's Inception-V4 ImageNet for visual recognition of videos.,../data/confluence_exports/TIKA/Home_109454051.html
TIKA : Home Statistical Machine Translation,"Statistical Machine Translation with Apache Joshua (Incubating)- A guide for leveraging Apache Joshua for language translation via the Tika.translate API.Neural Machine Translation powered by Reader Translator Generator toolkit- A guide for RTG integration with Tika.translate API  MetadataDiscussion- discussions on the design of MIME type detection and parsing for recursive metadata formats (and container formats) like Zip, etc.RecursiveMetadata- proposals for dealing with recursive metadata, based on theMetadataDiscussionpage.Tika JAX-RS Server- documentation on the recently contributed tika-server module.Metadata roadmap- Documentation and Discussion about the metadata roadmap for TikaErrors and Exceptions- What parsers should output/throw when, for empty/invalid/unsupported filesComposite Parsers discussion- How to give users sensible+clear control of multiple parsers for a given file typeTika 2.0 discussion- Roadmap for changes we would like to make for Tika 2.0Tika 2.0 Migration Guide- Guide for migrating to Tika 2.0 (once it is available)  Apache Tika MeetupsOpen Preserve Foundation Talk on Embedded Files  How to run tika-eval on the VM",../data/confluence_exports/TIKA/Home_109454051.html
TIKA : TikaAndVisionVideo 1. Tensorflow Using REST Server,"This is the recommended way for utilizing visual recognition capability of Tika.This approach uses Tensorflow over REST API.To get this working, we are going to start a python flask based REST API server and tell tika to connect to it.All these dependencies and setup complexities are isolated in docker image. Requirements : Docker -- VisitDocker.comand install latest version of Docker. (Note: tested on docker v17.03.1)",../data/confluence_exports/TIKA/TikaAndVisionVideo_148640959.html
TIKA : TikaAndVisionVideo Step 1. Setup REST Server,You can either start the REST server in an isolated docker container or natively on the host that runs tensorflow.,../data/confluence_exports/TIKA/TikaAndVisionVideo_148640959.html
TIKA : TikaAndVisionVideo a. Using docker (Recommended),"Toggle line numbers 1git clone https://github.com/USCDataScience/tika-dockers.git &&cdtika-dockers2docker build -f InceptionVideoRestDockerfile -t uscdatascience/inception-video-rest-tika .3docker run -p 8764:8764 -it uscdatascience/inception-video-rest-tika Once it is done, test the setup by visitinghttp://localhost:8764/inception/v4/classify/video?topn=5&min_confidence=0.015&url=http://dev.exiv2.org/attachments/344/MOV_0234.mp4&mode=fixed&ext=.m4vin your web browser. Sample output from API: {
        ""confidence"": [
                0.29454298615455626,
                0.2789864711463451,
                0.08245106576941907,
                0.07252519279718399,
                0.05011849589645863
        ],
        ""classnames"": [
                ""screen, CRT screen"",
                ""hand-held computer, hand-held microcomputer"",
                ""oscilloscope, scope, cathode-ray oscilloscope, CRO"",
                ""monitor"",
                ""cellular telephone, cellular phone, cellphone, cell, mobile phone""
        ],
        ""classids"": [
                783,
                591,
                689,
                665,
                488
        ],
        ""time"": {
                ""read"": 0,
                ""units"": ""ms"",
                ""classification"": 20298
        }
} Note: MAC USERS: If you are using an older version, say, 'Docker toolbox' instead of the newer 'Docker for Mac', you need to add port forwarding rules in your Virtual Box default machine. Open the Virtual Box Manager.Select your Docker Machine Virtual Box image.Open Settings -> Network -> Advanced -> Port Forwarding.Add an appname,Host IP 127.0.0.1 and set both ports to 8764.",../data/confluence_exports/TIKA/TikaAndVisionVideo_148640959.html
TIKA : TikaAndVisionVideo b. Without Using docker,"If you chose to setup REST server without a docker container, you are free to manually install all the required tools specified in thedocker file. Note: docker file has setup instructions for Ubuntu, you will have to transform those commands for your environment. Below are the main requirements - TensorflowOpenCV with python and video supportFlask and requests -pip install flask requests  Toggle line numbers 1python tika-parsers/src/main/resources/org/apache/tika/parser/recognition/tf/inceptionapi.py  --port 8764",../data/confluence_exports/TIKA/TikaAndVisionVideo_148640959.html
TIKA : TikaAndVisionVideo Step 2. Create a Tika-Config XML to enable Tensorflow Video parser.,"A sample config can be found in Tika source code attika-parsers/src/test/resources/org/apache/tika/parser/recognition/tika-config-tflow-video-rest.xml Here is an example: Toggle line numbers 1<properties>2<parsers>3<parserclass=""org.apache.tika.parser.recognition.ObjectRecognitionParser"">4<mime>video/mp4</mime>5<params>6<paramname=""topN""type=""int"">4</param>7<paramname=""minConfidence""type=""double"">0.015</param>8<paramname=""class""type=""string"">org.apache.tika.parser.recognition.tf.TensorflowRESTVideoRecogniser</param>9<paramname=""healthUri""type=""uri"">http://localhost:8764/inception/v4/ping</param>10<paramname=""apiUri""type=""uri"">http://localhost:8764/inception/v4/classify/video?mode=fixed</param>11</params>12</parser>13</parsers>14</properties> Description of parameters :  Param NameTypeMeaningRangeExampletopNintNumber of object names to outputa non-zero positive integer1 to receive top 1 object name.minConfidencedoubleMinimum confidence required to output the name of detected objects[0.0 to 1.0] inclusive0.9 for outputting object names iff at least 90% confident.classstringClass that implements object recognition functionalityconstant stringorg.apache.tika.parser.recognition.tf.TensorflowRESTVideoRecogniserhealthUriuriThe URI link to the ping service to ensure that the Video classification service is up and running.apiUriuriThe URI link to the Python OpenCV/ImageNet based Video classification service.",../data/confluence_exports/TIKA/TikaAndVisionVideo_148640959.html
TIKA : TikaAndVisionVideo Step 3: Demo,"To use the vision capability via Tensorflow, just supply the above configuration to Tika. For example, to use in Tika App (Assuming you havetika-appJAR and it is ready to run and you are running from the Tika src checkout dir):  $ java -jar tika-app/target/tika-app-1.15-SNAPSHOT.jar \
        --config=tika-parsers/src/test/resources/org/apache/tika/parser/recognition/tika-config-tflow-video-rest.xml \ 
       ./tika-parsers/src/test/resources/test-documents/testVideoMp4.mp4 The input video is from:Sample Videos.com. Big Rabbit coming out of his house And, the top 4 detections are: Toggle line numbers 1...2<metaname=""OBJECT""content=""king penguin, Aptenodytes patagonica (0.19076)""/>3<metaname=""OBJECT""content=""hare (0.13538)""/>4<metaname=""OBJECT""content=""wallaby, brush kangaroo (0.09441)""/>5<metaname=""OBJECT""content=""ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus (0.09350)""/>6...",../data/confluence_exports/TIKA/TikaAndVisionVideo_148640959.html
"TIKA : TikaAndVisionVideo Changing the default topN, API port or URL","To change the defaults, update the parameters in config XML file accordingly. Below is a description of parameters accepted by Inception REST API, they can be passed throughapiUriin config file. Param NameTypeDescriptionDefault ValuemodestringAvailable Modes of frame extractioncenter""center"" - Just one frame in center""interval"" - Extracts frames after fixed interval""fixed"" - Extract fixed number of framesframe-intervalintInterval for frame extraction to be used with INTERVAL mode. If frame_interval=10 then every 10th frame will be extracted10num-frameintNumber of frames to be extracted from video while using FIXED model. If num_frame=10 then 10 frames equally distant from each other will be extracted10 If you want to run video recognition by extracting labels from every 100th image you can make anapiUrilikehttp://localhost:8764/inception/v4/classify/video?mode=interval&frame-interval=100 Here is an example scenario: Run REST API on port 3030, and get top 4 object names if the confidence is above 10%. You may also change host to something else than 'localhost' if required. Example Config File <properties>
    <parsers>
        <parser class=""org.apache.tika.parser.recognition.ObjectRecognitionParser"">
            <mime>image/jpeg</mime>
            <params>
                <param name=""topN"" type=""int"">4</param>
                <param name=""minConfidence"" type=""double"">0.1</param>
                <param name=""class"" type=""string"">org.apache.tika.parser.recognition.tf.TensorflowRESTRecogniser</param>
                <param name=""healthUri"" type=""uri"">http://localhost:3030/inception/v4/ping</param>
                <param name=""apiUri"" type=""uri"">http://localhost:3030/inception/v4/classify/video?topk=4</param>
            </params>
        </parser>
    </parsers>
</properties> To Start the service on port 3030: Using Docker: docker run -it -p 3030:8764 uscdatascience/inception-video-rest-tika Without Using Docker: python tika-parsers/src/main/resources/org/apache/tika/parser/recognition/tf/inceptionapi.py  --port 3030",../data/confluence_exports/TIKA/TikaAndVisionVideo_148640959.html
TIKA : TikaAndVisionVideo Installing Tensorflow,"To install tensorflow, follow the instructions onthe official site herefor your environment.Unless you know what you are doing, you are recommended to follow pip installation. Then clone the repositorytensorflow/modelsor download thezip file. git clonehttps://github.com/tensorflow/models.git Add 'models/slim' folder to the environment variable, PYTHONPATH. $ export PYTHONPATH=""$PYTHONPATH:/path/to/models/slim"" To test the readiness of your environment : $ python -c 'import tensorflow, numpy, dataset; print(""OK"")' If the above command prints the message ""OK"", then the requirements are satisfied.",../data/confluence_exports/TIKA/TikaAndVisionVideo_148640959.html
TIKA : TikaAndVisionVideo Installing OpenCV,"OpenCV is a very popular computer vision library. We are using it to extract still images from video file. For this to work you need to install OpenCV with video support and python end point. Installation will vary from platform to platform so if you don't have enough experience with installing C packages it's a good idea to use docker build. - On Mac you can use homebrew to install OpenCV and make sure cv2.py is in your python path.  brew tap homebrew/science 
brew install opencv -docker commands on ubuntu To verify if your setup is correct you can run below commands and they should print ""SUCCESS"" Toggle line numbers 1curl -Lo testVideoMp4.mp4""https://github.com/apache/tika/blob/e141640891cd7adcfc1848b351c0db7eab00a2d2/tika-parsers/src/test/resources/test-documents/testVideoMp4.mp4?raw=true""2python -c""import cv2; cap = cv2.VideoCapture('testVideoMp4.mp4'); print 'SUCCESS' if cap.isOpened() else 'FAILURE'""",../data/confluence_exports/TIKA/TikaAndVisionVideo_148640959.html
TIKA : TikaAndVisionVideo Questions / Suggestions / Improvements / Feedback ?,"If it was useful, let us know on twitter by mentioning@ApacheTikaIf you have questions, let us know byusing Mailing ListsIf you find any bugs,use Jira to report them",../data/confluence_exports/TIKA/TikaAndVisionVideo_148640959.html
TIKA : CompositeParserDiscussion Classic,Sort the parsers by non-tika vs tika and then alphabetically by class name.  Pick the first parser that will handle a given mime type.,../data/confluence_exports/TIKA/CompositeParserDiscussion_109454044.html
TIKA : CompositeParserDiscussion Supplementary/Additive,"Concatenate the results (metadata and content) for several parsers  For Metadata, this merging should be configurable if multiple parsers output the same Metadata Key, between:  First/earliest parser's value(s) winLast/latest parser's value(s) winCapture all and return multiple values  TODOFor Content, decide how we could support appending or resetting of the SAX stream  We need a better name for this!",../data/confluence_exports/TIKA/CompositeParserDiscussion_109454044.html
TIKA : CompositeParserDiscussion Back-off,"Try one parser and if the output doesn't meet some criterion, apply another.  One use case for this might be: if a file is identified as XML, try the XMLParser and if that throws an exception, try the HTMLParser.",../data/confluence_exports/TIKA/CompositeParserDiscussion_109454044.html
TIKA : CompositeParserDiscussion Pick the Best Output,One use case for this: the charset detector identifies two equally likely charsets.  Apply both and use the wished-for junk detector (TIKA-1443) to determine which output is more likely to be not junk.,../data/confluence_exports/TIKA/CompositeParserDiscussion_109454044.html
TIKA : CompositeParserDiscussion Fastest,"If there are two parsers, use the faster one even if it might mean lower quality (eg avoid OCR)   Consider a case like:  application/vnd.ms-excelapplication/x-tika-msoffice  Or  application/dita+xml;format=conceptapplication/dita+xml;format=topicapplication/dita+xml  If there were two parsers available for application/vnd.ms-excel, and another for application/x-tika-msoffice, should it be possible to specify in a strategy that a parser for the parent type also be used? Should it be possible to set a strategy like ""use the dita concept, then the general dita, then the dita topic"", hopping around up and down the hierarchy?  Or do we keep the current behaviour where once a point in the hierachy with a parser is found, it is parsed at that point?   The right strategy for one user may not be the right for another. The right strategy for one file may not be the right one for another. We therefore need to allow users to pick their strategy, on an overall basis, and on a per-file basis",../data/confluence_exports/TIKA/CompositeParserDiscussion_109454044.html
TIKA : CompositeParserDiscussion FromTikaConfig,"Currently, a great many Tika users just callTikaConfig.getDefaultConfig()and go with that.  It might be nice if they could also do things likeTikaConfig.getMaxiumMetadataConfig()orTikaConfig.getTryEachInTurnConfig()to pick a different strategy  (Naming TBC, align with above)",../data/confluence_exports/TIKA/CompositeParserDiscussion_109454044.html
TIKA : CompositeParserDiscussion With a Tika Configuration file,"Users may wish to have full control over what parsers are used, what strategies are used for which mime types etc  For example, they might want default behaviour for most types, but to send XML through a fallback parser, and combine Image + GDAL + OCR for jpeg. The configuration file needs to support this  <parsers>
    <!-- Most things can use the default -->
    <parser class=""org.apache.tika.parser.DefaultParser"">
      <!-- Don't use DefaultParser for these mimetypes, alternate config below -->
      <mime-exclude>image/jpeg</mime-exclude>
      <mime-exclude>application/xml</mime-exclude>
      <mime-exclude>application/pdf</mime-exclude>
      <!-- Exclude (blacklist) these parsers, have them ignored by DefaultParser -->
      <parser-exclude class=""org.apache.tika.parser.jdbc.SQLite3Parser""/>
      <parser-exclude class=""org.apache.tika.parser.executable.ExecutableParser""/>
    </parser>

    <!-- No PDF, thank you! -->
    <parser class=""org.apache.tika.parser.EmptyParser"">
      <mime>application/pdf</mime>
    </parser>

    <!-- JPEG needs special handling - try+combine everything -->
    <parser class=""org.apache.tika.parser.(suppliment)"">
       <params>
          <!-- If several parsers output the same metadata key, first parser to do so wins -->
          <param name=""metadataPolicy"" value=""FIRST_WINS"" />
          <!-- If several parsers output the same metadata key, last parser to do so wins -->
          <!--
          <param name=""metadataPolicy"" value=""LAST_WINS"" />
           -->
          <!-- If several parsers output the same metadata key, store all their values -->
          <!--
          <param name=""metadataPolicy"" value=""KEEP_ALL"" />
           -->
       </params>
       <parser class=""org.apache.tika.parser.ocr.TesseractOCRParser"" />
       <parser class=""org.apache.tika.parser.image.ImageParser"" />
       <parser class=""org.apache.tika.parser.jpeg.JpegParser"" />
       <parser class=""org.apache.tika.parser.gdal.GDALParser"" />
       <!-- TODO DO we need to give mimetypes here too? Or can we get implicitly? -->
    </parser>

    <!-- XML needs special handling - use fallbacks to get something -->
    <parser class=""org.apache.tika.parser.(fallback)"">
       <params>
          <!-- If we move onto a second/third/etc parser, discard metadata from previous parsers -->
          <param name=""metadataPolicy"" value=""DISCARD"" />
          <!-- If we move onto a second parser, and both output the same metadata key, earlier parser wins -->
          <!--
          <param name=""metadataPolicy"" value=""FIRST_WINS"" />
           -->
          <!-- If we move onto a second parser, and both output the same metadata key, later parser wins -->
          <!--
          <param name=""metadataPolicy"" value=""LAST_WINS"" />
           -->
          <!-- If move onto a second parser, and both output the same metadata key, store all their values -->
          <!--
          <param name=""metadataPolicy"" value=""KEEP_ALL"" />
           -->
       </params>
       <parser class=""my.custom.xml.parser"" />
       <parser class=""org.apache.tika.parser.xml.XMLParser"" />
       <parser class=""org.apache.tika.parser.html.HTMLParser"" />
       <parser class=""org.apache.tika.parser.txt.TXTParser"" />
       <mime>application/xml</mime>
    </parser>
  </parsers>",../data/confluence_exports/TIKA/CompositeParserDiscussion_109454044.html
TIKA : CompositeParserDiscussion In Code,"Whatever we do, this must be available from code too, much as how today people can create customCompositeParserinstances, or wrap things up with customParserDecoratorinstances  We also need examples for all of these, not only in unit tests, but also in the examples package.",../data/confluence_exports/TIKA/CompositeParserDiscussion_109454044.html
TIKA : Open Preservation Foundation Talk -- 21 September 2022 Data,https://corpora.tika.apache.org/base/share/opf-embedded-files-data-20220920.tgz Read thenotes.txtfile for sources of the data.,../data/confluence_exports/TIKA/Open-Preservation-Foundation-Talk----21-September-2022_228495105.html
TIKA : Open Preservation Foundation Talk -- 21 September 2022 Prerequisites,"Must have Java >= 8 installed.If you have tesseract installed, you may want to move it/turn it off (e.g.sudo mv /usr/local/bin/tesseract /usr/local/bin/tesseract2)A json viewer: Notepad++ or Sublime Text or ....?",../data/confluence_exports/TIKA/Open-Preservation-Foundation-Talk----21-September-2022_228495105.html
TIKA : Open Preservation Foundation Talk -- 21 September 2022 Download,tika-app:https://dlcdn.apache.org/tika/2.4.1/tika-app-2.4.1.jaroptional:https://repo1.maven.org/maven2/com/github/jai-imageio/jai-imageio-jpeg2000/1.4.0/jai-imageio-jpeg2000-1.4.0.jar,../data/confluence_exports/TIKA/Open-Preservation-Foundation-Talk----21-September-2022_228495105.html
TIKA : Open Preservation Foundation Talk -- 21 September 2022 Commandlines,"Use the simple UI: java -jar tika-app-2.4.1.jar. Then drag and dropopf-embedded-files-data-20220920/ooxml/test_recursive_embedded.docxinto the window and select ""Recursive JSON.""Run Tika against a single file, pipe output to a json file:java -jar tika-app-2.4.1.jar -J -t opf-embedded-files-data-20220920/ooxml/test_recursive_embedded.docx > test_recursive.jsonRun Tika in batch mode:java -jar tika-app-2.4.1.jar -J -t -i opf-embedded-files-data-20220920 -o opf-embedded-files-data-20220920-extractsTo extract the literal embedded files (first level only!):java -jar tika-app-2.4.1.jar --extract-dir=261779-attachments -z opf-embedded-files-data-20220920/ppt/govdocs1/261779.ppt",../data/confluence_exports/TIKA/Open-Preservation-Foundation-Talk----21-September-2022_228495105.html
TIKA : Open Preservation Foundation Talk -- 21 September 2022 Advanced,"Because of license issues, Tika cannot bundle thejpeg2000 parser.  Thedigitally_signed_3D_Portfolio.pdffile contains jpeg2000. If the jpeg2000 parser's license is acceptable to you, put it in and the tika-app.X.Y.Z.jar in abindirectory and call tika-app's main class:java -cp ""bin/*"" org.apache.tika.cli.TikaCLI --extract-dir=portfolio-files -z opf-embedded-files-data-20220920/pdfs/OPF-format-corpus/digitally_signed_3D_Portfolio.pdf",../data/confluence_exports/TIKA/Open-Preservation-Foundation-Talk----21-September-2022_228495105.html
TIKA : CommonCrawl3 Office formats,"The top 10 file formats of this category include:  mimecountapplication/pdf4,128,740application/vnd.openxmlformats-officedocument.wordprocessingml.document53,579application/msword52,087application/rtf22,509application/vnd.ms-excel22,067application/vnd.openxmlformats-officedocument.spreadsheetml.sheet16,290application/vnd.oasis.opendocument.text8,314application/vnd.openxmlformats-officedocument.presentationml.presentation6,835application/vnd.ms-powerpoint5,799application/vnd.openxmlformats-officedocument.presentationml.slideshow2,465  select mime, sum(count) cnt
from detected_mimes
where 
(mime ilike '%pdf%' 
 OR 
 mime similar to '%(word|doc|power|ppt|excel|xls|application.*access|outlook|msg|visio|rtf|iwork|pages|numbers|keynot)%'
)
group by mime
order by cnt desc  Given how quickly the tail drops off, we could afford to take all of the non-PDFs.  For PDFs, we created a sampling frame by TLD.  We usedorg.tallison.cc.index.mappers.DownSampleto select files for downloading from Common Crawl.",../data/confluence_exports/TIKA/CommonCrawl3_109454042.html
TIKA : CommonCrawl3 Other Binaries,"These are the top 10 other binaries:  mimecntimage/jpeg6,207,029application/rss+xml3,495,173application/atom+xml2,868,625application/xml1,353,092image/png585,019application/octet-stream330,029application/json237,232application/rdf+xml229,766image/gif166,851application/gzip151,940  select mime, sum(count) cnt
from detected_mimes
where 
(mime not ilike '%pdf%' 
 and
 mime not similar to '%(word|doc|power|ppt|excel|xls|application.*access|outlook|msg|visio|rtf|iwork|pages|numbers|keynot)%'
 and mime not ilike '%html%'
 and mime not ilike '%text%'
)
group by mime
order by cnt desc  I created the sampling ratios from for these by preferring non-xml, but likely text-containing file types.  Further, I wanted to include a fairly large portion ofoctet-streamso that we might be able to see how we can improve Tika's file detection.  We usedorg.tallison.cc.index.mappers.DownSampleto select files for downloading from Common Crawl.",../data/confluence_exports/TIKA/CommonCrawl3_109454042.html
TIKA : CommonCrawl3 HTML/Text,"For the HTML/text files, I wanted to oversample files that were not ASCII/UTF-8 English, and I wanted to oversample files that had no charset detected.  We usedorg.tallison.cc.index.mappers.DownSampleLangCharsetto select the files for downloading from Common Crawl.",../data/confluence_exports/TIKA/CommonCrawl3_109454042.html
TIKA : CommonCrawl3 The Output,"In addition to storing the files, I generated a table for each pull that included information stored in the WARC file, including information from the http-headers as archived in Common Crawl.  The three table files are availablehere(116MB!).   Common Crawl truncates files at 1MB.  We've found it useful to have truncated files in our corpus, but this disproportionately affects some file formats, such as PDF and MSAccess files, and we wanted to have some recent, largish files in the corpus.  We selected those files that were close to 1MB or were marked as truncated:  select url,cc_digest from crawled_files
where
(cc_mime_detected ilike '%tika%'
or cc_mime_detected ilike '%power%'
or cc_mime_detected ilike '%access%'
or cc_mime_detected ilike '%rtf%'
or cc_mime_detected ilike '%pdf%'
or cc_mime_detected ilike '%sqlite%'
 or cc_mime_detected ilike '%openxml%'
 or cc_mime_detected ilike '%word%'
 or cc_mime_detected ilike '%rfc822%'
 or cc_mime_detected ilike '%apple%'
 or cc_mime_detected ilike '%excel%'
 or cc_mime_detected ilike '%sheet%'
 or cc_mime_detected ilike '%onenote%'
or cc_mime_detected ilike '%outlook%')
and (actual_length > 990000 or warc_is_truncated='TRUE')
order by random()  A rollup of the files that were to be refetched by mime type is here:  mimecountapplication/pdf121,386application/vnd.openxmlformats-officedocument.presentationml.presentation3,929application/x-tika-msoffice3,830application/vnd.ms-powerpoint2,942application/msword2,783application/vnd.openxmlformats-officedocument.wordprocessingml.document2,722application/x-tika-ooxml2,612application/vnd.openxmlformats-officedocument.presentationml.slideshow1,663application/rtf1,569application/vnd.ms-excel1,186  The full table ishere.  We usedorg.tallison.cc.WReGetter, a wrapper around 'wget' to re-fetch the files from the original URL.  If the refetched file was > 50MB, we deleted it; and if the refetch took longer than 2 minutes, we killed the process and deleted whatever bytes had been retrieved.  We refetched these files to a new directory and stored them by their new digest.  Each thread in WReGetter wrote to a table to record the mapping of the original digest to the new digest and whether the new file was successfully refetched and/or was too big.  Because of limitations of disc space, we stopped the refetch procedure after refetching 98,000 documents, comprising 440GB of data.  We then randomly deleted 80% of the original truncated files and moved the other 20% to /commoncrawl3_truncated.  Finally, we moved the refetched files into the /commoncrawl3_refetched directory.   We carried out this work on one of our TB drives.  We have to figure out what to keep from our older commoncrawl2 collection and then merge the two collections.  We may consider deleting some of the ISO-8859-1/Windows-1252/UTF-8, English text files.  We could also identify truncated files based on parser exceptions and move those into /commoncrawl3_truncated.   Top 20 ""container"" file mimes:  MimeCountapplication/pdf528,617text/plain; charset=ISO-8859-1184,019application/msword78,210application/vnd.openxmlformats-officedocument.wordprocessingml.document75,739text/html; charset=UTF-875,156text/plain; charset=windows-125274,144text/plain; charset=UTF-856,462application/octet-stream54,278application/zip44,989application/rss+xml34,213image/jpeg30,968application/atom+xml28,934image/png28,173text/html; charset=windows-125226,232application/xhtml+xml; charset=UTF-825,130text/html; charset=ISO-8859-124,515application/vnd.google-earth.kml+xml23,391application/xhtml+xml; charset=windows-125222,304application/vnd.openxmlformats-officedocument.spreadsheetml.sheet22,084application/rtf21,811  Top 20 Languages (including embedded files) as identified by language id:  LanguageNumber of Filesen1,803,350null242,442ru155,934de109,953fr96,192it73,781es59,069ja50,941pl47,044pt35,490ko35,251ca30,717fa26,202zh-cn25,379nl23,554ro23,259tr23,111da21,967br21,420vi19,305   Tobias Ospelt and Rohan Padhye (the author ofhttps://github.com/rohanpadhye/jqf) both noted on our dev list that we could use coverage analysis to identify a minimal corpus that would cover as much of our code base as possible.  Obviously, a minimal corpus designed for our current codebase would not be guaranteed to cover new features, and we'd want to leave plenty of extra files around in the hope that some of them would capture new code paths.  Nevertheless, if we could use jqf or another tool to reduce the corpus, that would help make our runs more efficient.  OnTIKA-2750, Tobias reported that his experiment withafl-cmin.pyshowed that it would take roughly four months on our single VM just to create traces (~300 files per hour).   SeeComparisonTikaAndPDFToText201811for notes on a comparison of the output of pdftotext and Tika.",../data/confluence_exports/TIKA/CommonCrawl3_109454042.html
TIKA : DeveloperResources Using Git,Pending a VOTEThis page will become valid if the VOTE for Apache Tika to move to using Git passes.  git clonehttps://git-wip-us.apache.org/repos/asf/tika.git,../data/confluence_exports/TIKA/DeveloperResources_109454047.html
TIKA : DeveloperResources (Deprecated) Old Method using SVN,"svn checkouthttps://svn.apache.org/repos/asf/tika/trunktika.trunk.  If you prefer git, you can clone repo mirror from github:git clonehttps://github.com/apache/tika.git.  To run all tests:mvn test  To run a single test:mvn test -Dtest=RTFParserTest -DfailIfNoTests=false  To run a single integration test:mvn verify -Dit.test=BundleIT -Dtest=non-existing-test -DfailIfNoTests=false,  to run integration test building only relevant part command likemvn -am -pl tika-bundle verify -Dit.test=BundleIT -Dtest=non-existing-test -DfailIfNoTests=falsecould be used.  Maven (Surefire plugin) by default suppresses the stack trace when you hit a failure; if you want to see it add-Dsurefire.useFile=falseto the command-line.",../data/confluence_exports/TIKA/DeveloperResources_109454047.html
