{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Chain of thoughts\n",
    "\n",
    "In this notebook we will try to implement \"chain of thoughts\" state-machine.\n",
    "It will use LLM to generate thoughts, actions and answers and will use tools to get results.\n",
    "\n",
    "We have only 3 dependencies:\n",
    "- transitions - to implement state-machine\n",
    "- transformers - to use LLM\n",
    "- accelerate - to use mixed precision for LLM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-28T13:58:17.499631100Z",
     "start_time": "2023-05-28T13:58:15.831092100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transitions transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create instance of LLM\n",
    "\n",
    "We will use T5 model from HuggingFace. But it could be any other LLM of relevant or greater size."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5cd417ebbe36423a9ffea4041433c45c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_id = 'google/flan-t5-xxl'\n",
    "default_pipeline = pipeline(\n",
    "    task='text2text-generation',\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-28T13:58:48.401293300Z",
     "start_time": "2023-05-28T13:58:17.500636700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementation\n",
    "\n",
    "We will create a class which implements \"chain of thoughts\" state-machine.\n",
    "It will use LLM to generate thoughts, actions and answers and will use tools to get results.\n",
    "\n",
    "Current state-machine will have following states:\n",
    "- INPUT - initial state, we will ask user a question\n",
    "- THOUGHT - we will use LLM to generate THOUGHT (answer or action)\n",
    "- ACTION - we will use LLM to generate ACTION (tool name)\n",
    "- ACTION_INPUT - we will ask user to provide input for ACTION\n",
    "- OBSERVATION - we will use LLM to generate OBSERVATION (result of tool usage)\n",
    "- ANSWER - we will use LLM to generate ANSWER (final answer)\n",
    "\n",
    "Transitions are running in following order:\n",
    "- INPUT -> THOUGHT -> ACTION -> ACTION_INPUT -> OBSERVATION -> THOUGHT -> ... -> THOUGHT -> ANSWER"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import logging\n",
    "from enum import Enum\n",
    "\n",
    "from transitions import Machine\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "THOUGHT_ELEMENTS = Enum('THOUGHT_ELEMENTS', ['INPUT', 'THOUGHT', 'ACTION', 'ACTION_INPUT', 'OBSERVATION', 'ANSWER'])\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "class ChainOfThoughts(Machine):\n",
    "    \"\"\"\n",
    "        Following class implements chain-of-thoughts state-machine.\n",
    "        LLM will decide when the answer for prompt is ready as well when it needs to use particular TOOL.\n",
    "    \"\"\"\n",
    "    states = THOUGHT_ELEMENTS\n",
    "    transitions = [\n",
    "        {'trigger': 'ask', 'source': THOUGHT_ELEMENTS.INPUT, 'dest': THOUGHT_ELEMENTS.THOUGHT},\n",
    "        {'trigger': 'act', 'source': THOUGHT_ELEMENTS.THOUGHT, 'dest': THOUGHT_ELEMENTS.ACTION},\n",
    "        {'trigger': 'answer', 'source': THOUGHT_ELEMENTS.THOUGHT, 'dest': THOUGHT_ELEMENTS.ANSWER},\n",
    "        {'trigger': 'pass_params', 'source': THOUGHT_ELEMENTS.ACTION, 'dest': THOUGHT_ELEMENTS.ACTION_INPUT},\n",
    "        {'trigger': 'observe', 'source': THOUGHT_ELEMENTS.ACTION_INPUT, 'dest': THOUGHT_ELEMENTS.OBSERVATION},\n",
    "        {'trigger': 'think', 'source': THOUGHT_ELEMENTS.OBSERVATION, 'dest': THOUGHT_ELEMENTS.THOUGHT},\n",
    "    ]\n",
    "\n",
    "    tools = None\n",
    "    llm_pipeline = None\n",
    "\n",
    "    def __init__(self,\n",
    "                 name: str,\n",
    "                 tools: list,\n",
    "                 llm_pipeline: pipeline\n",
    "                 ):\n",
    "        super().__init__(\n",
    "            states=ChainOfThoughts.states,\n",
    "            transitions=ChainOfThoughts.transitions,\n",
    "            initial=THOUGHT_ELEMENTS.INPUT,\n",
    "            send_event=True,\n",
    "        )\n",
    "        self.name = name\n",
    "        self.tools = tools\n",
    "        self.llm_pipeline = llm_pipeline\n",
    "\n",
    "    def on_enter_THOUGHT(self, events):\n",
    "        \"\"\"\n",
    "            This method will be called when we enter THOUGHT state.\n",
    "            We will use LLM to generate THOUGHT.\n",
    "            It will be one of the tools from the list.\n",
    "            If llm can produce answer without tool usages - we will go to ANSWER state.\n",
    "            If llm has to use tool and this tool exists in the list - we will go to ACTION state.\n",
    "        \"\"\"\n",
    "        question = events.kwargs.get('question')\n",
    "        history = events.kwargs.get('history', [])\n",
    "\n",
    "        tools_list = \", \".join( [f\"{tool['tool_name']} - {tool['description']}\" for tool in self.tools])\n",
    "\n",
    "        if len(history) == 0:\n",
    "            history.append(\"System: AI is here to help you.\")\n",
    "            history.append(\"User: \" + question)\n",
    "\n",
    "        history.append(\"System: AI has following tools to help you:\" + tools_list)\n",
    "        history.append(\"System: Do you need to use any of these tools to answer(yes/no)?\")\n",
    "        prompt = '\\nAI: Between yes and no, i will choose: '\n",
    "\n",
    "        assembled_prompt = \"\\n\".join(history) + prompt\n",
    "        tool_required = self.llm_pipeline(assembled_prompt)[0]['generated_text']\n",
    "        if 'yes' in tool_required.lower():\n",
    "            history.append(\"AI: I need to use a tool.\")\n",
    "            self.act(\n",
    "                history=history,\n",
    "                question=question\n",
    "            )\n",
    "        elif 'no' in tool_required.lower():\n",
    "            history.append(\"AI: I can answer without a tool.\")\n",
    "            self.answer(\n",
    "                history=history,\n",
    "                question=question\n",
    "            )\n",
    "        else:\n",
    "            logging.error(f'{self.name} is confused. Cannot understand tools requirements, got {tool_required}')\n",
    "            logging.log(f'History: {history}')\n",
    "\n",
    "    def on_enter_ACTION(self, events):\n",
    "        \"\"\"\n",
    "            This method will be called when we enter ACTION state.\n",
    "            We will use LLM to generate ACTION.\n",
    "            It will be one of the tools from the list.\n",
    "            If llm can produce answer without tool usages - we will go to ANSWER state.\n",
    "            If llm has to use tool and this tool exists in the list - we will go to ACTION state.\n",
    "        \"\"\"\n",
    "        history = events.kwargs.get('history', [])\n",
    "        question = events.kwargs.get('question')\n",
    "        history.append(\"System: Write a name of the tool you want to use\")\n",
    "        prompt = f\"AI: I will use a tool named: \"\n",
    "        assembled_prompt = \"\\n\".join(history) + prompt\n",
    "        action = self.llm_pipeline(assembled_prompt)[0]['generated_text']\n",
    "        if action in [tool['tool_name'] for tool in self.tools]:\n",
    "            history.append(f\"AI: {action}\")\n",
    "            self.pass_params(\n",
    "                history=history,\n",
    "                action=action,\n",
    "                question=question\n",
    "            )\n",
    "        else:\n",
    "            logging.error(f'{self.name} is confused. Cannot understand tool name {action}')\n",
    "            logging.debug(f'History: {history}')\n",
    "\n",
    "    def on_enter_ACTION_INPUT(self, events):\n",
    "        \"\"\"\n",
    "            This method will be called when we enter ACTION_INPUT state.\n",
    "            We will use tool to get result.\n",
    "            After that we will go to THOUGHT state again.\n",
    "        \"\"\"\n",
    "        history = events.kwargs.get('history', [])\n",
    "        question = events.kwargs.get('question')\n",
    "        action = events.kwargs.get('action')\n",
    "        prompt = \"AI: I need to pass there following input - \"\n",
    "        assembled_prompt = \"\\n\".join(history) + prompt\n",
    "        action_input = self.llm_pipeline(assembled_prompt)[0]['generated_text']\n",
    "        history.append(f\"AI: {action_input}\")\n",
    "        self.observe(\n",
    "            history=history,\n",
    "            action=action,\n",
    "            action_input=action_input,\n",
    "            question=question\n",
    "        )\n",
    "\n",
    "    def on_enter_OBSERVATION(self, events):\n",
    "        \"\"\"\n",
    "            This method will be called when we enter OBSERVATION state.\n",
    "            We will use tool to get result.\n",
    "            After that we will go to THOUGHT state again.\n",
    "        \"\"\"\n",
    "        history = events.kwargs.get('history', [])\n",
    "        question = events.kwargs.get('question')\n",
    "        action = events.kwargs.get('action')\n",
    "        action_input = events.kwargs.get('action_input')\n",
    "\n",
    "        tool = [tool for tool in self.tools if tool['tool_name'] == action][0]['func']\n",
    "        result = tool(action_input)\n",
    "        history.append(f\"System: This tool produced next result: {result}\")\n",
    "        self.think(history=history, question=question)\n",
    "\n",
    "    def on_enter_ANSWER(self, events):\n",
    "        \"\"\"\n",
    "            This method will be called when we enter ANSWER state.\n",
    "            We will use LLM to generate answer.\n",
    "        \"\"\"\n",
    "        history = events.kwargs.get('history', [])\n",
    "        question = events.kwargs.get('question')\n",
    "        prompt = f\"AI: Your original question was - {question}. And during my research I found following answer - \"\n",
    "        assembled_prompt = \"\\n\".join(history) + prompt\n",
    "        answer = self.llm_pipeline(assembled_prompt)[0]['generated_text']\n",
    "        history.append(f\"{prompt}{answer}\")\n",
    "        history.append(\"System: Thank you for using AI. Goodbye!\")\n",
    "        print(\"\\n\".join(history))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-28T13:58:48.405350600Z",
     "start_time": "2023-05-28T13:58:48.400297Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Usage\n",
    "\n",
    "It the current example we provided two stubs for tools - one is for getting articles from Wikipedia and another one is for googling."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transitions.core:solverFinished processing state INPUT exit callbacks.\n",
      "INFO:transitions.core:solverFinished processing state THOUGHT exit callbacks.\n",
      "INFO:transitions.core:solverFinished processing state ACTION exit callbacks.\n",
      "INFO:transitions.core:solverFinished processing state ACTION_INPUT exit callbacks.\n",
      "INFO:transitions.core:solverFinished processing state OBSERVATION exit callbacks.\n",
      "INFO:transitions.core:solverFinished processing state THOUGHT exit callbacks.\n",
      "INFO:transitions.core:solverExecuted callback 'on_enter_ANSWER'\n",
      "INFO:transitions.core:solverFinished processing state ANSWER enter callbacks.\n",
      "INFO:transitions.core:solverExecuted callback 'on_enter_THOUGHT'\n",
      "INFO:transitions.core:solverFinished processing state THOUGHT enter callbacks.\n",
      "INFO:transitions.core:solverExecuted callback 'on_enter_OBSERVATION'\n",
      "INFO:transitions.core:solverFinished processing state OBSERVATION enter callbacks.\n",
      "INFO:transitions.core:solverExecuted callback 'on_enter_ACTION_INPUT'\n",
      "INFO:transitions.core:solverFinished processing state ACTION_INPUT enter callbacks.\n",
      "INFO:transitions.core:solverExecuted callback 'on_enter_ACTION'\n",
      "INFO:transitions.core:solverFinished processing state ACTION enter callbacks.\n",
      "INFO:transitions.core:solverExecuted callback 'on_enter_THOUGHT'\n",
      "INFO:transitions.core:solverFinished processing state THOUGHT enter callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: AI is here to help you.\n",
      "User: What is the recent news in Czechia?\n",
      "System: AI has following tools to help you:Google - useful when you need to find some actual information, Wikipedia - useful when you need to get encyclopedic information about something\n",
      "System: Do you need to use any of these tools to answer(yes/no)?\n",
      "AI: I need to use a tool.\n",
      "System: Write a name of the tool you want to use\n",
      "AI: Google\n",
      "AI: Search for \"Czech Republic news\"\n",
      "System: This tool produced next result: Huge amount of people are protesting in Brno today is a news item\n",
      "System: AI has following tools to help you:Google - useful when you need to find some actual information, Wikipedia - useful when you need to get encyclopedic information about something\n",
      "System: Do you need to use any of these tools to answer(yes/no)?\n",
      "AI: I can answer without a tool.\n",
      "AI: Your original question was - What is the recent news in Czechia?. And during my research I found following answer - Huge amount of people are protesting in Brno today is a news item\n",
      "System: Thank you for using AI. Goodbye!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"tool_name\": \"Google\",\n",
    "        \"description\": \"useful when you need to find some actual information\",\n",
    "        \"func\": lambda query: \"Huge amount of people are protesting in Brno today is a news item\",\n",
    "    },\n",
    "    {\n",
    "        \"tool_name\": \"Wikipedia\",\n",
    "        \"description\": \"useful when you need to get encyclopedic information about something\",\n",
    "        \"func\": lambda query: \"404: Page not found\",\n",
    "    }\n",
    "]\n",
    "\n",
    "solver = ChainOfThoughts(name='solver', llm_pipeline=default_pipeline, tools=tools)\n",
    "\n",
    "solver.ask(question='What is the recent news in Czechia?')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-28T13:59:06.533029500Z",
     "start_time": "2023-05-28T13:58:48.408326400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
