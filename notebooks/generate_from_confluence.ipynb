{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning a model based on raw documents from Confluence\n",
    "\n",
    "This notebook contains code for fine-tuning a model based on raw documents from Confluence.\n",
    "\n",
    "## Introduction\n",
    "The process will contain several parts:\n",
    "\n",
    "- Data downloading\n",
    "We downloaded several examples from public available apache foundation Confluence to make a raw dataset. This step done outside of this notebook. You can read more about Confluence export here: [https://confluence.atlassian.com/doc/export-content-to-word-pdf-html-and-xml-139475.html](https://confluence.atlassian.com/doc/export-content-to-word-pdf-html-and-xml-139475.html)\n",
    "- Data extraction\n",
    "For data extraction from dumps we will use Apache Tika running on a separate docker container.\n",
    "Apache Tika - is a toolkit for detecting and extracting metadata and structured text content from various documents using existing parser libraries. You can read more about it here: [https://tika.apache.org/](https://tika.apache.org/)\n",
    "- Data processing\n",
    "We will use the dataset library to process the data. It is a library for loading and processing datasets in a few lines of code. You can read more about it here: [https://huggingface.co/docs/datasets/](https://huggingface.co/docs/datasets/)\n",
    "Also we have to extract instruction and data from the raw data.\n",
    "- Data augmentation\n",
    "Augmentation of dataset is a process of creating new data from existing data. In this case we use the model for paraphrasing to create new questions and answers.\n",
    "- Model fine-tuning\n",
    "Using modern techniques as PEFT, DeepSpeed, LoRA and Accelerate we will fine-tune the model on the dataset. You can read more about it here: [https://huggingface.co/transformers/training.html](https://huggingface.co/transformers/training.html) [https://huggingface.co/blog/peft](https://huggingface.co/blog/peft)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data extraction\n",
    "\n",
    "In this stage we will have output in the format of {path, header, text}[]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install datasets beautifulsoup4 requests tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T21:30:42.382025Z",
     "end_time": "2023-05-02T21:30:43.694064Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "TIKA_SERVER = \"http://localhost:9998/tika\"\n",
    "BATCH_SIZE = 10  # You can adjust the batch size according to your needs\n",
    "input_directory = os.path.join(\"..\", \"data\", \"confluence_exports\")\n",
    "output_path = os.path.join(\"..\", \"datasets\", \"confluence_exports.json\")\n",
    "include_extensions = [\".md\", \".html\", \".adoc\"]\n",
    "\n",
    "\n",
    "# Getting the list of articles to process\n",
    "def get_files_to_process(root_path):\n",
    "    for dirpath, _, filenames in os.walk(root_path):\n",
    "        for filename in filenames:\n",
    "            if any(filename.endswith(ext) for ext in include_extensions):\n",
    "                yield os.path.join(dirpath, filename)\n",
    "\n",
    "\n",
    "# Function to determine if the list is mostly links\n",
    "def is_mostly_links(tag):\n",
    "    if tag.name != \"li\":\n",
    "        return False\n",
    "    link_count = len(tag.find_all(\"a\"))\n",
    "    total_count = len(tag.find_all(recursive=False))\n",
    "    return link_count / total_count > 0.5 if total_count > 0 else False\n",
    "\n",
    "\n",
    "# Extract text from the tag excluding links\n",
    "def extract_text(tag):\n",
    "    if tag is None or is_mostly_links(tag):\n",
    "        return \"\"\n",
    "    if not hasattr(tag, \"contents\"):\n",
    "        return tag.strip()\n",
    "    return \"\".join([extract_text(child) for child in tag.contents])\n",
    "\n",
    "\n",
    "# Parse and clean HTML using BeautifulSoup library\n",
    "def parse_and_clean_html(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    for tag in soup([\"script\", \"style\", \"nav\"]):\n",
    "        tag.decompose()\n",
    "    main_header = soup.find(\"h1\")\n",
    "    main_header_text = main_header.get_text(strip=True) if main_header else \"\"\n",
    "    for subheader in soup.find_all([\"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]):\n",
    "        subheader_text = subheader.get_text(strip=True)\n",
    "        text = extract_text(subheader.find_next_sibling())\n",
    "        if main_header_text:\n",
    "            subheader_text = f\"{subheader_text}\"\n",
    "        yield {\n",
    "            \"header\": subheader_text,\n",
    "            \"text\": text.strip(),\n",
    "            \"topic\": main_header_text\n",
    "        }\n",
    "\n",
    "\n",
    "# Process files using Apache Tika server\n",
    "def process_file(files):\n",
    "    for filePath in tqdm(files, desc=\"Processing files\"):\n",
    "        with open(filePath, \"rb\") as f:\n",
    "            headers = {\"Accept\": \"text/html\"}\n",
    "            data = f.read()\n",
    "            response = requests.put(TIKA_SERVER, data=data, headers=headers)\n",
    "            html = response.text\n",
    "            subtexts = parse_and_clean_html(html)\n",
    "            for entry in subtexts:\n",
    "                yield {\n",
    "                    \"file\": filePath,\n",
    "                    \"header\": entry[\"header\"],\n",
    "                    \"text\": entry[\"text\"],\n",
    "                    \"topic\": entry[\"topic\"]\n",
    "                }\n",
    "\n",
    "\n",
    "# Filter out entries with empty content\n",
    "def has_content(example, min_length=3):\n",
    "    return len(example[\"header\"].strip().split(\" \")) >= min_length and len(example[\"text\"].strip().split(\" \")) >= min_length\n",
    "\n",
    "files_to_process = list(get_files_to_process(input_directory))\n",
    "dataframe = pd.DataFrame(process_file(files_to_process))\n",
    "\n",
    "filtered_dataframe = dataframe[dataframe.apply(has_content, axis=1)]\n",
    "filtered_dataframe.to_json(output_path, orient=\"records\", indent=4)\n",
    "filtered_dataframe.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T21:37:21.182342Z",
     "end_time": "2023-05-02T21:37:23.804852Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data processing\n",
    "\n",
    "First of all let's classify elements in the dataset into 3 categories: instruction, question-answer and trash. We will use the model for classification from the transformers library. You can read more about it here: [https://huggingface.co/transformers/task_summary.html#sequence-classification](https://huggingface.co/transformers/task_summary.html#sequence-classification)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install --upgrade --ignore-installed datasets transformers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T21:30:46.486065Z",
     "end_time": "2023-05-02T21:30:49.195727Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers.pipelines.base import KeyDataset\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "processed_data_path = os.path.join(\"..\", \"datasets\", \"confluence_exports-classified\")\n",
    "candidate_labels = [\"instruction\", \"question-answer\", \"other\"]\n",
    "\n",
    "classification_pipeline = pipeline(model=\"facebook/bart-large-mnli\", device='cuda:0')\n",
    "generation_pipeline = pipeline(\"text2text-generation\", model=\"t5-base\", tokenizer=\"t5-base\", device='cuda:0')\n",
    "\n",
    "def generate_query(samples):\n",
    "    return [f\"generate question: {sample['header']} context: {sample['text']}\" for sample in samples]\n",
    "\n",
    "ds = Dataset.from_json(output_path)\n",
    "ds = ds.add_column(\"query\", generate_query(ds))\n",
    "\n",
    "def extract_label(samples):\n",
    "    return [x[\"generated_text\"].split(\":\")[1].strip() for x in samples]\n",
    "\n",
    "classified_pipeline = tqdm(classification_pipeline(KeyDataset(ds, \"query\"), truncation=True, candidate_labels=candidate_labels, batch_size=BATCH_SIZE), total=len(ds), desc=\"Classifying dataset\")\n",
    "\n",
    "ds = ds.add_column(\"label\", [x[\"labels\"][0] for x in classified_pipeline])\n",
    "\n",
    "ds = ds.filter(lambda x: x[\"label\"] != \"other\")\n",
    "\n",
    "ds.save_to_disk(processed_data_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T21:38:47.802844Z",
     "end_time": "2023-05-02T21:40:42.882978Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data adjustments\n",
    "\n",
    "Since headers isn't a great to be directly used as a question, or instruction query - we will generate prompts for each required entity based on it's label."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T21:41:22.746751Z",
     "end_time": "2023-05-02T21:41:22.809564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'section': 'Creating the branch',\n 'text': \"Creating a branch is quick and easy#start off in the apache trunk\\ngit checkout trunk\\n#create a new branch from trunk\\ngit branch HDFS-775\\n#switch to it\\ngit checkout HDFS-775\\n#show what's branch you are in\\ngit branchRemember, this branch is local to your machine. Nobody else can see it until you push up your changes or generate a patch, or you make your machine visible over the network to interested parties.\",\n 'file': '../data/confluence_exports/HADOOP/Git-And-Hadoop_89071914.html',\n 'page': 'Hadoop : Git And Hadoop',\n 'query': \"Page: Hadoop : Git And Hadoop\\nSection: Creating the branch\\nText: Creating a branch is quick and easy#start off in the apache trunk\\ngit checkout trunk\\n#create a new branch from trunk\\ngit branch HDFS-775\\n#switch to it\\ngit checkout HDFS-775\\n#show what's branch you are in\\ngit branchRemember, this branch is local to your machine. Nobody else can see it until you push up your changes or generate a patch, or you make your machine visible over the network to interested parties.\",\n 'label': 'instruction'}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generation_pipeline = pipeline(\"text2text-generation\", model=\"t5-base\", tokenizer=\"t5-base\", device='cuda:0', truncation=True, batch_size=4)\n",
    "\n",
    "def generate_prompt(topic, header, label, text):\n",
    "    return f\"Generate a {label} prompt for the text about {header} - {topic}: {text}\"\n",
    "\n",
    "ds = Dataset.load_from_disk(processed_data_path)\n",
    "\n",
    "ds = ds.add_column(\"prompt\", [generate_prompt(x['topic'], x['header'], x[\"label\"], x[\"text\"]) for x in ds])\n",
    "\n",
    "\n",
    "for idx, out in enumerate(tqdm(generation_pipeline(KeyDataset(ds, \"prompt\")), total=len(ds), desc=\"Generating prompts\")):\n",
    "    print(out)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T22:29:40.097357Z",
     "end_time": "2023-05-02T22:31:30.283194Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T22:32:47.012202Z",
     "end_time": "2023-05-02T22:32:47.062773Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
