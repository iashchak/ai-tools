{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning a model based on raw documents from Confluence\n",
    "\n",
    "This notebook contains code for fine-tuning a model based on raw documents from Confluence.\n",
    "\n",
    "## Introduction\n",
    "The process will contain several parts:\n",
    "\n",
    "- Data downloading\n",
    "We downloaded several examples from public available apache foundation Confluence to make a raw dataset. This step done outside of this notebook. You can read more about Confluence export here: [https://confluence.atlassian.com/doc/export-content-to-word-pdf-html-and-xml-139475.html](https://confluence.atlassian.com/doc/export-content-to-word-pdf-html-and-xml-139475.html)\n",
    "- Data extraction\n",
    "For data extraction from dumps we will use Apache Tika running on a separate docker container.\n",
    "Apache Tika - is a toolkit for detecting and extracting metadata and structured text content from various documents using existing parser libraries. You can read more about it here: [https://tika.apache.org/](https://tika.apache.org/)\n",
    "- Data processing\n",
    "We will use the dataset library to process the data. It is a library for loading and processing datasets in a few lines of code. You can read more about it here: [https://huggingface.co/docs/datasets/](https://huggingface.co/docs/datasets/)\n",
    "Also we have to extract instruction and data from the raw data.\n",
    "- Data augmentation\n",
    "Augmentation of dataset is a process of creating new data from existing data. In this case we use the model for paraphrasing to create new questions and answers.\n",
    "- Model fine-tuning\n",
    "Using modern techniques as PEFT, DeepSpeed, LoRA and Accelerate we will fine-tune the model on the dataset. You can read more about it here: [https://huggingface.co/transformers/training.html](https://huggingface.co/transformers/training.html) [https://huggingface.co/blog/peft](https://huggingface.co/blog/peft)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup environment\n",
    "\n",
    "First of all we need to install all the dependencies needed for the project."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.10/site-packages (4.12.2)\n",
      "Requirement already satisfied: requests in /home/andrei/anaconda3/envs/ai-tools/lib/python3.10/site-packages (2.29.0)\n",
      "Requirement already satisfied: tqdm in /home/andrei/anaconda3/envs/ai-tools/lib/python3.10/site-packages (4.65.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.10/site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.10/site-packages (from requests) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.10/site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.10/site-packages (from requests) (2.0.4)\n",
      "Process is interrupted.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install beautifulsoup4 requests tqdm\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "pip install git+https://github.com/huggingface/peft.git\n",
    "pip install bitsandbytes transformers evaluate datasets accelerate loralib --upgrade --quiet\n",
    "pip install rouge-score tensorboard py7zr transformers[deepspeed] nltk"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T17:06:21.540452311Z",
     "start_time": "2023-05-08T17:06:20.122916335Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "\n",
    "### Data extraction\n",
    "\n",
    "In this stage we will have output in the format of\n",
    "```json\n",
    "{\n",
    "    \"file\": \"path/to/file\",\n",
    "    \"title\": \"page name\",\n",
    "    \"answer\": \"page content\",\n",
    "}[]\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "Processing files:   0%|          | 0/266 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a5e1b87a60f4a56a06d9dae953add48"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 24\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m filePath \u001B[38;5;129;01min\u001B[39;00m tqdm(fileList, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProcessing files\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(filePath, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m---> 24\u001B[0m         soup \u001B[38;5;241m=\u001B[39m \u001B[43mBeautifulSoup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhtml.parser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m         main_header \u001B[38;5;241m=\u001B[39m soup\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[1;32m     26\u001B[0m         header_tags \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh3\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh4\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh5\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mh6\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/site-packages/bs4/__init__.py:335\u001B[0m, in \u001B[0;36mBeautifulSoup.__init__\u001B[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001B[0m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39minitialize_soup(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    334\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 335\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_feed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    336\u001B[0m     success \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    337\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/site-packages/bs4/__init__.py:478\u001B[0m, in \u001B[0;36mBeautifulSoup._feed\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    475\u001B[0m \u001B[38;5;66;03m# Convert the document to Unicode.\u001B[39;00m\n\u001B[1;32m    476\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m--> 478\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeed\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmarkup\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    479\u001B[0m \u001B[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001B[39;00m\n\u001B[1;32m    480\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mendData()\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/site-packages/bs4/builder/_htmlparser.py:380\u001B[0m, in \u001B[0;36mHTMLParserTreeBuilder.feed\u001B[0;34m(self, markup)\u001B[0m\n\u001B[1;32m    378\u001B[0m parser\u001B[38;5;241m.\u001B[39msoup \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msoup\n\u001B[1;32m    379\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 380\u001B[0m     \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmarkup\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    382\u001B[0m     \u001B[38;5;66;03m# html.parser raises AssertionError in rare cases to\u001B[39;00m\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;66;03m# indicate a fatal problem with the markup, especially\u001B[39;00m\n\u001B[1;32m    384\u001B[0m     \u001B[38;5;66;03m# when there's an error in the doctype declaration.\u001B[39;00m\n\u001B[1;32m    385\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ParserRejectedMarkup(e)\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/html/parser.py:110\u001B[0m, in \u001B[0;36mHTMLParser.feed\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Feed data to the parser.\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \n\u001B[1;32m    106\u001B[0m \u001B[38;5;124;03mCall this as often as you want, with as little or as much text\u001B[39;00m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;124;03mas you want (may include '\\n').\u001B[39;00m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrawdata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrawdata \u001B[38;5;241m+\u001B[39m data\n\u001B[0;32m--> 110\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgoahead\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/html/parser.py:170\u001B[0m, in \u001B[0;36mHTMLParser.goahead\u001B[0;34m(self, end)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m startswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<\u001B[39m\u001B[38;5;124m'\u001B[39m, i):\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m starttagopen\u001B[38;5;241m.\u001B[39mmatch(rawdata, i): \u001B[38;5;66;03m# < + letter\u001B[39;00m\n\u001B[0;32m--> 170\u001B[0m         k \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse_starttag\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    171\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m startswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m</\u001B[39m\u001B[38;5;124m\"\u001B[39m, i):\n\u001B[1;32m    172\u001B[0m         k \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparse_endtag(i)\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/html/parser.py:344\u001B[0m, in \u001B[0;36mHTMLParser.parse_starttag\u001B[0;34m(self, i)\u001B[0m\n\u001B[1;32m    342\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_startendtag(tag, attrs)\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 344\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_starttag\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtag\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    345\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tag \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mCDATA_CONTENT_ELEMENTS:\n\u001B[1;32m    346\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_cdata_mode(tag)\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/site-packages/bs4/builder/_htmlparser.py:137\u001B[0m, in \u001B[0;36mBeautifulSoupHTMLParser.handle_starttag\u001B[0;34m(self, name, attrs, handle_empty_element)\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;66;03m#print(\"START\", name)\u001B[39;00m\n\u001B[1;32m    136\u001B[0m sourceline, sourcepos \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgetpos()\n\u001B[0;32m--> 137\u001B[0m tag \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msoup\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_starttag\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattr_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msourceline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msourceline\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    139\u001B[0m \u001B[43m    \u001B[49m\u001B[43msourcepos\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msourcepos\u001B[49m\n\u001B[1;32m    140\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tag \u001B[38;5;129;01mand\u001B[39;00m tag\u001B[38;5;241m.\u001B[39mis_empty_element \u001B[38;5;129;01mand\u001B[39;00m handle_empty_element:\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;66;03m# Unlike other parsers, html.parser doesn't send separate end tag\u001B[39;00m\n\u001B[1;32m    143\u001B[0m     \u001B[38;5;66;03m# events for empty-element tags. (It's handled in\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;66;03m# don't want handle_endtag() to cross off any previous end\u001B[39;00m\n\u001B[1;32m    150\u001B[0m     \u001B[38;5;66;03m# events for tags of this name.\u001B[39;00m\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_endtag(name, check_already_closed\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/site-packages/bs4/__init__.py:742\u001B[0m, in \u001B[0;36mBeautifulSoup.handle_starttag\u001B[0;34m(self, name, namespace, nsprefix, attrs, sourceline, sourcepos, namespaces)\u001B[0m\n\u001B[1;32m    724\u001B[0m \u001B[38;5;124;03m\"\"\"Called by the tree builder when a new tag is encountered.\u001B[39;00m\n\u001B[1;32m    725\u001B[0m \n\u001B[1;32m    726\u001B[0m \u001B[38;5;124;03m:param name: Name of the tag.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    739\u001B[0m \u001B[38;5;124;03mdon't call handle_endtag.\u001B[39;00m\n\u001B[1;32m    740\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    741\u001B[0m \u001B[38;5;66;03m# print(\"Start tag %s: %s\" % (name, attrs))\u001B[39;00m\n\u001B[0;32m--> 742\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mendData\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparse_only \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtagStack) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    745\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparse_only\u001B[38;5;241m.\u001B[39mtext\n\u001B[1;32m    746\u001B[0m          \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparse_only\u001B[38;5;241m.\u001B[39msearch_tag(name, attrs))):\n\u001B[1;32m    747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/site-packages/bs4/__init__.py:598\u001B[0m, in \u001B[0;36mBeautifulSoup.endData\u001B[0;34m(self, containerClass)\u001B[0m\n\u001B[1;32m    596\u001B[0m strippable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    597\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m current_data:\n\u001B[0;32m--> 598\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mASCII_SPACES:\n\u001B[1;32m    599\u001B[0m         strippable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    600\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "input_directory = os.path.join(\"..\", \"data\", \"confluence_exports\")\n",
    "include_extensions = [\".html\"]\n",
    "\n",
    "dataset_path = os.path.join(\"..\", \"datasets\", \"confluence_exports-inputs-augmented\")\n",
    "\n",
    "def get_files_to_process(root_path):\n",
    "    for dirpath, _, filenames in os.walk(root_path):\n",
    "        for filename in filenames:\n",
    "            if any(filename.endswith(ext) for ext in include_extensions):\n",
    "                yield os.path.join(dirpath, filename)\n",
    "\n",
    "\n",
    "articles_df = pd.DataFrame(columns=[\"source_raw\", \"target\", \"file\"])\n",
    "fileList = list(get_files_to_process(input_directory))\n",
    "\n",
    "for filePath in tqdm(fileList, desc=\"Processing files\"):\n",
    "    with open(filePath, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "        main_header = soup.find(\"h1\").text.strip()\n",
    "        header_tags = [\"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n",
    "        headers_stack = []\n",
    "        for header in soup.find_all(header_tags):\n",
    "            header_level = int(header.name[1])\n",
    "\n",
    "            while len(headers_stack) >= header_level:\n",
    "                headers_stack.pop()\n",
    "\n",
    "            headers_stack.append(header.text)\n",
    "\n",
    "            target = ''\n",
    "            current_element = header.next_element\n",
    "\n",
    "            while current_element is not None and (\n",
    "                    current_element.name is None or current_element.name not in header_tags):\n",
    "                if current_element.name is None:\n",
    "                    target = \" \".join([target, current_element.getText().strip()])\n",
    "                current_element = current_element.next_element\n",
    "\n",
    "            source_raw = \" : \".join([main_header] + headers_stack).replace(':', '>')\n",
    "            articles_df = pd.concat(\n",
    "                [articles_df, pd.DataFrame([[source_raw, target, filePath]], columns=[\"source_raw\", \"target\", \"file\"])])\n",
    "\n",
    "\n",
    "def has_content(row):\n",
    "    return len(row[\"source_raw\"].split()) > 2 and len(row[\"target\"].split()) > 5\n",
    "\n",
    "\n",
    "articles_df = articles_df.drop_duplicates(subset=[\"source_raw\"])\n",
    "articles_df = articles_df.drop_duplicates(subset=[\"target\"])\n",
    "articles_df = articles_df[articles_df.apply(has_content, axis=1)]\n",
    "\n",
    "articles_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "articles_df.sample(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T17:06:23.642745587Z",
     "start_time": "2023-05-08T17:06:21.546401404Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Rephrasing input data\n",
    "\n",
    "Since headers isn't a great to be directly used as a question, or instruction query - we will generate prompts for each required entity based on it's label."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T21:41:22.746751Z",
     "end_time": "2023-05-02T21:41:22.809564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'section': 'Creating the branch',\n 'text': \"Creating a branch is quick and easy#start off in the apache trunk\\ngit checkout trunk\\n#create a new branch from trunk\\ngit branch HDFS-775\\n#switch to it\\ngit checkout HDFS-775\\n#show what's branch you are in\\ngit branchRemember, this branch is local to your machine. Nobody else can see it until you push up your changes or generate a patch, or you make your machine visible over the network to interested parties.\",\n 'file': '../data/confluence_exports/HADOOP/Git-And-Hadoop_89071914.html',\n 'page': 'Hadoop : Git And Hadoop',\n 'query': \"Page: Hadoop : Git And Hadoop\\nSection: Creating the branch\\nText: Creating a branch is quick and easy#start off in the apache trunk\\ngit checkout trunk\\n#create a new branch from trunk\\ngit branch HDFS-775\\n#switch to it\\ngit checkout HDFS-775\\n#show what's branch you are in\\ngit branchRemember, this branch is local to your machine. Nobody else can see it until you push up your changes or generate a patch, or you make your machine visible over the network to interested parties.\",\n 'label': 'instruction'}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.base import KeyDataset\n",
    "from datasets import Dataset\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\", device_map='auto', framework=\"pt\")\n",
    "\n",
    "example_query = \"\"\"\n",
    "    Reword Title to be a valid prompt based on following examples:\n",
    "    Example 1:\n",
    "    Title: Hadoop > Hadoop 2.8.0 Release > Key Git Concepts > Forking onto GitHub\n",
    "    Q: Give me step-by-step guide on how to fork Hadoop 2.8.0 Release onto GitHub?\n",
    "    Example 2:\n",
    "    Title: Tomcat > WebSocket 1.1 TCK > Goals\n",
    "    Q: Tell to me what are the goals of WebSocket 1.1 TCK in Tomcat.\n",
    "    Example 3:\n",
    "    Title: Apache > Tomcat > What is the Native library?\n",
    "    Q: What is the Apache Tomcat Native library?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_query(row):\n",
    "    return f\"{example_query}\\nTitle: {row['source_raw']}\\nQ:\"\n",
    "\n",
    "\n",
    "def generate_queries(dataset):\n",
    "    \"\"\"This function will generate queries for each article in the dataset\"\"\"\n",
    "    for out in tqdm(pipe(KeyDataset(dataset, \"prompt\"), batch_size=8, return_text=True), desc=\"Generating queries\",\n",
    "                    total=len(dataset)):\n",
    "        for row in out:\n",
    "            yield row[\"generated_text\"]\n",
    "\n",
    "\n",
    "articles_df['prompt'] = articles_df.apply(create_query, axis=1)\n",
    "\n",
    "dataset = Dataset.from_pandas(articles_df)\n",
    "\n",
    "dataset = dataset.add_column(\"source\", list(generate_queries(dataset))) \\\n",
    "    .filter(lambda x: x[\"source\"] is not None and len(x[\"source\"].split()) > 3) \\\n",
    "    .remove_columns([\"prompt\", \"file\", \"source_raw\"])\n",
    "\n",
    "dataset.save_to_disk(dataset_path)\n",
    "\n",
    "dataset.to_pandas().sample(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remove bad queries\n",
    "\n",
    "Since some of the generated queries are empty or has some garbage in them - we will filter them out.\n",
    "We will use same classification model to filter out bad queries."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "BATCH_SIZE = 10  # You can adjust the batch size according to your needs\n",
    "\n",
    "candidate_labels = [\"question\", \"request\", \"nonsense\"]\n",
    "\n",
    "classification_pipeline = pipeline(model=\"facebook/bart-large-mnli\", device_map='auto', framework=\"pt\")\n",
    "\n",
    "classified_pipeline = tqdm(\n",
    "    classification_pipeline(KeyDataset(dataset, \"source\"),\n",
    "    truncation=True,\n",
    "    candidate_labels=candidate_labels,\n",
    "    batch_size=BATCH_SIZE),\n",
    "    total=len(dataset),\n",
    "    desc=\"Classifying dataset by correctness\"\n",
    ")\n",
    "\n",
    "dataset = dataset \\\n",
    "    .add_column(\"validity\", [x[\"labels\"][0] for x in classified_pipeline]) \\\n",
    "    .filter(lambda x: x[\"validity\"] != \"nonsense\") \\\n",
    "    .remove_columns([\"validity\"])\n",
    "\n",
    "dataset.save_to_disk(dataset_path)\n",
    "dataset.to_pandas().sample(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data augmentation\n",
    "\n",
    "Since we have a small dataset, we will augment it by replacing some words with their synonyms. We will use [wordnet](https://wordnet.princeton.edu/) for that."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "def random_insertion(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    word_to_insert = random.choice(words)\n",
    "    position = random.randint(0, len(words))\n",
    "    words.insert(position, word_to_insert)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def random_swap(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    if len(words) > 1:\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def random_deletion(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    if len(words) > 1:\n",
    "        idx = random.randint(0, len(words) - 1)\n",
    "        words.pop(idx)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def augment_text(text, augmentations):\n",
    "    for aug in augmentations:\n",
    "        if aug == \"random_insertion\":\n",
    "            text = random_insertion(text)\n",
    "        elif aug == \"random_swap\":\n",
    "            text = random_swap(text)\n",
    "        elif aug == \"random_deletion\":\n",
    "            text = random_deletion(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def augment_dataset(ds, column, augmentations, num_augmentations=4):\n",
    "    augmented_data = []\n",
    "    for idx in tqdm(range(len(ds)), desc=\"Augmenting dataset\"):\n",
    "        row = ds[idx]\n",
    "        text = row[column]\n",
    "        for _ in range(num_augmentations):\n",
    "            augmented_text = augment_text(text, augmentations)\n",
    "            new_row = row.copy()\n",
    "            new_row[column] = augmented_text\n",
    "            augmented_data.append(new_row)\n",
    "    return concatenate_datasets([ds, Dataset.from_pandas(pd.DataFrame(augmented_data))])\n",
    "\n",
    "\n",
    "augmentations = [\"random_insertion\", \"random_swap\", \"random_deletion\"]\n",
    "augmented_input_ds = augment_dataset(dataset, \"source\", augmentations)\n",
    "augmented_text_ds = augment_dataset(dataset, \"target\", augmentations)\n",
    "dataset = concatenate_datasets([dataset, augmented_input_ds, augmented_text_ds])\n",
    "\n",
    "dataset.save_to_disk(dataset_path)\n",
    "\n",
    "dataset.to_pandas().sample(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "For this stage we will use Peft, Lora, DeepSpeed, Accelerate and HuggingFace trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup deepspeed\n",
    "\n",
    "DeepSpeed is a deep learning optimization library that makes distributed training easy, efficient, and effective.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T18:22:47.444174204Z",
     "start_time": "2023-05-08T18:22:47.403754135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column to remove ['source', 'target'] not in the dataset. Current columns in the dataset: ['input_ids', 'attention_mask', 'labels']",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 14\u001B[0m\n\u001B[1;32m     10\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_id)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# The maximum total input sequence length after tokenization.\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Sequences longer than this will be truncated, sequences shorter will be padded.\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m tokenized_inputs \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msource\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msource\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtarget\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m input_lenghts \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mlen\u001B[39m(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m tokenized_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# take 85 percentile of max length for better utilization\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/site-packages/datasets/arrow_dataset.py:578\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    576\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    577\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 578\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    579\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    580\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[1;32m    581\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/site-packages/datasets/arrow_dataset.py:543\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    536\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    537\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[1;32m    538\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[1;32m    539\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[1;32m    540\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[1;32m    541\u001B[0m }\n\u001B[1;32m    542\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 543\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    544\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    545\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/site-packages/datasets/arrow_dataset.py:2991\u001B[0m, in \u001B[0;36mDataset.map\u001B[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[1;32m   2988\u001B[0m     remove_columns \u001B[38;5;241m=\u001B[39m [remove_columns]\n\u001B[1;32m   2990\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m remove_columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(col \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data\u001B[38;5;241m.\u001B[39mcolumn_names \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m remove_columns):\n\u001B[0;32m-> 2991\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2992\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumn to remove \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mfilter\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m col: col \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data\u001B[38;5;241m.\u001B[39mcolumn_names, remove_columns))\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in the dataset. Current columns in the dataset: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data\u001B[38;5;241m.\u001B[39mcolumn_names\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2993\u001B[0m     )\n\u001B[1;32m   2995\u001B[0m load_from_cache_file \u001B[38;5;241m=\u001B[39m load_from_cache_file \u001B[38;5;28;01mif\u001B[39;00m load_from_cache_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m is_caching_enabled()\n\u001B[1;32m   2997\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fn_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mValueError\u001B[0m: Column to remove ['source', 'target'] not in the dataset. Current columns in the dataset: ['input_ids', 'attention_mask', 'labels']"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, \\\n",
    "    DataCollatorForSeq2Seq\n",
    "from datasets import concatenate_datasets\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load tokenizer of FLAN-t5-XL\n",
    "model_id = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = dataset.map(lambda x: tokenizer(x[\"source\"], truncation=True), batched=True, remove_columns=[\"source\", \"target\"])\n",
    "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "# take 85 percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lenghts, 85))\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = dataset.map(lambda x: tokenizer(x[\"target\"], truncation=True), batched=True, remove_columns=[\"source\", \"target\"])\n",
    "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "# take 90 percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lenghts, 90))\n",
    "print(f\"Max target length: {max_target_length}\")\n",
    "\n",
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"Q: \" + item for item in sample[\"source\"]]\n",
    "    targets = [\"A: \" + item for item in sample[\"target\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"source\", \"target\"])\n",
    "print(f\"Keys of tokenized dataset: {list(dataset.features)}\")\n",
    "\n",
    "dataset.save_to_disk(dataset_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T18:22:54.343699728Z",
     "start_time": "2023-05-08T18:22:53.201614592Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/andrei/anaconda3/envs/ai-tools/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /home/andrei/anaconda3/envs/ai-tools/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrei/anaconda3/envs/ai-tools/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/andrei/anaconda3/envs/ai-tools did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/andrei/anaconda3/envs/ai-tools/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('local/andrei-pc-2'), PosixPath('@/tmp/.ICE-unix/2141,unix/andrei-pc-2')}\n",
      "  warn(msg)\n",
      "/home/andrei/anaconda3/envs/ai-tools/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/etc/xdg/xdg-ubuntu')}\n",
      "  warn(msg)\n",
      "/home/andrei/anaconda3/envs/ai-tools/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('0'), PosixPath('1')}\n",
      "  warn(msg)\n",
      "/home/andrei/anaconda3/envs/ai-tools/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4718592 || all params: 787868672 || trainable%: 0.5989059049678777\n",
      "[2023-05-08 21:37:05,127] [INFO] [comm.py:606:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2023-05-08 21:37:05,136] [INFO] [comm.py:656:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=192.168.50.234, master_port=29500\n",
      "[2023-05-08 21:37:05,136] [INFO] [comm.py:622:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "\u001B[93m [WARNING] \u001B[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/andrei/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/andrei/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load cpu_adam op: 2.1772849559783936 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module cpu_adam...\n",
      "Using /home/andrei/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/andrei/.cache/torch_extensions/py310_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load utils op: 0.04599118232727051 seconds\n",
      "Rank: 0 partition count [1] and sizes[(4718592, False)] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module utils...\n",
      "Using /home/andrei/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load utils op: 0.000217437744140625 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='2243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2243 : < :, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrei/anaconda3/envs/ai-tools/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    ")\n",
    "\n",
    "model_id = \"google/flan-t5-large\"\n",
    "batch_size = 4\n",
    "num_train_epochs = 2.5\n",
    "gradient_accumulation_steps = 3\n",
    "learning_rate = 1e-3\n",
    "label_pad_token_id = -100\n",
    "\n",
    "dataset_path = os.path.join(\"..\", \"datasets\", \"confluence_exports-inputs-augmented\")\n",
    "output_dir = os.path.join(\"..\", \"models\", f\"{model_id.replace('/', '-')}-lora-peft\")\n",
    "dataset = load_from_disk(dataset_path)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8,\n",
    ")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    deepspeed=\"ds_config_zero3.json\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n",
    "model.save_pretrained(os.path.join(output_dir, \"fine-tuned\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T20:30:26.980469688Z",
     "start_time": "2023-05-08T19:36:56.308085498Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference with the trained model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "'A: Apache Tika Apache Tika is a Java servlet container based on Apache Tomcat. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet container implementation of Apache Tika. It is a servlet'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import pipeline\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_id = \"google/flan-t5-large\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "model = PeftModel.from_pretrained(model, os.path.join(output_dir, \"fine-tuned\"))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def generate_simple(input_text):\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=512,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "input_text = \"\"\"Q: What is Apache Tika?\"\"\"\n",
    "generate_simple(input_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T21:02:51.408168342Z",
     "start_time": "2023-05-08T21:02:04.218559625Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'A: How do I deploy apache tika tomcat tomcat? See TomcatInTomcat'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"\"\"Q: How to deploy apache tika to tomcat?\"\"\"\n",
    "generate_simple(input_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-08T21:03:23.805725102Z",
     "start_time": "2023-05-08T21:03:21.676977761Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
