{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning a model based on raw documents from Confluence\n",
    "\n",
    "This notebook contains code for fine-tuning a model based on raw documents from Confluence.\n",
    "\n",
    "## Introduction\n",
    "The process will contain several parts:\n",
    "\n",
    "- Data downloading\n",
    "We downloaded several examples from public available apache foundation Confluence to make a raw dataset. This step done outside of this notebook. You can read more about Confluence export here: [https://confluence.atlassian.com/doc/export-content-to-word-pdf-html-and-xml-139475.html](https://confluence.atlassian.com/doc/export-content-to-word-pdf-html-and-xml-139475.html)\n",
    "- Data extraction\n",
    "For data extraction from dumps we will use Apache Tika running on a separate docker container.\n",
    "Apache Tika - is a toolkit for detecting and extracting metadata and structured text content from various documents using existing parser libraries. You can read more about it here: [https://tika.apache.org/](https://tika.apache.org/)\n",
    "- Data processing\n",
    "We will use the dataset library to process the data. It is a library for loading and processing datasets in a few lines of code. You can read more about it here: [https://huggingface.co/docs/datasets/](https://huggingface.co/docs/datasets/)\n",
    "Also we have to extract instruction and data from the raw data.\n",
    "- Data augmentation\n",
    "Augmentation of dataset is a process of creating new data from existing data. In this case we use the model for paraphrasing to create new questions and answers.\n",
    "- Model fine-tuning\n",
    "Using modern techniques as PEFT, DeepSpeed, LoRA and Accelerate we will fine-tune the model on the dataset. You can read more about it here: [https://huggingface.co/transformers/training.html](https://huggingface.co/transformers/training.html) [https://huggingface.co/blog/peft](https://huggingface.co/blog/peft)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup environment\n",
    "\n",
    "First of all we need to install all the dependencies needed for the project."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (4.12.2)\r\n",
      "Requirement already satisfied: requests in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (2.29.0)\r\n",
      "Requirement already satisfied: tqdm in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (4.65.0)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from beautifulsoup4) (2.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests) (3.1.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests) (1.26.15)\r\n",
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\r\n",
      "Requirement already satisfied: torch in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (2.0.0+cu118)\r\n",
      "Requirement already satisfied: torchvision in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (0.15.1+cu118)\r\n",
      "Requirement already satisfied: torchaudio in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (2.0.1+cu118)\r\n",
      "Requirement already satisfied: filelock in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torch) (3.12.0)\r\n",
      "Requirement already satisfied: typing-extensions in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torch) (4.5.0)\r\n",
      "Requirement already satisfied: sympy in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torch) (1.11.1)\r\n",
      "Requirement already satisfied: jinja2 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: networkx in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torch) (3.0)\r\n",
      "Requirement already satisfied: triton==2.0.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torch) (2.0.0)\r\n",
      "Requirement already satisfied: lit in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from triton==2.0.0->torch) (15.0.7)\r\n",
      "Requirement already satisfied: cmake in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from triton==2.0.0->torch) (3.25.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torchvision) (9.3.0)\r\n",
      "Requirement already satisfied: numpy in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torchvision) (1.24.3)\r\n",
      "Requirement already satisfied: requests in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torchvision) (2.29.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from jinja2->torch) (2.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests->torchvision) (3.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests->torchvision) (2022.12.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests->torchvision) (1.26.15)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests->torchvision) (3.4)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from sympy->torch) (1.2.1)\r\n",
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Collecting git+https://github.com/huggingface/peft.git\r\n",
      "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-hftbc1lh\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-hftbc1lh\r\n",
      "  Resolved https://github.com/huggingface/peft.git to commit 1a1cfe34791ee9d822fad5f6c9607b966f2b27c0\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.17 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from peft==0.3.0.dev0) (1.24.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from peft==0.3.0.dev0) (23.1)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from peft==0.3.0.dev0) (2.0.0+cu118)\r\n",
      "Requirement already satisfied: accelerate in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from peft==0.3.0.dev0) (0.18.0)\r\n",
      "Requirement already satisfied: pyyaml in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from peft==0.3.0.dev0) (6.0)\r\n",
      "Requirement already satisfied: transformers in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from peft==0.3.0.dev0) (4.28.1)\r\n",
      "Requirement already satisfied: psutil in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from peft==0.3.0.dev0) (5.9.0)\r\n",
      "Requirement already satisfied: networkx in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (3.0)\r\n",
      "Requirement already satisfied: jinja2 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (3.1.2)\r\n",
      "Requirement already satisfied: sympy in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (1.11.1)\r\n",
      "Requirement already satisfied: triton==2.0.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (2.0.0)\r\n",
      "Requirement already satisfied: typing-extensions in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (4.5.0)\r\n",
      "Requirement already satisfied: filelock in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torch>=1.13.0->peft==0.3.0.dev0) (3.12.0)\r\n",
      "Requirement already satisfied: cmake in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.3.0.dev0) (3.25.0)\r\n",
      "Requirement already satisfied: lit in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.3.0.dev0) (15.0.7)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from transformers->peft==0.3.0.dev0) (0.13.3)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from transformers->peft==0.3.0.dev0) (0.14.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from transformers->peft==0.3.0.dev0) (4.65.0)\r\n",
      "Requirement already satisfied: requests in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from transformers->peft==0.3.0.dev0) (2.29.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from transformers->peft==0.3.0.dev0) (2023.5.4)\r\n",
      "Requirement already satisfied: fsspec in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers->peft==0.3.0.dev0) (2023.4.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from jinja2->torch>=1.13.0->peft==0.3.0.dev0) (2.1.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests->transformers->peft==0.3.0.dev0) (2022.12.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests->transformers->peft==0.3.0.dev0) (1.26.15)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests->transformers->peft==0.3.0.dev0) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests->transformers->peft==0.3.0.dev0) (3.4)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from sympy->torch>=1.13.0->peft==0.3.0.dev0) (1.2.1)\r\n",
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Requirement already satisfied: rouge-score in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (0.1.2)\r\n",
      "Requirement already satisfied: tensorboard in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (2.12.3)\r\n",
      "Requirement already satisfied: py7zr in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (0.20.5)\r\n",
      "Requirement already satisfied: transformers[deepspeed] in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (4.28.1)\r\n",
      "Requirement already satisfied: spacy in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (3.5.2)\r\n",
      "Requirement already satisfied: nltk in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: six>=1.14.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from rouge-score) (1.16.0)\r\n",
      "Requirement already satisfied: numpy in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from rouge-score) (1.24.3)\r\n",
      "Requirement already satisfied: absl-py in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from rouge-score) (1.4.0)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from tensorboard) (2.29.0)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from tensorboard) (2.3.3)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from tensorboard) (0.7.0)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from tensorboard) (2.17.3)\r\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from tensorboard) (1.54.0)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from tensorboard) (0.38.4)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from tensorboard) (1.0.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from tensorboard) (3.4.3)\r\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from tensorboard) (4.22.3)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from tensorboard) (66.0.0)\r\n",
      "Requirement already satisfied: pyzstd>=0.14.4 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from py7zr) (0.15.7)\r\n",
      "Requirement already satisfied: pycryptodomex>=3.6.6 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from py7zr) (3.17)\r\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from py7zr) (0.2.3)\r\n",
      "Requirement already satisfied: pybcj>=0.6.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from py7zr) (1.0.1)\r\n",
      "Requirement already satisfied: texttable in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from py7zr) (1.6.7)\r\n",
      "Requirement already satisfied: psutil in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from py7zr) (5.9.0)\r\n",
      "Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from py7zr) (1.0.0)\r\n",
      "Requirement already satisfied: inflate64>=0.3.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from py7zr) (0.3.1)\r\n",
      "Requirement already satisfied: brotli>=1.0.9 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from py7zr) (1.0.9)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from transformers[deepspeed]) (23.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from transformers[deepspeed]) (0.14.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from transformers[deepspeed]) (6.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from transformers[deepspeed]) (0.13.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from transformers[deepspeed]) (4.65.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from transformers[deepspeed]) (2023.5.4)\r\n",
      "Requirement already satisfied: filelock in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from transformers[deepspeed]) (3.12.0)\r\n",
      "Requirement already satisfied: deepspeed>=0.8.3 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from transformers[deepspeed]) (0.9.1)\r\n",
      "Requirement already satisfied: accelerate>=0.10.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from transformers[deepspeed]) (0.18.0)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy) (1.0.4)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy) (2.0.7)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy) (1.0.9)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy) (6.3.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy) (1.10.7)\r\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy) (8.1.10)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy) (2.0.8)\r\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy) (0.10.1)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy) (3.0.12)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy) (1.1.1)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy) (3.3.0)\r\n",
      "Requirement already satisfied: jinja2 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy) (3.1.2)\r\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy) (0.7.0)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy) (2.4.6)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy) (3.0.8)\r\n",
      "Requirement already satisfied: click in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from nltk) (8.1.3)\r\n",
      "Requirement already satisfied: joblib in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from nltk) (1.2.0)\r\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from accelerate>=0.10.0->transformers[deepspeed]) (2.0.0+cu118)\r\n",
      "Requirement already satisfied: ninja in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from deepspeed>=0.8.3->transformers[deepspeed]) (1.11.1)\r\n",
      "Requirement already satisfied: hjson in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from deepspeed>=0.8.3->transformers[deepspeed]) (3.1.0)\r\n",
      "Requirement already satisfied: py-cpuinfo in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from deepspeed>=0.8.3->transformers[deepspeed]) (9.0.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.0)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\r\n",
      "Requirement already satisfied: fsspec in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers[deepspeed]) (2023.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers[deepspeed]) (4.5.0)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard) (6.0.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.1.0)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.11.0)\r\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\r\n",
      "Requirement already satisfied: triton==2.0.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torch>=1.4.0->accelerate>=0.10.0->transformers[deepspeed]) (2.0.0)\r\n",
      "Requirement already satisfied: networkx in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torch>=1.4.0->accelerate>=0.10.0->transformers[deepspeed]) (3.0)\r\n",
      "Requirement already satisfied: sympy in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from torch>=1.4.0->accelerate>=0.10.0->transformers[deepspeed]) (1.11.1)\r\n",
      "Requirement already satisfied: lit in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.4.0->accelerate>=0.10.0->transformers[deepspeed]) (15.0.7)\r\n",
      "Requirement already satisfied: cmake in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.4.0->accelerate>=0.10.0->transformers[deepspeed]) (3.25.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from sympy->torch>=1.4.0->accelerate>=0.10.0->transformers[deepspeed]) (1.2.1)\r\n"
     ]
    }
   ],
   "source": [
    "# install common dependencies\n",
    "!pip install beautifulsoup4 requests tqdm\n",
    "# install cuda\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# install Hugging Face Libraries\n",
    "!pip install git+https://github.com/huggingface/peft.git\n",
    "!pip install bitsandbytes transformers evaluate datasets accelerate loralib --upgrade --quiet\n",
    "# install additional dependencies needed for training\n",
    "!pip install rouge-score tensorboard py7zr transformers[deepspeed] spacy nltk"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T18:02:43.162006Z",
     "end_time": "2023-05-03T18:02:58.883065Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data extraction\n",
    "\n",
    "In this stage we will have output in the format of\n",
    "```json\n",
    "{\n",
    "    \"file\": \"path/to/file\",\n",
    "    \"page\": \"page name\",\n",
    "    \"section\": \"section name\",\n",
    "    \"text\": \"text\"\n",
    "}[]\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Processing files:   0%|          | 0/266 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efa27f2e7bfd4011a72261562cb0a10f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                                                  title   \n873   TIKA : TikaEval > Single Output from One Tool ...  \\\n1139  TIKA : TikaServer Windows Service > Here is a ...   \n481   Apache Tomcat : TomcatCreateNativeLaunchers > ...   \n898   TIKA : tika-pipes > Security Warning > From Fi...   \n124               Hadoop : Meetup agenda > Attachments:   \n1129  TIKA : TikaAndVision > Tika and Tensorflow Ima...   \n475    Apache Tomcat : Class Not Found Issues > Preface   \n510   Apache Tomcat : SupportAndTraining > Example c...   \n823   TIKA : SMTWithApacheJoshua > Introduction > La...   \n737   Apache Tomcat : HowTo > How do I add a questio...   \n\n                                                   text   \n873    Single Output from One Tool (Profile) NOTE: a...  \\\n1139   Attachments:     image2022-5-19_22-13-5.png (...   \n481    Available Options  What options do you have i...   \n898    From FileShare to FileShare Process all files...   \n124    Attachments:     HDFS-15547 Disk-level tierin...   \n1129   2. Tensorflow Using REST Server This is the r...   \n475    Preface This page discusses the various ways ...   \n510    Example company name  Use this example as a b...   \n823    Details The language pack being used in this ...   \n737    How do I set up multiple sites sharing the sa...   \n\n                                                   file  \n873   ../data/confluence_exports/TIKA/TikaEval_10945...  \n1139  ../data/confluence_exports/TIKA/TikaServer-Win...  \n481   ../data/confluence_exports/TOMCAT/TomcatCreate...  \n898   ../data/confluence_exports/TIKA/tika-pipes_181...  \n124   ../data/confluence_exports/HADOOP/Meetup-agend...  \n1129  ../data/confluence_exports/TIKA/TikaAndVision_...  \n475   ../data/confluence_exports/TOMCAT/Class-Not-Fo...  \n510   ../data/confluence_exports/TOMCAT/SupportAndTr...  \n823   ../data/confluence_exports/TIKA/SMTWithApacheJ...  \n737   ../data/confluence_exports/TOMCAT/HowTo_103099...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>text</th>\n      <th>file</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>873</th>\n      <td>TIKA : TikaEval &gt; Single Output from One Tool ...</td>\n      <td>Single Output from One Tool (Profile) NOTE: a...</td>\n      <td>../data/confluence_exports/TIKA/TikaEval_10945...</td>\n    </tr>\n    <tr>\n      <th>1139</th>\n      <td>TIKA : TikaServer Windows Service &gt; Here is a ...</td>\n      <td>Attachments:     image2022-5-19_22-13-5.png (...</td>\n      <td>../data/confluence_exports/TIKA/TikaServer-Win...</td>\n    </tr>\n    <tr>\n      <th>481</th>\n      <td>Apache Tomcat : TomcatCreateNativeLaunchers &gt; ...</td>\n      <td>Available Options  What options do you have i...</td>\n      <td>../data/confluence_exports/TOMCAT/TomcatCreate...</td>\n    </tr>\n    <tr>\n      <th>898</th>\n      <td>TIKA : tika-pipes &gt; Security Warning &gt; From Fi...</td>\n      <td>From FileShare to FileShare Process all files...</td>\n      <td>../data/confluence_exports/TIKA/tika-pipes_181...</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>Hadoop : Meetup agenda &gt; Attachments:</td>\n      <td>Attachments:     HDFS-15547 Disk-level tierin...</td>\n      <td>../data/confluence_exports/HADOOP/Meetup-agend...</td>\n    </tr>\n    <tr>\n      <th>1129</th>\n      <td>TIKA : TikaAndVision &gt; Tika and Tensorflow Ima...</td>\n      <td>2. Tensorflow Using REST Server This is the r...</td>\n      <td>../data/confluence_exports/TIKA/TikaAndVision_...</td>\n    </tr>\n    <tr>\n      <th>475</th>\n      <td>Apache Tomcat : Class Not Found Issues &gt; Preface</td>\n      <td>Preface This page discusses the various ways ...</td>\n      <td>../data/confluence_exports/TOMCAT/Class-Not-Fo...</td>\n    </tr>\n    <tr>\n      <th>510</th>\n      <td>Apache Tomcat : SupportAndTraining &gt; Example c...</td>\n      <td>Example company name  Use this example as a b...</td>\n      <td>../data/confluence_exports/TOMCAT/SupportAndTr...</td>\n    </tr>\n    <tr>\n      <th>823</th>\n      <td>TIKA : SMTWithApacheJoshua &gt; Introduction &gt; La...</td>\n      <td>Details The language pack being used in this ...</td>\n      <td>../data/confluence_exports/TIKA/SMTWithApacheJ...</td>\n    </tr>\n    <tr>\n      <th>737</th>\n      <td>Apache Tomcat : HowTo &gt; How do I add a questio...</td>\n      <td>How do I set up multiple sites sharing the sa...</td>\n      <td>../data/confluence_exports/TOMCAT/HowTo_103099...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "input_directory = os.path.join(\"..\", \"data\", \"confluence_exports\")\n",
    "include_extensions = [\".html\"]\n",
    "\n",
    "def get_files_to_process(root_path):\n",
    "    for dirpath, _, filenames in os.walk(root_path):\n",
    "        for filename in filenames:\n",
    "            if any(filename.endswith(ext) for ext in include_extensions):\n",
    "                yield os.path.join(dirpath, filename)\n",
    "\n",
    "articles_df = pd.DataFrame(columns=[\"title\", \"text\", \"file\"])\n",
    "fileList = list(get_files_to_process(input_directory))\n",
    "\n",
    "for filePath in tqdm(fileList, desc=\"Processing files\"):\n",
    "    with open(filePath, \"r\", encoding=\"utf-8\") as file:\n",
    "        main_header = soup.find(\"h1\").text.strip()\n",
    "        soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "        header_tags = [\"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n",
    "        headers_stack = []\n",
    "        for header in soup.find_all(header_tags):\n",
    "            header_level = int(header.name[1])\n",
    "\n",
    "            while len(headers_stack) >= header_level:\n",
    "                headers_stack.pop()\n",
    "\n",
    "            headers_stack.append(header.text)\n",
    "\n",
    "            text = ''\n",
    "            current_element = header.next_element\n",
    "\n",
    "            while current_element is not None and (current_element.name is None or current_element.name not in header_tags):\n",
    "                if current_element.name is None:\n",
    "                    text = \" \".join([text, current_element.getText().strip()])\n",
    "                current_element = current_element.next_element\n",
    "\n",
    "            title =  \" > \".join([main_header] + headers_stack)\n",
    "            articles_df = pd.concat([articles_df, pd.DataFrame([[title, text, filePath]], columns=[\"title\", \"text\", \"file\"])])\n",
    "\n",
    "def has_content(row):\n",
    "    return len(row[\"title\"].split()) > 2 and len(row[\"text\"].split()) > 5\n",
    "\n",
    "articles_df = articles_df.drop_duplicates(subset=[\"text\"])\n",
    "articles_df = articles_df.drop_duplicates(subset=[\"title\"])\n",
    "articles_df = articles_df[articles_df.apply(has_content, axis=1)]\n",
    "\n",
    "articles_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "articles_df.sample(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-05T18:47:10.539291827Z",
     "start_time": "2023-05-05T18:47:08.140540368Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data adjustments\n",
    "\n",
    "Since headers isn't a great to be directly used as a question, or instruction query - we will generate prompts for each required entity based on it's label."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T21:41:22.746751Z",
     "end_time": "2023-05-02T21:41:22.809564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'section': 'Creating the branch',\n 'text': \"Creating a branch is quick and easy#start off in the apache trunk\\ngit checkout trunk\\n#create a new branch from trunk\\ngit branch HDFS-775\\n#switch to it\\ngit checkout HDFS-775\\n#show what's branch you are in\\ngit branchRemember, this branch is local to your machine. Nobody else can see it until you push up your changes or generate a patch, or you make your machine visible over the network to interested parties.\",\n 'file': '../data/confluence_exports/HADOOP/Git-And-Hadoop_89071914.html',\n 'page': 'Hadoop : Git And Hadoop',\n 'query': \"Page: Hadoop : Git And Hadoop\\nSection: Creating the branch\\nText: Creating a branch is quick and easy#start off in the apache trunk\\ngit checkout trunk\\n#create a new branch from trunk\\ngit branch HDFS-775\\n#switch to it\\ngit checkout HDFS-775\\n#show what's branch you are in\\ngit branchRemember, this branch is local to your machine. Nobody else can see it until you push up your changes or generate a patch, or you make your machine visible over the network to interested parties.\",\n 'label': 'instruction'}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 27\u001B[0m\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m row \u001B[38;5;129;01min\u001B[39;00m tqdm(processor, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProcessing queries\u001B[39m\u001B[38;5;124m\"\u001B[39m, total\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(dataset)):\n\u001B[1;32m     25\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerated_text\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[0;32m---> 27\u001B[0m articles_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m get_queries(articles_df)\n\u001B[1;32m     29\u001B[0m articles_df\u001B[38;5;241m.\u001B[39mtail()\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.8/site-packages/pandas/core/frame.py:3960\u001B[0m, in \u001B[0;36mDataFrame.__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m   3957\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_setitem_array([key], value)\n\u001B[1;32m   3958\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3959\u001B[0m     \u001B[38;5;66;03m# set column\u001B[39;00m\n\u001B[0;32m-> 3960\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_set_item\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.8/site-packages/pandas/core/frame.py:4153\u001B[0m, in \u001B[0;36mDataFrame._set_item\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m   4143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_set_item\u001B[39m(\u001B[38;5;28mself\u001B[39m, key, value) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   4144\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4145\u001B[0m \u001B[38;5;124;03m    Add series to DataFrame in specified column.\u001B[39;00m\n\u001B[1;32m   4146\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4151\u001B[0m \u001B[38;5;124;03m    ensure homogeneity.\u001B[39;00m\n\u001B[1;32m   4152\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 4153\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sanitize_column\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4155\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   4156\u001B[0m         key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\n\u001B[1;32m   4157\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m value\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   4158\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_extension_array_dtype(value)\n\u001B[1;32m   4159\u001B[0m     ):\n\u001B[1;32m   4160\u001B[0m         \u001B[38;5;66;03m# broadcast across multiple columns if necessary\u001B[39;00m\n\u001B[1;32m   4161\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mis_unique \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns, MultiIndex):\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.8/site-packages/pandas/core/frame.py:4880\u001B[0m, in \u001B[0;36mDataFrame._sanitize_column\u001B[0;34m(self, value)\u001B[0m\n\u001B[1;32m   4877\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _reindex_for_setitem(Series(value), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex)\n\u001B[1;32m   4879\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_list_like(value):\n\u001B[0;32m-> 4880\u001B[0m     \u001B[43mcom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequire_length_match\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4881\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sanitize_array(value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, allow_2d\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.8/site-packages/pandas/core/common.py:575\u001B[0m, in \u001B[0;36mrequire_length_match\u001B[0;34m(data, index)\u001B[0m\n\u001B[1;32m    571\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequire_length_match\u001B[39m(data, index: Index) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    573\u001B[0m \u001B[38;5;124;03m    Check the length of data matches the length of the index.\u001B[39;00m\n\u001B[1;32m    574\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 575\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(index):\n\u001B[1;32m    576\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    577\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLength of values \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    578\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    579\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdoes not match length of index \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    580\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(index)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    581\u001B[0m         )\n",
      "\u001B[0;31mTypeError\u001B[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "from transformers.pipelines.base import KeyDataset\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\", device='cuda:0', framework=\"pt\")\n",
    "\n",
    "example_query = \"\"\"\n",
    "    <title>Hadoop : Hadoop 2.8.0 Release > Key Git Concepts > Forking onto GitHub</title><query>How to fork Hadoop 2.8.0 Release onto GitHub?</query>\n",
    "    <title>Apache Tomcat : WebSocket 1.1 TCK > Goals</title><query>What are the goals of WebSocket 1.1 TCK in Apache Tomcat?</query>\n",
    "\"\"\"\n",
    "\n",
    "articles_df = articles_df.sample(10)\n",
    "\n",
    "def create_query(row):\n",
    "    return f\"{example_query}\\n<title>{row['title']}</title>\\n\"\n",
    "\n",
    "articles_df[\"prompt\"] = articles_df[[\"title\"]].apply(create_query, axis=1)\n",
    "\n",
    "dataset = Dataset.from_pandas(articles_df)\n",
    "\n",
    "processed_queries = pipe(KeyDataset(dataset, \"title\"), truncation=True)\n",
    "\n",
    "for query in tqdm(processed_queries, desc=\"Processing queries\", total=len(dataset)):\n",
    "    print(query)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-05T18:47:48.881772011Z",
     "start_time": "2023-05-05T18:47:47.534487570Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove useless data\n",
    "\n",
    "Since some of the generated queries are empty or has some garbage in them - we will filter them out.\n",
    "We will use same classification model to filter out bad queries."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/438 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b08d27da1b524d3cba59753742b0417b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Classifying dataset by correctness:   0%|          | 0/415 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "201a186af75c4969b96c4adabd661948"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Flattening the indices:   0%|          | 0/415 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42d2903dfa354970a9358143936600c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/415 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d71a026baf5408da1361c3ce75615d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/377 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ab7d63070e1420892d8e6099d2deb29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers.pipelines.base import KeyDataset\n",
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 10  # You can adjust the batch size according to your needs\n",
    "\n",
    "inputs_dataset_path = os.path.join(\"..\", \"datasets\", \"confluence_exports-inputs\")\n",
    "valid_questions_dataset_path = os.path.join(\"..\", \"datasets\", \"confluence_exports-inputs-valid\")\n",
    "candidate_labels = [\"valid\",  \"nonsense\"]\n",
    "\n",
    "classification_pipeline = pipeline(model=\"facebook/bart-large-mnli\", device='cuda:0')\n",
    "generation_pipeline = pipeline(\"text2text-generation\", model=\"t5-base\", tokenizer=\"t5-base\", device='cuda:0')\n",
    "\n",
    "inputs_dataset = Dataset.load_from_disk(inputs_dataset_path).filter(lambda x: x[\"input\"] is not None and len(x[\"input\"].split(\" \")) > 3)\n",
    "\n",
    "classified_pipeline = tqdm(classification_pipeline(KeyDataset(inputs_dataset, \"input\"), truncation=True, candidate_labels=candidate_labels, batch_size=BATCH_SIZE), total=len(inputs_dataset), desc=\"Classifying dataset by correctness\")\n",
    "\n",
    "inputs_dataset = inputs_dataset.add_column(\"validity\", [x[\"labels\"][0] for x in classified_pipeline])\n",
    "\n",
    "inputs_dataset = inputs_dataset.filter(lambda x: x[\"validity\"] == \"valid\")\n",
    "\n",
    "inputs_dataset.save_to_disk(valid_questions_dataset_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T18:11:48.703100Z",
     "end_time": "2023-05-03T18:11:57.894119Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data augmentation\n",
    "\n",
    "Since we have a small dataset, we will augment it by replacing some words with their synonyms. We will use [wordnet](https://wordnet.princeton.edu/) for that."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Collecting en-core-web-sm==3.5.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\r\n",
      "\u001B[2K     \u001B[90m\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m77.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m0:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from en-core-web-sm==3.5.0) (3.5.2)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\r\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\r\n",
      "Requirement already satisfied: jinja2 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.29.0)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.3)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\r\n",
      "Requirement already satisfied: setuptools in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (66.0.0)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\r\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\r\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\r\n",
      "\u001B[38;5;2m Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T18:12:21.796981Z",
     "end_time": "2023-05-03T18:12:25.779984Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/andrei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/4147 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76654a34f23e4506aa006dcaed759d1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import torch\n",
    "from itertools import chain\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "valid_questions_dataset_path = os.path.join(\"..\", \"datasets\", \"confluence_exports-inputs-valid\")\n",
    "valid_questions_dataset = Dataset.load_from_disk(valid_questions_dataset_path)\n",
    "\n",
    "augmented_dataset_path = os.path.join(\"..\", \"datasets\", \"confluence_exports-inputs-augmented\")\n",
    "\n",
    "def synonym_augmentation(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        synsets = wn.synsets(word)\n",
    "        if synsets:\n",
    "            synonyms = set(chain.from_iterable([word.lemma_names() for word in synsets]))\n",
    "            if synonyms:\n",
    "                new_words.append(random.choice(list(synonyms)))\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def back_translation(text, src_lang=\"en\", tgt_lang=\"fr\"):\n",
    "    model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name).eval().to(\"cuda:0\")\n",
    "\n",
    "    # Forward translation\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(\"cuda:0\")\n",
    "    with torch.no_grad():\n",
    "        forward_outputs = model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(forward_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Backward translation\n",
    "    model_name = f'Helsinki-NLP/opus-mt-{tgt_lang}-{src_lang}'\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name).eval().to(\"cuda:0\")\n",
    "\n",
    "    inputs = tokenizer(translated_text, return_tensors=\"pt\", max_length=512, truncation=True).to(\"cuda:0\")\n",
    "    with torch.no_grad():\n",
    "        backward_outputs = model.generate(**inputs)\n",
    "    return tokenizer.decode(backward_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def random_insertion(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    word_to_insert = random.choice(words)\n",
    "    position = random.randint(0, len(words))\n",
    "    words.insert(position, word_to_insert)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def random_swap(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    if len(words) > 1:\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def random_deletion(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    if len(words) > 1:\n",
    "        idx = random.randint(0, len(words) - 1)\n",
    "        words.pop(idx)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def augment_text(text, augmentations):\n",
    "    for aug in augmentations:\n",
    "        if aug == \"synonym\":\n",
    "            text = synonym_augmentation(text)\n",
    "        elif aug == \"back_translation\":\n",
    "            text = back_translation(text)\n",
    "        elif aug == \"random_insertion\":\n",
    "            text = random_insertion(text)\n",
    "        elif aug == \"random_swap\":\n",
    "            text = random_swap(text)\n",
    "        elif aug == \"random_deletion\":\n",
    "            text = random_deletion(text)\n",
    "    return text\n",
    "\n",
    "def augment_dataset(ds, column, augmentations, num_augmentations=4):\n",
    "    augmented_data = []\n",
    "    for idx in range(len(ds)):\n",
    "        row = ds[idx]\n",
    "        text = row[column]\n",
    "        for _ in range(num_augmentations):\n",
    "            augmented_text = augment_text(text, augmentations)\n",
    "            new_row = row.copy()\n",
    "            new_row[column] = augmented_text\n",
    "            augmented_data.append(new_row)\n",
    "    return concatenate_datasets([ds, Dataset.from_pandas(pd.DataFrame(augmented_data))])\n",
    "\n",
    "augmentations = [\"synonym\", \"random_insertion\", \"random_swap\", \"random_deletion\"]\n",
    "augmented_input_ds = augment_dataset(valid_questions_dataset, \"input\", augmentations)\n",
    "augmented_text_ds = augment_dataset(valid_questions_dataset, \"text\", augmentations)\n",
    "augmented_ds = concatenate_datasets([valid_questions_dataset, augmented_input_ds, augmented_text_ds])\n",
    "columns_to_remove = [col for col in augmented_ds.column_names if col not in [\"input\", \"text\"]]\n",
    "augmented_ds = augmented_ds.remove_columns(columns_to_remove)\n",
    "augmented_ds.save_to_disk(augmented_dataset_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T18:12:26.858413Z",
     "end_time": "2023-05-03T18:12:35.333475Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "For this stage we will use Peft, Lora, DeepSpeed, Accelerate and HuggingFace trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/andrei/DataspellProjects/ai-tools/datasets/confluence_exports-inputs-augmented/cache-da2cca23ff28f88e.arrow and /home/andrei/DataspellProjects/ai-tools/datasets/confluence_exports-inputs-augmented/cache-8009923e98285385.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "augmented_dataset_path = os.path.join(\"..\", \"datasets\", \"confluence_exports-inputs-augmented\")\n",
    "\n",
    "model_id=\"google/flan-t5-xxl\"\n",
    "\n",
    "# Load tokenizer of FLAN-t5-XL\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "dataset = load_from_disk(augmented_dataset_path, )\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T18:24:52.542982Z",
     "end_time": "2023-05-03T18:24:53.097019Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 3317\n",
      "Test dataset size: 830\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T18:24:54.260215Z",
     "end_time": "2023-05-03T18:24:54.264529Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# huggingface hub model id\n",
    "model_id = \"google/flan-t5-base\"\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T18:24:55.463272Z",
     "end_time": "2023-05-03T18:24:59.161242Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/andrei/anaconda3/envs/ai-tools did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/share/gconf/ubuntu.mandatory.path')}\n",
      "  warn(msg)\n",
      "/home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/share/gconf/ubuntu.default.path')}\n",
      "  warn(msg)\n",
      "/home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('gnome-shell/DataSpell 2023.1/2663-0-ANDREI-PC-2_TIME32863')}\n",
      "  warn(msg)\n",
      "/home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('local/ANDREI-PC-2'), PosixPath('@/tmp/.ICE-unix/2638,unix/ANDREI-PC-2')}\n",
      "  warn(msg)\n",
      "/home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//debuginfod.ubuntu.com '), PosixPath('https')}\n",
      "  warn(msg)\n",
      "/home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/etc/xdg/xdg-ubuntu')}\n",
      "  warn(msg)\n",
      "/home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('0'), PosixPath('1')}\n",
      "  warn(msg)\n",
      "/home/andrei/anaconda3/envs/ai-tools/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1769472 || all params: 249347328 || trainable%: 0.7096414524241463\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T18:25:00.069501Z",
     "end_time": "2023-05-03T18:25:00.919565Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/andrei/DataspellProjects/ai-tools/datasets/confluence_exports-inputs-augmented/cache-62a2f695125de454.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/4147 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0720f300afc4856969c5f2daac1f28b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 512\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"input\"], truncation=True), batched=True, remove_columns=[\"input\", \"text\"])\n",
    "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "# take 85 percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lenghts, 85))\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"text\"], truncation=True), batched=True, remove_columns=[\"input\", \"text\"])\n",
    "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "# take 90 percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lenghts, 90))\n",
    "print(f\"Max target length: {max_target_length}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T18:25:02.246011Z",
     "end_time": "2023-05-03T18:25:03.146300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/andrei/DataspellProjects/ai-tools/datasets/confluence_exports-inputs-augmented/cache-5759f891d562ca86.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/830 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5ef0bed9c9c4131b8c7e526ecf66458"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/3317 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ccf1bd6da81a41e482a6c6054a4f4e85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/830 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e3596c94c194f568d0adf80d67a4653"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/830 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e3596c94c194f568d0adf80d67a4653"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"answer: \" + item for item in sample[\"input\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"text\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"input\", \"text\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "# save datasets to disk for later easy loading\n",
    "tokenized_dataset[\"train\"].save_to_disk(\"data/train\")\n",
    "tokenized_dataset[\"test\"].save_to_disk(\"data/eval\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T18:25:04.243290Z",
     "end_time": "2023-05-03T18:25:04.746719Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T18:25:05.246070Z",
     "end_time": "2023-05-03T18:25:05.260487Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "output_dir=\"lora-flan-t5-xxl\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # higher learning rate\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!# train model\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T18:25:11.837894Z",
     "end_time": "2023-05-03T18:35:49.908600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T20:31:58.714689Z",
     "end_time": "2023-05-03T20:31:58.747404Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are the steps to install Hadoop on\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# load model from the disk\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(output_dir)\n",
    "\n",
    "# load tokenizer from the disk\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# load pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Example question about apache tika\n",
    "question = \"What is Apache Tika?\"\n",
    "# Query\n",
    "result = qa_pipeline(question=question)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T16:10:46.339019Z",
     "end_time": "2023-05-05T16:10:49.179246Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
