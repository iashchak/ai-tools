{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning a model based on raw documents from Confluence\n",
    "\n",
    "This notebook contains code for fine-tuning a model based on raw documents from Confluence.\n",
    "\n",
    "## Introduction\n",
    "The process will contain several parts:\n",
    "\n",
    "- Data downloading\n",
    "We downloaded several examples from public available apache foundation Confluence to make a raw dataset. This step done outside of this notebook. You can read more about Confluence export here: [https://confluence.atlassian.com/doc/export-content-to-word-pdf-html-and-xml-139475.html](https://confluence.atlassian.com/doc/export-content-to-word-pdf-html-and-xml-139475.html)\n",
    "- Data extraction\n",
    "For data extraction from dumps we will use Apache Tika running on a separate docker container.\n",
    "Apache Tika - is a toolkit for detecting and extracting metadata and structured text content from various documents using existing parser libraries. You can read more about it here: [https://tika.apache.org/](https://tika.apache.org/)\n",
    "- Data processing\n",
    "We will use the dataset library to process the data. It is a library for loading and processing datasets in a few lines of code. You can read more about it here: [https://huggingface.co/docs/datasets/](https://huggingface.co/docs/datasets/)\n",
    "Also we have to extract instruction and data from the raw data.\n",
    "- Data augmentation\n",
    "Augmentation of dataset is a process of creating new data from existing data. In this case we use the model for paraphrasing to create new questions and answers.\n",
    "- Model fine-tuning\n",
    "Using modern techniques as PEFT, DeepSpeed, LoRA and Accelerate we will fine-tune the model on the dataset. You can read more about it here: [https://huggingface.co/transformers/training.html](https://huggingface.co/transformers/training.html) [https://huggingface.co/blog/peft](https://huggingface.co/blog/peft)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup environment\n",
    "\n",
    "First of all we need to install all the dependencies needed for the project."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# install common dependencies\n",
    "!pip install beautifulsoup4 requests tqdm\n",
    "# install cuda\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# install Hugging Face Libraries\n",
    "!pip install git+https://github.com/huggingface/peft.git\n",
    "!pip install bitsandbytes transformers evaluate datasets accelerate loralib --upgrade --quiet\n",
    "# install additional dependencies needed for training\n",
    "!pip install rouge-score tensorboard py7zr transformers[deepspeed] spacy nltk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data extraction\n",
    "\n",
    "In this stage we will have output in the format of\n",
    "```json\n",
    "{\n",
    "    \"file\": \"path/to/file\",\n",
    "    \"page\": \"page name\",\n",
    "    \"section\": \"section name\",\n",
    "    \"text\": \"text\"\n",
    "}[]\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "input_directory = os.path.join(\"..\", \"data\", \"confluence_exports\")\n",
    "include_extensions = [\".html\"]\n",
    "\n",
    "def get_files_to_process(root_path):\n",
    "    for dirpath, _, filenames in os.walk(root_path):\n",
    "        for filename in filenames:\n",
    "            if any(filename.endswith(ext) for ext in include_extensions):\n",
    "                yield os.path.join(dirpath, filename)\n",
    "\n",
    "articles_df = pd.DataFrame(columns=[\"title\", \"text\", \"file\"])\n",
    "fileList = list(get_files_to_process(input_directory))\n",
    "\n",
    "for filePath in tqdm(fileList, desc=\"Processing files\"):\n",
    "    with open(filePath, \"r\", encoding=\"utf-8\") as file:\n",
    "        main_header = soup.find(\"h1\").text.strip()\n",
    "        soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "        header_tags = [\"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n",
    "        headers_stack = []\n",
    "        for header in soup.find_all(header_tags):\n",
    "            header_level = int(header.name[1])\n",
    "\n",
    "            while len(headers_stack) >= header_level:\n",
    "                headers_stack.pop()\n",
    "\n",
    "            headers_stack.append(header.text)\n",
    "\n",
    "            text = ''\n",
    "            current_element = header.next_element\n",
    "\n",
    "            while current_element is not None and (current_element.name is None or current_element.name not in header_tags):\n",
    "                if current_element.name is None:\n",
    "                    text = \" \".join([text, current_element.getText().strip()])\n",
    "                current_element = current_element.next_element\n",
    "\n",
    "            title =  \" > \".join([main_header] + headers_stack)\n",
    "            articles_df = pd.concat([articles_df, pd.DataFrame([[title, text, filePath]], columns=[\"title\", \"text\", \"file\"])])\n",
    "\n",
    "def has_content(row):\n",
    "    return len(row[\"title\"].split()) > 2 and len(row[\"text\"].split()) > 5\n",
    "\n",
    "articles_df = articles_df.drop_duplicates(subset=[\"text\"])\n",
    "articles_df = articles_df.drop_duplicates(subset=[\"title\"])\n",
    "articles_df = articles_df[articles_df.apply(has_content, axis=1)]\n",
    "\n",
    "articles_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "articles_df.sample(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data adjustments\n",
    "\n",
    "Since headers isn't a great to be directly used as a question, or instruction query - we will generate prompts for each required entity based on it's label."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T21:41:22.746751Z",
     "end_time": "2023-05-02T21:41:22.809564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'section': 'Creating the branch',\n 'text': \"Creating a branch is quick and easy#start off in the apache trunk\\ngit checkout trunk\\n#create a new branch from trunk\\ngit branch HDFS-775\\n#switch to it\\ngit checkout HDFS-775\\n#show what's branch you are in\\ngit branchRemember, this branch is local to your machine. Nobody else can see it until you push up your changes or generate a patch, or you make your machine visible over the network to interested parties.\",\n 'file': '../data/confluence_exports/HADOOP/Git-And-Hadoop_89071914.html',\n 'page': 'Hadoop : Git And Hadoop',\n 'query': \"Page: Hadoop : Git And Hadoop\\nSection: Creating the branch\\nText: Creating a branch is quick and easy#start off in the apache trunk\\ngit checkout trunk\\n#create a new branch from trunk\\ngit branch HDFS-775\\n#switch to it\\ngit checkout HDFS-775\\n#show what's branch you are in\\ngit branchRemember, this branch is local to your machine. Nobody else can see it until you push up your changes or generate a patch, or you make your machine visible over the network to interested parties.\",\n 'label': 'instruction'}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "from transformers.pipelines.base import KeyDataset\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\", device='cuda:0', framework=\"pt\")\n",
    "\n",
    "example_query = \"\"\"\n",
    "    <title>Hadoop : Hadoop 2.8.0 Release > Key Git Concepts > Forking onto GitHub</title><query>How to fork Hadoop 2.8.0 Release onto GitHub?</query>\n",
    "    <title>Apache Tomcat : WebSocket 1.1 TCK > Goals</title><query>What are the goals of WebSocket 1.1 TCK in Apache Tomcat?</query>\n",
    "\"\"\"\n",
    "\n",
    "articles_df = articles_df.sample(10)\n",
    "\n",
    "def create_query(row):\n",
    "    return f\"{example_query}\\n<title>{row['title']}</title>\\n\"\n",
    "\n",
    "articles_df[\"prompt\"] = articles_df[[\"title\"]].apply(create_query, axis=1)\n",
    "\n",
    "dataset = Dataset.from_pandas(articles_df)\n",
    "\n",
    "processed_queries = pipe(KeyDataset(dataset, \"title\"), truncation=True)\n",
    "\n",
    "for query in tqdm(processed_queries, desc=\"Processing queries\", total=len(dataset)):\n",
    "    print(query)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Remove useless data\n",
    "\n",
    "Since some of the generated queries are empty or has some garbage in them - we will filter them out.\n",
    "We will use same classification model to filter out bad queries."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers.pipelines.base import KeyDataset\n",
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 10  # You can adjust the batch size according to your needs\n",
    "\n",
    "inputs_dataset_path = os.path.join(\"..\", \"datasets\", \"confluence_exports-inputs\")\n",
    "valid_questions_dataset_path = os.path.join(\"..\", \"datasets\", \"confluence_exports-inputs-valid\")\n",
    "candidate_labels = [\"valid\",  \"nonsense\"]\n",
    "\n",
    "classification_pipeline = pipeline(model=\"facebook/bart-large-mnli\", device='cuda:0')\n",
    "generation_pipeline = pipeline(\"text2text-generation\", model=\"t5-base\", tokenizer=\"t5-base\", device='cuda:0')\n",
    "\n",
    "inputs_dataset = Dataset.load_from_disk(inputs_dataset_path).filter(lambda x: x[\"input\"] is not None and len(x[\"input\"].split(\" \")) > 3)\n",
    "\n",
    "classified_pipeline = tqdm(classification_pipeline(KeyDataset(inputs_dataset, \"input\"), truncation=True, candidate_labels=candidate_labels, batch_size=BATCH_SIZE), total=len(inputs_dataset), desc=\"Classifying dataset by correctness\")\n",
    "\n",
    "inputs_dataset = inputs_dataset.add_column(\"validity\", [x[\"labels\"][0] for x in classified_pipeline])\n",
    "\n",
    "inputs_dataset = inputs_dataset.filter(lambda x: x[\"validity\"] == \"valid\")\n",
    "\n",
    "inputs_dataset.save_to_disk(valid_questions_dataset_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data augmentation\n",
    "\n",
    "Since we have a small dataset, we will augment it by replacing some words with their synonyms. We will use [wordnet](https://wordnet.princeton.edu/) for that."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import torch\n",
    "from itertools import chain\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "valid_questions_dataset_path = os.path.join(\"..\", \"datasets\", \"confluence_exports-inputs-valid\")\n",
    "valid_questions_dataset = Dataset.load_from_disk(valid_questions_dataset_path)\n",
    "\n",
    "augmented_dataset_path = os.path.join(\"..\", \"datasets\", \"confluence_exports-inputs-augmented\")\n",
    "\n",
    "def synonym_augmentation(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        synsets = wn.synsets(word)\n",
    "        if synsets:\n",
    "            synonyms = set(chain.from_iterable([word.lemma_names() for word in synsets]))\n",
    "            if synonyms:\n",
    "                new_words.append(random.choice(list(synonyms)))\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def back_translation(text, src_lang=\"en\", tgt_lang=\"fr\"):\n",
    "    model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}'\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name).eval().to(\"cuda:0\")\n",
    "\n",
    "    # Forward translation\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(\"cuda:0\")\n",
    "    with torch.no_grad():\n",
    "        forward_outputs = model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(forward_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Backward translation\n",
    "    model_name = f'Helsinki-NLP/opus-mt-{tgt_lang}-{src_lang}'\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name).eval().to(\"cuda:0\")\n",
    "\n",
    "    inputs = tokenizer(translated_text, return_tensors=\"pt\", max_length=512, truncation=True).to(\"cuda:0\")\n",
    "    with torch.no_grad():\n",
    "        backward_outputs = model.generate(**inputs)\n",
    "    return tokenizer.decode(backward_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def random_insertion(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    word_to_insert = random.choice(words)\n",
    "    position = random.randint(0, len(words))\n",
    "    words.insert(position, word_to_insert)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def random_swap(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    if len(words) > 1:\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def random_deletion(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    if len(words) > 1:\n",
    "        idx = random.randint(0, len(words) - 1)\n",
    "        words.pop(idx)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def augment_text(text, augmentations):\n",
    "    for aug in augmentations:\n",
    "        if aug == \"synonym\":\n",
    "            text = synonym_augmentation(text)\n",
    "        elif aug == \"back_translation\":\n",
    "            text = back_translation(text)\n",
    "        elif aug == \"random_insertion\":\n",
    "            text = random_insertion(text)\n",
    "        elif aug == \"random_swap\":\n",
    "            text = random_swap(text)\n",
    "        elif aug == \"random_deletion\":\n",
    "            text = random_deletion(text)\n",
    "    return text\n",
    "\n",
    "def augment_dataset(ds, column, augmentations, num_augmentations=4):\n",
    "    augmented_data = []\n",
    "    for idx in range(len(ds)):\n",
    "        row = ds[idx]\n",
    "        text = row[column]\n",
    "        for _ in range(num_augmentations):\n",
    "            augmented_text = augment_text(text, augmentations)\n",
    "            new_row = row.copy()\n",
    "            new_row[column] = augmented_text\n",
    "            augmented_data.append(new_row)\n",
    "    return concatenate_datasets([ds, Dataset.from_pandas(pd.DataFrame(augmented_data))])\n",
    "\n",
    "augmentations = [\"synonym\", \"random_insertion\", \"random_swap\", \"random_deletion\"]\n",
    "augmented_input_ds = augment_dataset(valid_questions_dataset, \"input\", augmentations)\n",
    "augmented_text_ds = augment_dataset(valid_questions_dataset, \"text\", augmentations)\n",
    "augmented_ds = concatenate_datasets([valid_questions_dataset, augmented_input_ds, augmented_text_ds])\n",
    "columns_to_remove = [col for col in augmented_ds.column_names if col not in [\"input\", \"text\"]]\n",
    "augmented_ds = augmented_ds.remove_columns(columns_to_remove)\n",
    "augmented_ds.save_to_disk(augmented_dataset_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "For this stage we will use Peft, Lora, DeepSpeed, Accelerate and HuggingFace trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "augmented_dataset_path = os.path.join(\"..\", \"datasets\", \"confluence_exports-inputs-augmented\")\n",
    "\n",
    "model_id=\"google/flan-t5-xxl\"\n",
    "\n",
    "# Load tokenizer of FLAN-t5-XL\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "dataset = load_from_disk(augmented_dataset_path, )\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# huggingface hub model id\n",
    "model_id = \"google/flan-t5-base\"\n",
    "\n",
    "# load model from the hub\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "# prepare int-8 model for training\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"input\"], truncation=True), batched=True, remove_columns=[\"input\", \"text\"])\n",
    "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "# take 85 percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lenghts, 85))\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"text\"], truncation=True), batched=True, remove_columns=[\"input\", \"text\"])\n",
    "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "# take 90 percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lenghts, 90))\n",
    "print(f\"Max target length: {max_target_length}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"answer: \" + item for item in sample[\"input\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"text\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"input\", \"text\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "# save datasets to disk for later easy loading\n",
    "tokenized_dataset[\"train\"].save_to_disk(\"data/train\")\n",
    "tokenized_dataset[\"test\"].save_to_disk(\"data/eval\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "output_dir=\"lora-flan-t5-xxl\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # higher learning rate\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!# train model\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# load model from the disk\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(output_dir)\n",
    "\n",
    "# load tokenizer from the disk\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# load pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Example question about apache tika\n",
    "question = \"What is Apache Tika?\"\n",
    "# Query\n",
    "result = qa_pipeline(question=question)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
