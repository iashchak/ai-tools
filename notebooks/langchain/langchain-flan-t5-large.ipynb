{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with LangChain and Flan-T5\n",
    "\n",
    "In this notebook we will experiment with the LangChain and Flan-T5 models. We will use the same data as in the previous notebooks.\n",
    "The goal is to get explanations with deep reasoning about particular topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local vector database\n",
    "\n",
    "We need to store embeddings of the documents in a local database. We will use embedded database - Weaviate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install tqdm beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install weaviate-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T21:20:17.647703494Z",
     "start_time": "2023-05-09T21:20:15.527336872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Processing files:   0%|          | 0/266 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76b31078cd924b029bea26722b5df1f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                                             source_raw   \n551   Hadoop > Hadoop 2.8.0 Release > Blocker/Critic...  \\\n1219  Apache Tomcat > ClusteringOverview > General T...   \n588   Hadoop > How To Contribute > Dev Environment S...   \n232   TIKA > API Bindings for Tika > Kubernetes Char...   \n947   Apache Tomcat > UsingDataSources > How do I us...   \n399   TIKA > VirtualMachine > Install software (this...   \n328   TIKA > ComparisonTikaAndPDFToText201811 > Lang...   \n858   Apache Tomcat > Community Review of DISA STIG ...   \n140   TIKA > Release Process for tika-helm > Apache ...   \n562   Hadoop > GitHub Integration > Git setup > Clos...   \n\n                                                 target   \n551    TODOs before RC TODO item Status - 1/4/2017 C...  \\\n1219   Linux in General You'll find that Linux is a ...   \n588    Integrated Development Environment (IDE) You ...   \n232    Ruby   Tika-Client: Ruby Bindings for Tika Se...   \n947    How do I use DataSources with Tomcat? When de...   \n399    prep nsfpolardata scp -r <user>@nsfpolardata....   \n328    Improvements to tika-eval   We observed a han...   \n858    V-222964 This finding misses multiple TLS set...   \n140    Creating Git Release Based on the above gener...   \n562    Closing a PR without committing (for committe...   \n\n                                                   file  \n551   ../../data/confluence_exports/HADOOP/Hadoop-2....  \n1219  ../../data/confluence_exports/TOMCAT/Clusterin...  \n588   ../../data/confluence_exports/HADOOP/How-To-Co...  \n232   ../../data/confluence_exports/TIKA/API-Binding...  \n947   ../../data/confluence_exports/TOMCAT/UsingData...  \n399   ../../data/confluence_exports/TIKA/VirtualMach...  \n328   ../../data/confluence_exports/TIKA/ComparisonT...  \n858   ../../data/confluence_exports/TOMCAT/Community...  \n140   ../../data/confluence_exports/TIKA/Release-Pro...  \n562   ../../data/confluence_exports/HADOOP/GitHub-In...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source_raw</th>\n      <th>target</th>\n      <th>file</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>551</th>\n      <td>Hadoop &gt; Hadoop 2.8.0 Release &gt; Blocker/Critic...</td>\n      <td>TODOs before RC TODO item Status - 1/4/2017 C...</td>\n      <td>../../data/confluence_exports/HADOOP/Hadoop-2....</td>\n    </tr>\n    <tr>\n      <th>1219</th>\n      <td>Apache Tomcat &gt; ClusteringOverview &gt; General T...</td>\n      <td>Linux in General You'll find that Linux is a ...</td>\n      <td>../../data/confluence_exports/TOMCAT/Clusterin...</td>\n    </tr>\n    <tr>\n      <th>588</th>\n      <td>Hadoop &gt; How To Contribute &gt; Dev Environment S...</td>\n      <td>Integrated Development Environment (IDE) You ...</td>\n      <td>../../data/confluence_exports/HADOOP/How-To-Co...</td>\n    </tr>\n    <tr>\n      <th>232</th>\n      <td>TIKA &gt; API Bindings for Tika &gt; Kubernetes Char...</td>\n      <td>Ruby   Tika-Client: Ruby Bindings for Tika Se...</td>\n      <td>../../data/confluence_exports/TIKA/API-Binding...</td>\n    </tr>\n    <tr>\n      <th>947</th>\n      <td>Apache Tomcat &gt; UsingDataSources &gt; How do I us...</td>\n      <td>How do I use DataSources with Tomcat? When de...</td>\n      <td>../../data/confluence_exports/TOMCAT/UsingData...</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>TIKA &gt; VirtualMachine &gt; Install software (this...</td>\n      <td>prep nsfpolardata scp -r &lt;user&gt;@nsfpolardata....</td>\n      <td>../../data/confluence_exports/TIKA/VirtualMach...</td>\n    </tr>\n    <tr>\n      <th>328</th>\n      <td>TIKA &gt; ComparisonTikaAndPDFToText201811 &gt; Lang...</td>\n      <td>Improvements to tika-eval   We observed a han...</td>\n      <td>../../data/confluence_exports/TIKA/ComparisonT...</td>\n    </tr>\n    <tr>\n      <th>858</th>\n      <td>Apache Tomcat &gt; Community Review of DISA STIG ...</td>\n      <td>V-222964 This finding misses multiple TLS set...</td>\n      <td>../../data/confluence_exports/TOMCAT/Community...</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>TIKA &gt; Release Process for tika-helm &gt; Apache ...</td>\n      <td>Creating Git Release Based on the above gener...</td>\n      <td>../../data/confluence_exports/TIKA/Release-Pro...</td>\n    </tr>\n    <tr>\n      <th>562</th>\n      <td>Hadoop &gt; GitHub Integration &gt; Git setup &gt; Clos...</td>\n      <td>Closing a PR without committing (for committe...</td>\n      <td>../../data/confluence_exports/HADOOP/GitHub-In...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "input_directory = os.path.join(\"..\", \"data\", \"confluence_exports\")\n",
    "include_extensions = [\".html\"]\n",
    "\n",
    "dataset_path = os.path.join(\"..\", \"datasets\", \"confluence_exports-inputs-augmented\")\n",
    "\n",
    "\n",
    "def get_files_to_process(root_path):\n",
    "    for dirpath, _, filenames in os.walk(root_path):\n",
    "        for filename in filenames:\n",
    "            if any(filename.endswith(ext) for ext in include_extensions):\n",
    "                yield os.path.join(dirpath, filename)\n",
    "\n",
    "\n",
    "articles_df = pd.DataFrame(columns=[\"source_raw\", \"target\", \"file\"])\n",
    "fileList = list(get_files_to_process(input_directory))\n",
    "\n",
    "for filePath in tqdm(fileList, desc=\"Processing files\"):\n",
    "    with open(filePath, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "        main_header = soup.find(\"h1\").text.strip()\n",
    "        header_tags = [\"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n",
    "        headers_stack = []\n",
    "        for header in soup.find_all(header_tags):\n",
    "            header_level = int(header.name[1])\n",
    "\n",
    "            while len(headers_stack) >= header_level:\n",
    "                headers_stack.pop()\n",
    "\n",
    "            headers_stack.append(header.text)\n",
    "\n",
    "            target = ''\n",
    "            current_element = header.next_element\n",
    "\n",
    "            while current_element is not None and (\n",
    "                    current_element.name is None or current_element.name not in header_tags):\n",
    "                if current_element.name is None:\n",
    "                    target = \" \".join([target, current_element.getText().strip()])\n",
    "                current_element = current_element.next_element\n",
    "\n",
    "            source_raw = \" : \".join([main_header] + headers_stack).replace(':', '>')\n",
    "            articles_df = pd.concat(\n",
    "                [articles_df, pd.DataFrame([[source_raw, target, filePath]], columns=[\"source_raw\", \"target\", \"file\"])])\n",
    "\n",
    "\n",
    "def has_content(row):\n",
    "    return len(row[\"source_raw\"].split()) > 2 and len(row[\"target\"].split()) > 5\n",
    "\n",
    "\n",
    "articles_df = articles_df.drop_duplicates(subset=[\"source_raw\"])\n",
    "articles_df = articles_df.drop_duplicates(subset=[\"target\"])\n",
    "articles_df = articles_df[articles_df.apply(has_content, axis=1)]\n",
    "\n",
    "articles_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "articles_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Weaviate Vector DB\n",
    "\n",
    "To complete following step you need to ensure that `docker` and `compose plugin` installed.\n",
    "Then you need to run:\n",
    "\n",
    "```sh\n",
    "docker compose up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T21:23:18.391568230Z",
     "start_time": "2023-05-09T21:20:21.320690938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Persisting articles to Weaviate:   0%|          | 0/1235 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07f9784d988e4aec912a6b5ab6e214c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import weaviate\n",
    "\n",
    "schema = {\n",
    "    \"classes\": [\n",
    "        {\n",
    "            \"class\": \"Document\",\n",
    "            \"description\": \"A class called document\",\n",
    "            \"moduleConfig\": {\n",
    "                \"text2vec-huggingface\": {\n",
    "                    \"model\": \"google/flan-t5-large\",\n",
    "                    \"options\": {\n",
    "                        \"waitForModel\": True,\n",
    "                        \"useGPU\": True,\n",
    "                        \"useCache\": True\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"dataType\": [\n",
    "                        \"text\"\n",
    "                    ],\n",
    "                    \"description\": \"Title of the document\",\n",
    "                    \"moduleConfig\": {\n",
    "                        \"text2vec-transformers\": {\n",
    "                            \"skip\": True,\n",
    "                            \"vectorizePropertyName\": True\n",
    "                        }\n",
    "                    },\n",
    "                    \"name\": \"title\"\n",
    "                },\n",
    "                {\n",
    "                    \"dataType\": [\n",
    "                        \"text\"\n",
    "                    ],\n",
    "                    \"description\": \"Content that will be vectorized\",\n",
    "                    \"moduleConfig\": {\n",
    "                        \"text2vec-transformers\": {\n",
    "                            \"skip\": True,\n",
    "                            \"vectorizePropertyName\": True\n",
    "                        }\n",
    "                    },\n",
    "                    \"name\": \"content\"\n",
    "                }\n",
    "            ],\n",
    "            \"vectorizer\": \"text2vec-transformers\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "client = weaviate.Client(\n",
    "    \"http://127.0.0.1:8080\",\n",
    "    startup_period=30\n",
    ")\n",
    "\n",
    "client.schema.delete_all()\n",
    "client.schema.create(schema)\n",
    "\n",
    "with client.batch as batch:\n",
    "    batch.batch_size=100\n",
    "    for index, article in tqdm(articles_df.iterrows(), total=len(articles_df), desc=\"Persisting articles to Weaviate\"):\n",
    "        data_obj = {\n",
    "            \"title\": article[\"source_raw\"],\n",
    "            \"content\": article[\"target\"]\n",
    "        }\n",
    "        client.batch.add_data_object(data_obj, \"Document\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of LangChain with Flan-T5\n",
    "\n",
    "We will use the Flan-T5 model from the HuggingFace library and text2text-generation pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install transformers langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T21:36:37.114705437Z",
     "start_time": "2023-05-09T21:36:32.095713291Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Document prompt requires documents to have metadata variables: ['source']. Received document with missing metadata: ['source'].",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 26\u001B[0m\n\u001B[1;32m     24\u001B[0m vectorstore \u001B[38;5;241m=\u001B[39m Weaviate(client, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDocument\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     25\u001B[0m docs \u001B[38;5;241m=\u001B[39m vectorstore\u001B[38;5;241m.\u001B[39msimilarity_search(query)\n\u001B[0;32m---> 26\u001B[0m \u001B[43mchain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_documents\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mquestion\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/site-packages/langchain/chains/base.py:140\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    139\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 140\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    141\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/site-packages/langchain/chains/base.py:134\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks)\u001B[0m\n\u001B[1;32m    128\u001B[0m run_manager \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_chain_start(\n\u001B[1;32m    129\u001B[0m     {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m    130\u001B[0m     inputs,\n\u001B[1;32m    131\u001B[0m )\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    133\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 134\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    135\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    136\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    137\u001B[0m     )\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    139\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:84\u001B[0m, in \u001B[0;36mBaseCombineDocumentsChain._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001B[39;00m\n\u001B[1;32m     83\u001B[0m other_keys \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_key}\n\u001B[0;32m---> 84\u001B[0m output, extra_return_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcombine_docs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_run_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mother_keys\u001B[49m\n\u001B[1;32m     86\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     87\u001B[0m extra_return_dict[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_key] \u001B[38;5;241m=\u001B[39m output\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m extra_return_dict\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py:85\u001B[0m, in \u001B[0;36mStuffDocumentsChain.combine_docs\u001B[0;34m(self, docs, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcombine_docs\u001B[39m(\n\u001B[1;32m     82\u001B[0m     \u001B[38;5;28mself\u001B[39m, docs: List[Document], callbacks: Callbacks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any\n\u001B[1;32m     83\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mdict\u001B[39m]:\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;124;03m\"\"\"Stuff all documents into one prompt and pass to LLM.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 85\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_inputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     86\u001B[0m     \u001B[38;5;66;03m# Call predict on the LLM.\u001B[39;00m\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_chain\u001B[38;5;241m.\u001B[39mpredict(callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs), {}\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py:65\u001B[0m, in \u001B[0;36mStuffDocumentsChain._get_inputs\u001B[0;34m(self, docs, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_inputs\u001B[39m(\u001B[38;5;28mself\u001B[39m, docs: List[Document], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m:\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;66;03m# Format each document according to the prompt\u001B[39;00m\n\u001B[0;32m---> 65\u001B[0m     doc_strings \u001B[38;5;241m=\u001B[39m [format_document(doc, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdocument_prompt) \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m docs]\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;66;03m# Join the documents together to put them in the prompt.\u001B[39;00m\n\u001B[1;32m     67\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     68\u001B[0m         k: v\n\u001B[1;32m     69\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m     70\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_chain\u001B[38;5;241m.\u001B[39mprompt\u001B[38;5;241m.\u001B[39minput_variables\n\u001B[1;32m     71\u001B[0m     }\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py:65\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_inputs\u001B[39m(\u001B[38;5;28mself\u001B[39m, docs: List[Document], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m:\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;66;03m# Format each document according to the prompt\u001B[39;00m\n\u001B[0;32m---> 65\u001B[0m     doc_strings \u001B[38;5;241m=\u001B[39m [\u001B[43mformat_document\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdocument_prompt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m docs]\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;66;03m# Join the documents together to put them in the prompt.\u001B[39;00m\n\u001B[1;32m     67\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     68\u001B[0m         k: v\n\u001B[1;32m     69\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m     70\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_chain\u001B[38;5;241m.\u001B[39mprompt\u001B[38;5;241m.\u001B[39minput_variables\n\u001B[1;32m     71\u001B[0m     }\n",
      "File \u001B[0;32m~/anaconda3/envs/ai-tools/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:27\u001B[0m, in \u001B[0;36mformat_document\u001B[0;34m(doc, prompt)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_metadata) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     24\u001B[0m     required_metadata \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     25\u001B[0m         iv \u001B[38;5;28;01mfor\u001B[39;00m iv \u001B[38;5;129;01min\u001B[39;00m prompt\u001B[38;5;241m.\u001B[39minput_variables \u001B[38;5;28;01mif\u001B[39;00m iv \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpage_content\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     26\u001B[0m     ]\n\u001B[0;32m---> 27\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     28\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDocument prompt requires documents to have metadata variables: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     29\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrequired_metadata\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Received document with missing metadata: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     30\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(missing_metadata)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     31\u001B[0m     )\n\u001B[1;32m     32\u001B[0m document_info \u001B[38;5;241m=\u001B[39m {k: base_info[k] \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m prompt\u001B[38;5;241m.\u001B[39minput_variables}\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m prompt\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdocument_info)\n",
      "\u001B[0;31mValueError\u001B[0m: Document prompt requires documents to have metadata variables: ['source']. Received document with missing metadata: ['source']."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Weaviate\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "vectorstore = Weaviate(client, \"Document\", \"content\")\n",
    "query = \"What is apache tika?\"\n",
    "docs = vectorstore.similarity_search(query)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
