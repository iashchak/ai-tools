{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Building a Question Answering Model with Transformers\n",
    "\n",
    "In this notebook, we demonstrate how to create a question answering model using the Transformers library from Hugging Face. We download and preprocess data from Wikipedia using natural language processing techniques and transformer-based models, fine-tune a pre-trained transformer model on the data, and generate answers to a set of predefined questions using the trained model.\n",
    "\n",
    "The notebook is divided into the following sections:\n",
    "\n",
    "1. Introduction ([link](#Section-1:-Introduction))\n",
    "2. Loading and Preprocessing Data ([link](#Section-2:-Loading-and-Preprocessing-Data))\n",
    "3. Creating the QADataset Class ([link](#Section-3:-Creating-the-QADataset-Class))\n",
    "4. Fine-Tuning the Transformer Model ([link](#Section-4:-Fine-Tuning-the-Transformer-Model))\n",
    "5. Generating Answers to Questions ([link](#Section-5:-Generating-Answers-to-Questions))\n",
    "6. Conclusions ([link](#Section-6:-Conclusions))\n",
    "7. References ([link](#Section-7:-References))\n",
    "\n",
    "The `QADataset` class is defined in Section 3, which is the core of this notebook. It loads the preprocessed data and creates PyTorch datasets for training and validation. The data loading and preprocessing functions are defined in Section 2. The transformer model is fine-tuned on the dataset in Section 4 using the PyTorch Lightning framework, and the trained model is used to generate answers to predefined questions in Section 5. Finally, Section 6 provides a summary of the results and discusses the potential improvements for the question answering model, while Section 7 provides links to resources related to the use of the Transformers library and transformer-based models for natural language processing.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 1: Introduction\n",
    "\n",
    "In this notebook, we will demonstrate the process of creating a question answering model using the Transformers library from Hugging Face. We will use natural language processing techniques and transformer-based models to achieve the best results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "articles_file_path = \"../data/articles.csv\"\n",
    "train_csv_path = \"../datasets/train_dataset.csv\"\n",
    "val_csv_path = \"../datasets/val_dataset.csv\"\n",
    "tokenizer_for_sentence_splitting = 'punkt'\n",
    "base_model_name = \"databricks/dolly-v2-3b\"\n",
    "model_path = \"./finetuned_model\"\n",
    "articles_category = 'Science_fiction_films'\n",
    "\n",
    "batch_size = 1\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T17:36:05.195708Z",
     "end_time": "2023-04-15T17:36:05.211228Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 2: Loading and Preprocessing Data\n",
    "\n",
    "This section is dedicated to downloading and preprocessing data for the question answering model. We will use the Wikipedia API to download articles on a given topic, split them into sentences, and create a dataset of question-answer pairs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mwparserfromhell in c:\\programdata\\anaconda3\\lib\\site-packages (0.6.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install mwparserfromhell"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T17:36:05.211228Z",
     "end_time": "2023-04-15T17:36:08.295376Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "                         title  \\\n0               Alfonso Cuarón   \n1                    Brad Bird   \n2         Brian Patrick Butler   \n3                 Charles Band   \n4  Chris Carter (screenwriter)   \n\n                                                text  \n0  Alfonso Cuarón Orozco ( , ; born 28 November 1...  \n1  Phillip Bradley Bird (born September 24, 1957)...  \n2  Brian Patrick Butler is an American actor, fil...  \n3  Charles Robert Band (born December 27, 1951) i...  \n4  Christopher Carl Carter (born October 13, 1956...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Alfonso Cuarón</td>\n      <td>Alfonso Cuarón Orozco ( , ; born 28 November 1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Brad Bird</td>\n      <td>Phillip Bradley Bird (born September 24, 1957)...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Brian Patrick Butler</td>\n      <td>Brian Patrick Butler is an American actor, fil...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Charles Band</td>\n      <td>Charles Robert Band (born December 27, 1951) i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Chris Carter (screenwriter)</td>\n      <td>Christopher Carl Carter (born October 13, 1956...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mwparserfromhell\n",
    "\n",
    "if not os.path.exists(articles_file_path):\n",
    "    def get_category_members(category: str, member_type: str) -> List[str]:\n",
    "        base_url = 'https://en.wikipedia.org/w/api.php'\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'list': 'categorymembers',\n",
    "            'cmtitle': category,\n",
    "            'cmtype': member_type,\n",
    "            'format': 'json',\n",
    "            'cmlimit': 500\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = json.loads(response.text)\n",
    "        members = [item['title'] for item in data['query']['categorymembers']]\n",
    "        return members\n",
    "\n",
    "\n",
    "    def get_article_texts(articles: List[str]) -> Dict[str, str]:\n",
    "        base_url = 'https://en.wikipedia.org/w/api.php'\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'prop': 'revisions',\n",
    "            'rvprop': 'content',\n",
    "            'format': 'json',\n",
    "            'titles': '|'.join(articles)\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = json.loads(response.text)\n",
    "        texts = {}\n",
    "        for page in data['query']['pages'].values():\n",
    "            raw_text = page['revisions'][0]['*']\n",
    "            parsed_text = mwparserfromhell.parse(raw_text)\n",
    "            cleaned_text = parsed_text.strip_code()\n",
    "            texts[page['title']] = cleaned_text\n",
    "        return texts\n",
    "\n",
    "\n",
    "    def download_articles(category: str) -> pd.DataFrame:\n",
    "        subcategories = get_category_members(f'Category:{category}', 'subcat')\n",
    "        all_articles = []\n",
    "\n",
    "        for subcategory in tqdm(subcategories, desc=\"Downloading subcategories\"):\n",
    "            articles = get_category_members(subcategory, 'page')\n",
    "            all_articles.extend(articles)\n",
    "\n",
    "        article_data = []\n",
    "        for i in tqdm(range(0, len(all_articles), 50), desc=\"Downloading articles\"):\n",
    "            batch = all_articles[i:i + 50]\n",
    "            texts = get_article_texts(batch)\n",
    "            for title, text in texts.items():\n",
    "                article_data.append({'title': title, 'text': text})\n",
    "\n",
    "        return pd.DataFrame(article_data)\n",
    "\n",
    "\n",
    "    articles_df = download_articles(articles_category)\n",
    "    articles_df.to_csv(articles_file_path, index=False)\n",
    "else:\n",
    "    articles_df = pd.read_csv(articles_file_path)\n",
    "\n",
    "articles_df.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T17:36:08.295376Z",
     "end_time": "2023-04-15T17:36:08.483688Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 3: Creating the QADataset Class\n",
    "\n",
    "In this section, we define the QADataset class, which is used to load the preprocessed data and create PyTorch datasets for training and validation. We also define helper functions for keyword extraction and sentence replacement, which are used to create question-answer pairs from sentences."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T17:36:08.483688Z",
     "end_time": "2023-04-15T17:36:11.545290Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\programdata\\anaconda3\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.22.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 34.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T17:36:11.549642Z",
     "end_time": "2023-04-15T17:36:21.910042Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 97224\n"
     ]
    },
    {
     "data": {
      "text/plain": "       index                                           question  \\\n55929     20  Who chose Carpenter to be the director because...   \n26177    949  On When 20, 2020, it was announced it will be ...   \n91154    270  Who appears in Teenage Mutant Ninja Turtles, p...   \n71692    756  Which organization Blades as Detective Danny A...   \n91295    523  It is the What part of the Creation Wars saga,...   \n\n                          answer  \n55929                    Douglas  \n26177              July 20, 2020  \n91154                      Karai  \n71692  Rubén Blades as Detective  \n91295                     second  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>55929</th>\n      <td>20</td>\n      <td>Who chose Carpenter to be the director because...</td>\n      <td>Douglas</td>\n    </tr>\n    <tr>\n      <th>26177</th>\n      <td>949</td>\n      <td>On When 20, 2020, it was announced it will be ...</td>\n      <td>July 20, 2020</td>\n    </tr>\n    <tr>\n      <th>91154</th>\n      <td>270</td>\n      <td>Who appears in Teenage Mutant Ninja Turtles, p...</td>\n      <td>Karai</td>\n    </tr>\n    <tr>\n      <th>71692</th>\n      <td>756</td>\n      <td>Which organization Blades as Detective Danny A...</td>\n      <td>Rubén Blades as Detective</td>\n    </tr>\n    <tr>\n      <th>91295</th>\n      <td>523</td>\n      <td>It is the What part of the Creation Wars saga,...</td>\n      <td>second</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import sent_tokenize\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "import random\n",
    "import re\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_question_word(entity_label):\n",
    "    question_word_map = {\n",
    "        \"PERSON\": \"Who\",\n",
    "        \"GPE\": \"Where\",\n",
    "        \"ORG\": \"Which organization\",\n",
    "        \"DATE\": \"When\",\n",
    "        \"TIME\": \"At what time\",\n",
    "        \"NOUN\": \"What\",\n",
    "        \"PROPN\": \"Which\",\n",
    "    }\n",
    "    return question_word_map.get(entity_label, \"What\")\n",
    "\n",
    "\n",
    "def replace_keywords(sentence, keywords, question_word):\n",
    "    words = sentence.split()\n",
    "    replaced = False\n",
    "    for i, word in enumerate(words):\n",
    "        if word.strip(\".,?;!\") in keywords and not replaced:\n",
    "            words[i] = question_word\n",
    "            replaced = True\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def extract_keywords(doc):\n",
    "    # For NER:\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    # For POS tagging (nouns and proper nouns):\n",
    "    nouns = [(token.text, token.pos_) for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"]]\n",
    "    keywords = entities + nouns\n",
    "    if not keywords:\n",
    "        return None\n",
    "    # Select the first keyword and its label\n",
    "    keyword, label = keywords[0]\n",
    "    question_word = get_question_word(label)\n",
    "    return keyword, question_word\n",
    "\n",
    "\n",
    "def create_qa_pairs(df):\n",
    "    qa_pairs = []\n",
    "    for index in tqdm(df.index, total=df.shape[0], desc=\"Creating QA pairs\"):\n",
    "        text = df.loc[index, \"text\"]\n",
    "        # Remove the section headings\n",
    "        cleaned_text = re.sub(r\"==.*?==+\", \"\", text)\n",
    "        sentences = sent_tokenize(cleaned_text)\n",
    "        docs = list(nlp.pipe(sentences))\n",
    "        for sentence, doc in zip(sentences, docs):\n",
    "            keyword_info = extract_keywords(doc)\n",
    "            if keyword_info is not None:\n",
    "                keyword, question_word = keyword_info\n",
    "                question = replace_keywords(sentence, keyword, question_word)\n",
    "                qa_pairs.append((index, question, keyword))\n",
    "    qa_df = pd.DataFrame(qa_pairs, columns=[\"index\", \"question\", \"answer\"])\n",
    "    return qa_df\n",
    "\n",
    "\n",
    "def split_data(qa_pairs, test_size=0.2, random_state=42):\n",
    "    train_data, val_data = train_test_split(qa_pairs, test_size=test_size, random_state=random_state)\n",
    "    return train_data, val_data\n",
    "\n",
    "def save_qa_pairs_to_csv(train_qa_pairs, val_qa_pairs, train_csv_path, val_csv_path):\n",
    "    train_df = pd.DataFrame(train_qa_pairs, columns=[\"index\", \"question\", \"answer\"])\n",
    "    val_df = pd.DataFrame(val_qa_pairs, columns=[\"index\", \"question\", \"answer\"])\n",
    "    train_df.to_csv(train_csv_path, index=False)\n",
    "    val_df.to_csv(val_csv_path, index=False)\n",
    "\n",
    "def load_qa_pairs_from_csv(train_csv_path, val_csv_path):\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    val_df = pd.read_csv(val_csv_path)\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, df, qa_df, tokenizer, max_input_length=512, max_output_length=512):\n",
    "        self.df = df\n",
    "        self.qa_df = qa_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qa_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index, question, answer = self.qa_df.loc[idx, [\"index\", \"question\", \"answer\"]]\n",
    "        tokenized_input = self.tokenizer(question, max_length=self.max_input_length, padding=\"max_length\",\n",
    "                                         truncation=True, return_tensors=\"pt\")\n",
    "        tokenized_output = self.tokenizer(answer, max_length=self.max_output_length, padding=\"max_length\",\n",
    "                                          truncation=True, return_tensors=\"pt\")\n",
    "        return {\"input_ids\": tokenized_input[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": tokenized_input[\"attention_mask\"].squeeze(),\n",
    "                \"labels\": tokenized_output[\"input_ids\"].squeeze()}\n",
    "\n",
    "# load train and validation datasets if they exist or create them and save them\n",
    "if os.path.exists(train_csv_path) and os.path.exists(val_csv_path):\n",
    "    train_qa_df, val_qa_df = load_qa_pairs_from_csv(train_csv_path, val_csv_path)\n",
    "else:\n",
    "    qa_df = create_qa_pairs(articles_df)\n",
    "    train_qa_df, val_qa_df = split_data(qa_df)\n",
    "    save_qa_pairs_to_csv(train_qa_df, val_qa_df, train_csv_path, val_csv_path)\n",
    "\n",
    "train_dataset = QADataset(articles_df, train_qa_df, tokenizer)\n",
    "val_dataset = QADataset(articles_df, val_qa_df, tokenizer)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "train_qa_df.sample(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T17:45:21.679888Z",
     "end_time": "2023-04-15T17:45:22.872150Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation samples: 24307\n"
     ]
    },
    {
     "data": {
      "text/plain": "       index                                           question        answer\n11361    220  Who Story, a 2009 audio story by Big Finish, c...  Mary's Story\n20039    756  It tells Which organization great deal of the ...      Harrigan\n4751     407  Plot At what time evening, an elderly woman te...   One evening\n2979     725  Who Maslin, reviewing for The New York Times, ...  Janet Maslin\n3369     264  The lifespan of What demon or half-demon as He...          half",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11361</th>\n      <td>220</td>\n      <td>Who Story, a 2009 audio story by Big Finish, c...</td>\n      <td>Mary's Story</td>\n    </tr>\n    <tr>\n      <th>20039</th>\n      <td>756</td>\n      <td>It tells Which organization great deal of the ...</td>\n      <td>Harrigan</td>\n    </tr>\n    <tr>\n      <th>4751</th>\n      <td>407</td>\n      <td>Plot At what time evening, an elderly woman te...</td>\n      <td>One evening</td>\n    </tr>\n    <tr>\n      <th>2979</th>\n      <td>725</td>\n      <td>Who Maslin, reviewing for The New York Times, ...</td>\n      <td>Janet Maslin</td>\n    </tr>\n    <tr>\n      <th>3369</th>\n      <td>264</td>\n      <td>The lifespan of What demon or half-demon as He...</td>\n      <td>half</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "val_qa_df.sample(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T17:45:27.961866Z",
     "end_time": "2023-04-15T17:45:27.972931Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 4: Fine-Tuning the Transformer Model\n",
    "\n",
    "This section is dedicated to fine-tuning the transformer-based language model using the PyTorch Lightning framework. We use the Trainer class from the transformers library to train the model on the QADataset, and evaluate its performance on a validation set. We also save the model and tokenizer for later use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    eval_accumulation_steps=1,\n",
    "    prediction_loss_only=True,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 5: Generating Answers to Questions\n",
    "\n",
    "In this section, we load the saved model and tokenizer, and use them to generate answers to a set of predefined questions. We use the pipeline function from the transformers library to generate text from input strings, and print the generated answers to the console."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "qa_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "questions = [\n",
    "    \"What is the main theme of the movie Blade Runner?\",\n",
    "    \"Who is the author of the novel Dune?\",\n",
    "    \"What is the name of the spaceship in the Alien movie?\",\n",
    "    \"Who directed the movie The Matrix?\",\n",
    "    \"What is the setting of the Star Wars series?\",\n",
    "    \"What is the similarity between the movie Matrix and Star Wars?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = qa_pipeline(question, max_length=50)[0][\"generated_text\"]\n",
    "    print(f\"Q: {question}\\nA: {answer}\\n\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 6: Conclusions\n",
    "\n",
    "This section provides a summary of the results and discusses the potential improvements for the question answering model. We also discuss the applications of the Transformers library for natural language processing tasks in general."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 7: References\n",
    "This section provides links to resources related to the use of the Transformers library and transformer-based models for natural language processing. Links to the official Hugging Face website, examples of transformer-based models, and research papers on transformer models are provided.\n",
    "\n",
    "Hugging Face official website: https://huggingface.co/\n",
    "Transformers: https://huggingface.co/transformers/\n",
    "Transformers examples: https://huggingface.co/examples\n",
    "\"Attention Is All You Need\" paper on Transformers: https://arxiv.org/abs/1706.03762\n",
    "\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" paper: https://arxiv.org/abs/1810.04805"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
