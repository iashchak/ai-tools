{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "In this Jupyter Notebook, our AI Tools team will take you on a journey to fine-tune the cutting-edge natural language processing model, Databricks Dolly v2-3b. Our objective is to enhance its capabilities to answer questions based on specific Wikipedia articles. By leveraging the vast knowledge available on Wikipedia, we aim to create a highly accurate and context-aware question-answering system. This project will encompass various stages, including data collection, preprocessing, model fine-tuning, and testing, providing a comprehensive overview of the entire process.\n",
    "\n",
    "Our fine-tuned Dolly v2-3b model is the result of advanced deep learning techniques applied to vast amounts of text data, enabling it to understand complex language patterns and generate meaningful responses in real-time. We have customized the model to excel at answering questions based on specific Wikipedia articles, providing a highly accurate and context-aware solution.\n",
    "\n",
    "Throughout this project, we will remain focused on achieving our primary goal: to create a powerful, effective, and highly customized model capable of understanding and responding to queries within the context of the chosen articles. We believe that this approach will provide a valuable tool for a wide range of applications, from customer support to research and education."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameters\n",
    "TBD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# File paths\n",
    "articles_file_path = \"../data/articles.csv\"\n",
    "\n",
    "# Dataset paths\n",
    "train_dataset_path = \"../datasets/train_dataset.pkl\"\n",
    "val_dataset_path = \"../datasets/val_dataset.pkl\"\n",
    "\n",
    "# Model parameters\n",
    "tokenizer_for_sentence_splitting = 'punkt'\n",
    "base_model_name = \"databricks/dolly-v2-3b\"\n",
    "model_path = \"./finetuned_model\"\n",
    "\n",
    "# Article parameters\n",
    "articles_category = 'Science_fiction_films'\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T17:07:52.019892Z",
     "end_time": "2023-04-14T17:07:52.035512Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Collection\n",
    "\n",
    "Code for downloading and processing the Wikipedia articles using the WikipediaBulkDownloader class provided in the first message."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "                         title  \\\n0               Alfonso Cuarón   \n1                    Brad Bird   \n2         Brian Patrick Butler   \n3                 Charles Band   \n4  Chris Carter (screenwriter)   \n\n                                                text  \n0  {{short description|Mexican filmmaker}}\\n{{Red...  \n1  {{Short description|American film director, sc...  \n2  {{Short description|American actor and filmmak...  \n3  {{short description|American film director}}\\n...  \n4  {{Short description|American television and fi...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Alfonso Cuarón</td>\n      <td>{{short description|Mexican filmmaker}}\\n{{Red...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Brad Bird</td>\n      <td>{{Short description|American film director, sc...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Brian Patrick Butler</td>\n      <td>{{Short description|American actor and filmmak...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Charles Band</td>\n      <td>{{short description|American film director}}\\n...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Chris Carter (screenwriter)</td>\n      <td>{{Short description|American television and fi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if not os.path.exists(articles_file_path):\n",
    "    # Retrieve elements (subcategories or articles) for the specified category.\n",
    "    def get_category_members(category: str, member_type: str) -> List[str]:\n",
    "        base_url = 'https://en.wikipedia.org/w/api.php'\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'list': 'categorymembers',\n",
    "            'cmtitle': category,\n",
    "            'cmtype': member_type,\n",
    "            'format': 'json',\n",
    "            'cmlimit': 500\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = json.loads(response.text)\n",
    "        members = [item['title'] for item in data['query']['categorymembers']]\n",
    "        return members\n",
    "\n",
    "\n",
    "    # Retrieve full wikitext content of articles.\n",
    "    def get_article_texts(articles: List[str]) -> Dict[str, str]:\n",
    "        base_url = 'https://en.wikipedia.org/w/api.php'\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'prop': 'revisions',\n",
    "            'rvprop': 'content',\n",
    "            'format': 'json',\n",
    "            'titles': '|'.join(articles)\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = json.loads(response.text)\n",
    "        texts = {}\n",
    "        for page in data['query']['pages'].values():\n",
    "            texts[page['title']] = page['revisions'][0]['*']\n",
    "        return texts\n",
    "\n",
    "\n",
    "    # Download articles from subcategories of the specified category.\n",
    "    def download_articles(category: str) -> pd.DataFrame:\n",
    "        # Get subcategories of the given category\n",
    "        subcategories = get_category_members(f'Category:{category}', 'subcat')\n",
    "        all_articles = []\n",
    "\n",
    "        # Loop through subcategories and get articles from each subcategory\n",
    "        for subcategory in tqdm(subcategories, desc=\"Downloading subcategories\"):\n",
    "            articles = get_category_members(subcategory, 'page')\n",
    "            all_articles.extend(articles)\n",
    "\n",
    "        article_data = []\n",
    "        # Process articles in batches to avoid hitting API limits\n",
    "        for i in tqdm(range(0, len(all_articles), 50), desc=\"Downloading articles\"):\n",
    "            batch = all_articles[i:i + 50]\n",
    "            texts = get_article_texts(batch)\n",
    "            for title, text in texts.items():\n",
    "                article_data.append({'title': title, 'text': text})\n",
    "\n",
    "        return pd.DataFrame(article_data)\n",
    "\n",
    "\n",
    "    # Download the articles and save them to a CSV file\n",
    "    articles_df = download_articles(articles_category)\n",
    "    articles_df.to_csv(articles_file_path, index=False)\n",
    "else:\n",
    "    # Load the articles from the existing CSV file\n",
    "    articles_df = pd.read_csv(articles_file_path)\n",
    "\n",
    "articles_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T17:07:52.035512Z",
     "end_time": "2023-04-14T17:07:52.445555Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a Dialogue Dataset\n",
    "\n",
    "Code for generating question-answer pairs based on the Wikipedia articles and storing them in a text file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "\n",
    "# Load the prerequisities\n",
    "nltk.download(tokenizer_for_sentence_splitting)  # Download the punkt tokenizer for sentence splitting\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)  # Load the tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T17:07:52.445555Z",
     "end_time": "2023-04-14T17:07:52.742956Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Load the articles\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, df, qa_indices, tokenizer, max_input_length=512, max_output_length=512):\n",
    "        self.df = df\n",
    "        self.qa_indices = qa_indices\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qa_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index, question, answer = self.qa_indices[idx]\n",
    "        tokenized_input = self.tokenizer(question, max_length=self.max_input_length, padding=\"max_length\",\n",
    "                                         truncation=True, return_tensors=\"pt\")\n",
    "        tokenized_output = self.tokenizer(answer, max_length=self.max_output_length, padding=\"max_length\",\n",
    "                                          truncation=True, return_tensors=\"pt\")\n",
    "        return {\"input_ids\": tokenized_input[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": tokenized_input[\"attention_mask\"].squeeze(),\n",
    "                \"labels\": tokenized_output[\"input_ids\"].squeeze()}\n",
    "\n",
    "\n",
    "# Replace keywords in a sentence with a placeholder\n",
    "def replace_keywords(sentence, keywords):\n",
    "    words = sentence.split()\n",
    "    replaced = False\n",
    "    for i, word in enumerate(words):\n",
    "        if word.strip(\".,?;!\") in keywords and not replaced:\n",
    "            words[i] = \"____\"\n",
    "            replaced = True\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# Extract keywords from a sentence\n",
    "def extract_keywords(sentence, num_keywords=1):\n",
    "    words = sentence.split()\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    words = sorted(words, key=lambda x: len(x), reverse=True)\n",
    "    return words[:num_keywords]\n",
    "\n",
    "\n",
    "# Create question-answer pairs from the articles\n",
    "def create_qa_pairs(df):\n",
    "    qa_pairs = []\n",
    "    for index in tqdm(df.index, total=df.shape[0], desc=\"Creating QA pairs\"):\n",
    "        text = df.loc[index, \"text\"]\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            keywords = extract_keywords(sentence)\n",
    "            question = replace_keywords(sentence, keywords)\n",
    "            if \"____\" in question:\n",
    "                qa_pairs.append((index, question, keywords[0]))\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "# Split data into train and validation sets\n",
    "def split_data(df, test_size=0.2, random_state=42):\n",
    "    train_df, val_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "# Check if datasets exist, otherwise create and save them\n",
    "if os.path.exists(train_dataset_path) and os.path.exists(val_dataset_path):\n",
    "    # Load the datasets\n",
    "    with open(train_dataset_path, \"rb\") as train_file:\n",
    "        train_dataset = pickle.load(train_file)\n",
    "\n",
    "    with open(val_dataset_path, \"rb\") as val_file:\n",
    "        val_dataset = pickle.load(val_file)\n",
    "else:\n",
    "    # Create the datasets\n",
    "    qa_pairs = create_qa_pairs(articles_df)\n",
    "    train_data, val_data = split_data(qa_pairs)\n",
    "\n",
    "    train_dataset = QADataset(train_data, tokenizer)\n",
    "    val_dataset = QADataset(val_data, tokenizer)\n",
    "\n",
    "    # Save the datasets\n",
    "    with open(train_dataset_path, \"wb\") as train_file:\n",
    "        pickle.dump(train_dataset, train_file)\n",
    "\n",
    "    with open(val_dataset_path, \"wb\") as val_file:\n",
    "        pickle.dump(val_dataset, val_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T17:07:52.742956Z",
     "end_time": "2023-04-14T17:07:53.330549Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model\n",
    "TBD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:399\u001B[0m, in \u001B[0;36mload_state_dict\u001B[1;34m(checkpoint_file)\u001B[0m\n\u001B[0;32m    398\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 399\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    400\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\serialization.py:809\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001B[0m\n\u001B[0;32m    808\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mUnpicklingError(UNSAFE_MESSAGE \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(e)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m--> 809\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _load(opened_zipfile, map_location, pickle_module, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args)\n\u001B[0;32m    810\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m weights_only:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\serialization.py:1172\u001B[0m, in \u001B[0;36m_load\u001B[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001B[0m\n\u001B[0;32m   1171\u001B[0m unpickler\u001B[38;5;241m.\u001B[39mpersistent_load \u001B[38;5;241m=\u001B[39m persistent_load\n\u001B[1;32m-> 1172\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43munpickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1174\u001B[0m torch\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39m_validate_loaded_sparse_tensors()\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\serialization.py:1142\u001B[0m, in \u001B[0;36m_load.<locals>.persistent_load\u001B[1;34m(saved_id)\u001B[0m\n\u001B[0;32m   1141\u001B[0m     nbytes \u001B[38;5;241m=\u001B[39m numel \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39m_element_size(dtype)\n\u001B[1;32m-> 1142\u001B[0m     typed_storage \u001B[38;5;241m=\u001B[39m \u001B[43mload_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_maybe_decode_ascii\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1144\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m typed_storage\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\serialization.py:1112\u001B[0m, in \u001B[0;36m_load.<locals>.load_tensor\u001B[1;34m(dtype, numel, key, location)\u001B[0m\n\u001B[0;32m   1110\u001B[0m name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m-> 1112\u001B[0m storage \u001B[38;5;241m=\u001B[39m \u001B[43mzip_file\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_storage_from_record\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mUntypedStorage\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_untyped_storage\n\u001B[0;32m   1113\u001B[0m \u001B[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001B[39;00m\n\u001B[0;32m   1114\u001B[0m \u001B[38;5;66;03m# stop wrapping with TypedStorage\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 257433600 bytes.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoModelForCausalLM\n\u001B[1;32m----> 3\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_model_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:463\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    461\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    462\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[1;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m    464\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    465\u001B[0m     )\n\u001B[0;32m    466\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    467\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    468\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    469\u001B[0m )\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:2184\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   2181\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m from_pt:\n\u001B[0;32m   2182\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sharded \u001B[38;5;129;01mand\u001B[39;00m state_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2183\u001B[0m         \u001B[38;5;66;03m# Time to load the checkpoint\u001B[39;00m\n\u001B[1;32m-> 2184\u001B[0m         state_dict \u001B[38;5;241m=\u001B[39m \u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresolved_archive_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2186\u001B[0m     \u001B[38;5;66;03m# set dtype to instantiate the model under:\u001B[39;00m\n\u001B[0;32m   2187\u001B[0m     \u001B[38;5;66;03m# 1. If torch_dtype is not None, we use that dtype\u001B[39;00m\n\u001B[0;32m   2188\u001B[0m     \u001B[38;5;66;03m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001B[39;00m\n\u001B[0;32m   2189\u001B[0m     \u001B[38;5;66;03m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001B[39;00m\n\u001B[0;32m   2190\u001B[0m     \u001B[38;5;66;03m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001B[39;00m\n\u001B[0;32m   2191\u001B[0m     dtype_orig \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py:403\u001B[0m, in \u001B[0;36mload_state_dict\u001B[1;34m(checkpoint_file)\u001B[0m\n\u001B[0;32m    401\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    402\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(checkpoint_file) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m--> 403\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mversion\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    404\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[0;32m    405\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou seem to have cloned a repository without having git-lfs installed. Please install \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    406\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgit-lfs and run `git lfs install` followed by `git lfs pull` in the folder \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    407\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou cloned.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    408\u001B[0m             )\n\u001B[0;32m    409\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mMemoryError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name)  # Load the model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T16:47:19.112710Z",
     "end_time": "2023-04-14T16:49:37.113875Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    eval_accumulation_steps=1,\n",
    "    prediction_loss_only=True,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Test the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Create a pipeline for question answering\n",
    "qa_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Test the model on some questions based on Science Fiction movies\n",
    "questions = [\n",
    "    \"What is the main theme of the movie Blade Runner?\",\n",
    "    \"Who is the author of the novel Dune?\",\n",
    "    \"What is the name of the spaceship in the Alien movie?\",\n",
    "    \"Who directed the movie The Matrix?\",\n",
    "    \"What is the setting of the Star Wars series?\",\n",
    "    \"What is the similarity between the movie Matrix and Star Wars?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = qa_pipeline(question, max_length=50)[0][\"generated_text\"]\n",
    "    print(f\"Q: {question}\\nA: {answer}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
