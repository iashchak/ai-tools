{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Building a Question Answering Model with Transformers\n",
    "\n",
    "In this notebook, we demonstrate how to create a question answering model using the Transformers library from Hugging Face. We download and preprocess data from Wikipedia using natural language processing techniques and transformer-based models, fine-tune a pre-trained transformer model on the data, and generate answers to a set of predefined questions using the trained model.\n",
    "\n",
    "The notebook is divided into the following sections:\n",
    "\n",
    "1. Introduction ([link](#Section-1:-Introduction))\n",
    "2. Loading and Preprocessing Data ([link](#Section-2:-Loading-and-Preprocessing-Data))\n",
    "3. Creating the QADataset Class ([link](#Section-3:-Creating-the-QADataset-Class))\n",
    "4. Fine-Tuning the Transformer Model ([link](#Section-4:-Fine-Tuning-the-Transformer-Model))\n",
    "5. Generating Answers to Questions ([link](#Section-5:-Generating-Answers-to-Questions))\n",
    "6. Conclusions ([link](#Section-6:-Conclusions))\n",
    "7. References ([link](#Section-7:-References))\n",
    "\n",
    "The `QADataset` class is defined in Section 3, which is the core of this notebook. It loads the preprocessed data and creates PyTorch datasets for training and validation. The data loading and preprocessing functions are defined in Section 2. The transformer model is fine-tuned on the dataset in Section 4 using the PyTorch Lightning framework, and the trained model is used to generate answers to predefined questions in Section 5. Finally, Section 6 provides a summary of the results and discusses the potential improvements for the question answering model, while Section 7 provides links to resources related to the use of the Transformers library and transformer-based models for natural language processing.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 1: Introduction\n",
    "\n",
    "In this notebook, we will demonstrate the process of creating a question answering model using the Transformers library from Hugging Face. We will use natural language processing techniques and transformer-based models to achieve the best results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T20:13:27.551973Z",
     "end_time": "2023-04-15T20:13:27.598849Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "data_path = \"../data\"\n",
    "articles_file_path = f\"{data_path}/articles.csv\"\n",
    "\n",
    "dataset_path = \"../datasets\"\n",
    "train_csv_path = f\"{dataset_path}/train_dataset.csv\"\n",
    "val_csv_path = f\"{dataset_path}/val_dataset.csv\"\n",
    "\n",
    "articles_category = 'Science_fiction_films'\n",
    "\n",
    "models_path = \"../models\"\n",
    "finetuned_model_path = f\"{models_path}/model.pt\"\n",
    "\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "if not os.path.exists(dataset_path):\n",
    "    os.makedirs(dataset_path, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T20:13:27.567599Z",
     "end_time": "2023-04-15T20:13:27.630099Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 2: Loading and Preprocessing Data\n",
    "\n",
    "This section is dedicated to downloading and preprocessing data for the question answering model. We will use the Wikipedia API to download articles on a given topic, split them into sentences, and create a dataset of question-answer pairs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mwparserfromhell in c:\\programdata\\anaconda3\\lib\\site-packages (0.6.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install mwparserfromhell"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T20:13:27.583231Z",
     "end_time": "2023-04-15T20:13:30.502013Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading subcategories:   0%|          | 0/28 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4b45a4cd0954adca068e81dff48b011"
      },
      "application/json": {
       "n": 0,
       "total": 28,
       "elapsed": 0.015625,
       "ncols": null,
       "nrows": null,
       "prefix": "Downloading subcategories",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading articles:   0%|          | 0/28 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7d1f91f1c48446984941ecf8dfae3d7"
      },
      "application/json": {
       "n": 0,
       "total": 28,
       "elapsed": 0.018436193466186523,
       "ncols": null,
       "nrows": null,
       "prefix": "Downloading articles",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                         title  \\\n0               Alfonso Cuarón   \n1                    Brad Bird   \n2         Brian Patrick Butler   \n3                 Charles Band   \n4  Chris Carter (screenwriter)   \n\n                                                text  \n0  Alfonso Cuarón Orozco ( , ; born 28 November 1...  \n1  Phillip Bradley Bird (born September 24, 1957)...  \n2  Brian Patrick Butler is an American actor, fil...  \n3  Charles Robert Band (born December 27, 1951) i...  \n4  Christopher Carl Carter (born October 13, 1956...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Alfonso Cuarón</td>\n      <td>Alfonso Cuarón Orozco ( , ; born 28 November 1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Brad Bird</td>\n      <td>Phillip Bradley Bird (born September 24, 1957)...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Brian Patrick Butler</td>\n      <td>Brian Patrick Butler is an American actor, fil...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Charles Band</td>\n      <td>Charles Robert Band (born December 27, 1951) i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Chris Carter (screenwriter)</td>\n      <td>Christopher Carl Carter (born October 13, 1956...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mwparserfromhell\n",
    "\n",
    "if not os.path.exists(articles_file_path):\n",
    "    def get_category_members(category: str, member_type: str) -> List[str]:\n",
    "        base_url = 'https://en.wikipedia.org/w/api.php'\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'list': 'categorymembers',\n",
    "            'cmtitle': category,\n",
    "            'cmtype': member_type,\n",
    "            'format': 'json',\n",
    "            'cmlimit': 500\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = json.loads(response.text)\n",
    "        members = [item['title'] for item in data['query']['categorymembers']]\n",
    "        return members\n",
    "\n",
    "\n",
    "    def get_article_texts(articles: List[str]) -> Dict[str, str]:\n",
    "        base_url = 'https://en.wikipedia.org/w/api.php'\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'prop': 'revisions',\n",
    "            'rvprop': 'content',\n",
    "            'format': 'json',\n",
    "            'titles': '|'.join(articles)\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = json.loads(response.text)\n",
    "        texts = {}\n",
    "        for page in data['query']['pages'].values():\n",
    "            raw_text = page['revisions'][0]['*']\n",
    "            parsed_text = mwparserfromhell.parse(raw_text)\n",
    "            cleaned_text = parsed_text.strip_code()\n",
    "            texts[page['title']] = cleaned_text\n",
    "        return texts\n",
    "\n",
    "\n",
    "    def download_articles(category: str) -> pd.DataFrame:\n",
    "        subcategories = get_category_members(f'Category:{category}', 'subcat')\n",
    "        all_articles = []\n",
    "\n",
    "        for subcategory in tqdm(subcategories, desc=\"Downloading subcategories\"):\n",
    "            articles = get_category_members(subcategory, 'page')\n",
    "            all_articles.extend(articles)\n",
    "\n",
    "        article_data = []\n",
    "        for i in tqdm(range(0, len(all_articles), 50), desc=\"Downloading articles\"):\n",
    "            batch = all_articles[i:i + 50]\n",
    "            texts = get_article_texts(batch)\n",
    "            for title, text in texts.items():\n",
    "                article_data.append({'title': title, 'text': text})\n",
    "\n",
    "        return pd.DataFrame(article_data)\n",
    "\n",
    "    articles_df = download_articles(articles_category)\n",
    "    articles_df.to_csv(articles_file_path, index=False)\n",
    "else:\n",
    "    articles_df = pd.read_csv(articles_file_path)\n",
    "\n",
    "articles_df.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T20:13:30.502013Z",
     "end_time": "2023-04-15T20:15:51.468420Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 3: Creating the QADataset Class\n",
    "\n",
    "In this section, we define the QADataset class, which is used to load the preprocessed data and create PyTorch datasets for training and validation. We also define helper functions for keyword extraction and sentence replacement, which are used to create question-answer pairs from sentences."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\programdata\\anaconda3\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.22.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 21.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy transformers\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m nltk.downloader punkt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T20:15:51.468420Z",
     "end_time": "2023-04-15T20:16:05.263712Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "Creating QA pairs:   0%|          | 0/1357 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d23c252e7e0d472ea27982c3a716d81d"
      },
      "application/json": {
       "n": 0,
       "total": 1357,
       "elapsed": 0.0069997310638427734,
       "ncols": null,
       "nrows": null,
       "prefix": "Creating QA pairs",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import sent_tokenize\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "base_model_name = \"databricks/dolly-v2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_question_word(entity_label):\n",
    "    question_word_map = {\n",
    "        \"PERSON\": \"Who\",\n",
    "        \"GPE\": \"Where\",\n",
    "        \"ORG\": \"Which organization\",\n",
    "        \"DATE\": \"When\",\n",
    "        \"TIME\": \"At what time\",\n",
    "        \"NOUN\": \"What\",\n",
    "        \"PROPN\": \"Which\",\n",
    "    }\n",
    "    return question_word_map.get(entity_label, \"What\")\n",
    "\n",
    "\n",
    "def replace_keywords(sentence, keywords, question_word):\n",
    "    words = sentence.split()\n",
    "    replaced = False\n",
    "    for i, word in enumerate(words):\n",
    "        if word.strip(\".,?;!\") in keywords and not replaced:\n",
    "            words[i] = question_word\n",
    "            replaced = True\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def extract_keywords(doc):\n",
    "    # For NER:\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    # For POS tagging (nouns and proper nouns):\n",
    "    nouns = [(token.text, token.pos_) for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"]]\n",
    "    keywords = entities + nouns\n",
    "    if not keywords:\n",
    "        return None\n",
    "    # Select the first keyword and its label\n",
    "    keyword, label = keywords[0]\n",
    "    question_word = get_question_word(label)\n",
    "    return keyword, question_word\n",
    "\n",
    "\n",
    "def create_qa_pairs(df):\n",
    "    qa_pairs = []\n",
    "    for index in tqdm(df.index, total=df.shape[0], desc=\"Creating QA pairs\"):\n",
    "        text = df.loc[index, \"text\"]\n",
    "        # Remove the section headings\n",
    "        cleaned_text = re.sub(r\"==.*?==+\", \"\", text)\n",
    "        sentences = sent_tokenize(cleaned_text)\n",
    "        docs = list(nlp.pipe(sentences))\n",
    "        for sentence, doc in zip(sentences, docs):\n",
    "            keyword_info = extract_keywords(doc)\n",
    "            if keyword_info is not None:\n",
    "                keyword, question_word = keyword_info\n",
    "                question = replace_keywords(sentence, keyword, question_word)\n",
    "                qa_pairs.append((index, question, keyword))\n",
    "    qa_df = pd.DataFrame(qa_pairs, columns=[\"index\", \"question\", \"answer\"])\n",
    "    return qa_df\n",
    "\n",
    "\n",
    "def split_data(qa_pairs, test_size=0.2, random_state=42):\n",
    "    train_data, val_data = train_test_split(qa_pairs, test_size=test_size, random_state=random_state)\n",
    "    return train_data, val_data\n",
    "\n",
    "def save_qa_pairs_to_csv(train_qa_pairs, val_qa_pairs, train_csv_path, val_csv_path):\n",
    "    train_df = pd.DataFrame(train_qa_pairs, columns=[\"index\", \"question\", \"answer\"])\n",
    "    val_df = pd.DataFrame(val_qa_pairs, columns=[\"index\", \"question\", \"answer\"])\n",
    "    train_df.to_csv(train_csv_path, index=False)\n",
    "    val_df.to_csv(val_csv_path, index=False)\n",
    "\n",
    "def load_qa_pairs_from_csv(train_csv_path, val_csv_path):\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    val_df = pd.read_csv(val_csv_path)\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, df, qa_df, tokenizer, max_input_length=512, max_output_length=512):\n",
    "        self.df = df\n",
    "        self.qa_df = qa_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qa_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index, question, answer = self.qa_df.loc[idx, [\"index\", \"question\", \"answer\"]]\n",
    "        tokenized_input = self.tokenizer(question, max_length=self.max_input_length, padding=\"max_length\",\n",
    "                                         truncation=True, return_tensors=\"pt\")\n",
    "        tokenized_output = self.tokenizer(answer, max_length=self.max_output_length, padding=\"max_length\",\n",
    "                                          truncation=True, return_tensors=\"pt\")\n",
    "        return {\"input_ids\": tokenized_input[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": tokenized_input[\"attention_mask\"].squeeze(),\n",
    "                \"labels\": tokenized_output[\"input_ids\"].squeeze()}\n",
    "\n",
    "# load train and validation datasets if they exist or create them and save them\n",
    "if os.path.exists(train_csv_path) and os.path.exists(val_csv_path):\n",
    "    train_qa_df, val_qa_df = load_qa_pairs_from_csv(train_csv_path, val_csv_path)\n",
    "else:\n",
    "    qa_df = create_qa_pairs(articles_df)\n",
    "    train_qa_df, val_qa_df = split_data(qa_df)\n",
    "    save_qa_pairs_to_csv(train_qa_df, val_qa_df, train_csv_path, val_csv_path)\n",
    "\n",
    "train_dataset = QADataset(articles_df, train_qa_df, tokenizer)\n",
    "val_dataset = QADataset(articles_df, val_qa_df, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T20:16:05.265849Z",
     "end_time": "2023-04-15T20:21:13.072772Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 97236\n"
     ]
    },
    {
     "data": {
      "text/plain": "        index                                           question  \\\n105798   1151  The new name Which organization derived from T...   \n101380   1105  Who later directed a feature-length stop-motio...   \n62058     706  In Who turmoil, the last Mother Box is left un...   \n24457     295  Which organization Cinema criticized the drast...   \n101670   1108  It grossed What million at the U.S. box office...   \n\n               answer  \n105798        Atragon  \n101380         Burton  \n62058      Mother Box  \n24457    Japan Cinema  \n101670  $19.4 million  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>105798</th>\n      <td>1151</td>\n      <td>The new name Which organization derived from T...</td>\n      <td>Atragon</td>\n    </tr>\n    <tr>\n      <th>101380</th>\n      <td>1105</td>\n      <td>Who later directed a feature-length stop-motio...</td>\n      <td>Burton</td>\n    </tr>\n    <tr>\n      <th>62058</th>\n      <td>706</td>\n      <td>In Who turmoil, the last Mother Box is left un...</td>\n      <td>Mother Box</td>\n    </tr>\n    <tr>\n      <th>24457</th>\n      <td>295</td>\n      <td>Which organization Cinema criticized the drast...</td>\n      <td>Japan Cinema</td>\n    </tr>\n    <tr>\n      <th>101670</th>\n      <td>1108</td>\n      <td>It grossed What million at the U.S. box office...</td>\n      <td>$19.4 million</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "train_qa_df.sample(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T20:21:13.072772Z",
     "end_time": "2023-04-15T20:21:13.088397Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation samples: 24310\n"
     ]
    },
    {
     "data": {
      "text/plain": "       index                                           question     answer\n94683   1029  Co-director Who reflected on the film in 2016 ...     Morton\n72616    806  While both aspects are present in Which organi...   MacReady\n79773    866  Which organization 1998 Fantasia's Toronto edi...  Fant-Asia\n1864      24                      \"Tobias, Scott (20 May 2007).     Tobias\n46641    567  What spoke to this in June 2018, stating that ...      Feige",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>94683</th>\n      <td>1029</td>\n      <td>Co-director Who reflected on the film in 2016 ...</td>\n      <td>Morton</td>\n    </tr>\n    <tr>\n      <th>72616</th>\n      <td>806</td>\n      <td>While both aspects are present in Which organi...</td>\n      <td>MacReady</td>\n    </tr>\n    <tr>\n      <th>79773</th>\n      <td>866</td>\n      <td>Which organization 1998 Fantasia's Toronto edi...</td>\n      <td>Fant-Asia</td>\n    </tr>\n    <tr>\n      <th>1864</th>\n      <td>24</td>\n      <td>\"Tobias, Scott (20 May 2007).</td>\n      <td>Tobias</td>\n    </tr>\n    <tr>\n      <th>46641</th>\n      <td>567</td>\n      <td>What spoke to this in June 2018, stating that ...</td>\n      <td>Feige</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "val_qa_df.sample(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T20:21:13.088397Z",
     "end_time": "2023-04-15T20:21:13.150897Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 4: Fine-Tuning the Transformer Model\n",
    "\n",
    "This section is dedicated to fine-tuning the transformer-based language model using the PyTorch Lightning framework. We use the Trainer class from the transformers library to train the model on the QADataset, and evaluate its performance on a validation set. We also save the model and tokenizer for later use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 76.00 MiB (GPU 0; 6.00 GiB total capacity; 5.26 GiB already allocated; 0 bytes free; 5.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[66], line 27\u001B[0m\n\u001B[0;32m      8\u001B[0m val_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(val_dataset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     10\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[0;32m     11\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./results\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     12\u001B[0m     num_train_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     24\u001B[0m     save_strategy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     25\u001B[0m )\n\u001B[1;32m---> 27\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     30\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     31\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m     35\u001B[0m trainer\u001B[38;5;241m.\u001B[39mevaluate()\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:437\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001B[0m\n\u001B[0;32m    434\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;241m=\u001B[39m tokenizer\n\u001B[0;32m    436\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mplace_model_on_device:\n\u001B[1;32m--> 437\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_move_model_to_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    439\u001B[0m \u001B[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001B[39;00m\n\u001B[0;32m    440\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_model_parallel:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:703\u001B[0m, in \u001B[0;36mTrainer._move_model_to_device\u001B[1;34m(self, model, device)\u001B[0m\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_move_model_to_device\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, device):\n\u001B[1;32m--> 703\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001B[39;00m\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mparallel_mode \u001B[38;5;241m==\u001B[39m ParallelMode\u001B[38;5;241m.\u001B[39mTPU \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtie_weights\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001B[0m, in \u001B[0;36mModule.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1141\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1142\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m   1143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[1;32m-> 1145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[0;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[0;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[1;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001B[0m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[0;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    816\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    817\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    818\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    819\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 820\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    821\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    822\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m   1140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m   1141\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1142\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m-> 1143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 76.00 MiB (GPU 0; 6.00 GiB total capacity; 5.26 GiB already allocated; 0 bytes free; 5.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    eval_accumulation_steps=1,\n",
    "    prediction_loss_only=True,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "model.save_pretrained(finetuned_model_path)\n",
    "tokenizer.save_pretrained(finetuned_model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 5: Generating Answers to Questions\n",
    "\n",
    "In this section, we load the saved model and tokenizer, and use them to generate answers to a set of predefined questions. We use the pipeline function from the transformers library to generate text from input strings, and print the generated answers to the console."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(finetuned_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path)\n",
    "qa_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "questions = [\n",
    "    \"What is the main theme of the movie Blade Runner?\",\n",
    "    \"Who is the author of the novel Dune?\",\n",
    "    \"What is the name of the spaceship in the Alien movie?\",\n",
    "    \"Who directed the movie The Matrix?\",\n",
    "    \"What is the setting of the Star Wars series?\",\n",
    "    \"What is the similarity between the movie Matrix and Star Wars?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = qa_pipeline(question, max_length=50)[0][\"generated_text\"]\n",
    "    print(f\"Q: {question}\\nA: {answer}\\n\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 6: Conclusions\n",
    "\n",
    "This section provides a summary of the results and discusses the potential improvements for the question answering model. We also discuss the applications of the Transformers library for natural language processing tasks in general."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 7: References\n",
    "This section provides links to resources related to the use of the Transformers library and transformer-based models for natural language processing. Links to the official Hugging Face website, examples of transformer-based models, and research papers on transformer models are provided.\n",
    "\n",
    "Hugging Face official website: https://huggingface.co/\n",
    "Transformers: https://huggingface.co/transformers/\n",
    "Transformers examples: https://huggingface.co/examples\n",
    "\"Attention Is All You Need\" paper on Transformers: https://arxiv.org/abs/1706.03762\n",
    "\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" paper: https://arxiv.org/abs/1810.04805"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
