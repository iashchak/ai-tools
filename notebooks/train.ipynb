{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Building a Question Answering Model with Transformers\n",
    "\n",
    "In this notebook, we demonstrate how to create a question answering model using the Transformers library from Hugging Face. We download and preprocess data from Wikipedia using natural language processing techniques and transformer-based models, fine-tune a pre-trained transformer model on the data, and generate answers to a set of predefined questions using the trained model.\n",
    "\n",
    "The notebook is divided into the following sections:\n",
    "\n",
    "1. Introduction ([link](#Section-1:-Introduction))\n",
    "2. Loading and Preprocessing Data ([link](#Section-2:-Loading-and-Preprocessing-Data))\n",
    "3. Creating the QADataset Class ([link](#Section-3:-Creating-the-QADataset-Class))\n",
    "4. Fine-Tuning the Transformer Model ([link](#Section-4:-Fine-Tuning-the-Transformer-Model))\n",
    "5. Generating Answers to Questions ([link](#Section-5:-Generating-Answers-to-Questions))\n",
    "6. Conclusions ([link](#Section-6:-Conclusions))\n",
    "7. References ([link](#Section-7:-References))\n",
    "\n",
    "The `QADataset` class is defined in Section 3, which is the core of this notebook. It loads the preprocessed data and creates PyTorch datasets for training and validation. The data loading and preprocessing functions are defined in Section 2. The transformer model is fine-tuned on the dataset in Section 4 using the PyTorch Lightning framework, and the trained model is used to generate answers to predefined questions in Section 5. Finally, Section 6 provides a summary of the results and discusses the potential improvements for the question answering model, while Section 7 provides links to resources related to the use of the Transformers library and transformer-based models for natural language processing.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 1: Introduction\n",
    "\n",
    "In this notebook, we will demonstrate the process of creating a question answering model using the Transformers library from Hugging Face. We will use natural language processing techniques and transformer-based models to achieve the best results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T20:13:27.551973Z",
     "end_time": "2023-04-15T20:13:27.598849Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_path = \"../data\"\n",
    "articles_file_path = f\"{data_path}/articles.csv\"\n",
    "\n",
    "dataset_path = \"../datasets\"\n",
    "train_csv_path = f\"{dataset_path}/train_dataset.csv\"\n",
    "val_csv_path = f\"{dataset_path}/val_dataset.csv\"\n",
    "\n",
    "articles_category = \"Science_fiction_films\"\n",
    "\n",
    "models_path = \"../models\"\n",
    "finetuned_model_path = f\"{models_path}/model.pt\"\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "if not os.path.exists(dataset_path):\n",
    "    os.makedirs(dataset_path, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T20:13:27.567599Z",
     "end_time": "2023-04-15T20:13:27.630099Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 2: Loading and Preprocessing Data\n",
    "\n",
    "This section is dedicated to downloading and preprocessing data for the question answering model. We will use the Wikipedia API to download articles on a given topic, split them into sentences, and create a dataset of question-answer pairs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install mwparserfromhell"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T20:13:27.583231Z",
     "end_time": "2023-04-15T20:13:30.502013Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mwparserfromhell\n",
    "\n",
    "if not os.path.exists(articles_file_path):\n",
    "\n",
    "    def get_category_members(category: str, member_type: str) -> List[str]:\n",
    "        base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"categorymembers\",\n",
    "            \"cmtitle\": category,\n",
    "            \"cmtype\": member_type,\n",
    "            \"format\": \"json\",\n",
    "            \"cmlimit\": 500,\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = json.loads(response.text)\n",
    "        members = [item[\"title\"] for item in data[\"query\"][\"categorymembers\"]]\n",
    "        return members\n",
    "\n",
    "    def get_article_texts(articles: List[str]) -> Dict[str, str]:\n",
    "        base_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"prop\": \"revisions\",\n",
    "            \"rvprop\": \"content\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": \"|\".join(articles),\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = json.loads(response.text)\n",
    "        texts = {}\n",
    "        for page in data[\"query\"][\"pages\"].values():\n",
    "            raw_text = page[\"revisions\"][0][\"*\"]\n",
    "            parsed_text = mwparserfromhell.parse(raw_text)\n",
    "            cleaned_text = parsed_text.strip_code()\n",
    "            texts[page[\"title\"]] = cleaned_text\n",
    "        return texts\n",
    "\n",
    "    def download_articles(category: str) -> pd.DataFrame:\n",
    "        subcategories = get_category_members(f\"Category:{category}\", \"subcat\")\n",
    "        all_articles = []\n",
    "        for subcategory in tqdm(subcategories, desc=\"Downloading subcategories\"):\n",
    "            articles = get_category_members(subcategory, \"page\")\n",
    "            all_articles.extend(articles)\n",
    "\n",
    "        article_data = []\n",
    "        for i in tqdm(range(0, len(all_articles), 50), desc=\"Downloading articles\"):\n",
    "            batch = all_articles[i : i + 50]\n",
    "            texts = get_article_texts(batch)\n",
    "            for title, text in texts.items():\n",
    "                article_data.append({\"title\": title, \"text\": text})\n",
    "\n",
    "        return pd.DataFrame(article_data)\n",
    "\n",
    "    articles_df = download_articles(articles_category)\n",
    "    articles_df.to_csv(articles_file_path, index=False)\n",
    "else:\n",
    "    articles_df = pd.read_csv(articles_file_path)\n",
    "\n",
    "articles_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T20:13:30.502013Z",
     "end_time": "2023-04-15T20:15:51.468420Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 3: Creating the QADataset Class\n",
    "\n",
    "In this section, we define the QADataset class, which is used to load the preprocessed data and create PyTorch datasets for training and validation. We also define helper functions for keyword extraction and sentence replacement, which are used to create question-answer pairs from sentences."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install spacy transformers\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m nltk.downloader punkt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T20:15:51.468420Z",
     "end_time": "2023-04-15T20:16:05.263712Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import sent_tokenize\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "base_model_name = \"databricks/dolly-v2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_question_word(entity_label):\n",
    "    question_word_map = {\n",
    "        \"PERSON\": \"Who\",\n",
    "        \"GPE\": \"Where\",\n",
    "        \"ORG\": \"Which organization\",\n",
    "        \"DATE\": \"When\",\n",
    "        \"TIME\": \"At what time\",\n",
    "        \"NOUN\": \"What\",\n",
    "        \"PROPN\": \"Which\",\n",
    "    }\n",
    "    return question_word_map.get(entity_label, \"What\")\n",
    "\n",
    "\n",
    "def replace_keywords(sentence, keywords, question_word):\n",
    "    words = sentence.split()\n",
    "    replaced = False\n",
    "    for i, word in enumerate(words):\n",
    "        if word.strip(\".,?;!\") in keywords and not replaced:\n",
    "            words[i] = question_word\n",
    "            replaced = True\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def extract_keywords(doc):\n",
    "    # For NER:\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    # For POS tagging (nouns and proper nouns):\n",
    "    nouns = [\n",
    "        (token.text, token.pos_) for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"]\n",
    "    ]\n",
    "    keywords = entities + nouns\n",
    "    if not keywords:\n",
    "        return None\n",
    "    # Select the first keyword and its label\n",
    "    keyword, label = keywords[0]\n",
    "    question_word = get_question_word(label)\n",
    "    return keyword, question_word\n",
    "\n",
    "\n",
    "def create_qa_pairs(df):\n",
    "    qa_pairs = []\n",
    "    for index in tqdm(df.index, total=df.shape[0], desc=\"Creating QA pairs\"):\n",
    "        text = df.loc[index, \"text\"]\n",
    "        # Remove the section headings\n",
    "        cleaned_text = re.sub(r\"==.*?==+\", \"\", text)\n",
    "        sentences = sent_tokenize(cleaned_text)\n",
    "        docs = list(nlp.pipe(sentences))\n",
    "        for sentence, doc in zip(sentences, docs):\n",
    "            keyword_info = extract_keywords(doc)\n",
    "            if keyword_info is not None:\n",
    "                keyword, question_word = keyword_info\n",
    "                question = replace_keywords(sentence, keyword, question_word)\n",
    "                qa_pairs.append((index, question, keyword))\n",
    "    qa_df = pd.DataFrame(qa_pairs, columns=[\"index\", \"question\", \"answer\"])\n",
    "    return qa_df\n",
    "\n",
    "\n",
    "def split_data(qa_pairs, test_size=0.2, random_state=42):\n",
    "    train_data, val_data = train_test_split(\n",
    "        qa_pairs, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "def save_qa_pairs_to_csv(train_qa_pairs, val_qa_pairs, train_csv_path, val_csv_path):\n",
    "    train_df = pd.DataFrame(train_qa_pairs, columns=[\"index\", \"question\", \"answer\"])\n",
    "    val_df = pd.DataFrame(val_qa_pairs, columns=[\"index\", \"question\", \"answer\"])\n",
    "    train_df.to_csv(train_csv_path, index=False)\n",
    "    val_df.to_csv(val_csv_path, index=False)\n",
    "\n",
    "\n",
    "def load_qa_pairs_from_csv(train_csv_path, val_csv_path):\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    val_df = pd.read_csv(val_csv_path)\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(\n",
    "        self, df, qa_df, tokenizer, max_input_length=512, max_output_length=512\n",
    "    ):\n",
    "        self.df = df\n",
    "        self.qa_df = qa_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qa_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index, question, answer = self.qa_df.loc[idx, [\"index\", \"question\", \"answer\"]]\n",
    "        tokenized_input = self.tokenizer(\n",
    "            question,\n",
    "            max_length=self.max_input_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        tokenized_output = self.tokenizer(\n",
    "            answer,\n",
    "            max_length=self.max_output_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": tokenized_input[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": tokenized_input[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": tokenized_output[\"input_ids\"].squeeze(),\n",
    "        }\n",
    "\n",
    "\n",
    "# load train and validation datasets if they exist or create them and save them\n",
    "if os.path.exists(train_csv_path) and os.path.exists(val_csv_path):\n",
    "    train_qa_df, val_qa_df = load_qa_pairs_from_csv(train_csv_path, val_csv_path)\n",
    "else:\n",
    "    qa_df = create_qa_pairs(articles_df)\n",
    "    train_qa_df, val_qa_df = split_data(qa_df)\n",
    "    save_qa_pairs_to_csv(train_qa_df, val_qa_df, train_csv_path, val_csv_path)\n",
    "\n",
    "train_dataset = QADataset(articles_df, train_qa_df, tokenizer)\n",
    "val_dataset = QADataset(articles_df, val_qa_df, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T20:16:05.265849Z",
     "end_time": "2023-04-15T20:21:13.072772Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "train_qa_df.sample(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T20:21:13.072772Z",
     "end_time": "2023-04-15T20:21:13.088397Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "val_qa_df.sample(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-15T20:21:13.088397Z",
     "end_time": "2023-04-15T20:21:13.150897Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 4: Fine-Tuning the Transformer Model\n",
    "\n",
    "This section is dedicated to fine-tuning the transformer-based language model using the PyTorch Lightning framework. We use the Trainer class from the transformers library to train the model on the QADataset, and evaluate its performance on a validation set. We also save the model and tokenizer for later use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    eval_accumulation_steps=1,\n",
    "    prediction_loss_only=True,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "model.save_pretrained(finetuned_model_path)\n",
    "tokenizer.save_pretrained(finetuned_model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 5: Generating Answers to Questions\n",
    "\n",
    "In this section, we load the saved model and tokenizer, and use them to generate answers to a set of predefined questions. We use the pipeline function from the transformers library to generate text from input strings, and print the generated answers to the console."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(finetuned_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path)\n",
    "qa_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "questions = [\n",
    "    \"What is the main theme of the movie Blade Runner?\",\n",
    "    \"Who is the author of the novel Dune?\",\n",
    "    \"What is the name of the spaceship in the Alien movie?\",\n",
    "    \"Who directed the movie The Matrix?\",\n",
    "    \"What is the setting of the Star Wars series?\",\n",
    "    \"What is the similarity between the movie Matrix and Star Wars?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = qa_pipeline(question, max_length=50)[0][\"generated_text\"]\n",
    "    print(f\"Q: {question}\\nA: {answer}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 6: Conclusions\n",
    "\n",
    "This section provides a summary of the results and discusses the potential improvements for the question answering model. We also discuss the applications of the Transformers library for natural language processing tasks in general."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 7: References\n",
    "This section provides links to resources related to the use of the Transformers library and transformer-based models for natural language processing. Links to the official Hugging Face website, examples of transformer-based models, and research papers on transformer models are provided.\n",
    "\n",
    "Hugging Face official website: https://huggingface.co/\n",
    "Transformers: https://huggingface.co/transformers/\n",
    "Transformers examples: https://huggingface.co/examples\n",
    "\"Attention Is All You Need\" paper on Transformers: https://arxiv.org/abs/1706.03762\n",
    "\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" paper: https://arxiv.org/abs/1810.04805\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
