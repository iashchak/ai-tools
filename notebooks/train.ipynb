{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Building a Question Answering Model with Transformers\n",
    "\n",
    "In this notebook, we demonstrate how to create a question answering model using the Transformers library from Hugging Face. We download and preprocess data from Wikipedia using natural language processing techniques and transformer-based models, fine-tune a pre-trained transformer model on the data, and generate answers to a set of predefined questions using the trained model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Install dependencies\n",
    "We install pytorch with built-in CUDA support. If you don't have CUDA, you can install pytorch without CUDA support. You can find more information [here](https://pytorch.org/get-started/locally/).\n",
    "Also we install transformers, pandas, mwparserfromhell, nltk, accelerate and nvidia-ml-py3.\n",
    "We use mwparserfromhell to parse the raw text of the Wikipedia articles, nltk for tokenization, accelerate for multi-GPU training and nvidia-ml-py3 for GPU monitoring."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages)\n",
      "ERROR: Directory '//' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (4.28.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: mwparserfromhell in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (0.6.4)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: accelerate in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: nvidia-ml-py3 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (7.352.0)\n",
      "Requirement already satisfied: datasets in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from accelerate) (2.0.0+cu117)\n",
      "Requirement already satisfied: responses<0.19 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: xxhash in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from torch>=1.4.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from torch>=1.4.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from torch>=1.4.0->accelerate) (3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from jinja2->torch>=1.4.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages (from sympy->torch>=1.4.0->accelerate) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\programdata\\anaconda3\\envs\\ai-tools\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https: // download.pytorch.org/whl/cu117\n",
    "!pip install transformers pandas mwparserfromhell nltk accelerate nvidia-ml-py3 datasets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T20:45:18.265699Z",
     "end_time": "2023-04-21T20:45:21.693384Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download data from Wikipedia\n",
    "We download each article about some specific category from Wikipedia. We use the category \"Science fiction films\" as an example. You can change the category to any other category you want. We also remove the references and external links sections from the articles and wiki markup and save the result to a CSV file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                       title   \n1361  The Tunnel (1933 German-language film)  \\\n1362                     Ureme (film series)   \n1363                  The Voice from the Sky   \n1364                      Welcome to Willits   \n1365                        Yuli (2018 film)   \n\n                                              wiki_text   \n1361  {{Infobox film\\n | name = Totò nella luna\\n | ...  \\\n1362  {{short description|1936 film by Del Lord}}\\n{...   \n1363  {{More citations needed|date=June 2019}}\\n''''...   \n1364  {{Short description|2016 film by Trevor Ryan}}...   \n1365  {{Infobox film\\n| name           = Yuli\\n| ima...   \n\n                                        wiki_text_clean   \n1361  {{Infobox film\\n | name = Totò nella luna\\n | ...  \\\n1362  {{short description|1936 film by Del Lord}}\\n{...   \n1363  {{More citations needed|date=June 2019}}\\n''''...   \n1364  {{Short description|2016 film by Trevor Ryan}}...   \n1365  {{Infobox film\\n| name           = Yuli\\n| ima...   \n\n                                                   text  \n1361  Totò nella luna (internationally released as T...  \n1362  Trapped by Television is a 1936 American comed...  \n1363  Ureme (also spelled ulemae, uroemae or wuroema...  \n1364  Welcome to Willits, also known as Alien Hunter...  \n1365  Yuli is a 2018 Peruvian science fiction action...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>wiki_text</th>\n      <th>wiki_text_clean</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1361</th>\n      <td>The Tunnel (1933 German-language film)</td>\n      <td>{{Infobox film\\n | name = Totò nella luna\\n | ...</td>\n      <td>{{Infobox film\\n | name = Totò nella luna\\n | ...</td>\n      <td>Totò nella luna (internationally released as T...</td>\n    </tr>\n    <tr>\n      <th>1362</th>\n      <td>Ureme (film series)</td>\n      <td>{{short description|1936 film by Del Lord}}\\n{...</td>\n      <td>{{short description|1936 film by Del Lord}}\\n{...</td>\n      <td>Trapped by Television is a 1936 American comed...</td>\n    </tr>\n    <tr>\n      <th>1363</th>\n      <td>The Voice from the Sky</td>\n      <td>{{More citations needed|date=June 2019}}\\n''''...</td>\n      <td>{{More citations needed|date=June 2019}}\\n''''...</td>\n      <td>Ureme (also spelled ulemae, uroemae or wuroema...</td>\n    </tr>\n    <tr>\n      <th>1364</th>\n      <td>Welcome to Willits</td>\n      <td>{{Short description|2016 film by Trevor Ryan}}...</td>\n      <td>{{Short description|2016 film by Trevor Ryan}}...</td>\n      <td>Welcome to Willits, also known as Alien Hunter...</td>\n    </tr>\n    <tr>\n      <th>1365</th>\n      <td>Yuli (2018 film)</td>\n      <td>{{Infobox film\\n| name           = Yuli\\n| ima...</td>\n      <td>{{Infobox film\\n| name           = Yuli\\n| ima...</td>\n      <td>Yuli is a 2018 Peruvian science fiction action...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "import mwparserfromhell\n",
    "import pandas as pd\n",
    "import requests\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Set the category you want to download\n",
    "csv_filename = \"../data/articles.csv\"\n",
    "articles_category = 'Science_fiction_films'\n",
    "\n",
    "# Get the list of subcategories and articles\n",
    "def get_category_members(category: str, member_type: str) -> List[str]:\n",
    "    base_url = 'https://en.wikipedia.org/w/api.php'\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'list': 'categorymembers',\n",
    "        'cmtitle': category,\n",
    "        'cmtype': member_type,\n",
    "        'format': 'json',\n",
    "        'cmlimit': 500\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "    for item in data['query']['categorymembers']:\n",
    "        yield item['title']\n",
    "\n",
    "# Get the raw text of the articles\n",
    "def get_article_texts(articles: List[str]) -> Dict[str, str]:\n",
    "    base_url = 'https://en.wikipedia.org/w/api.php'\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'prop': 'revisions',\n",
    "        'rvprop': 'content',\n",
    "        'format': 'json',\n",
    "        'titles': '|'.join(articles)\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "    for page in data['query']['pages'].values():\n",
    "        yield page['revisions'][0]['*']\n",
    "\n",
    "\n",
    "def parse_and_remove_references_and_external_links(text: str) -> mwparserfromhell.wikicode.Wikicode:\n",
    "    parsed_text = mwparserfromhell.parse(text)\n",
    "    for section in parsed_text.get_sections(levels=[2]):\n",
    "        if section.filter_headings()[0].title.strip().lower() in [\"references\", \"external links\", \"see also\", \"awards and nominations\", \"filmography\"]:\n",
    "            parsed_text.remove(section)\n",
    "    return parsed_text\n",
    "\n",
    "# Download the articles by chunks of 50 articles\n",
    "def download_articles(category: str):\n",
    "    # Get the list of subcategories and articles\n",
    "    subcategories = list(get_category_members(f'Category:{category}', 'subcat'))\n",
    "    all_articles = []\n",
    "\n",
    "    # Download the articles from the subcategories\n",
    "    for subcategory in tqdm(subcategories, desc=\"Downloading subcategories\"):\n",
    "        articles = list(get_category_members(subcategory, 'page'))\n",
    "        all_articles.extend(articles)\n",
    "\n",
    "    # Download the articles by chunks of 50 articles\n",
    "    for i in tqdm(range(0, len(all_articles), 50), desc=\"Downloading articles\"):\n",
    "        batch = all_articles[i:i + 50]\n",
    "        texts = dict(zip(batch, get_article_texts(batch)))\n",
    "        for title, wiki_text in texts.items():\n",
    "            wiki_text_clean = parse_and_remove_references_and_external_links(wiki_text)\n",
    "            text = wiki_text_clean.strip_code().strip()\n",
    "            # Remove the references and external links sections\n",
    "            yield {'title': title, 'wiki_text': wiki_text, 'wiki_text_clean': wiki_text_clean, 'text': text}\n",
    "\n",
    "# Download the articles and save them to a CSV file\n",
    "if not os.path.exists(csv_filename):\n",
    "    articles_df = pd.DataFrame(download_articles(articles_category))\n",
    "    articles_df = articles_df.dropna(subset=['text'])\n",
    "    articles_df.to_csv(csv_filename, index=False)\n",
    "else:\n",
    "    # If the CSV file already exists, we just load it\n",
    "    articles_df = pd.read_csv(csv_filename)\n",
    "    articles_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "articles_df.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T20:45:21.693384Z",
     "end_time": "2023-04-21T20:45:24.182411Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate question-answer pairs\n",
    "We use T5 to generate question-answer pairs from the Wikipedia articles. We use the T5-small model and the T5 tokenizer.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               question   \n1819  What is the full name of the actor who plays B...  \\\n1820         What was the first film made by the Ryans?   \n1821  What is the full name of the director of Welco...   \n1822  What is the name of the character that Yuli de...   \n1823  What was the name of the film that took place ...   \n\n                                 answer  \n1819                          Bill Sage  \n1820  Welcome to Willits: After Sundown  \n1821                        Trevor Ryan  \n1822            Jorge ‘Coco’ Gutiérrez.  \n1823                    Marisela Puicón  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1819</th>\n      <td>What is the full name of the actor who plays B...</td>\n      <td>Bill Sage</td>\n    </tr>\n    <tr>\n      <th>1820</th>\n      <td>What was the first film made by the Ryans?</td>\n      <td>Welcome to Willits: After Sundown</td>\n    </tr>\n    <tr>\n      <th>1821</th>\n      <td>What is the full name of the director of Welco...</td>\n      <td>Trevor Ryan</td>\n    </tr>\n    <tr>\n      <th>1822</th>\n      <td>What is the name of the character that Yuli de...</td>\n      <td>Jorge ‘Coco’ Gutiérrez.</td>\n    </tr>\n    <tr>\n      <th>1823</th>\n      <td>What was the name of the film that took place ...</td>\n      <td>Marisela Puicón</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# Prepare the model for distributed training\n",
    "accelerator = Accelerator()\n",
    "model, tokenizer = accelerator.prepare(model, tokenizer)\n",
    "device = accelerator.device\n",
    "\n",
    "# Load the dataset\n",
    "articles_df = pd.read_csv(csv_filename)\n",
    "csv_questions_filename = \"../data/questions.csv\"\n",
    "\n",
    "\n",
    "example_context = \"Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence, made significant contributions to the field of machine learning and AI. He coined the term \\\"machine learning\\\" in 1959 and also used the synonym \\\"self-teaching computers\\\" during that time period. His work laid the foundation for the development and growth of machine learning as a crucial aspect of artificial intelligence.\"\n",
    "example_section = \"The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.\"\n",
    "example_question = \"What was the contribution of Arthur Samuel in the field of machine learning and AI?\"\n",
    "example_answer = \"\"\"\n",
    "    Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence, made significant contributions to the field of machine learning and AI. He coined the term \"machine learning\" in 1959 and also used the synonym \"self-teaching computers\" during that time period. His work laid the foundation for the development and growth of machine learning as a crucial aspect of artificial intelligence.\n",
    "\"\"\"\n",
    "\n",
    "example_question_prompt = f\"\"\"\n",
    "    Context: {example_context}\n",
    "    Section: {example_section}\n",
    "    Question: {example_question}\n",
    "\"\"\"\n",
    "\n",
    "example_answer_prompt = f\"\"\"\n",
    "    Context: {example_context}\n",
    "    Section: {example_section}\n",
    "    Question: {example_question}\n",
    "    Answer: {example_answer.strip()}\n",
    "\"\"\"\n",
    "\n",
    "def make_query_to_model(query) -> str:\n",
    "    input_ids = tokenizer(query, return_tensors=\"pt\", max_length=512, truncation=True).input_ids.to(device)\n",
    "    outputs = model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True, temperature=0.7)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def is_valid_question(question, existing_questions):\n",
    "    return question.endswith(\"?\") and existing_questions[existing_questions[\"question\"] == question].empty\n",
    "\n",
    "\n",
    "def is_valid_answer(answer, existing_answers):\n",
    "    first_word = answer.split(\" \")[0]\n",
    "    return first_word not in [\"Yes\", \"No\"] and len(answer.split(\" \")) > 1 and existing_answers[existing_answers[\"answer\"] == answer].empty\n",
    "\n",
    "def generate_and_evaluate_questions_and_answers(row):\n",
    "    text_df = pd.DataFrame(columns=[\"question\", \"answer\"])\n",
    "    sections = mwparserfromhell.parse(row[\"wiki_text_clean\"]).get_sections(levels=[2])\n",
    "    article_summary = make_query_to_model(f\"Summary: {row['text']}\")\n",
    "    for section in sections:\n",
    "        section_text = section.strip_code().strip()\n",
    "        question_prompt = f\"\"\"\n",
    "            {example_question_prompt}\n",
    "\n",
    "            Context: {article_summary}\n",
    "            Section: {section_text}\n",
    "            Question:\n",
    "        \"\"\"\n",
    "        question = make_query_to_model(question_prompt)\n",
    "\n",
    "        if not is_valid_question(question, text_df):\n",
    "            continue\n",
    "\n",
    "        answer_prompt = f\"\"\"\n",
    "            {example_answer_prompt}\n",
    "\n",
    "            Context: {article_summary}\n",
    "            Section: {section_text}\n",
    "            Question: {question}\n",
    "            Answer:\n",
    "        \"\"\"\n",
    "        answer = make_query_to_model(answer_prompt)\n",
    "\n",
    "        if not is_valid_answer(answer, text_df):\n",
    "            continue\n",
    "\n",
    "        text_df = pd.concat([text_df, pd.DataFrame({\"question\": [question], \"answer\": [answer]})])\n",
    "\n",
    "    return text_df\n",
    "\n",
    "\n",
    "def load_or_generate_question_answers(csv_questions_filename, articles_df):\n",
    "    if os.path.exists(csv_questions_filename):\n",
    "        return pd.read_csv(csv_questions_filename)\n",
    "    else:\n",
    "        question_answers_df = generate_all_questions_and_answers(articles_df)\n",
    "        question_answers_df = question_answers_df.drop_duplicates(keep=False, subset='question')\n",
    "        question_answers_df = question_answers_df.drop_duplicates(keep=False, subset='answer')\n",
    "        question_answers_df.to_csv(csv_questions_filename, index=False)\n",
    "        return question_answers_df\n",
    "\n",
    "\n",
    "def generate_all_questions_and_answers(articles_df):\n",
    "    question_answers_df = pd.DataFrame(columns=[\"question\", \"answer\"])\n",
    "\n",
    "    with tqdm(articles_df.iterrows(), desc=\"Generating questions and answers\", total=len(articles_df)) as pbar:\n",
    "        for index, row in pbar:\n",
    "            created_question_answers = generate_and_evaluate_questions_and_answers(row)\n",
    "            question_answers_df = pd.concat([question_answers_df, created_question_answers])\n",
    "\n",
    "            if not question_answers_df.empty:\n",
    "                pbar.set_postfix_str(\"Last generated question-answer pair: \" + question_answers_df.iloc[-1][\"question\"] + \" - \" + question_answers_df.iloc[-1][\"answer\"])\n",
    "\n",
    "    return question_answers_df\n",
    "\n",
    "question_answers_df = load_or_generate_question_answers(csv_questions_filename, articles_df)\n",
    "question_answers_df.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T21:57:10.628235Z",
     "end_time": "2023-04-21T21:57:13.035448Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
