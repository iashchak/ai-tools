{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Building a Question Answering Model with Transformers\n",
    "\n",
    "In this notebook, we demonstrate how to create a question answering model using the Transformers library from Hugging Face. We download and preprocess data from Wikipedia using natural language processing techniques and transformer-based models, fine-tune a pre-trained transformer model on the data, and generate answers to a set of predefined questions using the trained model.\n",
    "\n",
    "The notebook is divided into the following sections:\n",
    "\n",
    "1. Introduction ([link](#Section-1:-Introduction))\n",
    "2. Loading and Preprocessing Data ([link](#Section-2:-Loading-and-Preprocessing-Data))\n",
    "3. Creating the QADataset Class ([link](#Section-3:-Creating-the-QADataset-Class))\n",
    "4. Fine-Tuning the Transformer Model ([link](#Section-4:-Fine-Tuning-the-Transformer-Model))\n",
    "5. Generating Answers to Questions ([link](#Section-5:-Generating-Answers-to-Questions))\n",
    "6. Conclusions ([link](#Section-6:-Conclusions))\n",
    "7. References ([link](#Section-7:-References))\n",
    "\n",
    "The `QADataset` class is defined in Section 3, which is the core of this notebook. It loads the preprocessed data and creates PyTorch datasets for training and validation. The data loading and preprocessing functions are defined in Section 2. The transformer model is fine-tuned on the dataset in Section 4 using the PyTorch Lightning framework, and the trained model is used to generate answers to predefined questions in Section 5. Finally, Section 6 provides a summary of the results and discusses the potential improvements for the question answering model, while Section 7 provides links to resources related to the use of the Transformers library and transformer-based models for natural language processing.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 1: Introduction\n",
    "\n",
    "In this notebook, we will demonstrate the process of creating a question answering model using the Transformers library from Hugging Face. We will use natural language processing techniques and transformer-based models to achieve the best results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "articles_file_path = \"../data/articles.csv\"\n",
    "train_dataset_path = \"../datasets/train_dataset.pkl\"\n",
    "val_dataset_path = \"../datasets/val_dataset.pkl\"\n",
    "tokenizer_for_sentence_splitting = 'punkt'\n",
    "base_model_name = \"databricks/dolly-v2-3b\"\n",
    "model_path = \"./finetuned_model\"\n",
    "articles_category = 'Science_fiction_films'\n",
    "\n",
    "batch_size = 1\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T19:24:37.529818Z",
     "end_time": "2023-04-14T19:24:38.411343Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 2: Loading and Preprocessing Data\n",
    "\n",
    "This section is dedicated to downloading and preprocessing data for the question answering model. We will use the Wikipedia API to download articles on a given topic, split them into sentences, and create a dataset of question-answer pairs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                         title  \\\n0               Alfonso Cuarón   \n1                    Brad Bird   \n2         Brian Patrick Butler   \n3                 Charles Band   \n4  Chris Carter (screenwriter)   \n\n                                                text  \n0  {{short description|Mexican filmmaker}}\\n{{Red...  \n1  {{Short description|American film director, sc...  \n2  {{Short description|American actor and filmmak...  \n3  {{short description|American film director}}\\n...  \n4  {{Short description|American television and fi...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Alfonso Cuarón</td>\n      <td>{{short description|Mexican filmmaker}}\\n{{Red...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Brad Bird</td>\n      <td>{{Short description|American film director, sc...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Brian Patrick Butler</td>\n      <td>{{Short description|American actor and filmmak...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Charles Band</td>\n      <td>{{short description|American film director}}\\n...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Chris Carter (screenwriter)</td>\n      <td>{{Short description|American television and fi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(articles_file_path):\n",
    "    def get_category_members(category: str, member_type: str) -> List[str]:\n",
    "        base_url = 'https://en.wikipedia.org/w/api.php'\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'list': 'categorymembers',\n",
    "            'cmtitle': category,\n",
    "            'cmtype': member_type,\n",
    "            'format': 'json',\n",
    "            'cmlimit': 500\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = json.loads(response.text)\n",
    "        members = [item['title'] for item in data['query']['categorymembers']]\n",
    "        return members\n",
    "\n",
    "\n",
    "    def get_article_texts(articles: List[str]) -> Dict[str, str]:\n",
    "        base_url = 'https://en.wikipedia.org/w/api.php'\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'prop': 'revisions',\n",
    "            'rvprop': 'content',\n",
    "            'format': 'json',\n",
    "            'titles': '|'.join(articles)\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = json.loads(response.text)\n",
    "        texts = {}\n",
    "        for page in data['query']['pages'].values():\n",
    "            texts[page['title']] = page['revisions'][0]['*']\n",
    "        return texts\n",
    "\n",
    "\n",
    "    def download_articles(category: str) -> pd.DataFrame:\n",
    "        subcategories = get_category_members(f'Category:{category}', 'subcat')\n",
    "        all_articles = []\n",
    "\n",
    "        for subcategory in tqdm(subcategories, desc=\"Downloading subcategories\"):\n",
    "            articles = get_category_members(subcategory, 'page')\n",
    "            all_articles.extend(articles)\n",
    "\n",
    "        article_data = []\n",
    "        for i in tqdm(range(0, len(all_articles), 50), desc=\"Downloading articles\"):\n",
    "            batch = all_articles[i:i + 50]\n",
    "            texts = get_article_texts(batch)\n",
    "            for title, text in texts.items():\n",
    "                article_data.append({'title': title, 'text': text})\n",
    "\n",
    "        return pd.DataFrame(article_data)\n",
    "\n",
    "\n",
    "    articles_df = download_articles(articles_category)\n",
    "    articles_df.to_csv(articles_file_path, index=False)\n",
    "else:\n",
    "    articles_df = pd.read_csv(articles_file_path)\n",
    "\n",
    "articles_df.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T19:24:38.411343Z",
     "end_time": "2023-04-14T19:24:38.872329Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 3: Creating the QADataset Class\n",
    "\n",
    "In this section, we define the QADataset class, which is used to load the preprocessed data and create PyTorch datasets for training and validation. We also define helper functions for keyword extraction and sentence replacement, which are used to create question-answer pairs from sentences."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python -m nltk.downloader stopwords"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "\n",
    "nltk.download(tokenizer_for_sentence_splitting)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, df, qa_indices, tokenizer, max_input_length=512, max_output_length=512):\n",
    "        self.df = df\n",
    "        self.qa_indices = qa_indices\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qa_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index, question, answer = self.qa_indices[idx]\n",
    "        tokenized_input = self.tokenizer(question, max_length=self.max_input_length, padding=\"max_length\",\n",
    "                                         truncation=True, return_tensors=\"pt\")\n",
    "        tokenized_output = self.tokenizer(answer, max_length=self.max_output_length, padding=\"max_length\",\n",
    "                                          truncation=True, return_tensors=\"pt\")\n",
    "        return {\"input_ids\": tokenized_input[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": tokenized_input[\"attention_mask\"].squeeze(),\n",
    "                \"labels\": tokenized_output[\"input_ids\"].squeeze()}\n",
    "\n",
    "\n",
    "def replace_keywords(sentence, keywords):\n",
    "    words = sentence.split()\n",
    "    replaced = False\n",
    "    for i, word in enumerate(words):\n",
    "        if word.strip(\".,?;!\") in keywords and not replaced:\n",
    "            words[i] = \"____\"\n",
    "            replaced = True\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def extract_keywords(sentence, num_keywords=1):\n",
    "    words = sentence.split()\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    words = sorted(words, key=lambda x: len(x), reverse=True)\n",
    "    return words[:num_keywords]\n",
    "\n",
    "\n",
    "def create_qa_pairs(df):\n",
    "    qa_pairs = []\n",
    "    for index in tqdm(df.index, total=df.shape[0], desc=\"Creating QA pairs\"):\n",
    "        text = df.loc[index, \"text\"]\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            keywords = extract_keywords(sentence)\n",
    "            question = replace_keywords(sentence, keywords)\n",
    "            if \"____\" in question:\n",
    "                qa_pairs.append((index, question, keywords[0]))\n",
    "    return qa_pairs\n",
    "\n",
    "def split_data(qa_pairs, test_size=0.2, random_state=42):\n",
    "    train_data, val_data = train_test_split(qa_pairs, test_size=test_size, random_state=random_state)\n",
    "    return train_data, val_data\n",
    "\n",
    "# load train and validation datasets if they exist or create them and save them\n",
    "if os.path.exists(train_dataset_path) and os.path.exists(val_dataset_path):\n",
    "    with open(train_dataset_path, \"rb\") as train_file:\n",
    "        train_dataset = pickle.load(train_file)\n",
    "\n",
    "    with open(val_dataset_path, \"rb\") as val_file:\n",
    "        val_dataset = pickle.load(val_file)\n",
    "else:\n",
    "    qa_pairs = create_qa_pairs(articles_df)\n",
    "    train_qa_pairs, val_qa_pairs = split_data(qa_pairs)\n",
    "\n",
    "    # create train and validation datasets\n",
    "    train_dataset = QADataset(articles_df, train_qa_pairs, tokenizer)\n",
    "    val_dataset = QADataset(articles_df, val_qa_pairs, tokenizer)\n",
    "\n",
    "    with open(train_dataset_path, \"wb\") as train_file:\n",
    "        pickle.dump(train_dataset, train_file)\n",
    "\n",
    "    with open(val_dataset_path, \"wb\") as val_file:\n",
    "        pickle.dump(val_dataset, val_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 4: Fine-Tuning the Transformer Model\n",
    "\n",
    "This section is dedicated to fine-tuning the transformer-based language model using the PyTorch Lightning framework. We use the Trainer class from the transformers library to train the model on the QADataset, and evaluate its performance on a validation set. We also save the model and tokenizer for later use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    eval_accumulation_steps=1,\n",
    "    prediction_loss_only=True,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 5: Generating Answers to Questions\n",
    "\n",
    "In this section, we load the saved model and tokenizer, and use them to generate answers to a set of predefined questions. We use the pipeline function from the transformers library to generate text from input strings, and print the generated answers to the console."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "qa_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "questions = [\n",
    "    \"What is the main theme of the movie Blade Runner?\",\n",
    "    \"Who is the author of the novel Dune?\",\n",
    "    \"What is the name of the spaceship in the Alien movie?\",\n",
    "    \"Who directed the movie The Matrix?\",\n",
    "    \"What is the setting of the Star Wars series?\",\n",
    "    \"What is the similarity between the movie Matrix and Star Wars?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = qa_pipeline(question, max_length=50)[0][\"generated_text\"]\n",
    "    print(f\"Q: {question}\\nA: {answer}\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 6: Conclusions\n",
    "\n",
    "This section provides a summary of the results and discusses the potential improvements for the question answering model. We also discuss the applications of the Transformers library for natural language processing tasks in general."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Section 7: References\n",
    "This section provides links to resources related to the use of the Transformers library and transformer-based models for natural language processing. Links to the official Hugging Face website, examples of transformer-based models, and research papers on transformer models are provided.\n",
    "\n",
    "Hugging Face official website: https://huggingface.co/\n",
    "Transformers: https://huggingface.co/transformers/\n",
    "Transformers examples: https://huggingface.co/examples\n",
    "\"Attention Is All You Need\" paper on Transformers: https://arxiv.org/abs/1706.03762\n",
    "\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" paper: https://arxiv.org/abs/1810.04805"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
