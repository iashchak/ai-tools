{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning model for interview question answering\n",
    "\n",
    "This notebook is a test for fine-tuning a model for interview question answering. It contains several sources for interview questions and answers. The goal is to fine-tune a model to answer interview questions.\n",
    "\n",
    "## Requirements:\n",
    "This code written using Local Machine with GPU Nvidia GTX 1660 Ti 6GB. But you can use Google Colab for free.\n",
    "\n",
    " - Anaconda\n",
    " - Nvidia CUDA Toolkit 11.1\n",
    " - Jupyter Notebook\n",
    "\n",
    "## Links to the sources:\n",
    "\n",
    "Sources for interview questions and answers:\n",
    "- [https://github.com/sudheerj/angular-interview-questions](https://github.com/sudheerj/angular-interview-questions)\n",
    "- [https://github.com/sudheerj/javascript-interview-questions](https://github.com/sudheerj/javascript-interview-questions)\n",
    "- [https://github.com/sudheerj/reactjs-interview-questions](https://github.com/sudheerj/reactjs-interview-questions)\n",
    "- [https://github.com/aershov24/full-stack-interview-questions](https://github.com/aershov24/full-stack-interview-questions)\n",
    "\n",
    "## Model for paraphrasing:\n",
    "Also we use the following model for paraphrasing:\n",
    "- [https://huggingface.co/google/flan-t5-small](https://huggingface.co/google/flan-t5-small)\n",
    "\n",
    "## Model for fine-tuning:\n",
    "And the following model for fine-tuning:\n",
    "- [https://huggingface.co/databricks/dolly-v2-3b](https://huggingface.co/databricks/dolly-v2-3b)\n",
    "- [https://huggingface.co/google/flan-t5-small](https://huggingface.co/google/flan-t5-small)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dataset preparation\n",
    "!pip install datasets markdown beautifulsoup4\n",
    "# GPU support\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
    "!pip install nvidia-ml-py3\n",
    "# Data processing\n",
    "!pip install numpy pandas nltk evaluate\n",
    "# Training software\n",
    "!pip install transformers accelerate\n",
    "# Training visualization\n",
    "!pip install tensorboard"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T21:45:44.377452Z",
     "end_time": "2023-04-24T21:45:59.728181Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create question and answer dataset from interviewing questions and answers\n",
    "\n",
    "This code is used to create a dataset from the sources above. It parses the markdown files and creates a dataset in JSON format.\n",
    "Most of questions are the h3 or h4 tags and answers are the content after the question tag."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import markdown\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "sudheerj_paths = [\n",
    "    os.path.join('..', 'data', 'interview', 'sudheerj', 'angular-interview-questions.md'),\n",
    "    os.path.join('..', 'data', 'interview', 'sudheerj', 'javascript-interview-questions.md'),\n",
    "    os.path.join('..', 'data', 'interview', 'sudheerj', 'reactjs-interview-questions.md'),\n",
    "]\n",
    "\n",
    "aershov24_paths = [\n",
    "    os.path.join('..', 'data', 'interview', 'aershov24', 'full-stack-interview-questions.md')\n",
    "]\n",
    "\n",
    "questions_answers_path = os.path.join('..', 'datasets', 'interview', 'interview_questions.json')\n",
    "augmented_questions_answers_path = os.path.join('..', 'datasets', 'interview', 'interview_questions_augmented.json')\n",
    "\n",
    "\n",
    "# Extract questions and answers from markdown files\n",
    "def parse_files(md_files, question_selector):\n",
    "    data = pd.DataFrame()\n",
    "    for md_file in md_files:\n",
    "        with open(md_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            md_content = file.read()\n",
    "            html_content = markdown.markdown(md_content)\n",
    "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "            questions = soup.select(question_selector)\n",
    "\n",
    "            for question in questions:\n",
    "                answer_elements = []\n",
    "                sibling = question.find_next_sibling()\n",
    "\n",
    "                while sibling and sibling.name != question_selector:\n",
    "                    answer_elements.append(str(sibling))\n",
    "                    sibling = sibling.find_next_sibling()\n",
    "\n",
    "                answer = BeautifulSoup(''.join(answer_elements).strip())\n",
    "\n",
    "                data = pd.concat([data, pd.DataFrame({\n",
    "                    'question': [question.text.strip()],\n",
    "                    'answer': [answer.text.strip()]\n",
    "                })], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "sudheerj_df = parse_files(sudheerj_paths, \"h3\")\n",
    "aershov24_df = parse_files(aershov24_paths, \"h4\")\n",
    "\n",
    "combine_df = pd.concat([sudheerj_df, aershov24_df], ignore_index=True)\n",
    "combine_df.to_json(questions_answers_path, orient='records')\n",
    "combine_df.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T21:45:59.735182Z",
     "end_time": "2023-04-24T21:46:02.931053Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Augment dataset with paraphrasing\n",
    "\n",
    "Augment is a process of creating new data from existing data. In this case we use the model for paraphrasing to create new questions and answers.\n",
    "It is done with following steps:\n",
    "1. Take precreated dataset\n",
    "2. Go through each question and answer and replace some words with synonyms\n",
    "3. Save it\n",
    "4.\n",
    "Yep, it is that simple. But it is enough to create a dataset with 4 times more data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-24T21:46:02.939050Z",
     "end_time": "2023-04-24T21:46:04.113045Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "combine_df = pd.read_json(questions_answers_path, orient='records')\n",
    "\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "\n",
    "def replace_with_synonym(sentence, max_replacements=2):\n",
    "    words = sentence.split()\n",
    "    replacements = 0\n",
    "    new_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if replacements < max_replacements and random.random() < 0.5:\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms:\n",
    "                word = random.choice(synonyms)\n",
    "                replacements += 1\n",
    "        new_words.append(word)\n",
    "\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "\n",
    "def generate_augmented_row(row):\n",
    "    question = row['question']\n",
    "    answer = row['answer']\n",
    "\n",
    "    augmented_question = replace_with_synonym(question)\n",
    "    augmented_answer = replace_with_synonym(answer)\n",
    "    rand = random.random()\n",
    "    if rand < 0.25:\n",
    "        return augmented_question, answer\n",
    "    elif rand > 0.25:\n",
    "        return question, augmented_answer\n",
    "    else:\n",
    "        return augmented_question, augmented_answer\n",
    "\n",
    "\n",
    "def generate_augmented_rows(df):\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc='Augmenting'):\n",
    "        yield generate_augmented_row(row)\n",
    "\n",
    "\n",
    "pd.DataFrame(generate_augmented_rows(df=combine_df), columns=['question', 'answer'])\n",
    "augmented_df = pd.concat(\n",
    "    [\n",
    "        combine_df,\n",
    "        pd.DataFrame(generate_augmented_rows(df=combine_df), columns=['question', 'answer']),\n",
    "        pd.DataFrame(generate_augmented_rows(df=combine_df), columns=['question', 'answer']),\n",
    "    ],\n",
    "    ignore_index=True\n",
    ").dropna().drop_duplicates(subset=['question', 'answer'], keep='first', ignore_index=True)\n",
    "\n",
    "augmented_df.to_json(augmented_questions_answers_path, orient='records')\n",
    "\n",
    "print(f'Original dataset size: {len(combine_df)}')\n",
    "print(f'Augmented dataset size: {len(augmented_df)}')\n",
    "augmented_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train a T5 model to generate answers from questions\n",
    "\n",
    "This code is used to train a T5 model to generate answers from questions. The model is trained on the augmented dataset above.\n",
    "Steps to train the model:\n",
    "1. Split the dataset into train and validation sets\n",
    "2. Create a T5 model and tokenizer\n",
    "3. Tokenize the questions and answers\n",
    "4. Create a `Seq2SeqTrainer` to train the model\n",
    "5. Train the model for 4 epochs\n",
    "6. Save the model\n",
    "The training is done on a single GPU and took only 44 minutes to train the model on GTX 1660 Ti."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config, Seq2SeqTrainingArguments, \\\n",
    "    DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# I set 4 epochs because i can\n",
    "num_train_epochs = 4\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Splitting the dataset into train and validation sets\n",
    "[train_ds, test_df] = load_dataset('json',\n",
    "                                   data_files=augmented_questions_answers_path,\n",
    "                                   split=['train[:90%]', 'train[-10%:]']\n",
    "                                   )\n",
    "# Creating the tokenizer and model\n",
    "model_name = \"google/flan-t5-small\"\n",
    "config = T5Config.from_pretrained(model_name)\n",
    "\n",
    "# Creating the model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, config=config)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Putting the model on the GPU to be FAAAAAST\n",
    "model, tokenizer = accelerator.prepare(model, tokenizer)\n",
    "\n",
    "# Tokenizing the questions and answers\n",
    "def preprocess_data(batch):\n",
    "    input_texts = [\"question: \" + example for example in batch[\"question\"]]\n",
    "    target_texts = [\"answer: \" + example for example in batch[\"answer\"]]\n",
    "    input_tokenized = tokenizer(input_texts, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"np\")\n",
    "    target_tokenized = tokenizer(target_texts, truncation=True, max_length=512, padding=\"max_length\",\n",
    "                                 return_tensors=\"np\")\n",
    "    input_tokenized, target_tokenized = accelerator.prepare(input_tokenized, target_tokenized)\n",
    "    return {\"input_ids\": input_tokenized.input_ids, \"attention_mask\": input_tokenized.attention_mask,\n",
    "            \"labels\": target_tokenized.input_ids}\n",
    "\n",
    "\n",
    "tokenized_train_dataset = train_ds.map(preprocess_data, batched=True)\n",
    "tokenized_test_dataset = test_df.map(preprocess_data, batched=True)\n",
    "\n",
    "# Data Collator - it is used to pad the data to the same length\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Creating the trainer\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"logs\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    optim='adamw_torch',\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Computing the metrics since it is hype thing to do\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\", 'rouge', 'bertscore', 'bleu', 'meteor', 'sacrebleu', 'accuracy'),\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Creating trainer and train the model! Rock!!!\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Saving the model for later use\n",
    "trainer.save_model(\"output/model\")\n",
    "tokenizer.save_pretrained(\"output/model\")\n",
    "\n",
    "# Displaying fancy digits\n",
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T20:29:39.518311Z",
     "end_time": "2023-04-24T20:29:52.758336Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference\n",
    "\n",
    "Let's compare the difference between the source model and the fine-tuned model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Creating the tokenizer and model\n",
    "model_name = \"google/flan-t5-small\"\n",
    "config = T5Config.from_pretrained(model_name)\n",
    "\n",
    "# Creating the model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, config=config)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Putting the model on the GPU to be FAAAAAST\n",
    "model, tokenizer = accelerator.prepare(model, tokenizer)\n",
    "\n",
    "# Inference on the source and fine-tuned models\n",
    "def generate_answer(question, model, tokenizer, max_length=128):\n",
    "    model.eval()\n",
    "    input_text = \"question: \" + question\n",
    "    input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    output_tokens = model.generate(input_tokens, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0,\n",
    "                                   early_stopping=True, num_beams=4, num_return_sequences=4)\n",
    "    answer = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    answer = answer.replace(\"answer: \", \"\")\n",
    "    return answer\n",
    "\n",
    "# Put whatever question you want\n",
    "question = \"What is the framework?\"\n",
    "\n",
    "# Loading the source model\n",
    "source_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
    "source_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# Generating the answer from the source model\n",
    "source_answer = generate_answer(question, source_model, source_tokenizer)\n",
    "\n",
    "# Loading the fine-tuned model\n",
    "tuned_model = T5ForConditionalGeneration.from_pretrained(\"output/model\")\n",
    "tuned_tokenizer = T5Tokenizer.from_pretrained(\"output/model\")\n",
    "\n",
    "# Generating the answer from the fine-tuned model\n",
    "tuned_answer = generate_answer(question, tuned_model, tuned_tokenizer)\n",
    "\n",
    "# Behold the difference between the source and fine-tuned models!!!\n",
    "print(f\"Question: {question}\")  # Question: What is the framework?\n",
    "print(f\"Source answer: {source_answer}\")  # a framework\n",
    "print(f\"Tuned answer: {tuned_answer}\")  # The framework is a set of tools that can be used to build and maintain the application. These tools are used to build applications, such as JavaScript, HTML, CSS, etc. In this framework, you can create your own custom code for your application. For example, let's take a look at the main features of the framework,javascript const template = []; console.log(message)  console.log(message);"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T22:16:56.772818Z",
     "end_time": "2023-04-24T22:17:03.120319Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Model able to answer questions with more deep understanding after fine-tuning on the augmented dataset. However, the model is still not perfect.\n",
    "\n",
    "To improve the model, you can try to:\n",
    "1. Increase the number of epochs\n",
    "2. Increase the number of training examples\n",
    "3. Use a larger model\n",
    "4. Use a different optimizer\n",
    "5. Use a different learning rate\n",
    "6. Use a different scheduler\n",
    "7. Use a different data augmentation technique\n",
    "8. Use a larger dataset\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
