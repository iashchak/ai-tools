{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning model for interview question answering\n",
    "\n",
    "This notebook is a test for fine-tuning a model for interview question answering. It contains several sources for interview questions and answers. The goal is to fine-tune a model to answer interview questions.\n",
    "\n",
    "## Requirements:\n",
    "This code written using Local Machine with GPU Nvidia GTX 1660 Ti 6GB. But you can use Google Colab for free.\n",
    "\n",
    " - Anaconda\n",
    " - Nvidia CUDA Toolkit 11.1\n",
    " - Jupyter Notebook\n",
    "\n",
    "## Links to the sources:\n",
    "\n",
    "Sources for interview questions and answers:\n",
    "- [https://github.com/sudheerj/angular-interview-questions](https://github.com/sudheerj/angular-interview-questions)\n",
    "- [https://github.com/sudheerj/javascript-interview-questions](https://github.com/sudheerj/javascript-interview-questions)\n",
    "- [https://github.com/sudheerj/reactjs-interview-questions](https://github.com/sudheerj/reactjs-interview-questions)\n",
    "- [https://github.com/aershov24/full-stack-interview-questions](https://github.com/aershov24/full-stack-interview-questions)\n",
    "\n",
    "## Model for paraphrasing:\n",
    "Also we use the following model for paraphrasing:\n",
    "- [https://huggingface.co/google/flan-t5-small](https://huggingface.co/google/flan-t5-small)\n",
    "\n",
    "## Model for fine-tuning:\n",
    "And the following model for fine-tuning:\n",
    "- [https://huggingface.co/databricks/dolly-v2-3b](https://huggingface.co/databricks/dolly-v2-3b)\n",
    "- [https://huggingface.co/google/flan-t5-small](https://huggingface.co/google/flan-t5-small)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install dependencies and libraries including CUDA for PyTorch\n",
    "!pip install datasets markdown beautifulsoup4\n",
    "!pip install torch torchvision torchaudio --index-url https: // download.pytorch.org/whl/cu117\n",
    "!pip install transformers pandas accelerate nvidia-ml-py3 datasets nltk"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T16:55:32.357283Z",
     "end_time": "2023-04-24T16:55:38.743042Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create question and answer dataset from interviewing questions and answers\n",
    "\n",
    "This code is used to create a dataset from the sources above. It parses the markdown files and creates a dataset in JSON format.\n",
    "Most of questions are the h3 or h4 tags and answers are the content after the question tag."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import markdown\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "sudheerj_paths = [\n",
    "    os.path.join('..', 'data', 'interview', 'sudheerj', 'angular-interview-questions.md'),\n",
    "    os.path.join('..', 'data', 'interview', 'sudheerj', 'javascript-interview-questions.md'),\n",
    "    os.path.join('..', 'data', 'interview', 'sudheerj', 'reactjs-interview-questions.md'),\n",
    "]\n",
    "\n",
    "aershov24_paths = [\n",
    "    os.path.join('..', 'data', 'interview', 'aershov24', 'full-stack-interview-questions.md')\n",
    "]\n",
    "\n",
    "# Extract questions and answers from markdown files\n",
    "def parse_files(md_files, question_selector):\n",
    "    data = pd.DataFrame()\n",
    "    for md_file in md_files:\n",
    "        with open(md_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            md_content = file.read()\n",
    "            html_content = markdown.markdown(md_content)\n",
    "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "            questions = soup.select(question_selector)\n",
    "\n",
    "            for question in questions:\n",
    "                answer_elements = []\n",
    "                sibling = question.find_next_sibling()\n",
    "\n",
    "                while sibling and sibling.name != question_selector:\n",
    "                    answer_elements.append(str(sibling))\n",
    "                    sibling = sibling.find_next_sibling()\n",
    "\n",
    "                answer = BeautifulSoup(''.join(answer_elements).strip())\n",
    "\n",
    "                data = pd.concat([data, pd.DataFrame({\n",
    "                    'question': [question.text.strip()],\n",
    "                    'answer': [answer.text.strip()]\n",
    "                })], ignore_index=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "sudheerj_df = parse_files(sudheerj_paths, \"h3\")\n",
    "aershov24_df = parse_files(aershov24_paths, \"h4\")\n",
    "\n",
    "combine_df = pd.concat([sudheerj_df, aershov24_df], ignore_index=True)\n",
    "combine_df.to_json(os.path.join('..', 'datasets', 'interview', 'interview_questions.json'), orient='records')\n",
    "combine_df.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T16:55:38.749041Z",
     "end_time": "2023-04-24T16:55:42.455959Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-24T16:55:42.460959Z",
     "end_time": "2023-04-24T16:55:47.020453Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "combine_df = pd.read_json(os.path.join('..', 'datasets', 'interview', 'interview_questions.json'), orient='records')\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "def replace_with_synonym(sentence, max_replacements=2):\n",
    "    words = sentence.split()\n",
    "    replacements = 0\n",
    "    new_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if replacements < max_replacements and random.random() < 0.5:\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms:\n",
    "                word = random.choice(synonyms)\n",
    "                replacements += 1\n",
    "        new_words.append(word)\n",
    "\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def generate_augmented_row(row):\n",
    "    question = row['question']\n",
    "    answer = row['answer']\n",
    "\n",
    "    augmented_question = replace_with_synonym(question)\n",
    "    augmented_answer = replace_with_synonym(answer)\n",
    "    rand = random.random()\n",
    "    if rand < 0.25:\n",
    "        return augmented_question, answer\n",
    "    elif rand > 0.25:\n",
    "        return question, augmented_answer\n",
    "    else:\n",
    "        return augmented_question, augmented_answer\n",
    "\n",
    "def generate_augmented_rows(df):\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc='Augmenting'):\n",
    "        yield generate_augmented_row(row)\n",
    "\n",
    "pd.DataFrame(generate_augmented_rows(df=combine_df), columns=['question', 'answer'])\n",
    "augmented_df = pd.concat(\n",
    "    [\n",
    "        combine_df,\n",
    "        pd.DataFrame(generate_augmented_rows(df=combine_df), columns=['question', 'answer']),\n",
    "        pd.DataFrame(generate_augmented_rows(df=combine_df), columns=['question', 'answer']),\n",
    "    ],\n",
    "    ignore_index=True\n",
    ").dropna().drop_duplicates(subset=['question', 'answer'], keep='first', ignore_index=True)\n",
    "\n",
    "augmented_df.to_json(os.path.join('..', 'datasets', 'interview', 'interview_questions_augmented.json'), orient='records')\n",
    "\n",
    "print(f'Original dataset size: {len(combine_df)}')\n",
    "print(f'Augmented dataset size: {len(augmented_df)}')\n",
    "augmented_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from accelerate import Accelerator\n",
    "\n",
    "num_train_epochs = 4\n",
    "\n",
    "accelerator = Accelerator()\n",
    "data = pd.read_json(os.path.join('..', 'datasets', 'interview', 'interview_questions_augmented.json'), orient='records')\n",
    "train_data = data.sample(frac=0.9, random_state=42)\n",
    "val_data = data.drop(train_data.index)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = accelerator.prepare(tokenizer)\n",
    "\n",
    "def preprocess_data(batch):\n",
    "    input_texts = [\"question: \" + example for example in batch[\"question\"]]\n",
    "    target_texts = [\"answer: \" + example for example in batch[\"answer\"]]\n",
    "    input_tokenized = tokenizer(input_texts, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"np\")\n",
    "    target_tokenized = tokenizer(target_texts, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"np\")\n",
    "    input_tokenized, target_tokenized = accelerator.prepare(input_tokenized, target_tokenized)\n",
    "    return {\"input_ids\": input_tokenized.input_ids, \"attention_mask\": input_tokenized.attention_mask, \"labels\": target_tokenized.input_ids}\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_data, batched=True)\n",
    "\n",
    "config = T5Config.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, config=config)\n",
    "\n",
    "model = accelerator.prepare(model)\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"logs\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"output/model\")\n",
    "tokenizer.save_pretrained(\"output/model\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T17:43:05.056329Z",
     "end_time": "2023-04-24T18:27:17.231037Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_answer(question, model, tokenizer, max_length=128):\n",
    "    model.eval()\n",
    "    input_text = \"question: \" + question\n",
    "    input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    output_tokens = model.generate(input_tokens, max_length=max_length, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True, num_beams=4, num_return_sequences=4)\n",
    "    answer = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    answer = answer.replace(\"answer: \", \"\")\n",
    "    return answer\n",
    "\n",
    "question = \"What is the difference between AngularJS and Angular?\"\n",
    "\n",
    "source_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n",
    "source_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "source_answer = generate_answer(question, source_model, source_tokenizer)\n",
    "\n",
    "tuned_model = T5ForConditionalGeneration.from_pretrained(\"output/model\")\n",
    "tuned_tokenizer = T5Tokenizer.from_pretrained(\"output/model\")\n",
    "\n",
    "tuned_answer = generate_answer(question, tuned_model, tuned_tokenizer)\n",
    "\n",
    "print(f\"Question: {question}\") # Question: What is the difference between AngularJS and Angular?\n",
    "print(f\"Source answer: {source_answer}\") # Source answer: AngularJS AngularJS may refer to:\n",
    "print(f\"Tuned answer: {tuned_answer}\") # Tuned answer: AngularJS is a JavaScript language that can be used to build web applications. It's also known as AngularJS or AngularJS. The main difference between AngularJS and AngularJS is that it uses the same syntax as AngularJS. For example, you can use AngularJS instead of AngularJS.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:50:17.016099Z",
     "end_time": "2023-04-24T18:50:22.191032Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T17:29:17.664330Z",
     "end_time": "2023-04-24T17:29:17.708324Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
